[{
    "Id": 495,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8c3822edbb31cf71cedaf49f2167e45c1e2d0b83",
    "date": "2023-03-13T12:53:59-07:00",
    "message": "Update the is_dtensor check to only run in eager mode.\n\nPiperOrigin-RevId: 516294602",
    "label": "NO",
    "changes": [
        {
            "name": "dtensor_device.py",
            "path": "tensorflow/dtensor/python/dtensor_device.py",
            "patches": [
                {
                    "old_start": 307,
                    "old_length": 7,
                    "new_start": 307,
                    "new_length": 12,
                    "hunk": "@@ -307,7 +307,12 @@ class DTensorDevice(object):\n \n     Returns:\n       bool, True if the given tensor is a DTensor.\n+\n+    Raises:\n+      RuntimeError: When not called eagerly.\n     \"\"\"\n+    if not context.executing_eagerly():\n+      raise RuntimeError(\"is_dtensor must be called eagerly.\")\n     if not tensor_util.is_tensor(tensor):\n       return False\n     if isinstance(tensor, variables.Variable):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+    Raises:\n+      RuntimeError: When not called eagerly.\n+    if not context.executing_eagerly():\n+      raise RuntimeError(\"is_dtensor must be called eagerly.\")\n",
            "whole_hunk": "@@ -307,7 +307,12 @@ class DTensorDevice(object):\n \n     Returns:\n       bool, True if the given tensor is a DTensor.\n+\n+    Raises:\n+      RuntimeError: When not called eagerly.\n     \"\"\"\n+    if not context.executing_eagerly():\n+      raise RuntimeError(\"is_dtensor must be called eagerly.\")\n     if not tensor_util.is_tensor(tensor):\n       return False\n     if isinstance(tensor, variables.Variable):"
        }
    ]
},
{
    "Id": 164,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e597f7dc6bbad2659af8f099b8e91e168a04bbf0",
    "date": "2024-02-07T09:30:20-08:00",
    "message": "[XLA:GPU] Add check that computation is found in TritonFilecheckTest.\n\nTritonFileCheckTest::CreateTritonIrAndFilecheck fails with an obscure segmentation fault in TritonFusionAnalysis::Execute if an incorrect name for the triton function is passed in. This makes it seem like the issue is around tiling analysis when it is just a test typo. This change adds a ret_check to clarify that the failure is from the call to GetComputationWithName.\nPiperOrigin-RevId: 605007087",
    "label": "YES",
    "changes": [
        {
            "name": "ir_emitter_triton_test.cc",
            "path": "third_party/xla/xla/service/gpu/ir_emitter_triton_test.cc",
            "patches": [
                {
                    "old_start": 121,
                    "old_length": 6,
                    "new_start": 121,
                    "new_length": 7,
                    "hunk": "@@ -121,6 +121,7 @@ absl::StatusOr<bool> TritonFilecheckTest::CreateTritonIrAndFileCheck(\n \n   auto* computation =\n       verified_module->GetComputationWithName(triton_fusion_name);\n+  TF_RET_CHECK(computation != nullptr);\n   TF_ASSIGN_OR_RETURN(auto analysis,\n                       TritonFusionAnalysis::Execute(*computation));\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  TF_RET_CHECK(computation != nullptr);\n",
            "whole_hunk": "@@ -121,6 +121,7 @@ absl::StatusOr<bool> TritonFilecheckTest::CreateTritonIrAndFileCheck(\n \n   auto* computation =\n       verified_module->GetComputationWithName(triton_fusion_name);\n+  TF_RET_CHECK(computation != nullptr);\n   TF_ASSIGN_OR_RETURN(auto analysis,\n                       TritonFusionAnalysis::Execute(*computation));\n "
        }
    ]
},
{
    "Id": 54,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/0a1a2f68de95cda539858eaf1dddbe512dbac4db",
    "date": "2024-05-08T21:03:51-07:00",
    "message": "gpu_delegate: Update GPU MUL compatibility checker\n\nADD, MUL only works when two inputs has the same number of dimension on GPU.\nMake sure if the condition is checked properly.\n\nPiperOrigin-RevId: 632015698",
    "label": "YES",
    "changes": [
        {
            "name": "gpu_compatibility.cc",
            "path": "tensorflow/lite/tools/versioning/gpu_compatibility.cc",
            "patches": [
                {
                    "old_start": 19,
                    "old_length": 6,
                    "new_start": 19,
                    "new_length": 7,
                    "hunk": "@@ -19,6 +19,7 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_join.h\"\n #include \"tensorflow/lite/builtin_op_data.h\"\n #include \"tensorflow/lite/builtin_ops.h\"\n #include \"tensorflow/lite/tools/versioning/op_signature.h\"\n"
                },
                {
                    "old_start": 446,
                    "old_length": 6,
                    "new_start": 447,
                    "new_length": 14,
                    "hunk": "@@ -446,6 +447,14 @@ absl::Status CheckGpuDelegateCompatibility(const OpSignature& op_sig) {\n       if (op_sig.inputs.size() != 2) {\n         return absl::UnimplementedError(\"ADD requires two input tensors.\");\n       }\n+      const auto& input0 = op_sig.inputs.at(0);\n+      const auto& input1 = op_sig.inputs.at(1);\n+      if (input0.dims.size() != input1.dims.size()) {\n+        return absl::UnimplementedError(\n+            absl::StrCat(\"ADD doesn't support broadcasting - input0: [\",\n+                         absl::StrJoin(input0.dims, \",\"), \"], input1: [\",\n+                         absl::StrJoin(input1.dims, \",\"), \"]\"));\n+      }\n       const TfLiteAddParams* tf_options;\n       return RetrieveBuiltinData(op_sig, &tf_options);\n     }\n"
                },
                {
                    "old_start": 690,
                    "old_length": 6,
                    "new_start": 699,
                    "new_length": 11,
                    "hunk": "@@ -690,6 +699,11 @@ absl::Status CheckGpuDelegateCompatibility(const OpSignature& op_sig) {\n               \"MUL requires one tensor that not less than second in all \"\n               \"dimensions.\");\n         }\n+      } else {\n+        return absl::UnimplementedError(\n+            absl::StrCat(\"MUL doesn't support broadcasting - input0: [\",\n+                         absl::StrJoin(input0.dims, \",\"), \"], input1: [\",\n+                         absl::StrJoin(input1.dims, \",\"), \"]\"));\n       }\n       const TfLiteMulParams* tf_options;\n       RETURN_IF_ERROR(RetrieveBuiltinData(op_sig, &tf_options));"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"absl/strings/str_join.h\"\n+      const auto& input0 = op_sig.inputs.at(0);\n+      const auto& input1 = op_sig.inputs.at(1);\n+      if (input0.dims.size() != input1.dims.size()) {\n+        return absl::UnimplementedError(\n+            absl::StrCat(\"ADD doesn't support broadcasting - input0: [\",\n+                         absl::StrJoin(input0.dims, \",\"), \"], input1: [\",\n+                         absl::StrJoin(input1.dims, \",\"), \"]\"));\n+      }\n+      } else {\n+        return absl::UnimplementedError(\n+            absl::StrCat(\"MUL doesn't support broadcasting - input0: [\",\n+                         absl::StrJoin(input0.dims, \",\"), \"], input1: [\",\n+                         absl::StrJoin(input1.dims, \",\"), \"]\"));\n",
            "whole_hunk": "@@ -19,6 +19,7 @@ limitations under the License.\n \n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_join.h\"\n #include \"tensorflow/lite/builtin_op_data.h\"\n #include \"tensorflow/lite/builtin_ops.h\"\n #include \"tensorflow/lite/tools/versioning/op_signature.h\"\n@@ -446,6 +447,14 @@ absl::Status CheckGpuDelegateCompatibility(const OpSignature& op_sig) {\n       if (op_sig.inputs.size() != 2) {\n         return absl::UnimplementedError(\"ADD requires two input tensors.\");\n       }\n+      const auto& input0 = op_sig.inputs.at(0);\n+      const auto& input1 = op_sig.inputs.at(1);\n+      if (input0.dims.size() != input1.dims.size()) {\n+        return absl::UnimplementedError(\n+            absl::StrCat(\"ADD doesn't support broadcasting - input0: [\",\n+                         absl::StrJoin(input0.dims, \",\"), \"], input1: [\",\n+                         absl::StrJoin(input1.dims, \",\"), \"]\"));\n+      }\n       const TfLiteAddParams* tf_options;\n       return RetrieveBuiltinData(op_sig, &tf_options);\n     }\n@@ -690,6 +699,11 @@ absl::Status CheckGpuDelegateCompatibility(const OpSignature& op_sig) {\n               \"MUL requires one tensor that not less than second in all \"\n               \"dimensions.\");\n         }\n+      } else {\n+        return absl::UnimplementedError(\n+            absl::StrCat(\"MUL doesn't support broadcasting - input0: [\",\n+                         absl::StrJoin(input0.dims, \",\"), \"], input1: [\",\n+                         absl::StrJoin(input1.dims, \",\"), \"]\"));\n       }\n       const TfLiteMulParams* tf_options;\n       RETURN_IF_ERROR(RetrieveBuiltinData(op_sig, &tf_options));"
        }
    ]
},
{
    "Id": 582,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/4fe875c71d97e84bb380539fb64179d01deb4f15",
    "date": "2023-01-20T04:36:45-08:00",
    "message": "Provide a set of TF_LITE_ENSURE macros that work with the opaque context.  This allows clients that use the stable delegate API to check conditions within their code and report errors if the conditions don't hold.\n\nPiperOrigin-RevId: 503411126",
    "label": "NO",
    "changes": [
        {
            "name": "c_api_opaque.cc",
            "path": "tensorflow/lite/c/c_api_opaque.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"tensorflow/lite/c/c_api_opaque.h\"\n \n+#include <cstdio>\n #include <unordered_map>\n #include <vector>\n \n"
                },
                {
                    "old_start": 362,
                    "old_length": 3,
                    "new_start": 363,
                    "new_length": 40,
                    "hunk": "@@ -362,3 +363,40 @@ TfLiteStatus TfLiteOpaqueContextResizeTensor(TfLiteOpaqueContext* context,\n   return tflite_context->ResizeTensor(\n       tflite_context, reinterpret_cast<TfLiteTensor*>(tensor), new_size);\n }\n+\n+void TfLiteOpaqueContextReportError(struct TfLiteOpaqueContext* opaque_context,\n+                                    const char* format, ...) {\n+  va_list vlist;\n+  va_start(vlist, format);\n+  TfLiteOpaqueContextReportErrorVa(opaque_context, format, vlist);\n+  va_end(vlist);\n+}\n+\n+void TfLiteOpaqueContextReportErrorVa(\n+    struct TfLiteOpaqueContext* opaque_context, const char* format,\n+    va_list vlist) {\n+  // Determine the length of the resulting error message.\n+  va_list copy;\n+  va_copy(copy, vlist);\n+  int n = vsnprintf(nullptr, 0, format, copy);\n+  if (n < 0) {\n+    return;\n+  }\n+\n+  size_t size = (size_t)n + 1;  // +1 for '\\0'.\n+  char* buffer = new char[size];\n+\n+  n = vsnprintf(buffer, size, format, vlist);\n+  if (n < 0) {\n+    delete[] buffer;\n+    return;\n+  }\n+\n+  // The following cast is safe only because this code is part of the\n+  // TF Lite runtime implementation.  Apps using TF Lite should not rely on\n+  // TfLiteOpaqueContext and TfLiteContext being equivalent.\n+  auto* context = reinterpret_cast<TfLiteContext*>(opaque_context);\n+  TF_LITE_KERNEL_LOG(context, \"%s\", buffer);\n+\n+  delete[] buffer;\n+}\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <cstdio>\n+\n+void TfLiteOpaqueContextReportError(struct TfLiteOpaqueContext* opaque_context,\n+                                    const char* format, ...) {\n+  va_list vlist;\n+  va_start(vlist, format);\n+  TfLiteOpaqueContextReportErrorVa(opaque_context, format, vlist);\n+  va_end(vlist);\n+}\n+\n+void TfLiteOpaqueContextReportErrorVa(\n+    struct TfLiteOpaqueContext* opaque_context, const char* format,\n+    va_list vlist) {\n+  // Determine the length of the resulting error message.\n+  va_list copy;\n+  va_copy(copy, vlist);\n+  int n = vsnprintf(nullptr, 0, format, copy);\n+  if (n < 0) {\n+    return;\n+  }\n+\n+  size_t size = (size_t)n + 1;  // +1 for '\\0'.\n+  char* buffer = new char[size];\n+\n+  n = vsnprintf(buffer, size, format, vlist);\n+  if (n < 0) {\n+    delete[] buffer;\n+    return;\n+  }\n+\n+  // The following cast is safe only because this code is part of the\n+  // TF Lite runtime implementation.  Apps using TF Lite should not rely on\n+  // TfLiteOpaqueContext and TfLiteContext being equivalent.\n+  auto* context = reinterpret_cast<TfLiteContext*>(opaque_context);\n+  TF_LITE_KERNEL_LOG(context, \"%s\", buffer);\n+\n+  delete[] buffer;\n+}\n",
            "whole_hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"tensorflow/lite/c/c_api_opaque.h\"\n \n+#include <cstdio>\n #include <unordered_map>\n #include <vector>\n \n@@ -362,3 +363,40 @@ TfLiteStatus TfLiteOpaqueContextResizeTensor(TfLiteOpaqueContext* context,\n   return tflite_context->ResizeTensor(\n       tflite_context, reinterpret_cast<TfLiteTensor*>(tensor), new_size);\n }\n+\n+void TfLiteOpaqueContextReportError(struct TfLiteOpaqueContext* opaque_context,\n+                                    const char* format, ...) {\n+  va_list vlist;\n+  va_start(vlist, format);\n+  TfLiteOpaqueContextReportErrorVa(opaque_context, format, vlist);\n+  va_end(vlist);\n+}\n+\n+void TfLiteOpaqueContextReportErrorVa(\n+    struct TfLiteOpaqueContext* opaque_context, const char* format,\n+    va_list vlist) {\n+  // Determine the length of the resulting error message.\n+  va_list copy;\n+  va_copy(copy, vlist);\n+  int n = vsnprintf(nullptr, 0, format, copy);\n+  if (n < 0) {\n+    return;\n+  }\n+\n+  size_t size = (size_t)n + 1;  // +1 for '\\0'.\n+  char* buffer = new char[size];\n+\n+  n = vsnprintf(buffer, size, format, vlist);\n+  if (n < 0) {\n+    delete[] buffer;\n+    return;\n+  }\n+\n+  // The following cast is safe only because this code is part of the\n+  // TF Lite runtime implementation.  Apps using TF Lite should not rely on\n+  // TfLiteOpaqueContext and TfLiteContext being equivalent.\n+  auto* context = reinterpret_cast<TfLiteContext*>(opaque_context);\n+  TF_LITE_KERNEL_LOG(context, \"%s\", buffer);\n+\n+  delete[] buffer;\n+}\n"
        },
        {
            "name": "c_api_opaque.h",
            "path": "tensorflow/lite/c/c_api_opaque.h",
            "patches": [
                {
                    "old_start": 353,
                    "old_length": 6,
                    "new_start": 353,
                    "new_length": 28,
                    "hunk": "@@ -353,6 +353,28 @@ TfLiteStatus TfLiteOpaqueContextResizeTensor(TfLiteOpaqueContext* context,\n                                              TfLiteOpaqueTensor* tensor,\n                                              TfLiteIntArray* new_size);\n \n+// Reports an error message formed by using the provided 'format' string in\n+// combination with the data provided via the unnamed arguments following the\n+// the 'format' parameter ('...').  The intended usage and behavior is the same\n+// as with 'printf' with regards to how the data and the formatting string\n+// interact.  E.g.\n+// 'TfLiteOpaqueContextReportError(opaque_context, \"a=%d b=%d\", a, b);'\n+//\n+// The provided 'opaque_context' will be used for reporting the resulting error\n+// message.\n+//\n+// Note that TF Lite clients can use macros like 'TF_LITE_OPAQUE_ENSURE' to\n+// check for certain conditions to be true, and print an error message if the\n+// condition does not hold.  Direct usage of this function from application code\n+// should therefore be rare.\n+TFL_CAPI_EXPORT\n+void TfLiteOpaqueContextReportError(struct TfLiteOpaqueContext* opaque_context,\n+                                    const char* format, ...);\n+TFL_CAPI_EXPORT\n+void TfLiteOpaqueContextReportErrorVa(\n+    struct TfLiteOpaqueContext* opaque_context, const char* format,\n+    va_list vlist);\n+\n #ifdef __cplusplus\n }  // extern \"C\"\n #endif  // __cplusplus\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// Reports an error message formed by using the provided 'format' string in\n+// combination with the data provided via the unnamed arguments following the\n+// the 'format' parameter ('...').  The intended usage and behavior is the same\n+// as with 'printf' with regards to how the data and the formatting string\n+// interact.  E.g.\n+// 'TfLiteOpaqueContextReportError(opaque_context, \"a=%d b=%d\", a, b);'\n+//\n+// The provided 'opaque_context' will be used for reporting the resulting error\n+// message.\n+//\n+// Note that TF Lite clients can use macros like 'TF_LITE_OPAQUE_ENSURE' to\n+// check for certain conditions to be true, and print an error message if the\n+// condition does not hold.  Direct usage of this function from application code\n+// should therefore be rare.\n+TFL_CAPI_EXPORT\n+void TfLiteOpaqueContextReportError(struct TfLiteOpaqueContext* opaque_context,\n+                                    const char* format, ...);\n+TFL_CAPI_EXPORT\n+void TfLiteOpaqueContextReportErrorVa(\n+    struct TfLiteOpaqueContext* opaque_context, const char* format,\n+    va_list vlist);\n+\n",
            "whole_hunk": "@@ -353,6 +353,28 @@ TfLiteStatus TfLiteOpaqueContextResizeTensor(TfLiteOpaqueContext* context,\n                                              TfLiteOpaqueTensor* tensor,\n                                              TfLiteIntArray* new_size);\n \n+// Reports an error message formed by using the provided 'format' string in\n+// combination with the data provided via the unnamed arguments following the\n+// the 'format' parameter ('...').  The intended usage and behavior is the same\n+// as with 'printf' with regards to how the data and the formatting string\n+// interact.  E.g.\n+// 'TfLiteOpaqueContextReportError(opaque_context, \"a=%d b=%d\", a, b);'\n+//\n+// The provided 'opaque_context' will be used for reporting the resulting error\n+// message.\n+//\n+// Note that TF Lite clients can use macros like 'TF_LITE_OPAQUE_ENSURE' to\n+// check for certain conditions to be true, and print an error message if the\n+// condition does not hold.  Direct usage of this function from application code\n+// should therefore be rare.\n+TFL_CAPI_EXPORT\n+void TfLiteOpaqueContextReportError(struct TfLiteOpaqueContext* opaque_context,\n+                                    const char* format, ...);\n+TFL_CAPI_EXPORT\n+void TfLiteOpaqueContextReportErrorVa(\n+    struct TfLiteOpaqueContext* opaque_context, const char* format,\n+    va_list vlist);\n+\n #ifdef __cplusplus\n }  // extern \"C\"\n #endif  // __cplusplus\n"
        },
        {
            "name": "common.h",
            "path": "tensorflow/lite/core/c/common.h",
            "patches": [
                {
                    "old_start": 42,
                    "old_length": 6,
                    "new_start": 42,
                    "new_length": 7,
                    "hunk": "@@ -42,6 +42,7 @@ limitations under the License.\n #ifndef TENSORFLOW_LITE_CORE_C_COMMON_H_\n #define TENSORFLOW_LITE_CORE_C_COMMON_H_\n \n+#include <stdarg.h>\n #include <stdbool.h>\n #include <stddef.h>\n #include <stdint.h>\n"
                },
                {
                    "old_start": 179,
                    "old_length": 10,
                    "new_start": 180,
                    "new_length": 26,
                    "hunk": "@@ -179,10 +180,26 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n       (context)->ReportError((context), __VA_ARGS__); \\\n     }                                                 \\\n   } while (false)\n+\n+#define TF_LITE_OPAQUE_KERNEL_LOG(opaque_context, ...)             \\\n+  do {                                                             \\\n+    TfLiteOpaqueContextReportError((opaque_context), __VA_ARGS__); \\\n+  } while (false)\n+\n+#define TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(opaque_context, ...)         \\\n+  do {                                                               \\\n+    if ((opaque_context) != nullptr) {                               \\\n+      TfLiteOpaqueContextReportError((opaque_context), __VA_ARGS__); \\\n+    }                                                                \\\n+  } while (false)\n+\n #else  // TF_LITE_STRIP_ERROR_STRINGS\n #define ARGS_UNUSED(...) (void)sizeof(#__VA_ARGS__)\n #define TF_LITE_KERNEL_LOG(context, ...) ARGS_UNUSED(__VA_ARGS__)\n #define TF_LITE_MAYBE_KERNEL_LOG(context, ...) ARGS_UNUSED(__VA_ARGS__)\n+#define TF_LITE_OPAQUE_KERNEL_LOG(opaque_context, ...) ARGS_UNUSED(__VA_ARGS__)\n+#define TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(opaque_context, ...) \\\n+  ARGS_UNUSED(__VA_ARGS__)\n #endif  // TF_LITE_STRIP_ERROR_STRINGS\n \n // Check whether value is true, and if not return kTfLiteError from\n"
                },
                {
                    "old_start": 195,
                    "old_length": 6,
                    "new_start": 212,
                    "new_length": 16,
                    "hunk": "@@ -195,6 +212,16 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n     }                                                  \\\n   } while (0)\n \n+// Check whether value is true, and if not return kTfLiteError from\n+// the current function (and report the error string msg).\n+#define TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, value, msg)        \\\n+  do {                                                               \\\n+    if (!(value)) {                                                  \\\n+      TF_LITE_OPAQUE_KERNEL_LOG((opaque_context), __FILE__ \" \" msg); \\\n+      return kTfLiteError;                                           \\\n+    }                                                                \\\n+  } while (0)\n+\n // Check whether the value `a` is true, and if not return kTfLiteError from\n // the current function, while also reporting the location of the error.\n #define TF_LITE_ENSURE(context, a)                                      \\\n"
                },
                {
                    "old_start": 206,
                    "old_length": 6,
                    "new_start": 233,
                    "new_length": 17,
                    "hunk": "@@ -206,6 +233,17 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n     }                                                                   \\\n   } while (0)\n \n+// Check whether the value `a` is true, and if not return kTfLiteError from\n+// the current function, while also reporting the location of the error.\n+#define TF_LITE_OPAQUE_ENSURE(opaque_context, a)                           \\\n+  do {                                                                     \\\n+    if (!(a)) {                                                            \\\n+      TF_LITE_OPAQUE_KERNEL_LOG(opaque_context, \"%s:%d: %s was not true.\", \\\n+                                __FILE__, __LINE__, #a);                   \\\n+      return kTfLiteError;                                                 \\\n+    }                                                                      \\\n+  } while (0)\n+\n #define TF_LITE_ENSURE_STATUS(a) \\\n   do {                           \\\n     const TfLiteStatus s = (a);  \\\n"
                },
                {
                    "old_start": 228,
                    "old_length": 6,
                    "new_start": 266,
                    "new_length": 20,
                    "hunk": "@@ -228,6 +266,20 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n     }                                                                      \\\n   } while (0)\n \n+// Check whether the value `a == b` is true, and if not return kTfLiteError from\n+// the current function, while also reporting the location of the error.\n+// `a` and `b` may be evaluated more than once, so no side effects or\n+// extremely expensive computations should be done.\n+// NOTE: Use TF_LITE_ENSURE_TYPES_EQ if comparing TfLiteTypes.\n+#define TF_LITE_OPAQUE_ENSURE_EQ(opaque_context, a, b)                         \\\n+  do {                                                                         \\\n+    if ((a) != (b)) {                                                          \\\n+      TF_LITE_OPAQUE_KERNEL_LOG((opaque_context), \"%s:%d %s != %s (%d != %d)\", \\\n+                                __FILE__, __LINE__, #a, #b, (a), (b));         \\\n+      return kTfLiteError;                                                     \\\n+    }                                                                          \\\n+  } while (0)\n+\n #define TF_LITE_ENSURE_TYPES_EQ(context, a, b)                             \\\n   do {                                                                     \\\n     if ((a) != (b)) {                                                      \\\n"
                },
                {
                    "old_start": 238,
                    "old_length": 6,
                    "new_start": 290,
                    "new_length": 16,
                    "hunk": "@@ -238,6 +290,16 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n     }                                                                      \\\n   } while (0)\n \n+#define TF_LITE_OPAQUE_ENSURE_TYPES_EQ(opaque_context, a, b)                   \\\n+  do {                                                                         \\\n+    if ((a) != (b)) {                                                          \\\n+      TF_LITE_OPAQUE_KERNEL_LOG((opaque_context), \"%s:%d %s != %s (%s != %s)\", \\\n+                                __FILE__, __LINE__, #a, #b,                    \\\n+                                TfLiteTypeGetName(a), TfLiteTypeGetName(b));   \\\n+      return kTfLiteError;                                                     \\\n+    }                                                                          \\\n+  } while (0)\n+\n #define TF_LITE_ENSURE_NEAR(context, a, b, epsilon)                          \\\n   do {                                                                       \\\n     auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));                    \\\n"
                },
                {
                    "old_start": 249,
                    "old_length": 6,
                    "new_start": 311,
                    "new_length": 17,
                    "hunk": "@@ -249,6 +311,17 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n     }                                                                        \\\n   } while (0)\n \n+#define TF_LITE_OPAQUE_ENSURE_NEAR(opaque_context, a, b, epsilon)            \\\n+  do {                                                                       \\\n+    auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));                    \\\n+    if (delta > epsilon) {                                                   \\\n+      TF_LITE_OPAQUE_KERNEL_LOG(                                             \\\n+          (opaque_context), \"%s:%d %s not near %s (%f != %f)\", __FILE__,     \\\n+          __LINE__, #a, #b, static_cast<double>(a), static_cast<double>(b)); \\\n+      return kTfLiteError;                                                   \\\n+    }                                                                        \\\n+  } while (0)\n+\n #define TF_LITE_ENSURE_OK(context, status) \\\n   do {                                     \\\n     const TfLiteStatus s = (status);       \\\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <stdarg.h>\n+\n+#define TF_LITE_OPAQUE_KERNEL_LOG(opaque_context, ...)             \\\n+  do {                                                             \\\n+    TfLiteOpaqueContextReportError((opaque_context), __VA_ARGS__); \\\n+  } while (false)\n+\n+#define TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(opaque_context, ...)         \\\n+  do {                                                               \\\n+    if ((opaque_context) != nullptr) {                               \\\n+      TfLiteOpaqueContextReportError((opaque_context), __VA_ARGS__); \\\n+    }                                                                \\\n+  } while (false)\n+\n+#define TF_LITE_OPAQUE_KERNEL_LOG(opaque_context, ...) ARGS_UNUSED(__VA_ARGS__)\n+#define TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(opaque_context, ...) \\\n+  ARGS_UNUSED(__VA_ARGS__)\n+// Check whether value is true, and if not return kTfLiteError from\n+// the current function (and report the error string msg).\n+#define TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, value, msg)        \\\n+  do {                                                               \\\n+    if (!(value)) {                                                  \\\n+      TF_LITE_OPAQUE_KERNEL_LOG((opaque_context), __FILE__ \" \" msg); \\\n+      return kTfLiteError;                                           \\\n+    }                                                                \\\n+  } while (0)\n+\n+// Check whether the value `a` is true, and if not return kTfLiteError from\n+// the current function, while also reporting the location of the error.\n+#define TF_LITE_OPAQUE_ENSURE(opaque_context, a)                           \\\n+  do {                                                                     \\\n+    if (!(a)) {                                                            \\\n+      TF_LITE_OPAQUE_KERNEL_LOG(opaque_context, \"%s:%d: %s was not true.\", \\\n+                                __FILE__, __LINE__, #a);                   \\\n+      return kTfLiteError;                                                 \\\n+    }                                                                      \\\n+  } while (0)\n+\n+// Check whether the value `a == b` is true, and if not return kTfLiteError from\n+// the current function, while also reporting the location of the error.\n+// `a` and `b` may be evaluated more than once, so no side effects or\n+// extremely expensive computations should be done.\n+// NOTE: Use TF_LITE_ENSURE_TYPES_EQ if comparing TfLiteTypes.\n+#define TF_LITE_OPAQUE_ENSURE_EQ(opaque_context, a, b)                         \\\n+  do {                                                                         \\\n+    if ((a) != (b)) {                                                          \\\n+      TF_LITE_OPAQUE_KERNEL_LOG((opaque_context), \"%s:%d %s != %s (%d != %d)\", \\\n+                                __FILE__, __LINE__, #a, #b, (a), (b));         \\\n+      return kTfLiteError;                                                     \\\n+    }                                                                          \\\n+  } while (0)\n+\n+#define TF_LITE_OPAQUE_ENSURE_TYPES_EQ(opaque_context, a, b)                   \\\n+  do {                                                                         \\\n+    if ((a) != (b)) {                                                          \\\n+      TF_LITE_OPAQUE_KERNEL_LOG((opaque_context), \"%s:%d %s != %s (%s != %s)\", \\\n+                                __FILE__, __LINE__, #a, #b,                    \\\n+                                TfLiteTypeGetName(a), TfLiteTypeGetName(b));   \\\n+      return kTfLiteError;                                                     \\\n+    }                                                                          \\\n+  } while (0)\n+\n+#define TF_LITE_OPAQUE_ENSURE_NEAR(opaque_context, a, b, epsilon)            \\\n+  do {                                                                       \\\n+    auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));                    \\\n+    if (delta > epsilon) {                                                   \\\n+      TF_LITE_OPAQUE_KERNEL_LOG(                                             \\\n+          (opaque_context), \"%s:%d %s not near %s (%f != %f)\", __FILE__,     \\\n+          __LINE__, #a, #b, static_cast<double>(a), static_cast<double>(b)); \\\n+      return kTfLiteError;                                                   \\\n+    }                                                                        \\\n+  } while (0)\n+\n",
            "whole_hunk": "@@ -42,6 +42,7 @@ limitations under the License.\n #ifndef TENSORFLOW_LITE_CORE_C_COMMON_H_\n #define TENSORFLOW_LITE_CORE_C_COMMON_H_\n \n+#include <stdarg.h>\n #include <stdbool.h>\n #include <stddef.h>\n #include <stdint.h>\n@@ -179,10 +180,26 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n       (context)->ReportError((context), __VA_ARGS__); \\\n     }                                                 \\\n   } while (false)\n+\n+#define TF_LITE_OPAQUE_KERNEL_LOG(opaque_context, ...)             \\\n+  do {                                                             \\\n+    TfLiteOpaqueContextReportError((opaque_context), __VA_ARGS__); \\\n+  } while (false)\n+\n+#define TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(opaque_context, ...)         \\\n+  do {                                                               \\\n+    if ((opaque_context) != nullptr) {                               \\\n+      TfLiteOpaqueContextReportError((opaque_context), __VA_ARGS__); \\\n+    }                                                                \\\n+  } while (false)\n+\n #else  // TF_LITE_STRIP_ERROR_STRINGS\n #define ARGS_UNUSED(...) (void)sizeof(#__VA_ARGS__)\n #define TF_LITE_KERNEL_LOG(context, ...) ARGS_UNUSED(__VA_ARGS__)\n #define TF_LITE_MAYBE_KERNEL_LOG(context, ...) ARGS_UNUSED(__VA_ARGS__)\n+#define TF_LITE_OPAQUE_KERNEL_LOG(opaque_context, ...) ARGS_UNUSED(__VA_ARGS__)\n+#define TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(opaque_context, ...) \\\n+  ARGS_UNUSED(__VA_ARGS__)\n #endif  // TF_LITE_STRIP_ERROR_STRINGS\n \n // Check whether value is true, and if not return kTfLiteError from\n@@ -195,6 +212,16 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n     }                                                  \\\n   } while (0)\n \n+// Check whether value is true, and if not return kTfLiteError from\n+// the current function (and report the error string msg).\n+#define TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, value, msg)        \\\n+  do {                                                               \\\n+    if (!(value)) {                                                  \\\n+      TF_LITE_OPAQUE_KERNEL_LOG((opaque_context), __FILE__ \" \" msg); \\\n+      return kTfLiteError;                                           \\\n+    }                                                                \\\n+  } while (0)\n+\n // Check whether the value `a` is true, and if not return kTfLiteError from\n // the current function, while also reporting the location of the error.\n #define TF_LITE_ENSURE(context, a)                                      \\\n@@ -206,6 +233,17 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n     }                                                                   \\\n   } while (0)\n \n+// Check whether the value `a` is true, and if not return kTfLiteError from\n+// the current function, while also reporting the location of the error.\n+#define TF_LITE_OPAQUE_ENSURE(opaque_context, a)                           \\\n+  do {                                                                     \\\n+    if (!(a)) {                                                            \\\n+      TF_LITE_OPAQUE_KERNEL_LOG(opaque_context, \"%s:%d: %s was not true.\", \\\n+                                __FILE__, __LINE__, #a);                   \\\n+      return kTfLiteError;                                                 \\\n+    }                                                                      \\\n+  } while (0)\n+\n #define TF_LITE_ENSURE_STATUS(a) \\\n   do {                           \\\n     const TfLiteStatus s = (a);  \\\n@@ -228,6 +266,20 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n     }                                                                      \\\n   } while (0)\n \n+// Check whether the value `a == b` is true, and if not return kTfLiteError from\n+// the current function, while also reporting the location of the error.\n+// `a` and `b` may be evaluated more than once, so no side effects or\n+// extremely expensive computations should be done.\n+// NOTE: Use TF_LITE_ENSURE_TYPES_EQ if comparing TfLiteTypes.\n+#define TF_LITE_OPAQUE_ENSURE_EQ(opaque_context, a, b)                         \\\n+  do {                                                                         \\\n+    if ((a) != (b)) {                                                          \\\n+      TF_LITE_OPAQUE_KERNEL_LOG((opaque_context), \"%s:%d %s != %s (%d != %d)\", \\\n+                                __FILE__, __LINE__, #a, #b, (a), (b));         \\\n+      return kTfLiteError;                                                     \\\n+    }                                                                          \\\n+  } while (0)\n+\n #define TF_LITE_ENSURE_TYPES_EQ(context, a, b)                             \\\n   do {                                                                     \\\n     if ((a) != (b)) {                                                      \\\n@@ -238,6 +290,16 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n     }                                                                      \\\n   } while (0)\n \n+#define TF_LITE_OPAQUE_ENSURE_TYPES_EQ(opaque_context, a, b)                   \\\n+  do {                                                                         \\\n+    if ((a) != (b)) {                                                          \\\n+      TF_LITE_OPAQUE_KERNEL_LOG((opaque_context), \"%s:%d %s != %s (%s != %s)\", \\\n+                                __FILE__, __LINE__, #a, #b,                    \\\n+                                TfLiteTypeGetName(a), TfLiteTypeGetName(b));   \\\n+      return kTfLiteError;                                                     \\\n+    }                                                                          \\\n+  } while (0)\n+\n #define TF_LITE_ENSURE_NEAR(context, a, b, epsilon)                          \\\n   do {                                                                       \\\n     auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));                    \\\n@@ -249,6 +311,17 @@ void TfLiteFloatArrayFree(TfLiteFloatArray* a);\n     }                                                                        \\\n   } while (0)\n \n+#define TF_LITE_OPAQUE_ENSURE_NEAR(opaque_context, a, b, epsilon)            \\\n+  do {                                                                       \\\n+    auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));                    \\\n+    if (delta > epsilon) {                                                   \\\n+      TF_LITE_OPAQUE_KERNEL_LOG(                                             \\\n+          (opaque_context), \"%s:%d %s not near %s (%f != %f)\", __FILE__,     \\\n+          __LINE__, #a, #b, static_cast<double>(a), static_cast<double>(b)); \\\n+      return kTfLiteError;                                                   \\\n+    }                                                                        \\\n+  } while (0)\n+\n #define TF_LITE_ENSURE_OK(context, status) \\\n   do {                                     \\\n     const TfLiteStatus s = (status);       \\\n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/lite/delegates/BUILD",
            "patches": [
                {
                    "old_start": 168,
                    "old_length": 15,
                    "new_start": 168,
                    "new_length": 34,
                    "hunk": "@@ -168,15 +168,34 @@ cc_test(\n         \"TFLITE_USE_OPAQUE_DELEGATE\",\n     ],\n     deps = [\n-        \":delegate_test_util\",\n-        \":interpreter_utils\",\n-        \":utils\",\n         \"//tensorflow/lite:framework\",\n         \"//tensorflow/lite/c:c_api_experimental\",\n         \"//tensorflow/lite/c:common\",\n         \"//tensorflow/lite/core/c:c_api_types\",\n         \"//tensorflow/lite/core/c:common\",\n         \"//tensorflow/lite/kernels:builtin_ops\",\n+        \"//tensorflow/lite/testing:util\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+cc_test(\n+    name = \"opaque_delegate_strip_error_strings_test\",\n+    size = \"small\",\n+    srcs = [\"opaque_delegate_test.cc\"],\n+    data = [\"//tensorflow/lite:testdata/add.bin\"],\n+    defines = [\n+        \"TFLITE_USE_OPAQUE_DELEGATE\",\n+        \"TF_LITE_STRIP_ERROR_STRINGS\",\n+    ],\n+    deps = [\n+        \"//tensorflow/lite:framework\",\n+        \"//tensorflow/lite/c:c_api_experimental\",\n+        \"//tensorflow/lite/c:common\",\n+        \"//tensorflow/lite/core/c:c_api_types\",\n+        \"//tensorflow/lite/core/c:common\",\n+        \"//tensorflow/lite/kernels:builtin_ops\",\n+        \"//tensorflow/lite/testing:util\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )\n"
                }
            ],
            "whole_deleted": "-        \":delegate_test_util\",\n-        \":interpreter_utils\",\n-        \":utils\",\n",
            "whole_added": "+        \"//tensorflow/lite/testing:util\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+cc_test(\n+    name = \"opaque_delegate_strip_error_strings_test\",\n+    size = \"small\",\n+    srcs = [\"opaque_delegate_test.cc\"],\n+    data = [\"//tensorflow/lite:testdata/add.bin\"],\n+    defines = [\n+        \"TFLITE_USE_OPAQUE_DELEGATE\",\n+        \"TF_LITE_STRIP_ERROR_STRINGS\",\n+    ],\n+    deps = [\n+        \"//tensorflow/lite:framework\",\n+        \"//tensorflow/lite/c:c_api_experimental\",\n+        \"//tensorflow/lite/c:common\",\n+        \"//tensorflow/lite/core/c:c_api_types\",\n+        \"//tensorflow/lite/core/c:common\",\n+        \"//tensorflow/lite/kernels:builtin_ops\",\n+        \"//tensorflow/lite/testing:util\",\n",
            "whole_hunk": "@@ -168,15 +168,34 @@ cc_test(\n         \"TFLITE_USE_OPAQUE_DELEGATE\",\n     ],\n     deps = [\n-        \":delegate_test_util\",\n-        \":interpreter_utils\",\n-        \":utils\",\n         \"//tensorflow/lite:framework\",\n         \"//tensorflow/lite/c:c_api_experimental\",\n         \"//tensorflow/lite/c:common\",\n         \"//tensorflow/lite/core/c:c_api_types\",\n         \"//tensorflow/lite/core/c:common\",\n         \"//tensorflow/lite/kernels:builtin_ops\",\n+        \"//tensorflow/lite/testing:util\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+cc_test(\n+    name = \"opaque_delegate_strip_error_strings_test\",\n+    size = \"small\",\n+    srcs = [\"opaque_delegate_test.cc\"],\n+    data = [\"//tensorflow/lite:testdata/add.bin\"],\n+    defines = [\n+        \"TFLITE_USE_OPAQUE_DELEGATE\",\n+        \"TF_LITE_STRIP_ERROR_STRINGS\",\n+    ],\n+    deps = [\n+        \"//tensorflow/lite:framework\",\n+        \"//tensorflow/lite/c:c_api_experimental\",\n+        \"//tensorflow/lite/c:common\",\n+        \"//tensorflow/lite/core/c:c_api_types\",\n+        \"//tensorflow/lite/core/c:common\",\n+        \"//tensorflow/lite/kernels:builtin_ops\",\n+        \"//tensorflow/lite/testing:util\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n )\n"
        },
        {
            "name": "opaque_delegate_test.cc",
            "path": "tensorflow/lite/delegates/opaque_delegate_test.cc",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 7,
                    "new_start": 14,
                    "new_length": 9,
                    "hunk": "@@ -14,7 +14,9 @@ limitations under the License.\n ==============================================================================*/\n \n #include <memory>\n+#include <string>\n \n+#include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"tensorflow/lite/c/c_api_opaque.h\"\n #include \"tensorflow/lite/c/common.h\"\n"
                },
                {
                    "old_start": 23,
                    "old_length": 6,
                    "new_start": 25,
                    "new_length": 9,
                    "hunk": "@@ -23,6 +25,9 @@ limitations under the License.\n #include \"tensorflow/lite/interpreter.h\"\n #include \"tensorflow/lite/interpreter_builder.h\"\n #include \"tensorflow/lite/kernels/register.h\"\n+#include \"tensorflow/lite/testing/util.h\"\n+\n+using ::testing::ContainsRegex;\n \n namespace tflite {\n namespace delegates {\n"
                },
                {
                    "old_start": 53,
                    "old_length": 6,
                    "new_start": 58,
                    "new_length": 251,
                    "hunk": "@@ -53,6 +58,251 @@ TEST(TestOpaqueDelegate, AddDelegate) {\n   TfLiteOpaqueDelegateDelete(opaque_delegate);\n }\n \n+class TestOpaqueMacros : public ::testing::Test {\n+  void DelegateImpl(TfLiteStatus expected_status) {\n+    // The 'opaque_delegate_builder_' is being configured in the test itself.\n+    opaque_delegate_ = TfLiteOpaqueDelegateCreate(&opaque_delegate_builder_);\n+    tflite::ops::builtin::BuiltinOpResolver resolver;\n+    tflite::InterpreterBuilder builder(*model_, resolver);\n+    builder.AddDelegate(opaque_delegate_);\n+    std::unique_ptr<tflite::Interpreter> interpreter;\n+    EXPECT_EQ(expected_status, builder(&interpreter));\n+  }\n+\n+  void SetUp() override {\n+    model_ = tflite::FlatBufferModel::BuildFromFile(\n+        \"third_party/tensorflow/lite/testdata/add.bin\", &reporter_);\n+    ASSERT_NE(model_, nullptr);\n+  }\n+\n+  void TearDown() override { TfLiteOpaqueDelegateDelete(opaque_delegate_); }\n+\n+ public:\n+  void EnsureDelegationSucceeds() { DelegateImpl(kTfLiteOk); }\n+  void EnsureDelegationFails() { DelegateImpl(kTfLiteDelegateError); }\n+\n+  ::tflite::TestErrorReporter reporter_;\n+  std::unique_ptr<tflite::FlatBufferModel> model_;\n+  TfLiteOpaqueDelegateBuilder opaque_delegate_builder_;\n+  TfLiteOpaqueDelegate* opaque_delegate_;\n+};\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE(opaque_context, false);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*false was not true.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE(opaque_context, true);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*was not true.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_EQ_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_EQ(opaque_context, true, false);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*true != false.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_EQ_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_EQ(opaque_context, true, true);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.* != *\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, false, \"custom error msg\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*custom error msg.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, true, \"custom error msg\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_TYPES_EQ_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_TYPES_EQ(opaque_context, kTfLiteFloat32,\n+                                   kTfLiteInt32);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*kTfLiteFloat32 != kTfLiteInt32.*\");\n+\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_TYPES_EQ_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_TYPES_EQ(opaque_context, kTfLiteFloat32,\n+                                   kTfLiteFloat32);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*!=.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_NEAR_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_NEAR(opaque_context, 1, 10, 5);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*1 not near 10.*\");\n+\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_NEAR_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_NEAR(opaque_context, 10, 10, 5);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*10 not near 10.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_MAYBE_KERNEL_LOG_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(\n+        opaque_context, \"Report through TF_LITE_OPAQUE_MAYBE_KERNEL_LOG\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*Report through TF_LITE_OPAQUE_MAYBE_KERNEL_LOG.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_MAYBE_KERNEL_LOG_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(nullptr, \"Should not be printed.\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*Report through TF_LITE_OPAQUE_MAYBE_KERNEL_LOG.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_EmptyString) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, false, \"\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_WithFormattingChars) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, false, \"%i %d\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+}\n+\n }  // anonymous namespace\n }  // namespace delegates\n }  // namespace tflite"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <string>\n+#include <gmock/gmock.h>\n+#include \"tensorflow/lite/testing/util.h\"\n+\n+using ::testing::ContainsRegex;\n+class TestOpaqueMacros : public ::testing::Test {\n+  void DelegateImpl(TfLiteStatus expected_status) {\n+    // The 'opaque_delegate_builder_' is being configured in the test itself.\n+    opaque_delegate_ = TfLiteOpaqueDelegateCreate(&opaque_delegate_builder_);\n+    tflite::ops::builtin::BuiltinOpResolver resolver;\n+    tflite::InterpreterBuilder builder(*model_, resolver);\n+    builder.AddDelegate(opaque_delegate_);\n+    std::unique_ptr<tflite::Interpreter> interpreter;\n+    EXPECT_EQ(expected_status, builder(&interpreter));\n+  }\n+\n+  void SetUp() override {\n+    model_ = tflite::FlatBufferModel::BuildFromFile(\n+        \"third_party/tensorflow/lite/testdata/add.bin\", &reporter_);\n+    ASSERT_NE(model_, nullptr);\n+  }\n+\n+  void TearDown() override { TfLiteOpaqueDelegateDelete(opaque_delegate_); }\n+\n+ public:\n+  void EnsureDelegationSucceeds() { DelegateImpl(kTfLiteOk); }\n+  void EnsureDelegationFails() { DelegateImpl(kTfLiteDelegateError); }\n+\n+  ::tflite::TestErrorReporter reporter_;\n+  std::unique_ptr<tflite::FlatBufferModel> model_;\n+  TfLiteOpaqueDelegateBuilder opaque_delegate_builder_;\n+  TfLiteOpaqueDelegate* opaque_delegate_;\n+};\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE(opaque_context, false);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*false was not true.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE(opaque_context, true);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*was not true.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_EQ_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_EQ(opaque_context, true, false);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*true != false.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_EQ_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_EQ(opaque_context, true, true);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.* != *\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, false, \"custom error msg\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*custom error msg.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, true, \"custom error msg\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_TYPES_EQ_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_TYPES_EQ(opaque_context, kTfLiteFloat32,\n+                                   kTfLiteInt32);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*kTfLiteFloat32 != kTfLiteInt32.*\");\n+\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_TYPES_EQ_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_TYPES_EQ(opaque_context, kTfLiteFloat32,\n+                                   kTfLiteFloat32);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*!=.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_NEAR_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_NEAR(opaque_context, 1, 10, 5);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*1 not near 10.*\");\n+\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_NEAR_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_NEAR(opaque_context, 10, 10, 5);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*10 not near 10.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_MAYBE_KERNEL_LOG_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(\n+        opaque_context, \"Report through TF_LITE_OPAQUE_MAYBE_KERNEL_LOG\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*Report through TF_LITE_OPAQUE_MAYBE_KERNEL_LOG.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_MAYBE_KERNEL_LOG_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(nullptr, \"Should not be printed.\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*Report through TF_LITE_OPAQUE_MAYBE_KERNEL_LOG.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_EmptyString) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, false, \"\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_WithFormattingChars) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, false, \"%i %d\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+}\n+\n",
            "whole_hunk": "@@ -14,7 +14,9 @@ limitations under the License.\n ==============================================================================*/\n \n #include <memory>\n+#include <string>\n \n+#include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"tensorflow/lite/c/c_api_opaque.h\"\n #include \"tensorflow/lite/c/common.h\"\n@@ -23,6 +25,9 @@ limitations under the License.\n #include \"tensorflow/lite/interpreter.h\"\n #include \"tensorflow/lite/interpreter_builder.h\"\n #include \"tensorflow/lite/kernels/register.h\"\n+#include \"tensorflow/lite/testing/util.h\"\n+\n+using ::testing::ContainsRegex;\n \n namespace tflite {\n namespace delegates {\n@@ -53,6 +58,251 @@ TEST(TestOpaqueDelegate, AddDelegate) {\n   TfLiteOpaqueDelegateDelete(opaque_delegate);\n }\n \n+class TestOpaqueMacros : public ::testing::Test {\n+  void DelegateImpl(TfLiteStatus expected_status) {\n+    // The 'opaque_delegate_builder_' is being configured in the test itself.\n+    opaque_delegate_ = TfLiteOpaqueDelegateCreate(&opaque_delegate_builder_);\n+    tflite::ops::builtin::BuiltinOpResolver resolver;\n+    tflite::InterpreterBuilder builder(*model_, resolver);\n+    builder.AddDelegate(opaque_delegate_);\n+    std::unique_ptr<tflite::Interpreter> interpreter;\n+    EXPECT_EQ(expected_status, builder(&interpreter));\n+  }\n+\n+  void SetUp() override {\n+    model_ = tflite::FlatBufferModel::BuildFromFile(\n+        \"third_party/tensorflow/lite/testdata/add.bin\", &reporter_);\n+    ASSERT_NE(model_, nullptr);\n+  }\n+\n+  void TearDown() override { TfLiteOpaqueDelegateDelete(opaque_delegate_); }\n+\n+ public:\n+  void EnsureDelegationSucceeds() { DelegateImpl(kTfLiteOk); }\n+  void EnsureDelegationFails() { DelegateImpl(kTfLiteDelegateError); }\n+\n+  ::tflite::TestErrorReporter reporter_;\n+  std::unique_ptr<tflite::FlatBufferModel> model_;\n+  TfLiteOpaqueDelegateBuilder opaque_delegate_builder_;\n+  TfLiteOpaqueDelegate* opaque_delegate_;\n+};\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE(opaque_context, false);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*false was not true.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE(opaque_context, true);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*was not true.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_EQ_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_EQ(opaque_context, true, false);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*true != false.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_EQ_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_EQ(opaque_context, true, true);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.* != *\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, false, \"custom error msg\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*custom error msg.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, true, \"custom error msg\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_TYPES_EQ_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_TYPES_EQ(opaque_context, kTfLiteFloat32,\n+                                   kTfLiteInt32);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*kTfLiteFloat32 != kTfLiteInt32.*\");\n+\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_TYPES_EQ_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_TYPES_EQ(opaque_context, kTfLiteFloat32,\n+                                   kTfLiteFloat32);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*!=.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_NEAR_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_NEAR(opaque_context, 1, 10, 5);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*1 not near 10.*\");\n+\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_NEAR_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_NEAR(opaque_context, 10, 10, 5);\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*third_party/tensorflow/lite/delegates/\"\n+      \"opaque_delegate_test\\\\.cc.*10 not near 10.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_MAYBE_KERNEL_LOG_REPORTS) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(\n+        opaque_context, \"Report through TF_LITE_OPAQUE_MAYBE_KERNEL_LOG\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*Report through TF_LITE_OPAQUE_MAYBE_KERNEL_LOG.*\");\n+#ifndef TF_LITE_STRIP_ERROR_STRINGS\n+  EXPECT_THAT(reporter_.error_messages(), ContainsRegex(txt_regex));\n+#else\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+#endif\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_MAYBE_KERNEL_LOG_SILENT) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_MAYBE_KERNEL_LOG(nullptr, \"Should not be printed.\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationSucceeds();\n+  const std::string txt_regex(\n+      \".*Report through TF_LITE_OPAQUE_MAYBE_KERNEL_LOG.*\");\n+  EXPECT_THAT(reporter_.error_messages(), Not(ContainsRegex(txt_regex)));\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_EmptyString) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, false, \"\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+}\n+\n+TEST_F(TestOpaqueMacros, TF_LITE_OPAQUE_ENSURE_MSG_WithFormattingChars) {\n+  opaque_delegate_builder_.Prepare = [](TfLiteOpaqueContext* opaque_context,\n+                                        TfLiteOpaqueDelegate* opaque_delegate,\n+                                        void* data) -> TfLiteStatus {\n+    TF_LITE_OPAQUE_ENSURE_MSG(opaque_context, false, \"%i %d\");\n+    return kTfLiteOk;\n+  };\n+  EnsureDelegationFails();\n+}\n+\n }  // anonymous namespace\n }  // namespace delegates\n }  // namespace tflite"
        }
    ]
},
{
    "Id": 608,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1e7fd2f447e37761b6f6c673de77d392b6d0650d",
    "date": "2022-12-16T13:14:12+03:00",
    "message": "Add checkers before derefence in conv Prepare()",
    "label": "YES",
    "changes": [
        {
            "name": "conv.cc",
            "path": "tensorflow/lite/kernels/conv.cc",
            "patches": [
                {
                    "old_start": 596,
                    "old_length": 6,
                    "new_start": 596,
                    "new_length": 8,
                    "hunk": "@@ -596,6 +596,8 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n       const auto* affine_quantization =\n           reinterpret_cast<TfLiteAffineQuantization*>(\n               filter->quantization.params);\n+      TF_LITE_ENSURE(context, affine_quantization);\n+      TF_LITE_ENSURE(context, affine_quantization->scale);\n       TF_LITE_ENSURE_EQ(\n           context, affine_quantization->scale->size,\n           filter->dims->data[affine_quantization->quantized_dimension]);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      TF_LITE_ENSURE(context, affine_quantization);\n+      TF_LITE_ENSURE(context, affine_quantization->scale);\n",
            "whole_hunk": "@@ -596,6 +596,8 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n       const auto* affine_quantization =\n           reinterpret_cast<TfLiteAffineQuantization*>(\n               filter->quantization.params);\n+      TF_LITE_ENSURE(context, affine_quantization);\n+      TF_LITE_ENSURE(context, affine_quantization->scale);\n       TF_LITE_ENSURE_EQ(\n           context, affine_quantization->scale->size,\n           filter->dims->data[affine_quantization->quantized_dimension]);"
        }
    ]
},
{
    "Id": 367,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/bc9028e1c9f1fa43c85e606bf8d367214db8ca23",
    "date": "2023-07-09T15:33:35-07:00",
    "message": "[XLA] Detect duplicate parameter numbers in HLO parser.\n\nThis enables early and more informative detection of wrong computations instead of crashing on a later check in the HLO computation's constructor.\n\nPiperOrigin-RevId: 546716740",
    "label": "NO",
    "changes": [
        {
            "name": "hlo_computation.cc",
            "path": "tensorflow/compiler/xla/hlo/ir/hlo_computation.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 16,
                    "new_start": 15,
                    "new_length": 12,
                    "hunk": "@@ -15,16 +15,12 @@ limitations under the License.\n \n #include \"tensorflow/compiler/xla/hlo/ir/hlo_computation.h\"\n \n-#include <algorithm>\n #include <cstddef>\n #include <cstdint>\n-#include <functional>\n #include <list>\n #include <memory>\n #include <optional>\n #include <queue>\n-#include <set>\n-#include <sstream>\n #include <string>\n #include <utility>\n #include <vector>\n"
                },
                {
                    "old_start": 32,
                    "old_length": 25,
                    "new_start": 28,
                    "new_length": 20,
                    "hunk": "@@ -32,25 +28,20 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n-#include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n-#include \"tensorflow/compiler/xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_clone_context.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instructions.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_module.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_opcode.h\"\n-#include \"tensorflow/compiler/xla/layout_util.h\"\n #include \"tensorflow/compiler/xla/map_util.h\"\n #include \"tensorflow/compiler/xla/printer.h\"\n #include \"tensorflow/compiler/xla/service/mapped_ptr_container_sorter.h\"\n #include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/status_macros.h\"\n-#include \"tensorflow/compiler/xla/types.h\"\n #include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n-#include \"tensorflow/tsl/platform/logging.h\"\n #include \"tensorflow/tsl/platform/status.h\"\n \n namespace xla {\n"
                }
            ],
            "whole_deleted": "-#include <algorithm>\n-#include <functional>\n-#include <set>\n-#include <sstream>\n-#include \"absl/strings/numbers.h\"\n-#include \"tensorflow/compiler/xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n-#include \"tensorflow/compiler/xla/layout_util.h\"\n-#include \"tensorflow/compiler/xla/types.h\"\n-#include \"tensorflow/tsl/platform/logging.h\"\n",
            "whole_added": "",
            "whole_hunk": "@@ -15,16 +15,12 @@ limitations under the License.\n \n #include \"tensorflow/compiler/xla/hlo/ir/hlo_computation.h\"\n \n-#include <algorithm>\n #include <cstddef>\n #include <cstdint>\n-#include <functional>\n #include <list>\n #include <memory>\n #include <optional>\n #include <queue>\n-#include <set>\n-#include <sstream>\n #include <string>\n #include <utility>\n #include <vector>\n@@ -32,25 +28,20 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n-#include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n-#include \"tensorflow/compiler/xla/hlo/ir/dfs_hlo_visitor_with_default.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_clone_context.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instructions.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_module.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_opcode.h\"\n-#include \"tensorflow/compiler/xla/layout_util.h\"\n #include \"tensorflow/compiler/xla/map_util.h\"\n #include \"tensorflow/compiler/xla/printer.h\"\n #include \"tensorflow/compiler/xla/service/mapped_ptr_container_sorter.h\"\n #include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/status_macros.h\"\n-#include \"tensorflow/compiler/xla/types.h\"\n #include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n-#include \"tensorflow/tsl/platform/logging.h\"\n #include \"tensorflow/tsl/platform/status.h\"\n \n namespace xla {\n"
        },
        {
            "name": "hlo_computation.h",
            "path": "tensorflow/compiler/xla/hlo/ir/hlo_computation.h",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 7,
                    "new_start": 16,
                    "new_length": 6,
                    "hunk": "@@ -16,7 +16,6 @@ limitations under the License.\n #ifndef TENSORFLOW_COMPILER_XLA_HLO_IR_HLO_COMPUTATION_H_\n #define TENSORFLOW_COMPILER_XLA_HLO_IR_HLO_COMPUTATION_H_\n \n-#include <functional>\n #include <list>\n #include <memory>\n #include <optional>\n"
                },
                {
                    "old_start": 34,
                    "old_length": 15,
                    "new_start": 33,
                    "new_length": 13,
                    "hunk": "@@ -34,15 +33,13 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_clone_context.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/iterator_util.h\"\n-#include \"tensorflow/compiler/xla/map_util.h\"\n #include \"tensorflow/compiler/xla/printer.h\"\n #include \"tensorflow/compiler/xla/service/hlo.pb.h\"\n #include \"tensorflow/compiler/xla/service/name_uniquer.h\"\n #include \"tensorflow/compiler/xla/shape_tree.h\"\n #include \"tensorflow/compiler/xla/statusor.h\"\n-#include \"tensorflow/compiler/xla/types.h\"\n+#include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/compiler/xla/xla_data.pb.h\"\n-#include \"tensorflow/tsl/platform/status.h\"\n \n namespace xla {\n \n"
                },
                {
                    "old_start": 102,
                    "old_length": 6,
                    "new_start": 99,
                    "new_length": 15,
                    "hunk": "@@ -102,6 +99,15 @@ class HloComputation {\n       return AddInstruction(std::move(instruction));\n     }\n \n+    StatusOr<HloInstruction*> AddParameter(\n+        std::unique_ptr<HloInstruction> parameter) {\n+      if (!parameter_numbers_.insert(parameter->parameter_number()).second) {\n+        return InternalError(\"Duplicate parameter number %d\",\n+                             parameter->parameter_number());\n+      }\n+      return AddInstruction(std::move(parameter));\n+    }\n+\n     Status ForEachInstruction(\n         absl::FunctionRef<Status(const HloInstruction*)> func) const {\n       for (const auto& instruction : instructions_) {\n"
                },
                {
                    "old_start": 118,
                    "old_length": 6,
                    "new_start": 124,
                    "new_length": 7,
                    "hunk": "@@ -118,6 +124,7 @@ class HloComputation {\n     const std::string name_;\n     HloInstruction* fusion_instruction_;\n     std::vector<std::unique_ptr<HloInstruction>> instructions_;\n+    absl::flat_hash_set<int> parameter_numbers_;\n \n     Builder(const Builder&) = delete;\n     Builder& operator=(const Builder&) = delete;\n"
                }
            ],
            "whole_deleted": "-#include <functional>\n-#include \"tensorflow/compiler/xla/map_util.h\"\n-#include \"tensorflow/compiler/xla/types.h\"\n-#include \"tensorflow/tsl/platform/status.h\"\n",
            "whole_added": "+#include \"tensorflow/compiler/xla/util.h\"\n+    StatusOr<HloInstruction*> AddParameter(\n+        std::unique_ptr<HloInstruction> parameter) {\n+      if (!parameter_numbers_.insert(parameter->parameter_number()).second) {\n+        return InternalError(\"Duplicate parameter number %d\",\n+                             parameter->parameter_number());\n+      }\n+      return AddInstruction(std::move(parameter));\n+    }\n+\n+    absl::flat_hash_set<int> parameter_numbers_;\n",
            "whole_hunk": "@@ -16,7 +16,6 @@ limitations under the License.\n #ifndef TENSORFLOW_COMPILER_XLA_HLO_IR_HLO_COMPUTATION_H_\n #define TENSORFLOW_COMPILER_XLA_HLO_IR_HLO_COMPUTATION_H_\n \n-#include <functional>\n #include <list>\n #include <memory>\n #include <optional>\n@@ -34,15 +33,13 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_clone_context.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/iterator_util.h\"\n-#include \"tensorflow/compiler/xla/map_util.h\"\n #include \"tensorflow/compiler/xla/printer.h\"\n #include \"tensorflow/compiler/xla/service/hlo.pb.h\"\n #include \"tensorflow/compiler/xla/service/name_uniquer.h\"\n #include \"tensorflow/compiler/xla/shape_tree.h\"\n #include \"tensorflow/compiler/xla/statusor.h\"\n-#include \"tensorflow/compiler/xla/types.h\"\n+#include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/compiler/xla/xla_data.pb.h\"\n-#include \"tensorflow/tsl/platform/status.h\"\n \n namespace xla {\n \n@@ -102,6 +99,15 @@ class HloComputation {\n       return AddInstruction(std::move(instruction));\n     }\n \n+    StatusOr<HloInstruction*> AddParameter(\n+        std::unique_ptr<HloInstruction> parameter) {\n+      if (!parameter_numbers_.insert(parameter->parameter_number()).second) {\n+        return InternalError(\"Duplicate parameter number %d\",\n+                             parameter->parameter_number());\n+      }\n+      return AddInstruction(std::move(parameter));\n+    }\n+\n     Status ForEachInstruction(\n         absl::FunctionRef<Status(const HloInstruction*)> func) const {\n       for (const auto& instruction : instructions_) {\n@@ -118,6 +124,7 @@ class HloComputation {\n     const std::string name_;\n     HloInstruction* fusion_instruction_;\n     std::vector<std::unique_ptr<HloInstruction>> instructions_;\n+    absl::flat_hash_set<int> parameter_numbers_;\n \n     Builder(const Builder&) = delete;\n     Builder& operator=(const Builder&) = delete;\n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/service/BUILD",
            "patches": [
                {
                    "old_start": 5810,
                    "old_length": 7,
                    "new_start": 5810,
                    "new_length": 6,
                    "hunk": "@@ -5810,7 +5810,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@com_google_absl//absl/types:variant\",\n     ],\n )\n \n"
                }
            ],
            "whole_deleted": "-        \"@com_google_absl//absl/types:variant\",\n",
            "whole_added": "",
            "whole_hunk": "@@ -5810,7 +5810,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"@com_google_absl//absl/types:variant\",\n     ],\n )\n \n"
        },
        {
            "name": "hlo_parser.cc",
            "path": "tensorflow/compiler/xla/service/hlo_parser.cc",
            "patches": [
                {
                    "old_start": 41,
                    "old_length": 7,
                    "new_start": 41,
                    "new_length": 6,
                    "hunk": "@@ -41,7 +41,6 @@ limitations under the License.\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n-#include \"absl/types/variant.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_casting_utils.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_domain_metadata.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_input_output_alias_config.h\"\n"
                },
                {
                    "old_start": 1326,
                    "old_length": 8,
                    "new_start": 1325,
                    "new_length": 9,
                    "hunk": "@@ -1326,8 +1325,9 @@ HloInstruction* HloParserImpl::CreateInstruction(  // NOLINT\n           !ParseInt64(&parameter_number)) {\n         return nullptr;\n       }\n+      const LocTy loc = lexer_.GetLoc();\n       if (parameter_number < 0) {\n-        Error(lexer_.GetLoc(), \"parameter number must be >= 0\");\n+        Error(loc, \"parameter number must be >= 0\");\n         return nullptr;\n       }\n       if (!ParseToken(TokKind::kRparen, \"expects ')' after parameter number\") ||\n"
                },
                {
                    "old_start": 1335,
                    "old_length": 8,
                    "new_start": 1335,
                    "new_length": 13,
                    "hunk": "@@ -1335,8 +1335,13 @@ HloInstruction* HloParserImpl::CreateInstruction(  // NOLINT\n         return nullptr;\n       }\n       std::string param_name(name);\n-      return builder->AddInstruction(HloInstruction::CreateParameter(\n+      auto result = builder->AddParameter(HloInstruction::CreateParameter(\n           parameter_number, *shape, param_name));\n+      if (!result.ok()) {\n+        Error(loc, result.status().message());\n+        return nullptr;\n+      }\n+      return result.value();\n     }\n     case HloOpcode::kConstant: {\n       Literal literal;\n"
                }
            ],
            "whole_deleted": "-#include \"absl/types/variant.h\"\n-        Error(lexer_.GetLoc(), \"parameter number must be >= 0\");\n-      return builder->AddInstruction(HloInstruction::CreateParameter(\n",
            "whole_added": "+      const LocTy loc = lexer_.GetLoc();\n+        Error(loc, \"parameter number must be >= 0\");\n+      auto result = builder->AddParameter(HloInstruction::CreateParameter(\n+      if (!result.ok()) {\n+        Error(loc, result.status().message());\n+        return nullptr;\n+      }\n+      return result.value();\n",
            "whole_hunk": "@@ -41,7 +41,6 @@ limitations under the License.\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n-#include \"absl/types/variant.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_casting_utils.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_domain_metadata.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_input_output_alias_config.h\"\n@@ -1326,8 +1325,9 @@ HloInstruction* HloParserImpl::CreateInstruction(  // NOLINT\n           !ParseInt64(&parameter_number)) {\n         return nullptr;\n       }\n+      const LocTy loc = lexer_.GetLoc();\n       if (parameter_number < 0) {\n-        Error(lexer_.GetLoc(), \"parameter number must be >= 0\");\n+        Error(loc, \"parameter number must be >= 0\");\n         return nullptr;\n       }\n       if (!ParseToken(TokKind::kRparen, \"expects ')' after parameter number\") ||\n@@ -1335,8 +1335,13 @@ HloInstruction* HloParserImpl::CreateInstruction(  // NOLINT\n         return nullptr;\n       }\n       std::string param_name(name);\n-      return builder->AddInstruction(HloInstruction::CreateParameter(\n+      auto result = builder->AddParameter(HloInstruction::CreateParameter(\n           parameter_number, *shape, param_name));\n+      if (!result.ok()) {\n+        Error(loc, result.status().message());\n+        return nullptr;\n+      }\n+      return result.value();\n     }\n     case HloOpcode::kConstant: {\n       Literal literal;\n"
        },
        {
            "name": "hlo_parser_test.cc",
            "path": "tensorflow/compiler/xla/service/hlo_parser_test.cc",
            "patches": [
                {
                    "old_start": 4160,
                    "old_length": 6,
                    "new_start": 4160,
                    "new_length": 20,
                    "hunk": "@@ -4160,6 +4160,20 @@ TEST_F(HloParserTest, NegativeParameterNumber) {\n               HasSubstr(\"parameter number must be >= 0\"));\n }\n \n+TEST_F(HloParserTest, DuplicateParameterNumberIsDetected) {\n+  const std::string kHloString = R\"(\n+  ENTRY e {\n+    a = s8[] parameter(0)\n+    b = s8[] parameter(0)\n+    ROOT a = s8[] add(a, b)\n+  }\n+  )\";\n+  auto result = ParseAndReturnUnverifiedModule(kHloString);\n+  ASSERT_FALSE(result.status().ok());\n+  EXPECT_THAT(result.status().message(),\n+              HasSubstr(\"Duplicate parameter number 0\"));\n+}\n+\n TEST_F(HloParserTest, WrongNumberOfParameterLeafBuffersInReplication) {\n   const std::string hlo_string =\n       \"par0 = (f32[3,5], f32[]) parameter(0), \""
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(HloParserTest, DuplicateParameterNumberIsDetected) {\n+  const std::string kHloString = R\"(\n+  ENTRY e {\n+    a = s8[] parameter(0)\n+    b = s8[] parameter(0)\n+    ROOT a = s8[] add(a, b)\n+  }\n+  )\";\n+  auto result = ParseAndReturnUnverifiedModule(kHloString);\n+  ASSERT_FALSE(result.status().ok());\n+  EXPECT_THAT(result.status().message(),\n+              HasSubstr(\"Duplicate parameter number 0\"));\n+}\n+\n",
            "whole_hunk": "@@ -4160,6 +4160,20 @@ TEST_F(HloParserTest, NegativeParameterNumber) {\n               HasSubstr(\"parameter number must be >= 0\"));\n }\n \n+TEST_F(HloParserTest, DuplicateParameterNumberIsDetected) {\n+  const std::string kHloString = R\"(\n+  ENTRY e {\n+    a = s8[] parameter(0)\n+    b = s8[] parameter(0)\n+    ROOT a = s8[] add(a, b)\n+  }\n+  )\";\n+  auto result = ParseAndReturnUnverifiedModule(kHloString);\n+  ASSERT_FALSE(result.status().ok());\n+  EXPECT_THAT(result.status().message(),\n+              HasSubstr(\"Duplicate parameter number 0\"));\n+}\n+\n TEST_F(HloParserTest, WrongNumberOfParameterLeafBuffersInReplication) {\n   const std::string hlo_string =\n       \"par0 = (f32[3,5], f32[]) parameter(0), \""
        }
    ]
},
{
    "Id": 371,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f32eaafca6748212d625738d6bd35330b1dfa82a",
    "date": "2023-07-05T11:08:53-07:00",
    "message": "[XLA:GPU] Outline cuBLAS padding requirements; make cuBLAS/Triton GEMM decision more robust.\n\nThe present order of passes in the compiler (padding for cuBLAS -> ... -> Triton GEMM rewrite -> cuBLAS GEMM rewrite) has a problem that padding for cuBLAS skips padding for those GEMMs which will be executed with Triton GEMM based on a complex check; if the condition for this check changes after that and before Triton GEMM rewrite and Triton rewriter skips that GEMM we end up with one that has to be executed by cuBLAS but is not padded. This change solves the problem by catching all non-padded ones in the Triton GEMM rewrite.\n\nPiperOrigin-RevId: 545729159",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 1170,
                    "old_length": 6,
                    "new_start": 1170,
                    "new_length": 7,
                    "hunk": "@@ -1170,6 +1170,7 @@ cc_library(\n     hdrs = [\"gemm_rewriter_triton.h\"],\n     deps = [\n         \":backend_configs_cc\",\n+        \":cublas_padding_requirements\",\n         \":gpu_types\",\n         \":ir_emission_utils\",\n         \":matmul_utils\",\n"
                },
                {
                    "old_start": 1220,
                    "old_length": 6,
                    "new_start": 1221,
                    "new_length": 7,
                    "hunk": "@@ -1220,6 +1221,7 @@ xla_cc_test(\n     name = \"gemm_rewriter_triton_test\",\n     srcs = [\"gemm_rewriter_triton_test.cc\"],\n     deps = [\n+        \":cublas_padding_requirements\",\n         \":gemm_rewriter_triton\",\n         \"//tensorflow/compiler/xla:shape_util\",\n         \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n"
                },
                {
                    "old_start": 2135,
                    "old_length": 8,
                    "new_start": 2137,
                    "new_length": 21,
                    "hunk": "@@ -2135,8 +2137,21 @@ cc_library(\n         \"//tensorflow/compiler/xla:literal_util\",\n         \"//tensorflow/compiler/xla:util\",\n         \"//tensorflow/compiler/xla:window_util\",\n+        \"//tensorflow/compiler/xla:xla_data_proto_cc\",\n         \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n         \"//tensorflow/compiler/xla/service:hlo_pass\",\n+        \"//tensorflow/compiler/xla/stream_executor:device_description\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"cublas_padding_requirements\",\n+    srcs = [\"cublas_padding_requirements.cc\"],\n+    hdrs = [\"cublas_padding_requirements.h\"],\n+    deps = [\n+        \"//tensorflow/compiler/xla:util\",\n+        \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n+        \"//tensorflow/compiler/xla/stream_executor:device_description\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 2522,
                    "old_length": 6,
                    "new_start": 2537,
                    "new_length": 7,
                    "hunk": "@@ -2522,6 +2537,7 @@ cc_library(\n         \":autotuner_util\",\n         \":cublas_cudnn\",\n         \":cublas_pad_for_gemms\",\n+        \":cublas_padding_requirements\",\n         \":cudnn_fused_conv_rewriter\",\n         \":cudnn_fused_mha_rewriter\",\n         \":cudnn_pad_for_convolutions\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \":cublas_padding_requirements\",\n+        \":cublas_padding_requirements\",\n+        \"//tensorflow/compiler/xla:xla_data_proto_cc\",\n+        \"//tensorflow/compiler/xla/stream_executor:device_description\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"cublas_padding_requirements\",\n+    srcs = [\"cublas_padding_requirements.cc\"],\n+    hdrs = [\"cublas_padding_requirements.h\"],\n+    deps = [\n+        \"//tensorflow/compiler/xla:util\",\n+        \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n+        \"//tensorflow/compiler/xla/stream_executor:device_description\",\n+        \":cublas_padding_requirements\",\n",
            "whole_hunk": "@@ -1170,6 +1170,7 @@ cc_library(\n     hdrs = [\"gemm_rewriter_triton.h\"],\n     deps = [\n         \":backend_configs_cc\",\n+        \":cublas_padding_requirements\",\n         \":gpu_types\",\n         \":ir_emission_utils\",\n         \":matmul_utils\",\n@@ -1220,6 +1221,7 @@ xla_cc_test(\n     name = \"gemm_rewriter_triton_test\",\n     srcs = [\"gemm_rewriter_triton_test.cc\"],\n     deps = [\n+        \":cublas_padding_requirements\",\n         \":gemm_rewriter_triton\",\n         \"//tensorflow/compiler/xla:shape_util\",\n         \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n@@ -2135,8 +2137,21 @@ cc_library(\n         \"//tensorflow/compiler/xla:literal_util\",\n         \"//tensorflow/compiler/xla:util\",\n         \"//tensorflow/compiler/xla:window_util\",\n+        \"//tensorflow/compiler/xla:xla_data_proto_cc\",\n         \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n         \"//tensorflow/compiler/xla/service:hlo_pass\",\n+        \"//tensorflow/compiler/xla/stream_executor:device_description\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"cublas_padding_requirements\",\n+    srcs = [\"cublas_padding_requirements.cc\"],\n+    hdrs = [\"cublas_padding_requirements.h\"],\n+    deps = [\n+        \"//tensorflow/compiler/xla:util\",\n+        \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n+        \"//tensorflow/compiler/xla/stream_executor:device_description\",\n     ],\n )\n \n@@ -2522,6 +2537,7 @@ cc_library(\n         \":autotuner_util\",\n         \":cublas_cudnn\",\n         \":cublas_pad_for_gemms\",\n+        \":cublas_padding_requirements\",\n         \":cudnn_fused_conv_rewriter\",\n         \":cudnn_fused_mha_rewriter\",\n         \":cudnn_pad_for_convolutions\",\n"
        },
        {
            "name": "cublas_pad_for_gemms.cc",
            "path": "tensorflow/compiler/xla/service/gpu/cublas_pad_for_gemms.cc",
            "patches": [
                {
                    "old_start": 168,
                    "old_length": 7,
                    "new_start": 168,
                    "new_length": 8,
                    "hunk": "@@ -168,7 +168,8 @@ static std::vector<HloDotInstruction*> GetRelevantDots(\n                 ->config()\n                 .debug_options()\n                 .xla_gpu_enable_triton_gemm() &&\n-            IsTritonHandledGEMM(*dot, cuda_compute_capability))) {\n+            CanTritonHandleGEMM(*dot, cuda_compute_capability) &&\n+            ShouldTritonHandleGEMM(*dot, cuda_compute_capability))) {\n         gemms.push_back(dot);\n       }\n     }\n"
                }
            ],
            "whole_deleted": "-            IsTritonHandledGEMM(*dot, cuda_compute_capability))) {\n",
            "whole_added": "+            CanTritonHandleGEMM(*dot, cuda_compute_capability) &&\n+            ShouldTritonHandleGEMM(*dot, cuda_compute_capability))) {\n",
            "whole_hunk": "@@ -168,7 +168,8 @@ static std::vector<HloDotInstruction*> GetRelevantDots(\n                 ->config()\n                 .debug_options()\n                 .xla_gpu_enable_triton_gemm() &&\n-            IsTritonHandledGEMM(*dot, cuda_compute_capability))) {\n+            CanTritonHandleGEMM(*dot, cuda_compute_capability) &&\n+            ShouldTritonHandleGEMM(*dot, cuda_compute_capability))) {\n         gemms.push_back(dot);\n       }\n     }\n"
        },
        {
            "name": "cublas_padding_requirements.cc",
            "path": "tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.cc",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 61,
                    "hunk": "@@ -0,0 +1,61 @@\n+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n+\n+#include <vector>\n+\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instructions.h\"\n+#include \"tensorflow/compiler/xla/util.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+namespace {\n+\n+bool DimensionRequiresPadding(const int64_t size, const PrimitiveType data_type,\n+                              const se::CudaComputeCapability cc) {\n+  for (const CublasPaddingRequirement& requirement :\n+       CublasPaddingRequirements) {\n+    if (cc.IsAtLeast(requirement.min_compute_capability) &&\n+        data_type == requirement.data_type &&\n+        size % requirement.multiple_of != 0) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ShapeRequiresPadding(const Shape& shape,\n+                          const se::CudaComputeCapability cc) {\n+  // Since dots are canonicalized before padding only the last two dimensions\n+  // of each operand represent non-batch dimensions and may need padding.\n+  return DimensionRequiresPadding(shape.dimensions(shape.rank() - 1),\n+                                  shape.element_type(), cc) ||\n+         DimensionRequiresPadding(shape.dimensions(shape.rank() - 2),\n+                                  shape.element_type(), cc);\n+}\n+\n+}  // namespace\n+\n+bool CublasRequiresPadding(const HloDotInstruction& dot,\n+                           const se::CudaComputeCapability cc) {\n+  return ShapeRequiresPadding(dot.operand(0)->shape(), cc) ||\n+         ShapeRequiresPadding(dot.operand(1)->shape(), cc);\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n+\n+#include <vector>\n+\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instructions.h\"\n+#include \"tensorflow/compiler/xla/util.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+namespace {\n+\n+bool DimensionRequiresPadding(const int64_t size, const PrimitiveType data_type,\n+                              const se::CudaComputeCapability cc) {\n+  for (const CublasPaddingRequirement& requirement :\n+       CublasPaddingRequirements) {\n+    if (cc.IsAtLeast(requirement.min_compute_capability) &&\n+        data_type == requirement.data_type &&\n+        size % requirement.multiple_of != 0) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ShapeRequiresPadding(const Shape& shape,\n+                          const se::CudaComputeCapability cc) {\n+  // Since dots are canonicalized before padding only the last two dimensions\n+  // of each operand represent non-batch dimensions and may need padding.\n+  return DimensionRequiresPadding(shape.dimensions(shape.rank() - 1),\n+                                  shape.element_type(), cc) ||\n+         DimensionRequiresPadding(shape.dimensions(shape.rank() - 2),\n+                                  shape.element_type(), cc);\n+}\n+\n+}  // namespace\n+\n+bool CublasRequiresPadding(const HloDotInstruction& dot,\n+                           const se::CudaComputeCapability cc) {\n+  return ShapeRequiresPadding(dot.operand(0)->shape(), cc) ||\n+         ShapeRequiresPadding(dot.operand(1)->shape(), cc);\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla\n",
            "whole_hunk": "@@ -0,0 +1,61 @@\n+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n+\n+#include <vector>\n+\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instructions.h\"\n+#include \"tensorflow/compiler/xla/util.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+namespace {\n+\n+bool DimensionRequiresPadding(const int64_t size, const PrimitiveType data_type,\n+                              const se::CudaComputeCapability cc) {\n+  for (const CublasPaddingRequirement& requirement :\n+       CublasPaddingRequirements) {\n+    if (cc.IsAtLeast(requirement.min_compute_capability) &&\n+        data_type == requirement.data_type &&\n+        size % requirement.multiple_of != 0) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ShapeRequiresPadding(const Shape& shape,\n+                          const se::CudaComputeCapability cc) {\n+  // Since dots are canonicalized before padding only the last two dimensions\n+  // of each operand represent non-batch dimensions and may need padding.\n+  return DimensionRequiresPadding(shape.dimensions(shape.rank() - 1),\n+                                  shape.element_type(), cc) ||\n+         DimensionRequiresPadding(shape.dimensions(shape.rank() - 2),\n+                                  shape.element_type(), cc);\n+}\n+\n+}  // namespace\n+\n+bool CublasRequiresPadding(const HloDotInstruction& dot,\n+                           const se::CudaComputeCapability cc) {\n+  return ShapeRequiresPadding(dot.operand(0)->shape(), cc) ||\n+         ShapeRequiresPadding(dot.operand(1)->shape(), cc);\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla\n"
        },
        {
            "name": "cublas_padding_requirements.h",
            "path": "tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 47,
                    "hunk": "@@ -0,0 +1,47 @@\n+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_COMPILER_XLA_SERVICE_GPU_CUBLAS_PADDING_REQUIREMENTS_H_\n+#define TENSORFLOW_COMPILER_XLA_SERVICE_GPU_CUBLAS_PADDING_REQUIREMENTS_H_\n+\n+#include <array>\n+#include <vector>\n+\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instructions.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/device_description.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+struct CublasPaddingRequirement {\n+  int min_compute_capability;\n+  PrimitiveType data_type;\n+  int multiple_of;\n+};\n+\n+// List of padding requirements per compute capability and data type.\n+constexpr std::array<CublasPaddingRequirement, 3> CublasPaddingRequirements{\n+    {{se::CudaComputeCapability::VOLTA, S8, 4},\n+     {se::CudaComputeCapability::VOLTA, F16, 8},\n+     {se::CudaComputeCapability::AMPERE, BF16, 8}}};\n+\n+// Tell if either of the operands of the dot requires padding.\n+bool CublasRequiresPadding(const HloDotInstruction& dot,\n+                           se::CudaComputeCapability cc);\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // TENSORFLOW_COMPILER_XLA_SERVICE_GPU_CUBLAS_PADDING_REQUIREMENTS_H_\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_COMPILER_XLA_SERVICE_GPU_CUBLAS_PADDING_REQUIREMENTS_H_\n+#define TENSORFLOW_COMPILER_XLA_SERVICE_GPU_CUBLAS_PADDING_REQUIREMENTS_H_\n+\n+#include <array>\n+#include <vector>\n+\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instructions.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/device_description.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+struct CublasPaddingRequirement {\n+  int min_compute_capability;\n+  PrimitiveType data_type;\n+  int multiple_of;\n+};\n+\n+// List of padding requirements per compute capability and data type.\n+constexpr std::array<CublasPaddingRequirement, 3> CublasPaddingRequirements{\n+    {{se::CudaComputeCapability::VOLTA, S8, 4},\n+     {se::CudaComputeCapability::VOLTA, F16, 8},\n+     {se::CudaComputeCapability::AMPERE, BF16, 8}}};\n+\n+// Tell if either of the operands of the dot requires padding.\n+bool CublasRequiresPadding(const HloDotInstruction& dot,\n+                           se::CudaComputeCapability cc);\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // TENSORFLOW_COMPILER_XLA_SERVICE_GPU_CUBLAS_PADDING_REQUIREMENTS_H_\n",
            "whole_hunk": "@@ -0,0 +1,47 @@\n+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_COMPILER_XLA_SERVICE_GPU_CUBLAS_PADDING_REQUIREMENTS_H_\n+#define TENSORFLOW_COMPILER_XLA_SERVICE_GPU_CUBLAS_PADDING_REQUIREMENTS_H_\n+\n+#include <array>\n+#include <vector>\n+\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instructions.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/device_description.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+struct CublasPaddingRequirement {\n+  int min_compute_capability;\n+  PrimitiveType data_type;\n+  int multiple_of;\n+};\n+\n+// List of padding requirements per compute capability and data type.\n+constexpr std::array<CublasPaddingRequirement, 3> CublasPaddingRequirements{\n+    {{se::CudaComputeCapability::VOLTA, S8, 4},\n+     {se::CudaComputeCapability::VOLTA, F16, 8},\n+     {se::CudaComputeCapability::AMPERE, BF16, 8}}};\n+\n+// Tell if either of the operands of the dot requires padding.\n+bool CublasRequiresPadding(const HloDotInstruction& dot,\n+                           se::CudaComputeCapability cc);\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // TENSORFLOW_COMPILER_XLA_SERVICE_GPU_CUBLAS_PADDING_REQUIREMENTS_H_\n"
        },
        {
            "name": "gemm_rewriter_triton.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gemm_rewriter_triton.cc",
            "patches": [
                {
                    "old_start": 41,
                    "old_length": 6,
                    "new_start": 41,
                    "new_length": 8,
                    "hunk": "@@ -41,6 +41,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/layout.h\"\n #include \"tensorflow/compiler/xla/literal_util.h\"\n #include \"tensorflow/compiler/xla/service/gpu/backend_configs.pb.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/gpu_types.h\"\n #include \"tensorflow/compiler/xla/service/gpu/ir_emission_utils.h\"\n #include \"tensorflow/compiler/xla/service/gpu/matmul_utils.h\"\n #include \"tensorflow/compiler/xla/service/hlo_creation_utils.h\"\n"
                },
                {
                    "old_start": 481,
                    "old_length": 7,
                    "new_start": 483,
                    "new_length": 18,
                    "hunk": "@@ -481,7 +483,18 @@ class GemmRewriterTritonVisitor : public DfsHloRewriteVisitor {\n   // and replaces the original dot() with a call to the computation.\n   Status HandleDot(HloInstruction* dot) override {\n     VLOG(5) << dot->ToString();\n-    if (!IsTritonHandledGEMM(*dot, gpu_version_)) {\n+\n+    if (!CanTritonHandleGEMM(*dot, gpu_version_)) {\n+      return OkStatus();\n+    }\n+\n+    // If a GEMM requiring padding for cuBLAS is encountered here this\n+    // happened because earlier ShouldTritonHandleGEMM() accepted it and padding\n+    // was skipped. Do not check ShouldTritonHandleGEMM() again then.\n+    if (!CublasRequiresPadding(\n+            *xla::Cast<HloDotInstruction>(dot),\n+            std::get<se::CudaComputeCapability>(gpu_version_)) &&\n+        !ShouldTritonHandleGEMM(*dot, gpu_version_)) {\n       return OkStatus();\n     }\n \n"
                },
                {
                    "old_start": 923,
                    "old_length": 7,
                    "new_start": 936,
                    "new_length": 7,
                    "hunk": "@@ -923,7 +936,7 @@ const DotFusionAnalysis::DimIterationSpec* DotFusionAnalysis::IterSpec(\n   return nullptr;\n }\n \n-bool IsTritonHandledGEMM(const HloInstruction& dot,\n+bool CanTritonHandleGEMM(const HloInstruction& dot,\n                          const GpuVersion gpu_version) {\n   if (dot.opcode() != HloOpcode::kDot ||\n       absl::c_any_of(dot.precision_config().operand_precision(),\n"
                },
                {
                    "old_start": 975,
                    "old_length": 6,
                    "new_start": 988,
                    "new_length": 11,
                    "hunk": "@@ -975,6 +988,11 @@ bool IsTritonHandledGEMM(const HloInstruction& dot,\n     return false;\n   }\n \n+  return true;\n+}\n+\n+bool ShouldTritonHandleGEMM(const HloInstruction& dot,\n+                            const GpuVersion gpu_version) {\n   if (dot.GetModule()->config().debug_options().xla_gpu_triton_gemm_any()) {\n     return true;\n   }\n"
                }
            ],
            "whole_deleted": "-    if (!IsTritonHandledGEMM(*dot, gpu_version_)) {\n-bool IsTritonHandledGEMM(const HloInstruction& dot,\n",
            "whole_added": "+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/gpu_types.h\"\n+\n+    if (!CanTritonHandleGEMM(*dot, gpu_version_)) {\n+      return OkStatus();\n+    }\n+\n+    // If a GEMM requiring padding for cuBLAS is encountered here this\n+    // happened because earlier ShouldTritonHandleGEMM() accepted it and padding\n+    // was skipped. Do not check ShouldTritonHandleGEMM() again then.\n+    if (!CublasRequiresPadding(\n+            *xla::Cast<HloDotInstruction>(dot),\n+            std::get<se::CudaComputeCapability>(gpu_version_)) &&\n+        !ShouldTritonHandleGEMM(*dot, gpu_version_)) {\n+bool CanTritonHandleGEMM(const HloInstruction& dot,\n+  return true;\n+}\n+\n+bool ShouldTritonHandleGEMM(const HloInstruction& dot,\n+                            const GpuVersion gpu_version) {\n",
            "whole_hunk": "@@ -41,6 +41,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/layout.h\"\n #include \"tensorflow/compiler/xla/literal_util.h\"\n #include \"tensorflow/compiler/xla/service/gpu/backend_configs.pb.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/gpu_types.h\"\n #include \"tensorflow/compiler/xla/service/gpu/ir_emission_utils.h\"\n #include \"tensorflow/compiler/xla/service/gpu/matmul_utils.h\"\n #include \"tensorflow/compiler/xla/service/hlo_creation_utils.h\"\n@@ -481,7 +483,18 @@ class GemmRewriterTritonVisitor : public DfsHloRewriteVisitor {\n   // and replaces the original dot() with a call to the computation.\n   Status HandleDot(HloInstruction* dot) override {\n     VLOG(5) << dot->ToString();\n-    if (!IsTritonHandledGEMM(*dot, gpu_version_)) {\n+\n+    if (!CanTritonHandleGEMM(*dot, gpu_version_)) {\n+      return OkStatus();\n+    }\n+\n+    // If a GEMM requiring padding for cuBLAS is encountered here this\n+    // happened because earlier ShouldTritonHandleGEMM() accepted it and padding\n+    // was skipped. Do not check ShouldTritonHandleGEMM() again then.\n+    if (!CublasRequiresPadding(\n+            *xla::Cast<HloDotInstruction>(dot),\n+            std::get<se::CudaComputeCapability>(gpu_version_)) &&\n+        !ShouldTritonHandleGEMM(*dot, gpu_version_)) {\n       return OkStatus();\n     }\n \n@@ -923,7 +936,7 @@ const DotFusionAnalysis::DimIterationSpec* DotFusionAnalysis::IterSpec(\n   return nullptr;\n }\n \n-bool IsTritonHandledGEMM(const HloInstruction& dot,\n+bool CanTritonHandleGEMM(const HloInstruction& dot,\n                          const GpuVersion gpu_version) {\n   if (dot.opcode() != HloOpcode::kDot ||\n       absl::c_any_of(dot.precision_config().operand_precision(),\n@@ -975,6 +988,11 @@ bool IsTritonHandledGEMM(const HloInstruction& dot,\n     return false;\n   }\n \n+  return true;\n+}\n+\n+bool ShouldTritonHandleGEMM(const HloInstruction& dot,\n+                            const GpuVersion gpu_version) {\n   if (dot.GetModule()->config().debug_options().xla_gpu_triton_gemm_any()) {\n     return true;\n   }\n"
        },
        {
            "name": "gemm_rewriter_triton.h",
            "path": "tensorflow/compiler/xla/service/gpu/gemm_rewriter_triton.h",
            "patches": [
                {
                    "old_start": 51,
                    "old_length": 8,
                    "new_start": 51,
                    "new_length": 11,
                    "hunk": "@@ -51,8 +51,11 @@ bool IsTritonSupportedElementwise(HloOpcode, PrimitiveType);\n Status MakeDotSplitKBatch(HloInstruction* dot_fusion,\n                           const AutotuneResult::TritonGemmKey& tiling);\n \n+// Filters GEMMs which can be handled using Triton.\n+bool CanTritonHandleGEMM(const HloInstruction&, GpuVersion gpu_version);\n+\n // Filters GEMMs which are better to handle using Triton.\n-bool IsTritonHandledGEMM(const HloInstruction&, GpuVersion gpu_version);\n+bool ShouldTritonHandleGEMM(const HloInstruction&, GpuVersion gpu_version);\n \n // Analysis of iteration of HLO shapes within a fusion around dot().\n class DotFusionAnalysis {\n"
                }
            ],
            "whole_deleted": "-bool IsTritonHandledGEMM(const HloInstruction&, GpuVersion gpu_version);\n",
            "whole_added": "+// Filters GEMMs which can be handled using Triton.\n+bool CanTritonHandleGEMM(const HloInstruction&, GpuVersion gpu_version);\n+\n+bool ShouldTritonHandleGEMM(const HloInstruction&, GpuVersion gpu_version);\n",
            "whole_hunk": "@@ -51,8 +51,11 @@ bool IsTritonSupportedElementwise(HloOpcode, PrimitiveType);\n Status MakeDotSplitKBatch(HloInstruction* dot_fusion,\n                           const AutotuneResult::TritonGemmKey& tiling);\n \n+// Filters GEMMs which can be handled using Triton.\n+bool CanTritonHandleGEMM(const HloInstruction&, GpuVersion gpu_version);\n+\n // Filters GEMMs which are better to handle using Triton.\n-bool IsTritonHandledGEMM(const HloInstruction&, GpuVersion gpu_version);\n+bool ShouldTritonHandleGEMM(const HloInstruction&, GpuVersion gpu_version);\n \n // Analysis of iteration of HLO shapes within a fusion around dot().\n class DotFusionAnalysis {\n"
        },
        {
            "name": "gemm_rewriter_triton_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gemm_rewriter_triton_test.cc",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 6,
                    "new_start": 20,
                    "new_length": 7,
                    "hunk": "@@ -20,6 +20,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_opcode.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n #include \"tensorflow/compiler/xla/service/pattern_matcher_gmock.h\"\n #include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/stream_executor/device_description.h\"\n"
                },
                {
                    "old_start": 771,
                    "old_length": 6,
                    "new_start": 772,
                    "new_length": 27,
                    "hunk": "@@ -771,6 +772,27 @@ ENTRY e {\n       tsl::testing::StatusIs(tsl::error::CANCELLED,\n                              \"Contracting dimension is too fragmented.\"));\n }\n+\n+TEST_F(GemmRewriterTritonTest, HandleDotIfCublasRequiresPadding) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+\n+ENTRY e {\n+  p0 = f16[5,3] parameter(0)\n+  p1 = f16[5,7] parameter(1)\n+  ROOT d = f16[3,7] dot(p0, p1),\n+    lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+})\"));\n+\n+  const se::CudaComputeCapability cc{se::CudaComputeCapability::VOLTA, 0};\n+  EXPECT_TRUE(CublasRequiresPadding(\n+      *xla::Cast<HloDotInstruction>(\n+          module->entry_computation()->root_instruction()),\n+      cc));\n+  EXPECT_TRUE(GemmRewriterTriton(cc).Run(module.get()).value());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n+\n+TEST_F(GemmRewriterTritonTest, HandleDotIfCublasRequiresPadding) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+\n+ENTRY e {\n+  p0 = f16[5,3] parameter(0)\n+  p1 = f16[5,7] parameter(1)\n+  ROOT d = f16[3,7] dot(p0, p1),\n+    lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+})\"));\n+\n+  const se::CudaComputeCapability cc{se::CudaComputeCapability::VOLTA, 0};\n+  EXPECT_TRUE(CublasRequiresPadding(\n+      *xla::Cast<HloDotInstruction>(\n+          module->entry_computation()->root_instruction()),\n+      cc));\n+  EXPECT_TRUE(GemmRewriterTriton(cc).Run(module.get()).value());\n+}\n+\n",
            "whole_hunk": "@@ -20,6 +20,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_opcode.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n #include \"tensorflow/compiler/xla/service/pattern_matcher_gmock.h\"\n #include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/stream_executor/device_description.h\"\n@@ -771,6 +772,27 @@ ENTRY e {\n       tsl::testing::StatusIs(tsl::error::CANCELLED,\n                              \"Contracting dimension is too fragmented.\"));\n }\n+\n+TEST_F(GemmRewriterTritonTest, HandleDotIfCublasRequiresPadding) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+HloModule m\n+\n+ENTRY e {\n+  p0 = f16[5,3] parameter(0)\n+  p1 = f16[5,7] parameter(1)\n+  ROOT d = f16[3,7] dot(p0, p1),\n+    lhs_contracting_dims={0}, rhs_contracting_dims={0}\n+})\"));\n+\n+  const se::CudaComputeCapability cc{se::CudaComputeCapability::VOLTA, 0};\n+  EXPECT_TRUE(CublasRequiresPadding(\n+      *xla::Cast<HloDotInstruction>(\n+          module->entry_computation()->root_instruction()),\n+      cc));\n+  EXPECT_TRUE(GemmRewriterTriton(cc).Run(module.get()).value());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla\n"
        },
        {
            "name": "nvptx_compiler.cc",
            "path": "tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc",
            "patches": [
                {
                    "old_start": 40,
                    "old_length": 6,
                    "new_start": 40,
                    "new_length": 7,
                    "hunk": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/service/gpu/autotuner_util.h\"\n #include \"tensorflow/compiler/xla/service/gpu/cublas_cudnn.h\"\n #include \"tensorflow/compiler/xla/service/gpu/cublas_pad_for_gemms.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n #include \"tensorflow/compiler/xla/service/gpu/cudnn_fused_conv_rewriter.h\"\n #include \"tensorflow/compiler/xla/service/gpu/cudnn_fused_mha_rewriter.h\"\n #include \"tensorflow/compiler/xla/service/gpu/cudnn_pad_for_convolutions.h\"\n"
                },
                {
                    "old_start": 214,
                    "old_length": 21,
                    "new_start": 215,
                    "new_length": 13,
                    "hunk": "@@ -214,21 +215,13 @@ Status NVPTXCompiler::OptimizeHloPostLayoutAssignment(\n     TF_RETURN_IF_ERROR(mha_fusion_pipeline.Run(hlo_module).status());\n   }\n \n-  if (cuda_compute_capability.IsAtLeast(se::CudaComputeCapability::AMPERE)) {\n-    pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n-                                            PrimitiveType::BF16,\n-                                            /*pad_to_multiple_of=*/8);\n-  }\n-  if (cuda_compute_capability.IsAtLeast(se::CudaComputeCapability::VOLTA)) {\n-    // Pad gemms over S8 to multiples of 4 so cuBLAS can run them.\n-    pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n-                                            PrimitiveType::S8,\n-                                            /*pad_to_multiple_of=*/4);\n-\n-    // Pad the dimensions of matrices in dot operations to multiples of 8.\n-    pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n-                                            PrimitiveType::F16,\n-                                            /*pad_to_multiple_of=*/8);\n+  for (const CublasPaddingRequirement& requirement :\n+       CublasPaddingRequirements) {\n+    if (cuda_compute_capability.IsAtLeast(requirement.min_compute_capability)) {\n+      pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n+                                              requirement.data_type,\n+                                              requirement.multiple_of);\n+    }\n   }\n   // Padding a gemm operand that's a constant results in pad(constant).  Run\n   // constant-folding to simplify this into a new constant."
                }
            ],
            "whole_deleted": "-  if (cuda_compute_capability.IsAtLeast(se::CudaComputeCapability::AMPERE)) {\n-    pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n-                                            PrimitiveType::BF16,\n-                                            /*pad_to_multiple_of=*/8);\n-  }\n-  if (cuda_compute_capability.IsAtLeast(se::CudaComputeCapability::VOLTA)) {\n-    // Pad gemms over S8 to multiples of 4 so cuBLAS can run them.\n-    pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n-                                            PrimitiveType::S8,\n-                                            /*pad_to_multiple_of=*/4);\n-\n-    // Pad the dimensions of matrices in dot operations to multiples of 8.\n-    pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n-                                            PrimitiveType::F16,\n-                                            /*pad_to_multiple_of=*/8);\n",
            "whole_added": "+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n+  for (const CublasPaddingRequirement& requirement :\n+       CublasPaddingRequirements) {\n+    if (cuda_compute_capability.IsAtLeast(requirement.min_compute_capability)) {\n+      pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n+                                              requirement.data_type,\n+                                              requirement.multiple_of);\n+    }\n",
            "whole_hunk": "@@ -40,6 +40,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/service/gpu/autotuner_util.h\"\n #include \"tensorflow/compiler/xla/service/gpu/cublas_cudnn.h\"\n #include \"tensorflow/compiler/xla/service/gpu/cublas_pad_for_gemms.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/cublas_padding_requirements.h\"\n #include \"tensorflow/compiler/xla/service/gpu/cudnn_fused_conv_rewriter.h\"\n #include \"tensorflow/compiler/xla/service/gpu/cudnn_fused_mha_rewriter.h\"\n #include \"tensorflow/compiler/xla/service/gpu/cudnn_pad_for_convolutions.h\"\n@@ -214,21 +215,13 @@ Status NVPTXCompiler::OptimizeHloPostLayoutAssignment(\n     TF_RETURN_IF_ERROR(mha_fusion_pipeline.Run(hlo_module).status());\n   }\n \n-  if (cuda_compute_capability.IsAtLeast(se::CudaComputeCapability::AMPERE)) {\n-    pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n-                                            PrimitiveType::BF16,\n-                                            /*pad_to_multiple_of=*/8);\n-  }\n-  if (cuda_compute_capability.IsAtLeast(se::CudaComputeCapability::VOLTA)) {\n-    // Pad gemms over S8 to multiples of 4 so cuBLAS can run them.\n-    pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n-                                            PrimitiveType::S8,\n-                                            /*pad_to_multiple_of=*/4);\n-\n-    // Pad the dimensions of matrices in dot operations to multiples of 8.\n-    pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n-                                            PrimitiveType::F16,\n-                                            /*pad_to_multiple_of=*/8);\n+  for (const CublasPaddingRequirement& requirement :\n+       CublasPaddingRequirements) {\n+    if (cuda_compute_capability.IsAtLeast(requirement.min_compute_capability)) {\n+      pre_pipeline.AddPass<CublasPadForGemms>(cuda_compute_capability,\n+                                              requirement.data_type,\n+                                              requirement.multiple_of);\n+    }\n   }\n   // Padding a gemm operand that's a constant results in pad(constant).  Run\n   // constant-folding to simplify this into a new constant."
        }
    ]
},
{
    "Id": 670,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2f95c49f1e72e1c95d150686a6ffeb23e76e3e71",
    "date": "2022-10-06T05:11:34-07:00",
    "message": "Make checks which opcodes support to_apply consistent.\n\nThere were switch statements in 3 different places, each with a different set\nof ops that are considered to have a to_apply() subcomputation. Add a\nhas_to_apply() function for checking whether an op has a to_apply()\nsubcomputation, and use it in these 3 places.\n\nPiperOrigin-RevId: 479284174",
    "label": "NO",
    "changes": [
        {
            "name": "hlo_instruction.cc",
            "path": "tensorflow/compiler/xla/service/hlo_instruction.cc",
            "patches": [
                {
                    "old_start": 2800,
                    "old_length": 45,
                    "new_start": 2800,
                    "new_length": 45,
                    "hunk": "@@ -2800,45 +2800,45 @@ bool HloInstruction::IsEffectiveBitcast() const {\n }\n \n HloComputation* HloInstruction::to_apply() const {\n-  switch (opcode_) {\n-    case HloOpcode::kCall:\n-    case HloOpcode::kMap:\n-    case HloOpcode::kReduceWindow:\n-    case HloOpcode::kReduce:\n-    case HloOpcode::kAllReduce:\n-    case HloOpcode::kReduceScatter:\n-    case HloOpcode::kAllReduceStart:\n-    case HloOpcode::kScatter:\n-    case HloOpcode::kSort:\n-    case HloOpcode::kCustomCall:\n-      CHECK_EQ(called_computations_.size(), 1);\n-      return called_computations_[0];\n-    default:\n-      LOG(FATAL) << \"Invalid opcode for to_apply(): \"\n-                 << HloOpcodeString(opcode());\n+  if (has_to_apply()) {\n+    CHECK_EQ(called_computations_.size(), 1)\n+        << \"Expected a to_apply computation for \" << HloOpcodeString(opcode());\n+    return called_computations_[0];\n   }\n+  LOG(FATAL) << \"Invalid opcode for to_apply(): \" << HloOpcodeString(opcode());\n }\n \n void HloInstruction::set_to_apply(HloComputation* computation) {\n   // Don't allow changing the computation for fused instructions so we don't\n   // have to recompute called_instructions for the entire fusion instruction.\n   CHECK(!IsFused());\n+  if (has_to_apply()) {\n+    CHECK_EQ(called_computations_.size(), 1)\n+        << \"Expected a to_apply computation for \" << HloOpcodeString(opcode());\n+    called_computations_[0] = computation;\n+    return;\n+  }\n+  LOG(FATAL) << \"Invalid opcode for to_apply(): \" << HloOpcodeString(opcode());\n+}\n+\n+bool HloInstruction::has_to_apply() const {\n   switch (opcode_) {\n+    case HloOpcode::kAllReduce:\n+    case HloOpcode::kAllReduceStart:\n     case HloOpcode::kCall:\n     case HloOpcode::kMap:\n-    case HloOpcode::kReduceWindow:\n     case HloOpcode::kReduce:\n-    case HloOpcode::kAllReduce:\n-    case HloOpcode::kAllReduceStart:\n+    case HloOpcode::kReduceScatter:\n+    case HloOpcode::kReduceWindow:\n     case HloOpcode::kScatter:\n     case HloOpcode::kSort:\n+      return true;\n     case HloOpcode::kCustomCall:\n-      CHECK_EQ(called_computations_.size(), 1);\n-      called_computations_[0] = computation;\n-      break;\n+      // CustomCall can have a to_apply computation, but it is not required to\n+      // have one.\n+      return called_computations_.size() == 1;\n     default:\n-      LOG(FATAL) << \"Invalid opcode for to_apply(): \"\n-                 << HloOpcodeString(opcode());\n+      return false;\n   }\n }\n \n"
                }
            ],
            "whole_deleted": "-  switch (opcode_) {\n-    case HloOpcode::kCall:\n-    case HloOpcode::kMap:\n-    case HloOpcode::kReduceWindow:\n-    case HloOpcode::kReduce:\n-    case HloOpcode::kAllReduce:\n-    case HloOpcode::kReduceScatter:\n-    case HloOpcode::kAllReduceStart:\n-    case HloOpcode::kScatter:\n-    case HloOpcode::kSort:\n-    case HloOpcode::kCustomCall:\n-      CHECK_EQ(called_computations_.size(), 1);\n-      return called_computations_[0];\n-    default:\n-      LOG(FATAL) << \"Invalid opcode for to_apply(): \"\n-                 << HloOpcodeString(opcode());\n-    case HloOpcode::kReduceWindow:\n-    case HloOpcode::kAllReduce:\n-    case HloOpcode::kAllReduceStart:\n-      CHECK_EQ(called_computations_.size(), 1);\n-      called_computations_[0] = computation;\n-      break;\n-      LOG(FATAL) << \"Invalid opcode for to_apply(): \"\n-                 << HloOpcodeString(opcode());\n",
            "whole_added": "+  if (has_to_apply()) {\n+    CHECK_EQ(called_computations_.size(), 1)\n+        << \"Expected a to_apply computation for \" << HloOpcodeString(opcode());\n+    return called_computations_[0];\n+  LOG(FATAL) << \"Invalid opcode for to_apply(): \" << HloOpcodeString(opcode());\n+  if (has_to_apply()) {\n+    CHECK_EQ(called_computations_.size(), 1)\n+        << \"Expected a to_apply computation for \" << HloOpcodeString(opcode());\n+    called_computations_[0] = computation;\n+    return;\n+  }\n+  LOG(FATAL) << \"Invalid opcode for to_apply(): \" << HloOpcodeString(opcode());\n+}\n+\n+bool HloInstruction::has_to_apply() const {\n+    case HloOpcode::kAllReduce:\n+    case HloOpcode::kAllReduceStart:\n+    case HloOpcode::kReduceScatter:\n+    case HloOpcode::kReduceWindow:\n+      return true;\n+      // CustomCall can have a to_apply computation, but it is not required to\n+      // have one.\n+      return called_computations_.size() == 1;\n+      return false;\n",
            "whole_hunk": "@@ -2800,45 +2800,45 @@ bool HloInstruction::IsEffectiveBitcast() const {\n }\n \n HloComputation* HloInstruction::to_apply() const {\n-  switch (opcode_) {\n-    case HloOpcode::kCall:\n-    case HloOpcode::kMap:\n-    case HloOpcode::kReduceWindow:\n-    case HloOpcode::kReduce:\n-    case HloOpcode::kAllReduce:\n-    case HloOpcode::kReduceScatter:\n-    case HloOpcode::kAllReduceStart:\n-    case HloOpcode::kScatter:\n-    case HloOpcode::kSort:\n-    case HloOpcode::kCustomCall:\n-      CHECK_EQ(called_computations_.size(), 1);\n-      return called_computations_[0];\n-    default:\n-      LOG(FATAL) << \"Invalid opcode for to_apply(): \"\n-                 << HloOpcodeString(opcode());\n+  if (has_to_apply()) {\n+    CHECK_EQ(called_computations_.size(), 1)\n+        << \"Expected a to_apply computation for \" << HloOpcodeString(opcode());\n+    return called_computations_[0];\n   }\n+  LOG(FATAL) << \"Invalid opcode for to_apply(): \" << HloOpcodeString(opcode());\n }\n \n void HloInstruction::set_to_apply(HloComputation* computation) {\n   // Don't allow changing the computation for fused instructions so we don't\n   // have to recompute called_instructions for the entire fusion instruction.\n   CHECK(!IsFused());\n+  if (has_to_apply()) {\n+    CHECK_EQ(called_computations_.size(), 1)\n+        << \"Expected a to_apply computation for \" << HloOpcodeString(opcode());\n+    called_computations_[0] = computation;\n+    return;\n+  }\n+  LOG(FATAL) << \"Invalid opcode for to_apply(): \" << HloOpcodeString(opcode());\n+}\n+\n+bool HloInstruction::has_to_apply() const {\n   switch (opcode_) {\n+    case HloOpcode::kAllReduce:\n+    case HloOpcode::kAllReduceStart:\n     case HloOpcode::kCall:\n     case HloOpcode::kMap:\n-    case HloOpcode::kReduceWindow:\n     case HloOpcode::kReduce:\n-    case HloOpcode::kAllReduce:\n-    case HloOpcode::kAllReduceStart:\n+    case HloOpcode::kReduceScatter:\n+    case HloOpcode::kReduceWindow:\n     case HloOpcode::kScatter:\n     case HloOpcode::kSort:\n+      return true;\n     case HloOpcode::kCustomCall:\n-      CHECK_EQ(called_computations_.size(), 1);\n-      called_computations_[0] = computation;\n-      break;\n+      // CustomCall can have a to_apply computation, but it is not required to\n+      // have one.\n+      return called_computations_.size() == 1;\n     default:\n-      LOG(FATAL) << \"Invalid opcode for to_apply(): \"\n-                 << HloOpcodeString(opcode());\n+      return false;\n   }\n }\n \n"
        },
        {
            "name": "hlo_instruction.h",
            "path": "tensorflow/compiler/xla/service/hlo_instruction.h",
            "patches": [
                {
                    "old_start": 1462,
                    "old_length": 6,
                    "new_start": 1462,
                    "new_length": 8,
                    "hunk": "@@ -1462,6 +1462,8 @@ class HloInstruction {\n   // Precondition: The instruction has a valid to_apply_ field.\n   HloComputation* to_apply() const;\n   void set_to_apply(HloComputation* computation);\n+  // Whether the instruction has a valid to_apply_ field.\n+  bool has_to_apply() const;\n \n   // Gets/sets the while_condition or while_body HloComputation for While. The\n   // setters should only be called by HloModule or HloComputation methods.\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // Whether the instruction has a valid to_apply_ field.\n+  bool has_to_apply() const;\n",
            "whole_hunk": "@@ -1462,6 +1462,8 @@ class HloInstruction {\n   // Precondition: The instruction has a valid to_apply_ field.\n   HloComputation* to_apply() const;\n   void set_to_apply(HloComputation* computation);\n+  // Whether the instruction has a valid to_apply_ field.\n+  bool has_to_apply() const;\n \n   // Gets/sets the while_condition or while_body HloComputation for While. The\n   // setters should only be called by HloModule or HloComputation methods.\n"
        },
        {
            "name": "hlo_module.cc",
            "path": "tensorflow/compiler/xla/service/hlo_module.cc",
            "patches": [
                {
                    "old_start": 172,
                    "old_length": 22,
                    "new_start": 172,
                    "new_length": 15,
                    "hunk": "@@ -172,22 +172,15 @@ void HloModule::ReplaceComputations(\n \n   for (std::unique_ptr<HloComputation>& computation : computations_) {\n     for (auto* instruction : computation->instructions()) {\n-      switch (instruction->opcode()) {\n-        case HloOpcode::kAllReduce:\n-        case HloOpcode::kCall:\n-        case HloOpcode::kMap:\n-        case HloOpcode::kReduce:\n-        case HloOpcode::kReduceScatter:\n-        case HloOpcode::kReduceWindow:\n-        case HloOpcode::kScatter:\n-        case HloOpcode::kSort: {\n-          HloComputation* new_arg = tsl::gtl::FindWithDefault(\n-              replacements, instruction->to_apply(), nullptr);\n-          if (new_arg != nullptr) {\n-            instruction->set_to_apply(new_arg);\n-          }\n-          break;\n+      if (instruction->has_to_apply()) {\n+        HloComputation* new_arg = tsl::gtl::FindWithDefault(\n+            replacements, instruction->to_apply(), nullptr);\n+        if (new_arg != nullptr) {\n+          instruction->set_to_apply(new_arg);\n         }\n+        continue;\n+      }\n+      switch (instruction->opcode()) {\n         case HloOpcode::kWhile: {\n           HloComputation* new_condition = tsl::gtl::FindWithDefault(\n               replacements, instruction->while_condition(), nullptr);\n"
                }
            ],
            "whole_deleted": "-      switch (instruction->opcode()) {\n-        case HloOpcode::kAllReduce:\n-        case HloOpcode::kCall:\n-        case HloOpcode::kMap:\n-        case HloOpcode::kReduce:\n-        case HloOpcode::kReduceScatter:\n-        case HloOpcode::kReduceWindow:\n-        case HloOpcode::kScatter:\n-        case HloOpcode::kSort: {\n-          HloComputation* new_arg = tsl::gtl::FindWithDefault(\n-              replacements, instruction->to_apply(), nullptr);\n-          if (new_arg != nullptr) {\n-            instruction->set_to_apply(new_arg);\n-          }\n-          break;\n",
            "whole_added": "+      if (instruction->has_to_apply()) {\n+        HloComputation* new_arg = tsl::gtl::FindWithDefault(\n+            replacements, instruction->to_apply(), nullptr);\n+        if (new_arg != nullptr) {\n+          instruction->set_to_apply(new_arg);\n+        continue;\n+      }\n+      switch (instruction->opcode()) {\n",
            "whole_hunk": "@@ -172,22 +172,15 @@ void HloModule::ReplaceComputations(\n \n   for (std::unique_ptr<HloComputation>& computation : computations_) {\n     for (auto* instruction : computation->instructions()) {\n-      switch (instruction->opcode()) {\n-        case HloOpcode::kAllReduce:\n-        case HloOpcode::kCall:\n-        case HloOpcode::kMap:\n-        case HloOpcode::kReduce:\n-        case HloOpcode::kReduceScatter:\n-        case HloOpcode::kReduceWindow:\n-        case HloOpcode::kScatter:\n-        case HloOpcode::kSort: {\n-          HloComputation* new_arg = tsl::gtl::FindWithDefault(\n-              replacements, instruction->to_apply(), nullptr);\n-          if (new_arg != nullptr) {\n-            instruction->set_to_apply(new_arg);\n-          }\n-          break;\n+      if (instruction->has_to_apply()) {\n+        HloComputation* new_arg = tsl::gtl::FindWithDefault(\n+            replacements, instruction->to_apply(), nullptr);\n+        if (new_arg != nullptr) {\n+          instruction->set_to_apply(new_arg);\n         }\n+        continue;\n+      }\n+      switch (instruction->opcode()) {\n         case HloOpcode::kWhile: {\n           HloComputation* new_condition = tsl::gtl::FindWithDefault(\n               replacements, instruction->while_condition(), nullptr);\n"
        },
        {
            "name": "hlo_module_test.cc",
            "path": "tensorflow/compiler/xla/service/hlo_module_test.cc",
            "patches": [
                {
                    "old_start": 471,
                    "old_length": 6,
                    "new_start": 471,
                    "new_length": 45,
                    "hunk": "@@ -471,6 +471,45 @@ ENTRY ReduceR3ToR2.v3 {\n   }\n }\n \n+TEST_F(HloModuleTest, VerifyReplaceComputationsWithReduceScatter) {\n+  const std::string text = R\"(\n+  HloModule reduce-scatter\n+  %sum (a: f32[], b: f32[]) -> f32[] {\n+    %a = f32[] parameter(0)\n+    %b = f32[] parameter(1)\n+    ROOT %add = f32[] add(f32[] a, f32[] b)\n+  }\n+  ENTRY main {\n+    %param = f32[16,8,128]{2,1,0} parameter(0)\n+    ROOT %rs = f32[4,8,128]{2,1,0} reduce-scatter(f32[16,8,128]{2,1,0} %param), replica_groups={}, to_apply=%sum, dimensions={0}\n+  }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(text));\n+\n+  // Create a replacement computation\n+  HloComputation* new_comp;\n+  {\n+    auto b = HloComputation::Builder(\"Fused\");\n+    auto p0 =\n+        b.AddInstruction(HloInstruction::CreateParameter(0, r0f32_, \"p0\"));\n+    auto p1 =\n+        b.AddInstruction(HloInstruction::CreateParameter(1, r0f32_, \"p1\"));\n+    b.AddInstruction(HloInstruction::CreateBinary(\n+        ShapeUtil::MakeShape(F32, {}), HloOpcode::kMultiply, p0, p1));\n+    new_comp = module->AddEmbeddedComputation(b.Build());\n+  }\n+\n+  HloComputation* entry = module->entry_computation();\n+  HloInstruction* root = entry->root_instruction();\n+  EXPECT_EQ(root->to_apply()->root_instruction()->opcode(), HloOpcode::kAdd);\n+\n+  absl::flat_hash_map<HloComputation*, HloComputation*> replacement;\n+  replacement[root->to_apply()] = new_comp;\n+  module->ReplaceComputations(replacement);\n+\n+  EXPECT_EQ(root->to_apply(), new_comp);\n+}\n+\n TEST_F(HloModuleTest, VerifyReplaceComputationsWithSortOp) {\n   const std::string text = R\"(\n   HloModule sort"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(HloModuleTest, VerifyReplaceComputationsWithReduceScatter) {\n+  const std::string text = R\"(\n+  HloModule reduce-scatter\n+  %sum (a: f32[], b: f32[]) -> f32[] {\n+    %a = f32[] parameter(0)\n+    %b = f32[] parameter(1)\n+    ROOT %add = f32[] add(f32[] a, f32[] b)\n+  }\n+  ENTRY main {\n+    %param = f32[16,8,128]{2,1,0} parameter(0)\n+    ROOT %rs = f32[4,8,128]{2,1,0} reduce-scatter(f32[16,8,128]{2,1,0} %param), replica_groups={}, to_apply=%sum, dimensions={0}\n+  }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(text));\n+\n+  // Create a replacement computation\n+  HloComputation* new_comp;\n+  {\n+    auto b = HloComputation::Builder(\"Fused\");\n+    auto p0 =\n+        b.AddInstruction(HloInstruction::CreateParameter(0, r0f32_, \"p0\"));\n+    auto p1 =\n+        b.AddInstruction(HloInstruction::CreateParameter(1, r0f32_, \"p1\"));\n+    b.AddInstruction(HloInstruction::CreateBinary(\n+        ShapeUtil::MakeShape(F32, {}), HloOpcode::kMultiply, p0, p1));\n+    new_comp = module->AddEmbeddedComputation(b.Build());\n+  }\n+\n+  HloComputation* entry = module->entry_computation();\n+  HloInstruction* root = entry->root_instruction();\n+  EXPECT_EQ(root->to_apply()->root_instruction()->opcode(), HloOpcode::kAdd);\n+\n+  absl::flat_hash_map<HloComputation*, HloComputation*> replacement;\n+  replacement[root->to_apply()] = new_comp;\n+  module->ReplaceComputations(replacement);\n+\n+  EXPECT_EQ(root->to_apply(), new_comp);\n+}\n+\n",
            "whole_hunk": "@@ -471,6 +471,45 @@ ENTRY ReduceR3ToR2.v3 {\n   }\n }\n \n+TEST_F(HloModuleTest, VerifyReplaceComputationsWithReduceScatter) {\n+  const std::string text = R\"(\n+  HloModule reduce-scatter\n+  %sum (a: f32[], b: f32[]) -> f32[] {\n+    %a = f32[] parameter(0)\n+    %b = f32[] parameter(1)\n+    ROOT %add = f32[] add(f32[] a, f32[] b)\n+  }\n+  ENTRY main {\n+    %param = f32[16,8,128]{2,1,0} parameter(0)\n+    ROOT %rs = f32[4,8,128]{2,1,0} reduce-scatter(f32[16,8,128]{2,1,0} %param), replica_groups={}, to_apply=%sum, dimensions={0}\n+  }\n+  )\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module, ParseAndReturnVerifiedModule(text));\n+\n+  // Create a replacement computation\n+  HloComputation* new_comp;\n+  {\n+    auto b = HloComputation::Builder(\"Fused\");\n+    auto p0 =\n+        b.AddInstruction(HloInstruction::CreateParameter(0, r0f32_, \"p0\"));\n+    auto p1 =\n+        b.AddInstruction(HloInstruction::CreateParameter(1, r0f32_, \"p1\"));\n+    b.AddInstruction(HloInstruction::CreateBinary(\n+        ShapeUtil::MakeShape(F32, {}), HloOpcode::kMultiply, p0, p1));\n+    new_comp = module->AddEmbeddedComputation(b.Build());\n+  }\n+\n+  HloComputation* entry = module->entry_computation();\n+  HloInstruction* root = entry->root_instruction();\n+  EXPECT_EQ(root->to_apply()->root_instruction()->opcode(), HloOpcode::kAdd);\n+\n+  absl::flat_hash_map<HloComputation*, HloComputation*> replacement;\n+  replacement[root->to_apply()] = new_comp;\n+  module->ReplaceComputations(replacement);\n+\n+  EXPECT_EQ(root->to_apply(), new_comp);\n+}\n+\n TEST_F(HloModuleTest, VerifyReplaceComputationsWithSortOp) {\n   const std::string text = R\"(\n   HloModule sort"
        }
    ]
},
{
    "Id": 583,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c9666f72eb0c97846c82bc3aadff9214f466585e",
    "date": "2023-01-19T17:19:30-08:00",
    "message": "This CL adds a new RewritePattern to check if a BatchMatMulOp, with no broadcasting necessary, can be lowered to FullyConnectedOp. If possible, lowers a TF::BatchMatMulOp to TFL::FullyConnectedOp.\n\nPiperOrigin-RevId: 503305389",
    "label": "NO",
    "changes": [
        {
            "name": "legalize-tf.mlir",
            "path": "tensorflow/compiler/mlir/lite/tests/legalize-tf.mlir",
            "patches": [
                {
                    "old_start": 2504,
                    "old_length": 3,
                    "new_start": 2504,
                    "new_length": 17,
                    "hunk": "@@ -2504,3 +2504,17 @@ func.func @sigmoidGrad(%arg0: tensor<?x32xf32>, %arg1: tensor<?x32xf32>) -> tens\n // CHECK-NEXT: [[MUL1:%.+]] =  tfl.mul %arg1, [[MUL0]] {fused_activation_function = \"NONE\"} : tensor<?x32xf32>\n // CHECK: return [[MUL1]]\n }\n+\n+func.func @batchmatmul2fullyconnected(%arg0: tensor<4x128x2xf32>) -> (tensor<4x128x1xf32>) {\n+  %0 = \"tf.Const\"() {value = dense<[[1.0], [2.0]]> : tensor<2x1xf32>} : () -> tensor<2x1xf32>\n+  %1 = \"tf.BatchMatMulV2\"(%arg0, %0) : (tensor<4x128x2xf32>, tensor<2x1xf32>) -> tensor<4x128x1xf32>\n+  func.return %1 : tensor<4x128x1xf32>\n+\n+  // CHECK-LABEL: batchmatmul2fullyconnected\n+  // CHECK-DAG:  %cst_0 = arith.constant dense<[1, 0]> : tensor<2xi32> \n+  // CHECK:  %0 = \"tfl.transpose\"(%cst, %cst_0) : (tensor<2x1xf32>, tensor<2xi32>) -> tensor<1x2xf32> \n+  // CHECK-DAG:  %1 = \"tfl.no_value\"() {value} : () -> none\n+  // CHECK:  %2 = \"tfl.fully_connected\"(%arg0, %0, %1) {fused_activation_function = \"NONE\", keep_num_dims = true, weights_format = \"DEFAULT\"} : (tensor<4x128x2xf32>, tensor<1x2xf32>, none) -> tensor<4x128x1xf32>\n+  // CHECK:  return %2 : tensor<4x128x1xf32>\n+}\n+\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+func.func @batchmatmul2fullyconnected(%arg0: tensor<4x128x2xf32>) -> (tensor<4x128x1xf32>) {\n+  %0 = \"tf.Const\"() {value = dense<[[1.0], [2.0]]> : tensor<2x1xf32>} : () -> tensor<2x1xf32>\n+  %1 = \"tf.BatchMatMulV2\"(%arg0, %0) : (tensor<4x128x2xf32>, tensor<2x1xf32>) -> tensor<4x128x1xf32>\n+  func.return %1 : tensor<4x128x1xf32>\n+\n+  // CHECK-LABEL: batchmatmul2fullyconnected\n+  // CHECK-DAG:  %cst_0 = arith.constant dense<[1, 0]> : tensor<2xi32> \n+  // CHECK:  %0 = \"tfl.transpose\"(%cst, %cst_0) : (tensor<2x1xf32>, tensor<2xi32>) -> tensor<1x2xf32> \n+  // CHECK-DAG:  %1 = \"tfl.no_value\"() {value} : () -> none\n+  // CHECK:  %2 = \"tfl.fully_connected\"(%arg0, %0, %1) {fused_activation_function = \"NONE\", keep_num_dims = true, weights_format = \"DEFAULT\"} : (tensor<4x128x2xf32>, tensor<1x2xf32>, none) -> tensor<4x128x1xf32>\n+  // CHECK:  return %2 : tensor<4x128x1xf32>\n+}\n+\n",
            "whole_hunk": "@@ -2504,3 +2504,17 @@ func.func @sigmoidGrad(%arg0: tensor<?x32xf32>, %arg1: tensor<?x32xf32>) -> tens\n // CHECK-NEXT: [[MUL1:%.+]] =  tfl.mul %arg1, [[MUL0]] {fused_activation_function = \"NONE\"} : tensor<?x32xf32>\n // CHECK: return [[MUL1]]\n }\n+\n+func.func @batchmatmul2fullyconnected(%arg0: tensor<4x128x2xf32>) -> (tensor<4x128x1xf32>) {\n+  %0 = \"tf.Const\"() {value = dense<[[1.0], [2.0]]> : tensor<2x1xf32>} : () -> tensor<2x1xf32>\n+  %1 = \"tf.BatchMatMulV2\"(%arg0, %0) : (tensor<4x128x2xf32>, tensor<2x1xf32>) -> tensor<4x128x1xf32>\n+  func.return %1 : tensor<4x128x1xf32>\n+\n+  // CHECK-LABEL: batchmatmul2fullyconnected\n+  // CHECK-DAG:  %cst_0 = arith.constant dense<[1, 0]> : tensor<2xi32> \n+  // CHECK:  %0 = \"tfl.transpose\"(%cst, %cst_0) : (tensor<2x1xf32>, tensor<2xi32>) -> tensor<1x2xf32> \n+  // CHECK-DAG:  %1 = \"tfl.no_value\"() {value} : () -> none\n+  // CHECK:  %2 = \"tfl.fully_connected\"(%arg0, %0, %1) {fused_activation_function = \"NONE\", keep_num_dims = true, weights_format = \"DEFAULT\"} : (tensor<4x128x2xf32>, tensor<1x2xf32>, none) -> tensor<4x128x1xf32>\n+  // CHECK:  return %2 : tensor<4x128x1xf32>\n+}\n+\n"
        },
        {
            "name": "legalize_tf.cc",
            "path": "tensorflow/compiler/mlir/lite/transforms/legalize_tf.cc",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 6,
                    "new_start": 21,
                    "new_length": 7,
                    "hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n // constant folding opportunities from the extra ops can be exploited by the\n // constant folding support for the TensorFlow ops.\n \n+#include <algorithm>\n #include <climits>\n #include <complex>\n #include <cstdint>\n"
                },
                {
                    "old_start": 169,
                    "old_length": 6,
                    "new_start": 170,
                    "new_length": 9,
                    "hunk": "@@ -169,6 +170,9 @@ mlir::TFL::MirrorPaddingType GetTFLMirrorPaddingFromString(\n \n DECL_CONVERT_OP(Assert);\n DECL_CONVERT_OP(ConcatV2);\n+DECL_CONVERT_OP(BatchMatMul);\n+DECL_CONVERT_OP(BatchMatMulV2);\n+DECL_CONVERT_OP(BatchMatMulV3);\n DECL_CONVERT_OP(MatMul);\n DECL_CONVERT_OP(MatrixDiagV2);\n DECL_CONVERT_OP(MatrixDiagV3);\n"
                },
                {
                    "old_start": 224,
                    "old_length": 6,
                    "new_start": 228,
                    "new_length": 128,
                    "hunk": "@@ -224,6 +228,128 @@ LogicalResult ConvertTFConcatV2Op::matchAndRewrite(\n   return success();\n }\n \n+template <typename BatchMatMulOpType>\n+bool ConvertTFBatchMatMulOp2TFLFullyConnectedOp(Operation* bmm_op,\n+                                                PatternRewriter& rewriter) {\n+  // If `value` is produced by tf.Dequantize op, then return the Dequantize op's\n+  // input. Otherwise return `value`.\n+  auto get_real_input_value = [](Value value) -> Value {\n+    Operation* defining_op = value.getDefiningOp();\n+    if (auto dequantize = dyn_cast_or_null<TF::DequantizeOp>(defining_op)) {\n+      return dequantize.getInput();\n+    } else if (auto dequantize =\n+                   dyn_cast_or_null<TFL::DequantizeOp>(defining_op)) {\n+      return dequantize.getInput();\n+    } else {\n+      return value;\n+    }\n+  };\n+\n+  // Returns true if the TF::BatchMatMul operation can be converted to\n+  // tfl.fully_connected.\n+  auto can_convert_to_fully_connected =\n+      [&](BatchMatMulOpType& batch_matmul_op) {\n+        Value input_rhs = get_real_input_value(batch_matmul_op.getY());\n+\n+        DenseElementsAttr constant;\n+        if (!matchPattern(input_rhs, m_Constant(&constant))) {\n+          return false;\n+        }\n+\n+        // The rhs matrix must be 2D for fully connected op.\n+        return (constant.getType().getRank() == 2);\n+      };\n+\n+  auto op = cast<BatchMatMulOpType>(bmm_op);\n+\n+  // Create a tfl.transpose op that performs ZX transpose on `input`.\n+  auto create_z_x_transpose_op = [&](Value input) -> Value {\n+    RankedTensorType input_type = input.getType().cast<RankedTensorType>();\n+    const int input_rank = input_type.getRank();\n+\n+    // Create a 1D I32 tensor for representing the dimension permutation.\n+    auto permuation_tensor_type =\n+        RankedTensorType::get({input_rank}, rewriter.getIntegerType(32));\n+    llvm::SmallVector<Attribute, 4> permute;\n+    permute.reserve(input_rank);\n+    // First create an identity permutation tensor.\n+    for (int i = 0; i < input_rank; i++) {\n+      permute.push_back(rewriter.getI32IntegerAttr(i));\n+    }\n+    // Swaps the last two dimension since the last two dimension will be mapped\n+    // to X and Z dimension.\n+    std::iter_swap(permute.begin() + input_rank - 1,\n+                   permute.begin() + input_rank - 2);\n+    auto permutation_tensor_op = rewriter.create<arith::ConstantOp>(\n+        op->getLoc(), permuation_tensor_type,\n+        DenseElementsAttr::get(permuation_tensor_type, permute));\n+\n+    auto input_shape = input_type.getShape();\n+    llvm::SmallVector<int64_t, 4> permuted_shape(input_shape.begin(),\n+                                                 input_shape.end());\n+    // Swaps z dimension and x dimension to get permuted shape.\n+    std::iter_swap(permuted_shape.begin() + input_rank - 1,\n+                   permuted_shape.begin() + input_rank - 2);\n+    return rewriter.create<TFL::TransposeOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(permuted_shape, input_type.getElementType()),\n+        input, permutation_tensor_op.getResult());\n+  };\n+\n+  if (!can_convert_to_fully_connected(op)) {\n+    return false;\n+  }\n+\n+  Value input_lhs = get_real_input_value(op.getX());\n+  Value input_rhs = get_real_input_value(op.getY());\n+\n+  Value legalized_lhs =\n+      op.getAdjX() ? create_z_x_transpose_op(input_lhs) : input_lhs;\n+\n+  // The rhs need to be transposed if adj_y == false AND this matmul will be\n+  // legalized to tfl.fully_connected\n+  Value legalized_rhs =\n+      !op.getAdjY() ? create_z_x_transpose_op(input_rhs) : input_rhs;\n+\n+  Type output_type = op.getResult().getType();\n+  auto no_input = rewriter.create<TFL::NoValueOp>(\n+      op->getLoc(), rewriter.getNoneType(), rewriter.getUnitAttr());\n+  auto fc_op = rewriter.create<TFL::FullyConnectedOp>(\n+      op->getLoc(), ArrayRef<Type>{output_type},\n+      /*input=*/legalized_lhs, /*filter=*/legalized_rhs, /*bias=*/no_input,\n+      /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n+      /*weights_format=*/rewriter.getStringAttr(\"DEFAULT\"),\n+      /*keep_num_dims=*/rewriter.getBoolAttr(true),\n+      /*asymmetric_quantize_inputs=*/mlir::BoolAttr());\n+  rewriter.replaceOp(op, {fc_op.getResult(0)});\n+\n+  return true;\n+}\n+\n+LogicalResult ConvertTFBatchMatMulOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  if (ConvertTFBatchMatMulOp2TFLFullyConnectedOp<TF::BatchMatMulOp>(op,\n+                                                                    rewriter))\n+    return success();\n+  return failure();\n+}\n+\n+LogicalResult ConvertTFBatchMatMulV2Op::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  if (ConvertTFBatchMatMulOp2TFLFullyConnectedOp<TF::BatchMatMulV2Op>(op,\n+                                                                      rewriter))\n+    return success();\n+  return failure();\n+}\n+\n+LogicalResult ConvertTFBatchMatMulV3Op::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  if (ConvertTFBatchMatMulOp2TFLFullyConnectedOp<TF::BatchMatMulV3Op>(op,\n+                                                                      rewriter))\n+    return success();\n+  return failure();\n+}\n+\n LogicalResult ConvertTFMatMulOp::matchAndRewrite(\n     Operation* op, PatternRewriter& rewriter) const {\n   auto tf_matmul_op = cast<TF::MatMulOp>(op);\n"
                },
                {
                    "old_start": 904,
                    "old_length": 7,
                    "new_start": 1030,
                    "new_length": 9,
                    "hunk": "@@ -904,7 +1030,9 @@ void addPatterns(MLIRContext* context, RewritePatternSet& patterns,\n \n   // Add the generated patterns to the list.\n   populateWithGenerated(patterns);\n-  patterns.add<ConvertTFConcatV2Op, ConvertTFMatMulOp, ConvertTFMatrixDiagV2Op,\n+  patterns.add<ConvertTFConcatV2Op, ConvertTFBatchMatMulOp,\n+               ConvertTFBatchMatMulV2Op, ConvertTFBatchMatMulV3Op,\n+               ConvertTFMatMulOp, ConvertTFMatrixDiagV2Op,\n                ConvertTFMatrixDiagV3Op, ConvertTFPackOp, ConvertTFSplitOp,\n                ConvertTFSplitVOp, ConvertTFUnpackOp, ConvertTFConv3DOp,\n                ConvertTFConv3DBackpropInputV2Op>(context);"
                }
            ],
            "whole_deleted": "-  patterns.add<ConvertTFConcatV2Op, ConvertTFMatMulOp, ConvertTFMatrixDiagV2Op,\n",
            "whole_added": "+#include <algorithm>\n+DECL_CONVERT_OP(BatchMatMul);\n+DECL_CONVERT_OP(BatchMatMulV2);\n+DECL_CONVERT_OP(BatchMatMulV3);\n+template <typename BatchMatMulOpType>\n+bool ConvertTFBatchMatMulOp2TFLFullyConnectedOp(Operation* bmm_op,\n+                                                PatternRewriter& rewriter) {\n+  // If `value` is produced by tf.Dequantize op, then return the Dequantize op's\n+  // input. Otherwise return `value`.\n+  auto get_real_input_value = [](Value value) -> Value {\n+    Operation* defining_op = value.getDefiningOp();\n+    if (auto dequantize = dyn_cast_or_null<TF::DequantizeOp>(defining_op)) {\n+      return dequantize.getInput();\n+    } else if (auto dequantize =\n+                   dyn_cast_or_null<TFL::DequantizeOp>(defining_op)) {\n+      return dequantize.getInput();\n+    } else {\n+      return value;\n+    }\n+  };\n+\n+  // Returns true if the TF::BatchMatMul operation can be converted to\n+  // tfl.fully_connected.\n+  auto can_convert_to_fully_connected =\n+      [&](BatchMatMulOpType& batch_matmul_op) {\n+        Value input_rhs = get_real_input_value(batch_matmul_op.getY());\n+\n+        DenseElementsAttr constant;\n+        if (!matchPattern(input_rhs, m_Constant(&constant))) {\n+          return false;\n+        }\n+\n+        // The rhs matrix must be 2D for fully connected op.\n+        return (constant.getType().getRank() == 2);\n+      };\n+\n+  auto op = cast<BatchMatMulOpType>(bmm_op);\n+\n+  // Create a tfl.transpose op that performs ZX transpose on `input`.\n+  auto create_z_x_transpose_op = [&](Value input) -> Value {\n+    RankedTensorType input_type = input.getType().cast<RankedTensorType>();\n+    const int input_rank = input_type.getRank();\n+\n+    // Create a 1D I32 tensor for representing the dimension permutation.\n+    auto permuation_tensor_type =\n+        RankedTensorType::get({input_rank}, rewriter.getIntegerType(32));\n+    llvm::SmallVector<Attribute, 4> permute;\n+    permute.reserve(input_rank);\n+    // First create an identity permutation tensor.\n+    for (int i = 0; i < input_rank; i++) {\n+      permute.push_back(rewriter.getI32IntegerAttr(i));\n+    }\n+    // Swaps the last two dimension since the last two dimension will be mapped\n+    // to X and Z dimension.\n+    std::iter_swap(permute.begin() + input_rank - 1,\n+                   permute.begin() + input_rank - 2);\n+    auto permutation_tensor_op = rewriter.create<arith::ConstantOp>(\n+        op->getLoc(), permuation_tensor_type,\n+        DenseElementsAttr::get(permuation_tensor_type, permute));\n+\n+    auto input_shape = input_type.getShape();\n+    llvm::SmallVector<int64_t, 4> permuted_shape(input_shape.begin(),\n+                                                 input_shape.end());\n+    // Swaps z dimension and x dimension to get permuted shape.\n+    std::iter_swap(permuted_shape.begin() + input_rank - 1,\n+                   permuted_shape.begin() + input_rank - 2);\n+    return rewriter.create<TFL::TransposeOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(permuted_shape, input_type.getElementType()),\n+        input, permutation_tensor_op.getResult());\n+  };\n+\n+  if (!can_convert_to_fully_connected(op)) {\n+    return false;\n+  }\n+\n+  Value input_lhs = get_real_input_value(op.getX());\n+  Value input_rhs = get_real_input_value(op.getY());\n+\n+  Value legalized_lhs =\n+      op.getAdjX() ? create_z_x_transpose_op(input_lhs) : input_lhs;\n+\n+  // The rhs need to be transposed if adj_y == false AND this matmul will be\n+  // legalized to tfl.fully_connected\n+  Value legalized_rhs =\n+      !op.getAdjY() ? create_z_x_transpose_op(input_rhs) : input_rhs;\n+\n+  Type output_type = op.getResult().getType();\n+  auto no_input = rewriter.create<TFL::NoValueOp>(\n+      op->getLoc(), rewriter.getNoneType(), rewriter.getUnitAttr());\n+  auto fc_op = rewriter.create<TFL::FullyConnectedOp>(\n+      op->getLoc(), ArrayRef<Type>{output_type},\n+      /*input=*/legalized_lhs, /*filter=*/legalized_rhs, /*bias=*/no_input,\n+      /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n+      /*weights_format=*/rewriter.getStringAttr(\"DEFAULT\"),\n+      /*keep_num_dims=*/rewriter.getBoolAttr(true),\n+      /*asymmetric_quantize_inputs=*/mlir::BoolAttr());\n+  rewriter.replaceOp(op, {fc_op.getResult(0)});\n+\n+  return true;\n+}\n+\n+LogicalResult ConvertTFBatchMatMulOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  if (ConvertTFBatchMatMulOp2TFLFullyConnectedOp<TF::BatchMatMulOp>(op,\n+                                                                    rewriter))\n+    return success();\n+  return failure();\n+}\n+\n+LogicalResult ConvertTFBatchMatMulV2Op::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  if (ConvertTFBatchMatMulOp2TFLFullyConnectedOp<TF::BatchMatMulV2Op>(op,\n+                                                                      rewriter))\n+    return success();\n+  return failure();\n+}\n+\n+LogicalResult ConvertTFBatchMatMulV3Op::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  if (ConvertTFBatchMatMulOp2TFLFullyConnectedOp<TF::BatchMatMulV3Op>(op,\n+                                                                      rewriter))\n+    return success();\n+  return failure();\n+}\n+\n+  patterns.add<ConvertTFConcatV2Op, ConvertTFBatchMatMulOp,\n+               ConvertTFBatchMatMulV2Op, ConvertTFBatchMatMulV3Op,\n+               ConvertTFMatMulOp, ConvertTFMatrixDiagV2Op,\n",
            "whole_hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n // constant folding opportunities from the extra ops can be exploited by the\n // constant folding support for the TensorFlow ops.\n \n+#include <algorithm>\n #include <climits>\n #include <complex>\n #include <cstdint>\n@@ -169,6 +170,9 @@ mlir::TFL::MirrorPaddingType GetTFLMirrorPaddingFromString(\n \n DECL_CONVERT_OP(Assert);\n DECL_CONVERT_OP(ConcatV2);\n+DECL_CONVERT_OP(BatchMatMul);\n+DECL_CONVERT_OP(BatchMatMulV2);\n+DECL_CONVERT_OP(BatchMatMulV3);\n DECL_CONVERT_OP(MatMul);\n DECL_CONVERT_OP(MatrixDiagV2);\n DECL_CONVERT_OP(MatrixDiagV3);\n@@ -224,6 +228,128 @@ LogicalResult ConvertTFConcatV2Op::matchAndRewrite(\n   return success();\n }\n \n+template <typename BatchMatMulOpType>\n+bool ConvertTFBatchMatMulOp2TFLFullyConnectedOp(Operation* bmm_op,\n+                                                PatternRewriter& rewriter) {\n+  // If `value` is produced by tf.Dequantize op, then return the Dequantize op's\n+  // input. Otherwise return `value`.\n+  auto get_real_input_value = [](Value value) -> Value {\n+    Operation* defining_op = value.getDefiningOp();\n+    if (auto dequantize = dyn_cast_or_null<TF::DequantizeOp>(defining_op)) {\n+      return dequantize.getInput();\n+    } else if (auto dequantize =\n+                   dyn_cast_or_null<TFL::DequantizeOp>(defining_op)) {\n+      return dequantize.getInput();\n+    } else {\n+      return value;\n+    }\n+  };\n+\n+  // Returns true if the TF::BatchMatMul operation can be converted to\n+  // tfl.fully_connected.\n+  auto can_convert_to_fully_connected =\n+      [&](BatchMatMulOpType& batch_matmul_op) {\n+        Value input_rhs = get_real_input_value(batch_matmul_op.getY());\n+\n+        DenseElementsAttr constant;\n+        if (!matchPattern(input_rhs, m_Constant(&constant))) {\n+          return false;\n+        }\n+\n+        // The rhs matrix must be 2D for fully connected op.\n+        return (constant.getType().getRank() == 2);\n+      };\n+\n+  auto op = cast<BatchMatMulOpType>(bmm_op);\n+\n+  // Create a tfl.transpose op that performs ZX transpose on `input`.\n+  auto create_z_x_transpose_op = [&](Value input) -> Value {\n+    RankedTensorType input_type = input.getType().cast<RankedTensorType>();\n+    const int input_rank = input_type.getRank();\n+\n+    // Create a 1D I32 tensor for representing the dimension permutation.\n+    auto permuation_tensor_type =\n+        RankedTensorType::get({input_rank}, rewriter.getIntegerType(32));\n+    llvm::SmallVector<Attribute, 4> permute;\n+    permute.reserve(input_rank);\n+    // First create an identity permutation tensor.\n+    for (int i = 0; i < input_rank; i++) {\n+      permute.push_back(rewriter.getI32IntegerAttr(i));\n+    }\n+    // Swaps the last two dimension since the last two dimension will be mapped\n+    // to X and Z dimension.\n+    std::iter_swap(permute.begin() + input_rank - 1,\n+                   permute.begin() + input_rank - 2);\n+    auto permutation_tensor_op = rewriter.create<arith::ConstantOp>(\n+        op->getLoc(), permuation_tensor_type,\n+        DenseElementsAttr::get(permuation_tensor_type, permute));\n+\n+    auto input_shape = input_type.getShape();\n+    llvm::SmallVector<int64_t, 4> permuted_shape(input_shape.begin(),\n+                                                 input_shape.end());\n+    // Swaps z dimension and x dimension to get permuted shape.\n+    std::iter_swap(permuted_shape.begin() + input_rank - 1,\n+                   permuted_shape.begin() + input_rank - 2);\n+    return rewriter.create<TFL::TransposeOp>(\n+        op->getLoc(),\n+        RankedTensorType::get(permuted_shape, input_type.getElementType()),\n+        input, permutation_tensor_op.getResult());\n+  };\n+\n+  if (!can_convert_to_fully_connected(op)) {\n+    return false;\n+  }\n+\n+  Value input_lhs = get_real_input_value(op.getX());\n+  Value input_rhs = get_real_input_value(op.getY());\n+\n+  Value legalized_lhs =\n+      op.getAdjX() ? create_z_x_transpose_op(input_lhs) : input_lhs;\n+\n+  // The rhs need to be transposed if adj_y == false AND this matmul will be\n+  // legalized to tfl.fully_connected\n+  Value legalized_rhs =\n+      !op.getAdjY() ? create_z_x_transpose_op(input_rhs) : input_rhs;\n+\n+  Type output_type = op.getResult().getType();\n+  auto no_input = rewriter.create<TFL::NoValueOp>(\n+      op->getLoc(), rewriter.getNoneType(), rewriter.getUnitAttr());\n+  auto fc_op = rewriter.create<TFL::FullyConnectedOp>(\n+      op->getLoc(), ArrayRef<Type>{output_type},\n+      /*input=*/legalized_lhs, /*filter=*/legalized_rhs, /*bias=*/no_input,\n+      /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n+      /*weights_format=*/rewriter.getStringAttr(\"DEFAULT\"),\n+      /*keep_num_dims=*/rewriter.getBoolAttr(true),\n+      /*asymmetric_quantize_inputs=*/mlir::BoolAttr());\n+  rewriter.replaceOp(op, {fc_op.getResult(0)});\n+\n+  return true;\n+}\n+\n+LogicalResult ConvertTFBatchMatMulOp::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  if (ConvertTFBatchMatMulOp2TFLFullyConnectedOp<TF::BatchMatMulOp>(op,\n+                                                                    rewriter))\n+    return success();\n+  return failure();\n+}\n+\n+LogicalResult ConvertTFBatchMatMulV2Op::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  if (ConvertTFBatchMatMulOp2TFLFullyConnectedOp<TF::BatchMatMulV2Op>(op,\n+                                                                      rewriter))\n+    return success();\n+  return failure();\n+}\n+\n+LogicalResult ConvertTFBatchMatMulV3Op::matchAndRewrite(\n+    Operation* op, PatternRewriter& rewriter) const {\n+  if (ConvertTFBatchMatMulOp2TFLFullyConnectedOp<TF::BatchMatMulV3Op>(op,\n+                                                                      rewriter))\n+    return success();\n+  return failure();\n+}\n+\n LogicalResult ConvertTFMatMulOp::matchAndRewrite(\n     Operation* op, PatternRewriter& rewriter) const {\n   auto tf_matmul_op = cast<TF::MatMulOp>(op);\n@@ -904,7 +1030,9 @@ void addPatterns(MLIRContext* context, RewritePatternSet& patterns,\n \n   // Add the generated patterns to the list.\n   populateWithGenerated(patterns);\n-  patterns.add<ConvertTFConcatV2Op, ConvertTFMatMulOp, ConvertTFMatrixDiagV2Op,\n+  patterns.add<ConvertTFConcatV2Op, ConvertTFBatchMatMulOp,\n+               ConvertTFBatchMatMulV2Op, ConvertTFBatchMatMulV3Op,\n+               ConvertTFMatMulOp, ConvertTFMatrixDiagV2Op,\n                ConvertTFMatrixDiagV3Op, ConvertTFPackOp, ConvertTFSplitOp,\n                ConvertTFSplitVOp, ConvertTFUnpackOp, ConvertTFConv3DOp,\n                ConvertTFConv3DBackpropInputV2Op>(context);"
        }
    ]
},
{
    "Id": 310,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/94eb84b051eb1435c1930ddad8f474c188a234b9",
    "date": "2023-08-23T02:22:34-07:00",
    "message": "[XLA:GPU][NFC] Refactor Triton GEMM rewriter: use a context class to hold attributes of a fusion scope.\n\nIn addition generalize the way the parameters of the dimension split are passed between dot fusion scopes and update the method checking for supported dimension orders.\n\nPiperOrigin-RevId: 559358482",
    "label": "NO",
    "changes": [
        {
            "name": "gemm_rewriter_triton.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gemm_rewriter_triton.cc",
            "patches": [
                {
                    "old_start": 165,
                    "old_length": 16,
                    "new_start": 165,
                    "new_length": 13,
                    "hunk": "@@ -165,16 +165,13 @@ FusionDecision RequireTritonFusibleConvert(const HloInstruction* input,\n // Used to calculate cumulative index transformations done by non-elementwise\n // instructions between source and target.\n class DimensionOrder {\n+ public:\n+  DimensionOrder() = default;\n+\n   // Dimension order constructed for the output shape of `hlo`.\n-  // `hlo` is currently supposed to be either an operand or the output of dot();\n-  // properties describing the dimensions are stored for later analysis.\n-  explicit DimensionOrder(\n-      const HloInstruction* hlo, const int64_t splittable_dimension_index = -1,\n-      const int64_t splittable_dimension_supported_major_size = 0,\n-      const int split_k_dimension_index = -1)\n-      : splittable_dimension_index_(splittable_dimension_index),\n-        splittable_dimension_supported_major_part_size_(\n-            splittable_dimension_supported_major_size) {\n+  // `hlo` is currently supposed to be either an operand or the output of dot().\n+  explicit DimensionOrder(const HloInstruction* hlo,\n+                          const int split_k_dimension_index = -1) {\n     tensor_fragments_order_.reserve(hlo->shape().rank());\n     for (const int i : hlo->shape().layout().minor_to_major()) {\n       int target_dim_number = i;\n"
                },
                {
                    "old_start": 191,
                    "old_length": 14,
                    "new_start": 188,
                    "new_length": 6,
                    "hunk": "@@ -191,14 +188,6 @@ class DimensionOrder {\n     }\n   }\n \n-  explicit DimensionOrder(\n-      const int64_t splittable_dimension_index,\n-      const int64_t splittable_dimension_supported_major_size)\n-      : splittable_dimension_index_(splittable_dimension_index),\n-        splittable_dimension_supported_major_part_size_(\n-            splittable_dimension_supported_major_size) {}\n-\n- public:\n   // Description of a continuous fragment of one dimension of a tensor.\n   struct Fragment {\n     // Label carrying the dimension number of an defining operation.\n"
                },
                {
                    "old_start": 214,
                    "old_length": 27,
                    "new_start": 203,
                    "new_length": 6,
                    "hunk": "@@ -214,27 +203,6 @@ class DimensionOrder {\n   using Fragments = std::vector<Fragment>;\n   using FragmentOrders = absl::flat_hash_map<int, std::vector<int>>;\n \n-  DimensionOrder(const DimensionOrder&) = default;\n-\n-  // Copies fusion context attributes from `other` leaving internal structures\n-  // describing dimension fragments empty. Used to create derived dimension\n-  // orders.\n-  static DimensionOrder EmptyLike(const DimensionOrder& other) {\n-    return DimensionOrder(\n-        other.splittable_dimension_index_,\n-        other.splittable_dimension_supported_major_part_size_);\n-  }\n-\n-  // Create dimension order describing a dot operand according to\n-  // the currently supported configurations.\n-  static DimensionOrder FromDotOperand(const HloInstruction& dot,\n-                                       int operand_number, int split_k = 1);\n-\n-  // Create dimension order describing dot's output.\n-  static DimensionOrder FromDotOutput(\n-      const HloInstruction& dot, int split_k = 1,\n-      int64_t splittable_dimension_supported_major_part_size = 0);\n-\n   const Fragments& TensorFragmentsOrder() const {\n     return tensor_fragments_order_;\n   }\n"
                },
                {
                    "old_start": 245,
                    "old_length": 19,
                    "new_start": 213,
                    "new_length": 6,
                    "hunk": "@@ -245,19 +213,6 @@ class DimensionOrder {\n   }\n   FragmentOrders& DimFragmentsOrders() { return dim_fragments_orders_; }\n \n-  // Index of dot dimension that can be split.\n-  // Currently typically LHS non-contracting one.\n-  int64_t SplittableDimensionIndex() const {\n-    return splittable_dimension_index_;\n-  }\n-\n-  // Tells whether `size` major part of a dimension can be physically split.\n-  bool IsSupportedSplittableDimensionMajorPartSize(int64_t size) const {\n-    // 0 means no specific size requirement.\n-    return splittable_dimension_supported_major_part_size_ == 0 ||\n-           splittable_dimension_supported_major_part_size_ == size;\n-  }\n-\n   // Tells that two dimension orders describe the same tensor physical layout.\n   bool IsPhysicallyEquivalent(const DimensionOrder& other) const;\n \n"
                },
                {
                    "old_start": 281,
                    "old_length": 9,
                    "new_start": 236,
                    "new_length": 6,
                    "hunk": "@@ -281,9 +236,6 @@ class DimensionOrder {\n   // (fragments can be physically unordered and disconnected within\n   // the shape due to reshapes and transposes).\n   FragmentOrders dim_fragments_orders_;\n-\n-  const int64_t splittable_dimension_index_ = -1;\n-  const int64_t splittable_dimension_supported_major_part_size_ = 0;\n };\n \n using DimIterationSpec = TensorIterationSpec::DimIterationSpec;\n"
                },
                {
                    "old_start": 292,
                    "old_length": 6,
                    "new_start": 244,
                    "new_length": 11,
                    "hunk": "@@ -292,6 +244,11 @@ using Fragments = DimensionOrder::Fragments;\n using FragmentOrders = DimensionOrder::FragmentOrders;\n using DimOrderMap = absl::flat_hash_map<const HloInstruction*, DimensionOrder>;\n \n+struct DimOrderUpdates {\n+  DimOrderMap map;\n+  int64_t splittable_dimension_major_part_size = 0;\n+};\n+\n TensorIterationSpec DimensionOrderToTensorIterationSpec(\n     const DimensionOrder& order) {\n   const Fragments& dim_fragments = order.TensorFragmentsOrder();\n"
                },
                {
                    "old_start": 344,
                    "old_length": 53,
                    "new_start": 301,
                    "new_length": 12,
                    "hunk": "@@ -344,53 +301,12 @@ bool DimensionOrder::IsPhysicallyEquivalent(const DimensionOrder& other) const {\n          DimensionOrderToTensorIterationSpec(other);\n }\n \n-DimensionOrder DimensionOrder::FromDotOperand(const HloInstruction& dot,\n-                                              const int operand_number,\n-                                              const int split_k) {\n-  const HloInstruction* operand = dot.operand(operand_number);\n-  // There can be either none or one split-K batch dimension.\n-  const int num_split_k_batch_dims = split_k > 1;\n-  int split_k_dimension_index = -1;\n-  if (split_k > 1) {\n-    split_k_dimension_index =\n-        ContractingDimensionIndex(dot, operand_number) - 1;\n-  }\n-  int splittable_dimension_index = -1;\n-  // LHS non-contracting dimension can be split if non-splitK batch is absent.\n-  if (operand_number == 0 &&\n-      dot.dot_dimension_numbers().lhs_batch_dimensions_size() -\n-              num_split_k_batch_dims ==\n-          0) {\n-    splittable_dimension_index =\n-        NonContractingDimensionIndex(dot, operand_number);\n-  }\n-  return DimensionOrder(operand, splittable_dimension_index,\n-                        /*splittable_dimension_supported_major_size=*/0,\n-                        split_k_dimension_index);\n-}\n-\n-DimensionOrder DimensionOrder::FromDotOutput(\n-    const HloInstruction& dot, const int split_k,\n-    const int64_t splittable_dimension_supported_major_part_size) {\n-  // Allow non-contracting dimension originating from LHS to split if\n-  // this dimension is split at the output at the same ratio as\n-  // at the input.\n-  int64_t splittable_dimension_index = -1;\n-  if (splittable_dimension_supported_major_part_size > 1) {\n-    // Split-K dimension is the first one in the output if present;\n-    // LHS non-contracting follows (batch is absent in this case).\n-    splittable_dimension_index = (split_k > 1) ? 1 : 0;\n-  }\n-  return DimensionOrder(&dot, splittable_dimension_index,\n-                        splittable_dimension_supported_major_part_size);\n-}\n-\n enum class TransformDirection { kInputToOutput, kOutputToInput };\n \n-using DimOrderMapOrError = std::variant<FusionDecision, DimOrderMap>;\n+using DimOrderUpdatesOrError = std::variant<FusionDecision, DimOrderUpdates>;\n \n-DimOrderMapOrError HandleElementwise(const HloInstruction* hlo,\n-                                     const DimOrderMap& dim_orders) {\n+DimOrderUpdatesOrError HandleElementwise(const HloInstruction* hlo,\n+                                         const DimOrderMap& dim_orders) {\n   // The output and all the input dimension orders of `hlo` have to be the same.\n   const HloInstruction* src = nullptr;\n   const DimensionOrder* src_dim_order;\n"
                },
                {
                    "old_start": 410,
                    "old_length": 17,
                    "new_start": 326,
                    "new_length": 17,
                    "hunk": "@@ -410,17 +326,17 @@ DimOrderMapOrError HandleElementwise(const HloInstruction* hlo,\n     CHECK_NE(src, nullptr);\n   }\n \n-  DimOrderMap result;\n-  result.insert({hlo, DimensionOrder(*src_dim_order)});\n+  DimOrderUpdates result;\n+  result.map.insert({hlo, DimensionOrder(*src_dim_order)});\n   for (const HloInstruction* operand : hlo->operands()) {\n-    result.insert({operand, DimensionOrder(dim_orders.at(src))});\n+    result.map.insert({operand, DimensionOrder(dim_orders.at(src))});\n   }\n   return result;\n }\n \n-DimOrderMapOrError HandleBitcast(const HloInstruction* hlo,\n-                                 const DimOrderMap& dim_orders,\n-                                 const TransformDirection direction) {\n+DimOrderUpdatesOrError HandleBitcast(const HloInstruction* hlo,\n+                                     const DimOrderMap& dim_orders,\n+                                     const TransformDirection direction) {\n   const HloInstruction* src =\n       (direction == TransformDirection::kOutputToInput) ? hlo : hlo->operand(0);\n   const HloInstruction* dst =\n"
                },
                {
                    "old_start": 428,
                    "old_length": 10,
                    "new_start": 344,
                    "new_length": 9,
                    "hunk": "@@ -428,10 +344,9 @@ DimOrderMapOrError HandleBitcast(const HloInstruction* hlo,\n   const Shape& dst_shape = dst->shape();\n   const Fragments& src_fragments_order =\n       dim_orders.at(src).TensorFragmentsOrder();\n-  DimOrderMap result;\n+  DimOrderUpdates result;\n   DimensionOrder& dst_dim_order =\n-      result.insert({dst, DimensionOrder::EmptyLike(dim_orders.at(src))})\n-          .first->second;\n+      result.map.insert({dst, DimensionOrder()}).first->second;\n   Fragments& dst_fragments_order = dst_dim_order.TensorFragmentsOrder();\n   // Size of not yet assigned part of current target dimension.\n   int64_t dst_remaining_size = 1;\n"
                },
                {
                    "old_start": 525,
                    "old_length": 7,
                    "new_start": 440,
                    "new_length": 7,
                    "hunk": "@@ -525,7 +440,7 @@ DimOrderMapOrError HandleBitcast(const HloInstruction* hlo,\n   return result;\n }\n \n-DimOrderMapOrError HandleCopyOrTransposeOrBroadcast(\n+DimOrderUpdatesOrError HandleCopyOrTransposeOrBroadcast(\n     const HloInstruction* hlo, const DimOrderMap& dim_orders,\n     const TransformDirection direction) {\n   const HloInstruction* src =\n"
                },
                {
                    "old_start": 534,
                    "old_length": 10,
                    "new_start": 449,
                    "new_length": 9,
                    "hunk": "@@ -534,10 +449,9 @@ DimOrderMapOrError HandleCopyOrTransposeOrBroadcast(\n       (direction == TransformDirection::kOutputToInput) ? hlo->operand(0) : hlo;\n   const Fragments& src_fragments_order =\n       dim_orders.at(src).TensorFragmentsOrder();\n-  DimOrderMap result;\n+  DimOrderUpdates result;\n   DimensionOrder& dst_dim_order =\n-      result.insert({dst, DimensionOrder::EmptyLike(dim_orders.at(src))})\n-          .first->second;\n+      result.map.insert({dst, DimensionOrder()}).first->second;\n   Fragments& dst_fragments_order = dst_dim_order.TensorFragmentsOrder();\n   // Every HLO dimension can correspond to a group of subdimensions in\n   // dim_order_. For the easier handling of permutations: group dim_order_ by\n"
                },
                {
                    "old_start": 615,
                    "old_length": 13,
                    "new_start": 529,
                    "new_length": 13,
                    "hunk": "@@ -615,13 +529,13 @@ DimOrderMapOrError HandleCopyOrTransposeOrBroadcast(\n \n // Infers DimensionOrders of all unknown sides (output, operands)\n // of `hlo` from the known ones.\n-DimOrderMapOrError HandleInstruction(const HloInstruction* hlo,\n-                                     const DimOrderMap& dim_orders,\n-                                     TransformDirection direction) {\n+DimOrderUpdatesOrError HandleInstruction(const HloInstruction* hlo,\n+                                         const DimOrderMap& dim_orders,\n+                                         TransformDirection direction) {\n   VLOG(7) << hlo->ToString();\n   if (hlo->opcode() == HloOpcode::kParameter ||\n       hlo_query::IsScalarConstant(hlo)) {\n-    return DimOrderMap{};\n+    return DimOrderUpdates{};\n   } else if (hlo->opcode() == HloOpcode::kTranspose ||\n              hlo->opcode() == HloOpcode::kCopy) {\n     return HandleCopyOrTransposeOrBroadcast(hlo, dim_orders, direction);\n"
                },
                {
                    "old_start": 645,
                    "old_length": 52,
                    "new_start": 559,
                    "new_length": 199,
                    "hunk": "@@ -645,52 +559,199 @@ DimOrderMapOrError HandleInstruction(const HloInstruction* hlo,\n   return \"Unimplemented instruction.\";\n }\n \n-// Tells if the dimension order is supported by the triton GEMM emitter.\n-// Only the dimension indicated by SplittableDimensionIndex() can be split\n-// physically once by other dimensions. Other ones can be only split logically.\n-// All subdimensions within a dimension have to be ordered.\n-FusionDecision RequireTritonGemmSupportedDimOrder(const DimensionOrder& order) {\n+class FusionContext {\n+  explicit FusionContext(\n+      const int64_t splittable_dimension_index,\n+      const int64_t splittable_dimension_supported_major_size)\n+      : splittable_dimension_index_(splittable_dimension_index),\n+        splittable_dimension_supported_major_part_size_(\n+            splittable_dimension_supported_major_size) {}\n+\n+ public:\n+  // Create fusion context from a dot operand according to\n+  // the currently supported configurations.\n+  static FusionContext FromDotOperand(const HloInstruction& dot,\n+                                      int operand_number, int split_k = 1);\n+\n+  // Create fusion context from dot's output.\n+  static FusionContext FromDotOutput(\n+      const HloInstruction& dot, int split_k,\n+      int64_t splittable_dimension_supported_major_part_size);\n+\n+  // Tells if the dimension order is supported by the triton GEMM emitter.\n+  // Only the dimension indicated by SplittableDimensionIndex() can be split\n+  // physically once by other dimensions. Other ones can be only split\n+  // logically. All subdimensions within a dimension have to be ordered.\n+  // Return major part of splittable dimension in split_dim_major_part if a\n+  // supported split is detected.\n+  FusionDecision RequireTritonGemmSupportedDimOrder(\n+      const DimensionOrder& order, int64_t& split_dim_major_part) const;\n+  // Apply RequireTritonGemmSupportedDimOrder() to all known dimension orders\n+  // around `hlo`.\n+  FusionDecision RequireTritonGemmSupportedDimOrders(\n+      const HloInstruction& hlo, DimOrderUpdates& updates) const;\n+  // Checks if the instruction is possible and profitable to fuse.\n+  // If so tries to transform dim_order describing one side of `hlo` into\n+  // description(s) of its other side if it is supported.\n+  DimOrderUpdatesOrError AnalyzeForFusion(\n+      const HloInstruction& hlo, bool as_input,\n+      absl::flat_hash_map<const HloInstruction*, HloInstruction*>&\n+          old_to_new_mapping,\n+      GpuVersion gpu_version) const;\n+  // Add dimension orders from `updates` to `dim_orders_` and update the\n+  // splittable dimension ratio if all of them are compatible.\n+  bool MergeUpdates(const DimOrderUpdates& updates);\n+  // Fuse an instruction with all its fusible inputs.\n+  // If an input is not fusible stop there and make a parameter of the new\n+  // fusion, otherwise put it onto stack and check its own inputs first.\n+  void TryToFuseWithInputsRecursively(\n+      HloInstruction& root, GpuVersion gpu_version,\n+      absl::flat_hash_map<const HloInstruction*, HloInstruction*>&\n+          old_to_new_mapping,\n+      std::vector<HloInstruction*>& fusion_inputs,\n+      HloComputation::Builder& builder);\n+  // Propagate dimension orders in consumer->producer direction starting at\n+  // `origin` with output `origin_dim_order` till parameters of the computation.\n+  // Store the found parameters and their iteration specs.\n+  Status PropagateDimensionOrdersToParameters(\n+      const HloInstruction& origin,\n+      absl::flat_hash_set<const HloInstruction*>& parameters,\n+      absl::flat_hash_map<const HloInstruction*, TensorIterationSpec>&\n+          iter_specs);\n+\n+  // Index of dot dimension that can be split.\n+  // Currently typically LHS non-contracting one.\n+  int64_t SplittableDimensionIndex() const {\n+    return splittable_dimension_index_;\n+  }\n+  // Tells whether `size` major part of a dimension can be physically split.\n+  bool IsSupportedSplittableDimensionMajorPartSize(const int64_t size) const {\n+    CHECK_NE(size, 0);\n+    // 0 means no specific size requirement.\n+    return splittable_dimension_supported_major_part_size_ == 0 ||\n+           splittable_dimension_supported_major_part_size_ == size;\n+  }\n+  int SplittableDimensionMajorPartSize() const {\n+    return splittable_dimension_supported_major_part_size_;\n+  }\n+  const DimOrderMap& DimOrders() const { return dim_orders_; }\n+\n+ private:\n+  bool SetSplittableDimensionMajorPartSize(const int64_t size) {\n+    if (IsSupportedSplittableDimensionMajorPartSize(size)) {\n+      splittable_dimension_supported_major_part_size_ = size;\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  const int splittable_dimension_index_;\n+  int64_t splittable_dimension_supported_major_part_size_;\n+  DimOrderMap dim_orders_;\n+};\n+\n+FusionContext FusionContext::FromDotOperand(const HloInstruction& dot,\n+                                            const int operand_number,\n+                                            const int split_k) {\n+  // There can be either none or one split-K batch dimension.\n+  const int num_split_k_batch_dims = split_k > 1;\n+  int split_k_dimension_index = -1;\n+  if (split_k > 1) {\n+    split_k_dimension_index =\n+        ContractingDimensionIndex(dot, operand_number) - 1;\n+  }\n+  int splittable_dimension_index = -1;\n+  // LHS non-contracting dimension can be split if non-splitK batch is absent.\n+  if (operand_number == 0 &&\n+      dot.dot_dimension_numbers().lhs_batch_dimensions_size() -\n+              num_split_k_batch_dims ==\n+          0) {\n+    splittable_dimension_index =\n+        NonContractingDimensionIndex(dot, operand_number);\n+  }\n+  FusionContext context(splittable_dimension_index,\n+                        /*splittable_dimension_supported_major_size=*/0);\n+  context.dim_orders_[dot.operand(operand_number)] =\n+      DimensionOrder(dot.operand(operand_number), split_k_dimension_index);\n+  return context;\n+}\n+\n+FusionContext FusionContext::FromDotOutput(\n+    const HloInstruction& dot, const int split_k,\n+    const int64_t splittable_dimension_supported_major_part_size) {\n+  // Allow non-contracting dimension originating from LHS to split if\n+  // this dimension is split at the output at the same ratio as\n+  // at the input.\n+  int64_t splittable_dimension_index = -1;\n+  if (splittable_dimension_supported_major_part_size > 1) {\n+    // Split-K dimension is the first one in the output if present;\n+    // LHS non-contracting follows (batch is absent in this case).\n+    splittable_dimension_index = (split_k > 1) ? 1 : 0;\n+  }\n+  FusionContext context(splittable_dimension_index,\n+                        splittable_dimension_supported_major_part_size);\n+  context.dim_orders_[&dot] = DimensionOrder(&dot);\n+  return context;\n+}\n+\n+FusionDecision FusionContext::RequireTritonGemmSupportedDimOrder(\n+    const DimensionOrder& order, int64_t& split_dim_major_part) const {\n   VLOG(8) << order.ToString();\n   const Fragments& tensor_dim_fragments = order.TensorFragmentsOrder();\n   for (const auto& [dim_index, dim_fragments] : order.DimFragmentsOrders()) {\n-    int last_fragment_number = -1;\n     int split_counter = -1;\n-    for (const int fragment_number : dim_fragments) {\n-      CHECK_EQ(tensor_dim_fragments[fragment_number].dst_dim_number, dim_index);\n-      const int size = tensor_dim_fragments[fragment_number].size;\n-      if (fragment_number <= last_fragment_number) {\n-        return \"Transpose within a dimension.\";\n+    auto fragment = dim_fragments.cbegin();\n+    while (true) {\n+      if (fragment == dim_fragments.cend()) {\n+        break;\n       }\n-      if (size == 1) {\n-        last_fragment_number = fragment_number;\n+      int64_t grouped_size = tensor_dim_fragments[*fragment].size;\n+      // Gather contiguous fragments.\n+      while ((fragment + 1) != dim_fragments.cend() &&\n+             *(fragment + 1) == *fragment + 1) {\n+        ++fragment;\n+        grouped_size *= tensor_dim_fragments[*fragment].size;\n+      }\n+\n+      if (grouped_size == 1) {\n+        ++fragment;\n         continue;\n       }\n-      if (fragment_number == 0 ||\n-          tensor_dim_fragments[fragment_number - 1].dst_dim_number !=\n-              dim_index) {\n-        ++split_counter;\n-        if (dim_index == order.SplittableDimensionIndex() &&\n-            order.IsSupportedSplittableDimensionMajorPartSize(size)) {\n-          if (split_counter > 1) {\n+\n+      if (fragment != dim_fragments.cbegin() && *fragment < *(fragment - 1)) {\n+        return \"Transpose within a dimension.\";\n+      }\n+\n+      ++split_counter;\n+      if (split_counter > 0) {\n+        if (dim_index == SplittableDimensionIndex() &&\n+            IsSupportedSplittableDimensionMajorPartSize(grouped_size)) {\n+          if (split_counter == 1) {\n+            if (split_dim_major_part != 0 &&\n+                split_dim_major_part != grouped_size) {\n+              return \"Conflicting splits of splittable dimension\";\n+            }\n+            split_dim_major_part = grouped_size;\n+          } else if (split_counter > 1) {\n             return \"2nd split of a splittable dimension.\";\n           }\n-        } else if (split_counter > 0) {\n-          return \"Split of a non-splittable dimension.\";\n+        } else {\n+          return \"Unsupported split of a dimension.\";\n         }\n       }\n-      last_fragment_number = fragment_number;\n+\n+      ++fragment;\n     }\n   }\n   return FusionDecision{};\n }\n \n-// Apply RequireTritonGemmSupportedDimOrder() to all known dimension orders\n-// around `hlo`.\n-FusionDecision RequireTritonGemmSupportedDimOrders(\n-    const HloInstruction& hlo, const DimOrderMap& dim_orders) {\n+FusionDecision FusionContext::RequireTritonGemmSupportedDimOrders(\n+    const HloInstruction& hlo, DimOrderUpdates& updates) const {\n   auto check_if_present = [&](const HloInstruction* instr) {\n-    if (auto it = dim_orders.find(instr); it != dim_orders.end()) {\n-      return RequireTritonGemmSupportedDimOrder(it->second);\n+    if (auto it = updates.map.find(instr); it != updates.map.end()) {\n+      return RequireTritonGemmSupportedDimOrder(\n+          it->second, updates.splittable_dimension_major_part_size);\n     }\n     return FusionDecision{};\n   };\n"
                },
                {
                    "old_start": 741,
                    "old_length": 14,
                    "new_start": 802,
                    "new_length": 11,
                    "hunk": "@@ -741,14 +802,11 @@ bool IsOutputWorthFusing(const HloInstruction& hlo) {\n          InputMinusOutputBytes(hlo) >= -kIoToleranceBytes;\n }\n \n-// Checks if the instruction is possible and profitable to fuse.\n-// If so tries to transform dim_order describing one side of `hlo` into\n-// description(s) of its other side if it is supported.\n-DimOrderMapOrError AnalyzeForFusion(\n-    const HloInstruction& hlo, bool as_input, DimOrderMap& dim_orders,\n+DimOrderUpdatesOrError FusionContext::AnalyzeForFusion(\n+    const HloInstruction& hlo, bool as_input,\n     absl::flat_hash_map<const HloInstruction*, HloInstruction*>&\n         old_to_new_mapping,\n-    const GpuVersion gpu_version) {\n+    const GpuVersion gpu_version) const {\n   int fusion_level =\n       hlo.GetModule()->config().debug_options().xla_gpu_triton_fusion_level();\n   if (!std::get<se::CudaComputeCapability>(gpu_version)\n"
                },
                {
                    "old_start": 812,
                    "old_length": 19,
                    "new_start": 870,
                    "new_length": 19,
                    "hunk": "@@ -812,19 +870,19 @@ DimOrderMapOrError AnalyzeForFusion(\n   }\n \n   auto result =\n-      HandleInstruction(&hlo, dim_orders,\n+      HandleInstruction(&hlo, dim_orders_,\n                         as_input ? TransformDirection::kOutputToInput\n                                  : TransformDirection::kInputToOutput);\n-  if (!std::holds_alternative<DimOrderMap>(result)) {\n+  if (!std::holds_alternative<DimOrderUpdates>(result)) {\n     return std::get<FusionDecision>(result);\n   }\n \n   if (FusionDecision supported = RequireTritonGemmSupportedDimOrders(\n-          hlo, std::get<DimOrderMap>(result));\n+          hlo, std::get<DimOrderUpdates>(result));\n       !supported) {\n     return supported;\n   }\n-  return std::get<DimOrderMap>(result);\n+  return std::get<DimOrderUpdates>(result);\n }\n \n // Clone an instruction into the fusion.\n"
                },
                {
                    "old_start": 875,
                    "old_length": 26,
                    "new_start": 933,
                    "new_length": 26,
                    "hunk": "@@ -875,26 +933,26 @@ int64_t NumAddedParameters(const HloInstruction& hlo) {\n   return hlo.operand_count() - 1;\n }\n \n-bool MergeDimOrderMapUpdates(DimOrderMap& target, const DimOrderMap& updates) {\n+bool FusionContext::MergeUpdates(const DimOrderUpdates& updates) {\n   // First check that all updates to insert are compatible to avoid\n   // incomplete merges.\n-  for (const auto& [key, value] : updates) {\n-    auto it = target.find(key);\n-    if (it != target.cend() && !it->second.IsPhysicallyEquivalent(value)) {\n+  for (const auto& [key, value] : updates.map) {\n+    auto it = dim_orders_.find(key);\n+    if (it != dim_orders_.cend() && !it->second.IsPhysicallyEquivalent(value)) {\n       return false;\n     }\n   }\n-  target.insert(updates.begin(), updates.end());\n+  if (updates.splittable_dimension_major_part_size > 1 &&\n+      !SetSplittableDimensionMajorPartSize(\n+          updates.splittable_dimension_major_part_size)) {\n+    return false;\n+  }\n+  dim_orders_.insert(updates.map.begin(), updates.map.end());\n   return true;\n }\n \n-// Fuse an instruction with all its fusible inputs.\n-// If an input is not fusible stop there and make a parameter of the new\n-// fusion, otherwise put it onto stack and check its own inputs first.\n-void TryToFuseWithInputsRecursively(\n-    HloInstruction& root,\n-    // Dimension orders describing outputs of corresponding instructions.\n-    DimOrderMap& dim_orders, const GpuVersion gpu_version,\n+void FusionContext::TryToFuseWithInputsRecursively(\n+    HloInstruction& root, const GpuVersion gpu_version,\n     absl::flat_hash_map<const HloInstruction*, HloInstruction*>&\n         old_to_new_mapping,\n     std::vector<HloInstruction*>& fusion_inputs,\n"
                },
                {
                    "old_start": 910,
                    "old_length": 21,
                    "new_start": 968,
                    "new_length": 21,
                    "hunk": "@@ -910,21 +968,21 @@ void TryToFuseWithInputsRecursively(\n   // of them to be physically compatible.\n   const HloInstruction* reference_dim_order_hlo = nullptr;\n   auto try_fuse_one = [&](HloInstruction& hlo) {\n-    const DimOrderMapOrError result = AnalyzeForFusion(\n-        hlo, /*as_input=*/true, dim_orders, old_to_new_mapping, gpu_version);\n-    if (!std::holds_alternative<DimOrderMap>(result)) {\n+    const DimOrderUpdatesOrError result = AnalyzeForFusion(\n+        hlo, /*as_input=*/true, old_to_new_mapping, gpu_version);\n+    if (!std::holds_alternative<DimOrderUpdates>(result)) {\n       return false;\n     }\n     for (const HloInstruction* operand : hlo.operands()) {\n       const DimensionOrder& dim_order =\n-          std::get<DimOrderMap>(result).at(operand);\n+          std::get<DimOrderUpdates>(result).map.at(operand);\n       if (reference_dim_order_hlo != nullptr &&\n           !dim_order.IsPhysicallyEquivalent(\n-              dim_orders.at(reference_dim_order_hlo))) {\n+              dim_orders_.at(reference_dim_order_hlo))) {\n         return false;\n       }\n     }\n-    if (!MergeDimOrderMapUpdates(dim_orders, std::get<DimOrderMap>(result))) {\n+    if (!MergeUpdates(std::get<DimOrderUpdates>(result))) {\n       return false;\n     }\n     to_fuse.push(&hlo);\n"
                },
                {
                    "old_start": 986,
                    "old_length": 45,
                    "new_start": 1044,
                    "new_length": 19,
                    "hunk": "@@ -986,45 +1044,19 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n   // differently shaped tiles but may go through same HLO graph nodes.\n   // Direct dot inputs have well defined dimension orders.\n \n-  auto fuse_inputs = [&](int operand_number) -> StatusOr<DimOrderMap> {\n+  auto fuse_inputs = [&](int operand_number) -> StatusOr<FusionContext> {\n     const int operand_count_before = fusion_inputs.size();\n-    DimOrderMap dim_orders;\n     // Direct dot inputs have well defined dimension orders.\n-    dim_orders.insert({dot.operand(operand_number),\n-                       DimensionOrder::FromDotOperand(dot, operand_number)});\n-    TryToFuseWithInputsRecursively(*dot.mutable_operand(operand_number),\n-                                   dim_orders, gpu_version, old_to_new_mapping,\n-                                   fusion_inputs, builder);\n+    auto context = FusionContext::FromDotOperand(dot, operand_number);\n+    context.TryToFuseWithInputsRecursively(*dot.mutable_operand(operand_number),\n+                                           gpu_version, old_to_new_mapping,\n+                                           fusion_inputs, builder);\n     TF_RET_CHECK(fusion_inputs.size() - operand_count_before <=\n                  DotFusionAnalysis::kMaxParameterPerScope);\n-    return dim_orders;\n+    return context;\n   };\n-  // Check if non-contracting dimension originating from LHS operand in the\n-  // output can be split. This currently requires this dimension being split\n-  // in the operand the same way.\n-  int64_t lhs_nc_split_major_part = -1;\n-  {\n-    TF_ASSIGN_OR_RETURN(const auto lhs_dim_orders, fuse_inputs(0));\n-    // Looking at first LHS parameter to find split non-contracting dimension\n-    // is sufficient because currently all parameters of one scope have to use\n-    // the same tiling.\n-    auto first_lhs_parameter_it = lhs_dim_orders.cbegin();\n-    while (first_lhs_parameter_it != lhs_dim_orders.cend()) {\n-      if (auto it = old_to_new_mapping.find(first_lhs_parameter_it->first);\n-          it != old_to_new_mapping.cend() &&\n-          it->second->opcode() == HloOpcode::kParameter) {\n-        break;\n-      }\n-      ++first_lhs_parameter_it;\n-    }\n-    if (first_lhs_parameter_it != lhs_dim_orders.cend()) {\n-      const auto lhs_nc_iter_spec = DimensionOrderToTensorIterationSpec(\n-          first_lhs_parameter_it->second)[NonContractingDimensionIndex(dot, 0)];\n-      if (lhs_nc_iter_spec.size() > 1) {\n-        lhs_nc_split_major_part = lhs_nc_iter_spec.at(1).count;\n-      }\n-    }\n-  }\n+\n+  TF_ASSIGN_OR_RETURN(const FusionContext lhs_context, fuse_inputs(0));\n   if (auto result = fuse_inputs(1); !result.ok()) {\n     return result.status();\n   }\n"
                },
                {
                    "old_start": 1034,
                    "old_length": 10,
                    "new_start": 1066,
                    "new_length": 8,
                    "hunk": "@@ -1034,10 +1066,8 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n   // Fusion at dot's output.\n \n   // These describe _outputs_ of corresponding HLOs.\n-  DimOrderMap out_dim_orders;\n-  out_dim_orders.insert(\n-      {&dot, DimensionOrder::FromDotOutput(dot, /*split_k=*/1,\n-                                           lhs_nc_split_major_part)});\n+  auto context = FusionContext::FromDotOutput(\n+      dot, /*split_k=*/1, lhs_context.SplittableDimensionMajorPartSize());\n   HloInstruction* fusion_output = &dot;\n   bool output_changed = true;\n   while (output_changed) {\n"
                },
                {
                    "old_start": 1049,
                    "old_length": 18,
                    "new_start": 1079,
                    "new_length": 16,
                    "hunk": "@@ -1049,18 +1079,16 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n     if (!IsDistributiveOverAddition(*user)) {\n       break;\n     }\n-    auto result = AnalyzeForFusion(*user, /*as_input=*/false, out_dim_orders,\n-                                   old_to_new_mapping, gpu_version);\n-    if (!std::holds_alternative<DimOrderMap>(result)) {\n+    auto result = context.AnalyzeForFusion(*user, /*as_input=*/false,\n+                                           old_to_new_mapping, gpu_version);\n+    if (!std::holds_alternative<DimOrderUpdates>(result)) {\n       continue;\n     }\n-    TF_RET_CHECK(\n-        MergeDimOrderMapUpdates(out_dim_orders, std::get<DimOrderMap>(result)));\n+    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n     for (HloInstruction* operand : user->operands()) {\n       if (!old_to_new_mapping.contains(operand)) {\n-        TryToFuseWithInputsRecursively(*operand, out_dim_orders, gpu_version,\n-                                       old_to_new_mapping, fusion_inputs,\n-                                       builder);\n+        context.TryToFuseWithInputsRecursively(\n+            *operand, gpu_version, old_to_new_mapping, fusion_inputs, builder);\n       }\n     }\n     Fuse(*user, old_to_new_mapping, fusion_inputs, builder);\n"
                },
                {
                    "old_start": 1368,
                    "old_length": 20,
                    "new_start": 1396,
                    "new_length": 14,
                    "hunk": "@@ -1368,20 +1396,14 @@ Status MakeDotComputationSplitKBatch(\n   return OkStatus();\n }\n \n-// Propagate dimension orders in consumer->producer direction starting at\n-// `origin` with output `origin_dim_order` till parameters of the computation.\n-// Store the found parameters and their iteration specs.\n-Status PropagateDimensionOrdersToParameters(\n-    const HloInstruction& origin, DimensionOrder origin_dim_order,\n+Status FusionContext::PropagateDimensionOrdersToParameters(\n+    const HloInstruction& origin,\n     absl::flat_hash_set<const HloInstruction*>& parameters,\n     absl::flat_hash_map<const HloInstruction*, TensorIterationSpec>&\n         iter_specs) {\n   absl::flat_hash_set<const HloInstruction*> visited;\n   std::queue<const HloInstruction*> to_process;\n   // Dimension orders describing outputs of corresponding instructions.\n-  DimOrderMap dim_orders;\n-  TF_RET_CHECK(RequireTritonGemmSupportedDimOrder(origin_dim_order));\n-  dim_orders.insert({&origin, origin_dim_order});\n   visited.insert(&origin);\n   to_process.push(&origin);\n   while (!to_process.empty()) {\n"
                },
                {
                    "old_start": 1397,
                    "old_length": 12,
                    "new_start": 1419,
                    "new_length": 11,
                    "hunk": "@@ -1397,12 +1419,11 @@ Status PropagateDimensionOrdersToParameters(\n       VLOG(5) << hlo->ToString();\n     }\n     auto result =\n-        HandleInstruction(hlo, dim_orders, TransformDirection::kOutputToInput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderMap>(result));\n-    TF_RET_CHECK(\n-        MergeDimOrderMapUpdates(dim_orders, std::get<DimOrderMap>(result)));\n-    TF_RET_CHECK(\n-        RequireTritonGemmSupportedDimOrders(*hlo, dim_orders).CanFuse());\n+        HandleInstruction(hlo, dim_orders_, TransformDirection::kOutputToInput);\n+    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n+    TF_RET_CHECK(RequireTritonGemmSupportedDimOrders(\n+        *hlo, std::get<DimOrderUpdates>(result)));\n+    TF_RET_CHECK(MergeUpdates(std::get<DimOrderUpdates>(result)));\n     for (const HloInstruction* operand : hlo->operands()) {\n       if (!visited.insert(operand).second) {\n         continue;\n"
                },
                {
                    "old_start": 1417,
                    "old_length": 10,
                    "new_start": 1438,
                    "new_length": 10,
                    "hunk": "@@ -1417,10 +1438,10 @@ Status PropagateDimensionOrdersToParameters(\n   }\n   // For now all parameters of one scope have to use the same tiling.\n   for (const HloInstruction* parameter : parameters) {\n-    TF_RET_CHECK(dim_orders.at(parameter).IsPhysicallyEquivalent(\n-        dim_orders.at(*parameters.cbegin())));\n+    TF_RET_CHECK(dim_orders_.at(parameter).IsPhysicallyEquivalent(\n+        dim_orders_.at(*parameters.cbegin())));\n     iter_specs[parameter] =\n-        DimensionOrderToTensorIterationSpec(dim_orders.at(parameter));\n+        DimensionOrderToTensorIterationSpec(dim_orders_.at(parameter));\n   }\n   return OkStatus();\n }\n"
                },
                {
                    "old_start": 1575,
                    "old_length": 49,
                    "new_start": 1596,
                    "new_length": 40,
                    "hunk": "@@ -1575,49 +1596,40 @@ Status DotFusionAnalysis::ExecuteImpl(const HloComputation* computation,\n   const HloInstruction* dot =\n       hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n \n+  int64_t lhs_nc_split_major_part_size = -1;\n   for (const Scope scope : {Scope::LHS, Scope::RHS}) {\n     const int operand_number = static_cast<int>(scope);\n-    TF_RETURN_IF_ERROR(PropagateDimensionOrdersToParameters(\n-        *dot->operand(operand_number),\n-        DimensionOrder::FromDotOperand(*dot, operand_number, split_k),\n-        parameters_[scope], iter_specs_[scope]));\n-  }\n-\n-  int64_t lhs_nc_split_major_part_size = -1;\n-  if (!ScopeParameters(Scope::LHS).empty()) {\n-    const DimIterationSpec* lhs_nc_iter_spec =\n-        IterSpec(Scope::LHS, *ScopeParameters(Scope::LHS).cbegin(),\n-                 NonContractingDimensionIndex(*dot, 0));\n-    if (lhs_nc_iter_spec != nullptr && lhs_nc_iter_spec->size() > 1) {\n-      lhs_nc_split_major_part_size = lhs_nc_iter_spec->at(1).count;\n+    auto context = FusionContext::FromDotOperand(*dot, operand_number, split_k);\n+    TF_RETURN_IF_ERROR(context.PropagateDimensionOrdersToParameters(\n+        *dot->operand(operand_number), parameters_[scope], iter_specs_[scope]));\n+    if (scope == Scope::LHS && context.SplittableDimensionMajorPartSize() > 1) {\n+      lhs_nc_split_major_part_size = context.SplittableDimensionMajorPartSize();\n     }\n   }\n-  DimOrderMap dim_orders;\n-  dim_orders.insert({dot, DimensionOrder::FromDotOutput(\n-                              *dot, split_k, lhs_nc_split_major_part_size)});\n+\n+  auto context =\n+      FusionContext::FromDotOutput(*dot, split_k, lhs_nc_split_major_part_size);\n   const HloInstruction* output = dot;\n   // Currently supported is one fusion output and one path from dot to it.\n   // Propagate dimension order from dot to root.\n   while (!output->IsRoot()) {\n     TF_RET_CHECK(output->user_count() == 1);\n     output = output->users()[0];\n-    auto result = HandleInstruction(output, dim_orders,\n+    auto result = HandleInstruction(output, context.DimOrders(),\n                                     TransformDirection::kInputToOutput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderMap>(result));\n-    TF_RET_CHECK(RequireTritonGemmSupportedDimOrder(\n-        std::get<DimOrderMap>(result).at(output)));\n-    TF_RET_CHECK(\n-        MergeDimOrderMapUpdates(dim_orders, std::get<DimOrderMap>(result)));\n+    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n+    TF_RET_CHECK(context.RequireTritonGemmSupportedDimOrders(\n+        *output, std::get<DimOrderUpdates>(result)));\n+    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n   }\n   TF_RET_CHECK(iter_specs_[Scope::OUTPUT]\n                    .insert({output, DimensionOrderToTensorIterationSpec(\n-                                        dim_orders.at(output))})\n+                                        context.DimOrders().at(output))})\n                    .second);\n   if (output != dot) {\n     // Propagate back to parameters of the output fusion.\n-    TF_RETURN_IF_ERROR(PropagateDimensionOrdersToParameters(\n-        *output, dim_orders.at(output), parameters_[Scope::OUTPUT],\n-        iter_specs_[Scope::OUTPUT]));\n+    TF_RETURN_IF_ERROR(context.PropagateDimensionOrdersToParameters(\n+        *output, parameters_[Scope::OUTPUT], iter_specs_[Scope::OUTPUT]));\n   }\n   return OkStatus();\n }"
                }
            ],
            "whole_deleted": "-  // `hlo` is currently supposed to be either an operand or the output of dot();\n-  // properties describing the dimensions are stored for later analysis.\n-  explicit DimensionOrder(\n-      const HloInstruction* hlo, const int64_t splittable_dimension_index = -1,\n-      const int64_t splittable_dimension_supported_major_size = 0,\n-      const int split_k_dimension_index = -1)\n-      : splittable_dimension_index_(splittable_dimension_index),\n-        splittable_dimension_supported_major_part_size_(\n-            splittable_dimension_supported_major_size) {\n-  explicit DimensionOrder(\n-      const int64_t splittable_dimension_index,\n-      const int64_t splittable_dimension_supported_major_size)\n-      : splittable_dimension_index_(splittable_dimension_index),\n-        splittable_dimension_supported_major_part_size_(\n-            splittable_dimension_supported_major_size) {}\n-\n- public:\n-  DimensionOrder(const DimensionOrder&) = default;\n-\n-  // Copies fusion context attributes from `other` leaving internal structures\n-  // describing dimension fragments empty. Used to create derived dimension\n-  // orders.\n-  static DimensionOrder EmptyLike(const DimensionOrder& other) {\n-    return DimensionOrder(\n-        other.splittable_dimension_index_,\n-        other.splittable_dimension_supported_major_part_size_);\n-  }\n-\n-  // Create dimension order describing a dot operand according to\n-  // the currently supported configurations.\n-  static DimensionOrder FromDotOperand(const HloInstruction& dot,\n-                                       int operand_number, int split_k = 1);\n-\n-  // Create dimension order describing dot's output.\n-  static DimensionOrder FromDotOutput(\n-      const HloInstruction& dot, int split_k = 1,\n-      int64_t splittable_dimension_supported_major_part_size = 0);\n-\n-  // Index of dot dimension that can be split.\n-  // Currently typically LHS non-contracting one.\n-  int64_t SplittableDimensionIndex() const {\n-    return splittable_dimension_index_;\n-  }\n-\n-  // Tells whether `size` major part of a dimension can be physically split.\n-  bool IsSupportedSplittableDimensionMajorPartSize(int64_t size) const {\n-    // 0 means no specific size requirement.\n-    return splittable_dimension_supported_major_part_size_ == 0 ||\n-           splittable_dimension_supported_major_part_size_ == size;\n-  }\n-\n-\n-  const int64_t splittable_dimension_index_ = -1;\n-  const int64_t splittable_dimension_supported_major_part_size_ = 0;\n-DimensionOrder DimensionOrder::FromDotOperand(const HloInstruction& dot,\n-                                              const int operand_number,\n-                                              const int split_k) {\n-  const HloInstruction* operand = dot.operand(operand_number);\n-  // There can be either none or one split-K batch dimension.\n-  const int num_split_k_batch_dims = split_k > 1;\n-  int split_k_dimension_index = -1;\n-  if (split_k > 1) {\n-    split_k_dimension_index =\n-        ContractingDimensionIndex(dot, operand_number) - 1;\n-  }\n-  int splittable_dimension_index = -1;\n-  // LHS non-contracting dimension can be split if non-splitK batch is absent.\n-  if (operand_number == 0 &&\n-      dot.dot_dimension_numbers().lhs_batch_dimensions_size() -\n-              num_split_k_batch_dims ==\n-          0) {\n-    splittable_dimension_index =\n-        NonContractingDimensionIndex(dot, operand_number);\n-  }\n-  return DimensionOrder(operand, splittable_dimension_index,\n-                        /*splittable_dimension_supported_major_size=*/0,\n-                        split_k_dimension_index);\n-}\n-\n-DimensionOrder DimensionOrder::FromDotOutput(\n-    const HloInstruction& dot, const int split_k,\n-    const int64_t splittable_dimension_supported_major_part_size) {\n-  // Allow non-contracting dimension originating from LHS to split if\n-  // this dimension is split at the output at the same ratio as\n-  // at the input.\n-  int64_t splittable_dimension_index = -1;\n-  if (splittable_dimension_supported_major_part_size > 1) {\n-    // Split-K dimension is the first one in the output if present;\n-    // LHS non-contracting follows (batch is absent in this case).\n-    splittable_dimension_index = (split_k > 1) ? 1 : 0;\n-  }\n-  return DimensionOrder(&dot, splittable_dimension_index,\n-                        splittable_dimension_supported_major_part_size);\n-}\n-\n-using DimOrderMapOrError = std::variant<FusionDecision, DimOrderMap>;\n-DimOrderMapOrError HandleElementwise(const HloInstruction* hlo,\n-                                     const DimOrderMap& dim_orders) {\n-  DimOrderMap result;\n-  result.insert({hlo, DimensionOrder(*src_dim_order)});\n-    result.insert({operand, DimensionOrder(dim_orders.at(src))});\n-DimOrderMapOrError HandleBitcast(const HloInstruction* hlo,\n-                                 const DimOrderMap& dim_orders,\n-                                 const TransformDirection direction) {\n-  DimOrderMap result;\n-      result.insert({dst, DimensionOrder::EmptyLike(dim_orders.at(src))})\n-          .first->second;\n-DimOrderMapOrError HandleCopyOrTransposeOrBroadcast(\n-  DimOrderMap result;\n-      result.insert({dst, DimensionOrder::EmptyLike(dim_orders.at(src))})\n-          .first->second;\n-DimOrderMapOrError HandleInstruction(const HloInstruction* hlo,\n-                                     const DimOrderMap& dim_orders,\n-                                     TransformDirection direction) {\n-    return DimOrderMap{};\n-// Tells if the dimension order is supported by the triton GEMM emitter.\n-// Only the dimension indicated by SplittableDimensionIndex() can be split\n-// physically once by other dimensions. Other ones can be only split logically.\n-// All subdimensions within a dimension have to be ordered.\n-FusionDecision RequireTritonGemmSupportedDimOrder(const DimensionOrder& order) {\n-    int last_fragment_number = -1;\n-    for (const int fragment_number : dim_fragments) {\n-      CHECK_EQ(tensor_dim_fragments[fragment_number].dst_dim_number, dim_index);\n-      const int size = tensor_dim_fragments[fragment_number].size;\n-      if (fragment_number <= last_fragment_number) {\n-        return \"Transpose within a dimension.\";\n-      if (size == 1) {\n-        last_fragment_number = fragment_number;\n-      if (fragment_number == 0 ||\n-          tensor_dim_fragments[fragment_number - 1].dst_dim_number !=\n-              dim_index) {\n-        ++split_counter;\n-        if (dim_index == order.SplittableDimensionIndex() &&\n-            order.IsSupportedSplittableDimensionMajorPartSize(size)) {\n-          if (split_counter > 1) {\n-        } else if (split_counter > 0) {\n-          return \"Split of a non-splittable dimension.\";\n-      last_fragment_number = fragment_number;\n-// Apply RequireTritonGemmSupportedDimOrder() to all known dimension orders\n-// around `hlo`.\n-FusionDecision RequireTritonGemmSupportedDimOrders(\n-    const HloInstruction& hlo, const DimOrderMap& dim_orders) {\n-    if (auto it = dim_orders.find(instr); it != dim_orders.end()) {\n-      return RequireTritonGemmSupportedDimOrder(it->second);\n-// Checks if the instruction is possible and profitable to fuse.\n-// If so tries to transform dim_order describing one side of `hlo` into\n-// description(s) of its other side if it is supported.\n-DimOrderMapOrError AnalyzeForFusion(\n-    const HloInstruction& hlo, bool as_input, DimOrderMap& dim_orders,\n-    const GpuVersion gpu_version) {\n-      HandleInstruction(&hlo, dim_orders,\n-  if (!std::holds_alternative<DimOrderMap>(result)) {\n-          hlo, std::get<DimOrderMap>(result));\n-  return std::get<DimOrderMap>(result);\n-bool MergeDimOrderMapUpdates(DimOrderMap& target, const DimOrderMap& updates) {\n-  for (const auto& [key, value] : updates) {\n-    auto it = target.find(key);\n-    if (it != target.cend() && !it->second.IsPhysicallyEquivalent(value)) {\n-  target.insert(updates.begin(), updates.end());\n-// Fuse an instruction with all its fusible inputs.\n-// If an input is not fusible stop there and make a parameter of the new\n-// fusion, otherwise put it onto stack and check its own inputs first.\n-void TryToFuseWithInputsRecursively(\n-    HloInstruction& root,\n-    // Dimension orders describing outputs of corresponding instructions.\n-    DimOrderMap& dim_orders, const GpuVersion gpu_version,\n-    const DimOrderMapOrError result = AnalyzeForFusion(\n-        hlo, /*as_input=*/true, dim_orders, old_to_new_mapping, gpu_version);\n-    if (!std::holds_alternative<DimOrderMap>(result)) {\n-          std::get<DimOrderMap>(result).at(operand);\n-              dim_orders.at(reference_dim_order_hlo))) {\n-    if (!MergeDimOrderMapUpdates(dim_orders, std::get<DimOrderMap>(result))) {\n-  auto fuse_inputs = [&](int operand_number) -> StatusOr<DimOrderMap> {\n-    DimOrderMap dim_orders;\n-    dim_orders.insert({dot.operand(operand_number),\n-                       DimensionOrder::FromDotOperand(dot, operand_number)});\n-    TryToFuseWithInputsRecursively(*dot.mutable_operand(operand_number),\n-                                   dim_orders, gpu_version, old_to_new_mapping,\n-                                   fusion_inputs, builder);\n-    return dim_orders;\n-  // Check if non-contracting dimension originating from LHS operand in the\n-  // output can be split. This currently requires this dimension being split\n-  // in the operand the same way.\n-  int64_t lhs_nc_split_major_part = -1;\n-  {\n-    TF_ASSIGN_OR_RETURN(const auto lhs_dim_orders, fuse_inputs(0));\n-    // Looking at first LHS parameter to find split non-contracting dimension\n-    // is sufficient because currently all parameters of one scope have to use\n-    // the same tiling.\n-    auto first_lhs_parameter_it = lhs_dim_orders.cbegin();\n-    while (first_lhs_parameter_it != lhs_dim_orders.cend()) {\n-      if (auto it = old_to_new_mapping.find(first_lhs_parameter_it->first);\n-          it != old_to_new_mapping.cend() &&\n-          it->second->opcode() == HloOpcode::kParameter) {\n-        break;\n-      }\n-      ++first_lhs_parameter_it;\n-    }\n-    if (first_lhs_parameter_it != lhs_dim_orders.cend()) {\n-      const auto lhs_nc_iter_spec = DimensionOrderToTensorIterationSpec(\n-          first_lhs_parameter_it->second)[NonContractingDimensionIndex(dot, 0)];\n-      if (lhs_nc_iter_spec.size() > 1) {\n-        lhs_nc_split_major_part = lhs_nc_iter_spec.at(1).count;\n-      }\n-    }\n-  }\n-  DimOrderMap out_dim_orders;\n-  out_dim_orders.insert(\n-      {&dot, DimensionOrder::FromDotOutput(dot, /*split_k=*/1,\n-                                           lhs_nc_split_major_part)});\n-    auto result = AnalyzeForFusion(*user, /*as_input=*/false, out_dim_orders,\n-                                   old_to_new_mapping, gpu_version);\n-    if (!std::holds_alternative<DimOrderMap>(result)) {\n-    TF_RET_CHECK(\n-        MergeDimOrderMapUpdates(out_dim_orders, std::get<DimOrderMap>(result)));\n-        TryToFuseWithInputsRecursively(*operand, out_dim_orders, gpu_version,\n-                                       old_to_new_mapping, fusion_inputs,\n-                                       builder);\n-// Propagate dimension orders in consumer->producer direction starting at\n-// `origin` with output `origin_dim_order` till parameters of the computation.\n-// Store the found parameters and their iteration specs.\n-Status PropagateDimensionOrdersToParameters(\n-    const HloInstruction& origin, DimensionOrder origin_dim_order,\n-  DimOrderMap dim_orders;\n-  TF_RET_CHECK(RequireTritonGemmSupportedDimOrder(origin_dim_order));\n-  dim_orders.insert({&origin, origin_dim_order});\n-        HandleInstruction(hlo, dim_orders, TransformDirection::kOutputToInput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderMap>(result));\n-    TF_RET_CHECK(\n-        MergeDimOrderMapUpdates(dim_orders, std::get<DimOrderMap>(result)));\n-    TF_RET_CHECK(\n-        RequireTritonGemmSupportedDimOrders(*hlo, dim_orders).CanFuse());\n-    TF_RET_CHECK(dim_orders.at(parameter).IsPhysicallyEquivalent(\n-        dim_orders.at(*parameters.cbegin())));\n-        DimensionOrderToTensorIterationSpec(dim_orders.at(parameter));\n-    TF_RETURN_IF_ERROR(PropagateDimensionOrdersToParameters(\n-        *dot->operand(operand_number),\n-        DimensionOrder::FromDotOperand(*dot, operand_number, split_k),\n-        parameters_[scope], iter_specs_[scope]));\n-  }\n-\n-  int64_t lhs_nc_split_major_part_size = -1;\n-  if (!ScopeParameters(Scope::LHS).empty()) {\n-    const DimIterationSpec* lhs_nc_iter_spec =\n-        IterSpec(Scope::LHS, *ScopeParameters(Scope::LHS).cbegin(),\n-                 NonContractingDimensionIndex(*dot, 0));\n-    if (lhs_nc_iter_spec != nullptr && lhs_nc_iter_spec->size() > 1) {\n-      lhs_nc_split_major_part_size = lhs_nc_iter_spec->at(1).count;\n-  DimOrderMap dim_orders;\n-  dim_orders.insert({dot, DimensionOrder::FromDotOutput(\n-                              *dot, split_k, lhs_nc_split_major_part_size)});\n-    auto result = HandleInstruction(output, dim_orders,\n-    TF_RET_CHECK(std::holds_alternative<DimOrderMap>(result));\n-    TF_RET_CHECK(RequireTritonGemmSupportedDimOrder(\n-        std::get<DimOrderMap>(result).at(output)));\n-    TF_RET_CHECK(\n-        MergeDimOrderMapUpdates(dim_orders, std::get<DimOrderMap>(result)));\n-                                        dim_orders.at(output))})\n-    TF_RETURN_IF_ERROR(PropagateDimensionOrdersToParameters(\n-        *output, dim_orders.at(output), parameters_[Scope::OUTPUT],\n-        iter_specs_[Scope::OUTPUT]));\n",
            "whole_added": "+ public:\n+  DimensionOrder() = default;\n+\n+  // `hlo` is currently supposed to be either an operand or the output of dot().\n+  explicit DimensionOrder(const HloInstruction* hlo,\n+                          const int split_k_dimension_index = -1) {\n+struct DimOrderUpdates {\n+  DimOrderMap map;\n+  int64_t splittable_dimension_major_part_size = 0;\n+};\n+\n+using DimOrderUpdatesOrError = std::variant<FusionDecision, DimOrderUpdates>;\n+DimOrderUpdatesOrError HandleElementwise(const HloInstruction* hlo,\n+                                         const DimOrderMap& dim_orders) {\n+  DimOrderUpdates result;\n+  result.map.insert({hlo, DimensionOrder(*src_dim_order)});\n+    result.map.insert({operand, DimensionOrder(dim_orders.at(src))});\n+DimOrderUpdatesOrError HandleBitcast(const HloInstruction* hlo,\n+                                     const DimOrderMap& dim_orders,\n+                                     const TransformDirection direction) {\n+  DimOrderUpdates result;\n+      result.map.insert({dst, DimensionOrder()}).first->second;\n+DimOrderUpdatesOrError HandleCopyOrTransposeOrBroadcast(\n+  DimOrderUpdates result;\n+      result.map.insert({dst, DimensionOrder()}).first->second;\n+DimOrderUpdatesOrError HandleInstruction(const HloInstruction* hlo,\n+                                         const DimOrderMap& dim_orders,\n+                                         TransformDirection direction) {\n+    return DimOrderUpdates{};\n+class FusionContext {\n+  explicit FusionContext(\n+      const int64_t splittable_dimension_index,\n+      const int64_t splittable_dimension_supported_major_size)\n+      : splittable_dimension_index_(splittable_dimension_index),\n+        splittable_dimension_supported_major_part_size_(\n+            splittable_dimension_supported_major_size) {}\n+\n+ public:\n+  // Create fusion context from a dot operand according to\n+  // the currently supported configurations.\n+  static FusionContext FromDotOperand(const HloInstruction& dot,\n+                                      int operand_number, int split_k = 1);\n+\n+  // Create fusion context from dot's output.\n+  static FusionContext FromDotOutput(\n+      const HloInstruction& dot, int split_k,\n+      int64_t splittable_dimension_supported_major_part_size);\n+\n+  // Tells if the dimension order is supported by the triton GEMM emitter.\n+  // Only the dimension indicated by SplittableDimensionIndex() can be split\n+  // physically once by other dimensions. Other ones can be only split\n+  // logically. All subdimensions within a dimension have to be ordered.\n+  // Return major part of splittable dimension in split_dim_major_part if a\n+  // supported split is detected.\n+  FusionDecision RequireTritonGemmSupportedDimOrder(\n+      const DimensionOrder& order, int64_t& split_dim_major_part) const;\n+  // Apply RequireTritonGemmSupportedDimOrder() to all known dimension orders\n+  // around `hlo`.\n+  FusionDecision RequireTritonGemmSupportedDimOrders(\n+      const HloInstruction& hlo, DimOrderUpdates& updates) const;\n+  // Checks if the instruction is possible and profitable to fuse.\n+  // If so tries to transform dim_order describing one side of `hlo` into\n+  // description(s) of its other side if it is supported.\n+  DimOrderUpdatesOrError AnalyzeForFusion(\n+      const HloInstruction& hlo, bool as_input,\n+      absl::flat_hash_map<const HloInstruction*, HloInstruction*>&\n+          old_to_new_mapping,\n+      GpuVersion gpu_version) const;\n+  // Add dimension orders from `updates` to `dim_orders_` and update the\n+  // splittable dimension ratio if all of them are compatible.\n+  bool MergeUpdates(const DimOrderUpdates& updates);\n+  // Fuse an instruction with all its fusible inputs.\n+  // If an input is not fusible stop there and make a parameter of the new\n+  // fusion, otherwise put it onto stack and check its own inputs first.\n+  void TryToFuseWithInputsRecursively(\n+      HloInstruction& root, GpuVersion gpu_version,\n+      absl::flat_hash_map<const HloInstruction*, HloInstruction*>&\n+          old_to_new_mapping,\n+      std::vector<HloInstruction*>& fusion_inputs,\n+      HloComputation::Builder& builder);\n+  // Propagate dimension orders in consumer->producer direction starting at\n+  // `origin` with output `origin_dim_order` till parameters of the computation.\n+  // Store the found parameters and their iteration specs.\n+  Status PropagateDimensionOrdersToParameters(\n+      const HloInstruction& origin,\n+      absl::flat_hash_set<const HloInstruction*>& parameters,\n+      absl::flat_hash_map<const HloInstruction*, TensorIterationSpec>&\n+          iter_specs);\n+\n+  // Index of dot dimension that can be split.\n+  // Currently typically LHS non-contracting one.\n+  int64_t SplittableDimensionIndex() const {\n+    return splittable_dimension_index_;\n+  }\n+  // Tells whether `size` major part of a dimension can be physically split.\n+  bool IsSupportedSplittableDimensionMajorPartSize(const int64_t size) const {\n+    CHECK_NE(size, 0);\n+    // 0 means no specific size requirement.\n+    return splittable_dimension_supported_major_part_size_ == 0 ||\n+           splittable_dimension_supported_major_part_size_ == size;\n+  }\n+  int SplittableDimensionMajorPartSize() const {\n+    return splittable_dimension_supported_major_part_size_;\n+  }\n+  const DimOrderMap& DimOrders() const { return dim_orders_; }\n+\n+ private:\n+  bool SetSplittableDimensionMajorPartSize(const int64_t size) {\n+    if (IsSupportedSplittableDimensionMajorPartSize(size)) {\n+      splittable_dimension_supported_major_part_size_ = size;\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  const int splittable_dimension_index_;\n+  int64_t splittable_dimension_supported_major_part_size_;\n+  DimOrderMap dim_orders_;\n+};\n+\n+FusionContext FusionContext::FromDotOperand(const HloInstruction& dot,\n+                                            const int operand_number,\n+                                            const int split_k) {\n+  // There can be either none or one split-K batch dimension.\n+  const int num_split_k_batch_dims = split_k > 1;\n+  int split_k_dimension_index = -1;\n+  if (split_k > 1) {\n+    split_k_dimension_index =\n+        ContractingDimensionIndex(dot, operand_number) - 1;\n+  }\n+  int splittable_dimension_index = -1;\n+  // LHS non-contracting dimension can be split if non-splitK batch is absent.\n+  if (operand_number == 0 &&\n+      dot.dot_dimension_numbers().lhs_batch_dimensions_size() -\n+              num_split_k_batch_dims ==\n+          0) {\n+    splittable_dimension_index =\n+        NonContractingDimensionIndex(dot, operand_number);\n+  }\n+  FusionContext context(splittable_dimension_index,\n+                        /*splittable_dimension_supported_major_size=*/0);\n+  context.dim_orders_[dot.operand(operand_number)] =\n+      DimensionOrder(dot.operand(operand_number), split_k_dimension_index);\n+  return context;\n+}\n+\n+FusionContext FusionContext::FromDotOutput(\n+    const HloInstruction& dot, const int split_k,\n+    const int64_t splittable_dimension_supported_major_part_size) {\n+  // Allow non-contracting dimension originating from LHS to split if\n+  // this dimension is split at the output at the same ratio as\n+  // at the input.\n+  int64_t splittable_dimension_index = -1;\n+  if (splittable_dimension_supported_major_part_size > 1) {\n+    // Split-K dimension is the first one in the output if present;\n+    // LHS non-contracting follows (batch is absent in this case).\n+    splittable_dimension_index = (split_k > 1) ? 1 : 0;\n+  }\n+  FusionContext context(splittable_dimension_index,\n+                        splittable_dimension_supported_major_part_size);\n+  context.dim_orders_[&dot] = DimensionOrder(&dot);\n+  return context;\n+}\n+\n+FusionDecision FusionContext::RequireTritonGemmSupportedDimOrder(\n+    const DimensionOrder& order, int64_t& split_dim_major_part) const {\n+    auto fragment = dim_fragments.cbegin();\n+    while (true) {\n+      if (fragment == dim_fragments.cend()) {\n+        break;\n+      int64_t grouped_size = tensor_dim_fragments[*fragment].size;\n+      // Gather contiguous fragments.\n+      while ((fragment + 1) != dim_fragments.cend() &&\n+             *(fragment + 1) == *fragment + 1) {\n+        ++fragment;\n+        grouped_size *= tensor_dim_fragments[*fragment].size;\n+      }\n+\n+      if (grouped_size == 1) {\n+        ++fragment;\n+\n+      if (fragment != dim_fragments.cbegin() && *fragment < *(fragment - 1)) {\n+        return \"Transpose within a dimension.\";\n+      }\n+\n+      ++split_counter;\n+      if (split_counter > 0) {\n+        if (dim_index == SplittableDimensionIndex() &&\n+            IsSupportedSplittableDimensionMajorPartSize(grouped_size)) {\n+          if (split_counter == 1) {\n+            if (split_dim_major_part != 0 &&\n+                split_dim_major_part != grouped_size) {\n+              return \"Conflicting splits of splittable dimension\";\n+            }\n+            split_dim_major_part = grouped_size;\n+          } else if (split_counter > 1) {\n+        } else {\n+          return \"Unsupported split of a dimension.\";\n+\n+      ++fragment;\n+FusionDecision FusionContext::RequireTritonGemmSupportedDimOrders(\n+    const HloInstruction& hlo, DimOrderUpdates& updates) const {\n+    if (auto it = updates.map.find(instr); it != updates.map.end()) {\n+      return RequireTritonGemmSupportedDimOrder(\n+          it->second, updates.splittable_dimension_major_part_size);\n+DimOrderUpdatesOrError FusionContext::AnalyzeForFusion(\n+    const HloInstruction& hlo, bool as_input,\n+    const GpuVersion gpu_version) const {\n+      HandleInstruction(&hlo, dim_orders_,\n+  if (!std::holds_alternative<DimOrderUpdates>(result)) {\n+          hlo, std::get<DimOrderUpdates>(result));\n+  return std::get<DimOrderUpdates>(result);\n+bool FusionContext::MergeUpdates(const DimOrderUpdates& updates) {\n+  for (const auto& [key, value] : updates.map) {\n+    auto it = dim_orders_.find(key);\n+    if (it != dim_orders_.cend() && !it->second.IsPhysicallyEquivalent(value)) {\n+  if (updates.splittable_dimension_major_part_size > 1 &&\n+      !SetSplittableDimensionMajorPartSize(\n+          updates.splittable_dimension_major_part_size)) {\n+    return false;\n+  }\n+  dim_orders_.insert(updates.map.begin(), updates.map.end());\n+void FusionContext::TryToFuseWithInputsRecursively(\n+    HloInstruction& root, const GpuVersion gpu_version,\n+    const DimOrderUpdatesOrError result = AnalyzeForFusion(\n+        hlo, /*as_input=*/true, old_to_new_mapping, gpu_version);\n+    if (!std::holds_alternative<DimOrderUpdates>(result)) {\n+          std::get<DimOrderUpdates>(result).map.at(operand);\n+              dim_orders_.at(reference_dim_order_hlo))) {\n+    if (!MergeUpdates(std::get<DimOrderUpdates>(result))) {\n+  auto fuse_inputs = [&](int operand_number) -> StatusOr<FusionContext> {\n+    auto context = FusionContext::FromDotOperand(dot, operand_number);\n+    context.TryToFuseWithInputsRecursively(*dot.mutable_operand(operand_number),\n+                                           gpu_version, old_to_new_mapping,\n+                                           fusion_inputs, builder);\n+    return context;\n+\n+  TF_ASSIGN_OR_RETURN(const FusionContext lhs_context, fuse_inputs(0));\n+  auto context = FusionContext::FromDotOutput(\n+      dot, /*split_k=*/1, lhs_context.SplittableDimensionMajorPartSize());\n+    auto result = context.AnalyzeForFusion(*user, /*as_input=*/false,\n+                                           old_to_new_mapping, gpu_version);\n+    if (!std::holds_alternative<DimOrderUpdates>(result)) {\n+    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n+        context.TryToFuseWithInputsRecursively(\n+            *operand, gpu_version, old_to_new_mapping, fusion_inputs, builder);\n+Status FusionContext::PropagateDimensionOrdersToParameters(\n+    const HloInstruction& origin,\n+        HandleInstruction(hlo, dim_orders_, TransformDirection::kOutputToInput);\n+    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n+    TF_RET_CHECK(RequireTritonGemmSupportedDimOrders(\n+        *hlo, std::get<DimOrderUpdates>(result)));\n+    TF_RET_CHECK(MergeUpdates(std::get<DimOrderUpdates>(result)));\n+    TF_RET_CHECK(dim_orders_.at(parameter).IsPhysicallyEquivalent(\n+        dim_orders_.at(*parameters.cbegin())));\n+        DimensionOrderToTensorIterationSpec(dim_orders_.at(parameter));\n+  int64_t lhs_nc_split_major_part_size = -1;\n+    auto context = FusionContext::FromDotOperand(*dot, operand_number, split_k);\n+    TF_RETURN_IF_ERROR(context.PropagateDimensionOrdersToParameters(\n+        *dot->operand(operand_number), parameters_[scope], iter_specs_[scope]));\n+    if (scope == Scope::LHS && context.SplittableDimensionMajorPartSize() > 1) {\n+      lhs_nc_split_major_part_size = context.SplittableDimensionMajorPartSize();\n+\n+  auto context =\n+      FusionContext::FromDotOutput(*dot, split_k, lhs_nc_split_major_part_size);\n+    auto result = HandleInstruction(output, context.DimOrders(),\n+    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n+    TF_RET_CHECK(context.RequireTritonGemmSupportedDimOrders(\n+        *output, std::get<DimOrderUpdates>(result)));\n+    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n+                                        context.DimOrders().at(output))})\n+    TF_RETURN_IF_ERROR(context.PropagateDimensionOrdersToParameters(\n+        *output, parameters_[Scope::OUTPUT], iter_specs_[Scope::OUTPUT]));\n",
            "whole_hunk": "@@ -165,16 +165,13 @@ FusionDecision RequireTritonFusibleConvert(const HloInstruction* input,\n // Used to calculate cumulative index transformations done by non-elementwise\n // instructions between source and target.\n class DimensionOrder {\n+ public:\n+  DimensionOrder() = default;\n+\n   // Dimension order constructed for the output shape of `hlo`.\n-  // `hlo` is currently supposed to be either an operand or the output of dot();\n-  // properties describing the dimensions are stored for later analysis.\n-  explicit DimensionOrder(\n-      const HloInstruction* hlo, const int64_t splittable_dimension_index = -1,\n-      const int64_t splittable_dimension_supported_major_size = 0,\n-      const int split_k_dimension_index = -1)\n-      : splittable_dimension_index_(splittable_dimension_index),\n-        splittable_dimension_supported_major_part_size_(\n-            splittable_dimension_supported_major_size) {\n+  // `hlo` is currently supposed to be either an operand or the output of dot().\n+  explicit DimensionOrder(const HloInstruction* hlo,\n+                          const int split_k_dimension_index = -1) {\n     tensor_fragments_order_.reserve(hlo->shape().rank());\n     for (const int i : hlo->shape().layout().minor_to_major()) {\n       int target_dim_number = i;\n@@ -191,14 +188,6 @@ class DimensionOrder {\n     }\n   }\n \n-  explicit DimensionOrder(\n-      const int64_t splittable_dimension_index,\n-      const int64_t splittable_dimension_supported_major_size)\n-      : splittable_dimension_index_(splittable_dimension_index),\n-        splittable_dimension_supported_major_part_size_(\n-            splittable_dimension_supported_major_size) {}\n-\n- public:\n   // Description of a continuous fragment of one dimension of a tensor.\n   struct Fragment {\n     // Label carrying the dimension number of an defining operation.\n@@ -214,27 +203,6 @@ class DimensionOrder {\n   using Fragments = std::vector<Fragment>;\n   using FragmentOrders = absl::flat_hash_map<int, std::vector<int>>;\n \n-  DimensionOrder(const DimensionOrder&) = default;\n-\n-  // Copies fusion context attributes from `other` leaving internal structures\n-  // describing dimension fragments empty. Used to create derived dimension\n-  // orders.\n-  static DimensionOrder EmptyLike(const DimensionOrder& other) {\n-    return DimensionOrder(\n-        other.splittable_dimension_index_,\n-        other.splittable_dimension_supported_major_part_size_);\n-  }\n-\n-  // Create dimension order describing a dot operand according to\n-  // the currently supported configurations.\n-  static DimensionOrder FromDotOperand(const HloInstruction& dot,\n-                                       int operand_number, int split_k = 1);\n-\n-  // Create dimension order describing dot's output.\n-  static DimensionOrder FromDotOutput(\n-      const HloInstruction& dot, int split_k = 1,\n-      int64_t splittable_dimension_supported_major_part_size = 0);\n-\n   const Fragments& TensorFragmentsOrder() const {\n     return tensor_fragments_order_;\n   }\n@@ -245,19 +213,6 @@ class DimensionOrder {\n   }\n   FragmentOrders& DimFragmentsOrders() { return dim_fragments_orders_; }\n \n-  // Index of dot dimension that can be split.\n-  // Currently typically LHS non-contracting one.\n-  int64_t SplittableDimensionIndex() const {\n-    return splittable_dimension_index_;\n-  }\n-\n-  // Tells whether `size` major part of a dimension can be physically split.\n-  bool IsSupportedSplittableDimensionMajorPartSize(int64_t size) const {\n-    // 0 means no specific size requirement.\n-    return splittable_dimension_supported_major_part_size_ == 0 ||\n-           splittable_dimension_supported_major_part_size_ == size;\n-  }\n-\n   // Tells that two dimension orders describe the same tensor physical layout.\n   bool IsPhysicallyEquivalent(const DimensionOrder& other) const;\n \n@@ -281,9 +236,6 @@ class DimensionOrder {\n   // (fragments can be physically unordered and disconnected within\n   // the shape due to reshapes and transposes).\n   FragmentOrders dim_fragments_orders_;\n-\n-  const int64_t splittable_dimension_index_ = -1;\n-  const int64_t splittable_dimension_supported_major_part_size_ = 0;\n };\n \n using DimIterationSpec = TensorIterationSpec::DimIterationSpec;\n@@ -292,6 +244,11 @@ using Fragments = DimensionOrder::Fragments;\n using FragmentOrders = DimensionOrder::FragmentOrders;\n using DimOrderMap = absl::flat_hash_map<const HloInstruction*, DimensionOrder>;\n \n+struct DimOrderUpdates {\n+  DimOrderMap map;\n+  int64_t splittable_dimension_major_part_size = 0;\n+};\n+\n TensorIterationSpec DimensionOrderToTensorIterationSpec(\n     const DimensionOrder& order) {\n   const Fragments& dim_fragments = order.TensorFragmentsOrder();\n@@ -344,53 +301,12 @@ bool DimensionOrder::IsPhysicallyEquivalent(const DimensionOrder& other) const {\n          DimensionOrderToTensorIterationSpec(other);\n }\n \n-DimensionOrder DimensionOrder::FromDotOperand(const HloInstruction& dot,\n-                                              const int operand_number,\n-                                              const int split_k) {\n-  const HloInstruction* operand = dot.operand(operand_number);\n-  // There can be either none or one split-K batch dimension.\n-  const int num_split_k_batch_dims = split_k > 1;\n-  int split_k_dimension_index = -1;\n-  if (split_k > 1) {\n-    split_k_dimension_index =\n-        ContractingDimensionIndex(dot, operand_number) - 1;\n-  }\n-  int splittable_dimension_index = -1;\n-  // LHS non-contracting dimension can be split if non-splitK batch is absent.\n-  if (operand_number == 0 &&\n-      dot.dot_dimension_numbers().lhs_batch_dimensions_size() -\n-              num_split_k_batch_dims ==\n-          0) {\n-    splittable_dimension_index =\n-        NonContractingDimensionIndex(dot, operand_number);\n-  }\n-  return DimensionOrder(operand, splittable_dimension_index,\n-                        /*splittable_dimension_supported_major_size=*/0,\n-                        split_k_dimension_index);\n-}\n-\n-DimensionOrder DimensionOrder::FromDotOutput(\n-    const HloInstruction& dot, const int split_k,\n-    const int64_t splittable_dimension_supported_major_part_size) {\n-  // Allow non-contracting dimension originating from LHS to split if\n-  // this dimension is split at the output at the same ratio as\n-  // at the input.\n-  int64_t splittable_dimension_index = -1;\n-  if (splittable_dimension_supported_major_part_size > 1) {\n-    // Split-K dimension is the first one in the output if present;\n-    // LHS non-contracting follows (batch is absent in this case).\n-    splittable_dimension_index = (split_k > 1) ? 1 : 0;\n-  }\n-  return DimensionOrder(&dot, splittable_dimension_index,\n-                        splittable_dimension_supported_major_part_size);\n-}\n-\n enum class TransformDirection { kInputToOutput, kOutputToInput };\n \n-using DimOrderMapOrError = std::variant<FusionDecision, DimOrderMap>;\n+using DimOrderUpdatesOrError = std::variant<FusionDecision, DimOrderUpdates>;\n \n-DimOrderMapOrError HandleElementwise(const HloInstruction* hlo,\n-                                     const DimOrderMap& dim_orders) {\n+DimOrderUpdatesOrError HandleElementwise(const HloInstruction* hlo,\n+                                         const DimOrderMap& dim_orders) {\n   // The output and all the input dimension orders of `hlo` have to be the same.\n   const HloInstruction* src = nullptr;\n   const DimensionOrder* src_dim_order;\n@@ -410,17 +326,17 @@ DimOrderMapOrError HandleElementwise(const HloInstruction* hlo,\n     CHECK_NE(src, nullptr);\n   }\n \n-  DimOrderMap result;\n-  result.insert({hlo, DimensionOrder(*src_dim_order)});\n+  DimOrderUpdates result;\n+  result.map.insert({hlo, DimensionOrder(*src_dim_order)});\n   for (const HloInstruction* operand : hlo->operands()) {\n-    result.insert({operand, DimensionOrder(dim_orders.at(src))});\n+    result.map.insert({operand, DimensionOrder(dim_orders.at(src))});\n   }\n   return result;\n }\n \n-DimOrderMapOrError HandleBitcast(const HloInstruction* hlo,\n-                                 const DimOrderMap& dim_orders,\n-                                 const TransformDirection direction) {\n+DimOrderUpdatesOrError HandleBitcast(const HloInstruction* hlo,\n+                                     const DimOrderMap& dim_orders,\n+                                     const TransformDirection direction) {\n   const HloInstruction* src =\n       (direction == TransformDirection::kOutputToInput) ? hlo : hlo->operand(0);\n   const HloInstruction* dst =\n@@ -428,10 +344,9 @@ DimOrderMapOrError HandleBitcast(const HloInstruction* hlo,\n   const Shape& dst_shape = dst->shape();\n   const Fragments& src_fragments_order =\n       dim_orders.at(src).TensorFragmentsOrder();\n-  DimOrderMap result;\n+  DimOrderUpdates result;\n   DimensionOrder& dst_dim_order =\n-      result.insert({dst, DimensionOrder::EmptyLike(dim_orders.at(src))})\n-          .first->second;\n+      result.map.insert({dst, DimensionOrder()}).first->second;\n   Fragments& dst_fragments_order = dst_dim_order.TensorFragmentsOrder();\n   // Size of not yet assigned part of current target dimension.\n   int64_t dst_remaining_size = 1;\n@@ -525,7 +440,7 @@ DimOrderMapOrError HandleBitcast(const HloInstruction* hlo,\n   return result;\n }\n \n-DimOrderMapOrError HandleCopyOrTransposeOrBroadcast(\n+DimOrderUpdatesOrError HandleCopyOrTransposeOrBroadcast(\n     const HloInstruction* hlo, const DimOrderMap& dim_orders,\n     const TransformDirection direction) {\n   const HloInstruction* src =\n@@ -534,10 +449,9 @@ DimOrderMapOrError HandleCopyOrTransposeOrBroadcast(\n       (direction == TransformDirection::kOutputToInput) ? hlo->operand(0) : hlo;\n   const Fragments& src_fragments_order =\n       dim_orders.at(src).TensorFragmentsOrder();\n-  DimOrderMap result;\n+  DimOrderUpdates result;\n   DimensionOrder& dst_dim_order =\n-      result.insert({dst, DimensionOrder::EmptyLike(dim_orders.at(src))})\n-          .first->second;\n+      result.map.insert({dst, DimensionOrder()}).first->second;\n   Fragments& dst_fragments_order = dst_dim_order.TensorFragmentsOrder();\n   // Every HLO dimension can correspond to a group of subdimensions in\n   // dim_order_. For the easier handling of permutations: group dim_order_ by\n@@ -615,13 +529,13 @@ DimOrderMapOrError HandleCopyOrTransposeOrBroadcast(\n \n // Infers DimensionOrders of all unknown sides (output, operands)\n // of `hlo` from the known ones.\n-DimOrderMapOrError HandleInstruction(const HloInstruction* hlo,\n-                                     const DimOrderMap& dim_orders,\n-                                     TransformDirection direction) {\n+DimOrderUpdatesOrError HandleInstruction(const HloInstruction* hlo,\n+                                         const DimOrderMap& dim_orders,\n+                                         TransformDirection direction) {\n   VLOG(7) << hlo->ToString();\n   if (hlo->opcode() == HloOpcode::kParameter ||\n       hlo_query::IsScalarConstant(hlo)) {\n-    return DimOrderMap{};\n+    return DimOrderUpdates{};\n   } else if (hlo->opcode() == HloOpcode::kTranspose ||\n              hlo->opcode() == HloOpcode::kCopy) {\n     return HandleCopyOrTransposeOrBroadcast(hlo, dim_orders, direction);\n@@ -645,52 +559,199 @@ DimOrderMapOrError HandleInstruction(const HloInstruction* hlo,\n   return \"Unimplemented instruction.\";\n }\n \n-// Tells if the dimension order is supported by the triton GEMM emitter.\n-// Only the dimension indicated by SplittableDimensionIndex() can be split\n-// physically once by other dimensions. Other ones can be only split logically.\n-// All subdimensions within a dimension have to be ordered.\n-FusionDecision RequireTritonGemmSupportedDimOrder(const DimensionOrder& order) {\n+class FusionContext {\n+  explicit FusionContext(\n+      const int64_t splittable_dimension_index,\n+      const int64_t splittable_dimension_supported_major_size)\n+      : splittable_dimension_index_(splittable_dimension_index),\n+        splittable_dimension_supported_major_part_size_(\n+            splittable_dimension_supported_major_size) {}\n+\n+ public:\n+  // Create fusion context from a dot operand according to\n+  // the currently supported configurations.\n+  static FusionContext FromDotOperand(const HloInstruction& dot,\n+                                      int operand_number, int split_k = 1);\n+\n+  // Create fusion context from dot's output.\n+  static FusionContext FromDotOutput(\n+      const HloInstruction& dot, int split_k,\n+      int64_t splittable_dimension_supported_major_part_size);\n+\n+  // Tells if the dimension order is supported by the triton GEMM emitter.\n+  // Only the dimension indicated by SplittableDimensionIndex() can be split\n+  // physically once by other dimensions. Other ones can be only split\n+  // logically. All subdimensions within a dimension have to be ordered.\n+  // Return major part of splittable dimension in split_dim_major_part if a\n+  // supported split is detected.\n+  FusionDecision RequireTritonGemmSupportedDimOrder(\n+      const DimensionOrder& order, int64_t& split_dim_major_part) const;\n+  // Apply RequireTritonGemmSupportedDimOrder() to all known dimension orders\n+  // around `hlo`.\n+  FusionDecision RequireTritonGemmSupportedDimOrders(\n+      const HloInstruction& hlo, DimOrderUpdates& updates) const;\n+  // Checks if the instruction is possible and profitable to fuse.\n+  // If so tries to transform dim_order describing one side of `hlo` into\n+  // description(s) of its other side if it is supported.\n+  DimOrderUpdatesOrError AnalyzeForFusion(\n+      const HloInstruction& hlo, bool as_input,\n+      absl::flat_hash_map<const HloInstruction*, HloInstruction*>&\n+          old_to_new_mapping,\n+      GpuVersion gpu_version) const;\n+  // Add dimension orders from `updates` to `dim_orders_` and update the\n+  // splittable dimension ratio if all of them are compatible.\n+  bool MergeUpdates(const DimOrderUpdates& updates);\n+  // Fuse an instruction with all its fusible inputs.\n+  // If an input is not fusible stop there and make a parameter of the new\n+  // fusion, otherwise put it onto stack and check its own inputs first.\n+  void TryToFuseWithInputsRecursively(\n+      HloInstruction& root, GpuVersion gpu_version,\n+      absl::flat_hash_map<const HloInstruction*, HloInstruction*>&\n+          old_to_new_mapping,\n+      std::vector<HloInstruction*>& fusion_inputs,\n+      HloComputation::Builder& builder);\n+  // Propagate dimension orders in consumer->producer direction starting at\n+  // `origin` with output `origin_dim_order` till parameters of the computation.\n+  // Store the found parameters and their iteration specs.\n+  Status PropagateDimensionOrdersToParameters(\n+      const HloInstruction& origin,\n+      absl::flat_hash_set<const HloInstruction*>& parameters,\n+      absl::flat_hash_map<const HloInstruction*, TensorIterationSpec>&\n+          iter_specs);\n+\n+  // Index of dot dimension that can be split.\n+  // Currently typically LHS non-contracting one.\n+  int64_t SplittableDimensionIndex() const {\n+    return splittable_dimension_index_;\n+  }\n+  // Tells whether `size` major part of a dimension can be physically split.\n+  bool IsSupportedSplittableDimensionMajorPartSize(const int64_t size) const {\n+    CHECK_NE(size, 0);\n+    // 0 means no specific size requirement.\n+    return splittable_dimension_supported_major_part_size_ == 0 ||\n+           splittable_dimension_supported_major_part_size_ == size;\n+  }\n+  int SplittableDimensionMajorPartSize() const {\n+    return splittable_dimension_supported_major_part_size_;\n+  }\n+  const DimOrderMap& DimOrders() const { return dim_orders_; }\n+\n+ private:\n+  bool SetSplittableDimensionMajorPartSize(const int64_t size) {\n+    if (IsSupportedSplittableDimensionMajorPartSize(size)) {\n+      splittable_dimension_supported_major_part_size_ = size;\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  const int splittable_dimension_index_;\n+  int64_t splittable_dimension_supported_major_part_size_;\n+  DimOrderMap dim_orders_;\n+};\n+\n+FusionContext FusionContext::FromDotOperand(const HloInstruction& dot,\n+                                            const int operand_number,\n+                                            const int split_k) {\n+  // There can be either none or one split-K batch dimension.\n+  const int num_split_k_batch_dims = split_k > 1;\n+  int split_k_dimension_index = -1;\n+  if (split_k > 1) {\n+    split_k_dimension_index =\n+        ContractingDimensionIndex(dot, operand_number) - 1;\n+  }\n+  int splittable_dimension_index = -1;\n+  // LHS non-contracting dimension can be split if non-splitK batch is absent.\n+  if (operand_number == 0 &&\n+      dot.dot_dimension_numbers().lhs_batch_dimensions_size() -\n+              num_split_k_batch_dims ==\n+          0) {\n+    splittable_dimension_index =\n+        NonContractingDimensionIndex(dot, operand_number);\n+  }\n+  FusionContext context(splittable_dimension_index,\n+                        /*splittable_dimension_supported_major_size=*/0);\n+  context.dim_orders_[dot.operand(operand_number)] =\n+      DimensionOrder(dot.operand(operand_number), split_k_dimension_index);\n+  return context;\n+}\n+\n+FusionContext FusionContext::FromDotOutput(\n+    const HloInstruction& dot, const int split_k,\n+    const int64_t splittable_dimension_supported_major_part_size) {\n+  // Allow non-contracting dimension originating from LHS to split if\n+  // this dimension is split at the output at the same ratio as\n+  // at the input.\n+  int64_t splittable_dimension_index = -1;\n+  if (splittable_dimension_supported_major_part_size > 1) {\n+    // Split-K dimension is the first one in the output if present;\n+    // LHS non-contracting follows (batch is absent in this case).\n+    splittable_dimension_index = (split_k > 1) ? 1 : 0;\n+  }\n+  FusionContext context(splittable_dimension_index,\n+                        splittable_dimension_supported_major_part_size);\n+  context.dim_orders_[&dot] = DimensionOrder(&dot);\n+  return context;\n+}\n+\n+FusionDecision FusionContext::RequireTritonGemmSupportedDimOrder(\n+    const DimensionOrder& order, int64_t& split_dim_major_part) const {\n   VLOG(8) << order.ToString();\n   const Fragments& tensor_dim_fragments = order.TensorFragmentsOrder();\n   for (const auto& [dim_index, dim_fragments] : order.DimFragmentsOrders()) {\n-    int last_fragment_number = -1;\n     int split_counter = -1;\n-    for (const int fragment_number : dim_fragments) {\n-      CHECK_EQ(tensor_dim_fragments[fragment_number].dst_dim_number, dim_index);\n-      const int size = tensor_dim_fragments[fragment_number].size;\n-      if (fragment_number <= last_fragment_number) {\n-        return \"Transpose within a dimension.\";\n+    auto fragment = dim_fragments.cbegin();\n+    while (true) {\n+      if (fragment == dim_fragments.cend()) {\n+        break;\n       }\n-      if (size == 1) {\n-        last_fragment_number = fragment_number;\n+      int64_t grouped_size = tensor_dim_fragments[*fragment].size;\n+      // Gather contiguous fragments.\n+      while ((fragment + 1) != dim_fragments.cend() &&\n+             *(fragment + 1) == *fragment + 1) {\n+        ++fragment;\n+        grouped_size *= tensor_dim_fragments[*fragment].size;\n+      }\n+\n+      if (grouped_size == 1) {\n+        ++fragment;\n         continue;\n       }\n-      if (fragment_number == 0 ||\n-          tensor_dim_fragments[fragment_number - 1].dst_dim_number !=\n-              dim_index) {\n-        ++split_counter;\n-        if (dim_index == order.SplittableDimensionIndex() &&\n-            order.IsSupportedSplittableDimensionMajorPartSize(size)) {\n-          if (split_counter > 1) {\n+\n+      if (fragment != dim_fragments.cbegin() && *fragment < *(fragment - 1)) {\n+        return \"Transpose within a dimension.\";\n+      }\n+\n+      ++split_counter;\n+      if (split_counter > 0) {\n+        if (dim_index == SplittableDimensionIndex() &&\n+            IsSupportedSplittableDimensionMajorPartSize(grouped_size)) {\n+          if (split_counter == 1) {\n+            if (split_dim_major_part != 0 &&\n+                split_dim_major_part != grouped_size) {\n+              return \"Conflicting splits of splittable dimension\";\n+            }\n+            split_dim_major_part = grouped_size;\n+          } else if (split_counter > 1) {\n             return \"2nd split of a splittable dimension.\";\n           }\n-        } else if (split_counter > 0) {\n-          return \"Split of a non-splittable dimension.\";\n+        } else {\n+          return \"Unsupported split of a dimension.\";\n         }\n       }\n-      last_fragment_number = fragment_number;\n+\n+      ++fragment;\n     }\n   }\n   return FusionDecision{};\n }\n \n-// Apply RequireTritonGemmSupportedDimOrder() to all known dimension orders\n-// around `hlo`.\n-FusionDecision RequireTritonGemmSupportedDimOrders(\n-    const HloInstruction& hlo, const DimOrderMap& dim_orders) {\n+FusionDecision FusionContext::RequireTritonGemmSupportedDimOrders(\n+    const HloInstruction& hlo, DimOrderUpdates& updates) const {\n   auto check_if_present = [&](const HloInstruction* instr) {\n-    if (auto it = dim_orders.find(instr); it != dim_orders.end()) {\n-      return RequireTritonGemmSupportedDimOrder(it->second);\n+    if (auto it = updates.map.find(instr); it != updates.map.end()) {\n+      return RequireTritonGemmSupportedDimOrder(\n+          it->second, updates.splittable_dimension_major_part_size);\n     }\n     return FusionDecision{};\n   };\n@@ -741,14 +802,11 @@ bool IsOutputWorthFusing(const HloInstruction& hlo) {\n          InputMinusOutputBytes(hlo) >= -kIoToleranceBytes;\n }\n \n-// Checks if the instruction is possible and profitable to fuse.\n-// If so tries to transform dim_order describing one side of `hlo` into\n-// description(s) of its other side if it is supported.\n-DimOrderMapOrError AnalyzeForFusion(\n-    const HloInstruction& hlo, bool as_input, DimOrderMap& dim_orders,\n+DimOrderUpdatesOrError FusionContext::AnalyzeForFusion(\n+    const HloInstruction& hlo, bool as_input,\n     absl::flat_hash_map<const HloInstruction*, HloInstruction*>&\n         old_to_new_mapping,\n-    const GpuVersion gpu_version) {\n+    const GpuVersion gpu_version) const {\n   int fusion_level =\n       hlo.GetModule()->config().debug_options().xla_gpu_triton_fusion_level();\n   if (!std::get<se::CudaComputeCapability>(gpu_version)\n@@ -812,19 +870,19 @@ DimOrderMapOrError AnalyzeForFusion(\n   }\n \n   auto result =\n-      HandleInstruction(&hlo, dim_orders,\n+      HandleInstruction(&hlo, dim_orders_,\n                         as_input ? TransformDirection::kOutputToInput\n                                  : TransformDirection::kInputToOutput);\n-  if (!std::holds_alternative<DimOrderMap>(result)) {\n+  if (!std::holds_alternative<DimOrderUpdates>(result)) {\n     return std::get<FusionDecision>(result);\n   }\n \n   if (FusionDecision supported = RequireTritonGemmSupportedDimOrders(\n-          hlo, std::get<DimOrderMap>(result));\n+          hlo, std::get<DimOrderUpdates>(result));\n       !supported) {\n     return supported;\n   }\n-  return std::get<DimOrderMap>(result);\n+  return std::get<DimOrderUpdates>(result);\n }\n \n // Clone an instruction into the fusion.\n@@ -875,26 +933,26 @@ int64_t NumAddedParameters(const HloInstruction& hlo) {\n   return hlo.operand_count() - 1;\n }\n \n-bool MergeDimOrderMapUpdates(DimOrderMap& target, const DimOrderMap& updates) {\n+bool FusionContext::MergeUpdates(const DimOrderUpdates& updates) {\n   // First check that all updates to insert are compatible to avoid\n   // incomplete merges.\n-  for (const auto& [key, value] : updates) {\n-    auto it = target.find(key);\n-    if (it != target.cend() && !it->second.IsPhysicallyEquivalent(value)) {\n+  for (const auto& [key, value] : updates.map) {\n+    auto it = dim_orders_.find(key);\n+    if (it != dim_orders_.cend() && !it->second.IsPhysicallyEquivalent(value)) {\n       return false;\n     }\n   }\n-  target.insert(updates.begin(), updates.end());\n+  if (updates.splittable_dimension_major_part_size > 1 &&\n+      !SetSplittableDimensionMajorPartSize(\n+          updates.splittable_dimension_major_part_size)) {\n+    return false;\n+  }\n+  dim_orders_.insert(updates.map.begin(), updates.map.end());\n   return true;\n }\n \n-// Fuse an instruction with all its fusible inputs.\n-// If an input is not fusible stop there and make a parameter of the new\n-// fusion, otherwise put it onto stack and check its own inputs first.\n-void TryToFuseWithInputsRecursively(\n-    HloInstruction& root,\n-    // Dimension orders describing outputs of corresponding instructions.\n-    DimOrderMap& dim_orders, const GpuVersion gpu_version,\n+void FusionContext::TryToFuseWithInputsRecursively(\n+    HloInstruction& root, const GpuVersion gpu_version,\n     absl::flat_hash_map<const HloInstruction*, HloInstruction*>&\n         old_to_new_mapping,\n     std::vector<HloInstruction*>& fusion_inputs,\n@@ -910,21 +968,21 @@ void TryToFuseWithInputsRecursively(\n   // of them to be physically compatible.\n   const HloInstruction* reference_dim_order_hlo = nullptr;\n   auto try_fuse_one = [&](HloInstruction& hlo) {\n-    const DimOrderMapOrError result = AnalyzeForFusion(\n-        hlo, /*as_input=*/true, dim_orders, old_to_new_mapping, gpu_version);\n-    if (!std::holds_alternative<DimOrderMap>(result)) {\n+    const DimOrderUpdatesOrError result = AnalyzeForFusion(\n+        hlo, /*as_input=*/true, old_to_new_mapping, gpu_version);\n+    if (!std::holds_alternative<DimOrderUpdates>(result)) {\n       return false;\n     }\n     for (const HloInstruction* operand : hlo.operands()) {\n       const DimensionOrder& dim_order =\n-          std::get<DimOrderMap>(result).at(operand);\n+          std::get<DimOrderUpdates>(result).map.at(operand);\n       if (reference_dim_order_hlo != nullptr &&\n           !dim_order.IsPhysicallyEquivalent(\n-              dim_orders.at(reference_dim_order_hlo))) {\n+              dim_orders_.at(reference_dim_order_hlo))) {\n         return false;\n       }\n     }\n-    if (!MergeDimOrderMapUpdates(dim_orders, std::get<DimOrderMap>(result))) {\n+    if (!MergeUpdates(std::get<DimOrderUpdates>(result))) {\n       return false;\n     }\n     to_fuse.push(&hlo);\n@@ -986,45 +1044,19 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n   // differently shaped tiles but may go through same HLO graph nodes.\n   // Direct dot inputs have well defined dimension orders.\n \n-  auto fuse_inputs = [&](int operand_number) -> StatusOr<DimOrderMap> {\n+  auto fuse_inputs = [&](int operand_number) -> StatusOr<FusionContext> {\n     const int operand_count_before = fusion_inputs.size();\n-    DimOrderMap dim_orders;\n     // Direct dot inputs have well defined dimension orders.\n-    dim_orders.insert({dot.operand(operand_number),\n-                       DimensionOrder::FromDotOperand(dot, operand_number)});\n-    TryToFuseWithInputsRecursively(*dot.mutable_operand(operand_number),\n-                                   dim_orders, gpu_version, old_to_new_mapping,\n-                                   fusion_inputs, builder);\n+    auto context = FusionContext::FromDotOperand(dot, operand_number);\n+    context.TryToFuseWithInputsRecursively(*dot.mutable_operand(operand_number),\n+                                           gpu_version, old_to_new_mapping,\n+                                           fusion_inputs, builder);\n     TF_RET_CHECK(fusion_inputs.size() - operand_count_before <=\n                  DotFusionAnalysis::kMaxParameterPerScope);\n-    return dim_orders;\n+    return context;\n   };\n-  // Check if non-contracting dimension originating from LHS operand in the\n-  // output can be split. This currently requires this dimension being split\n-  // in the operand the same way.\n-  int64_t lhs_nc_split_major_part = -1;\n-  {\n-    TF_ASSIGN_OR_RETURN(const auto lhs_dim_orders, fuse_inputs(0));\n-    // Looking at first LHS parameter to find split non-contracting dimension\n-    // is sufficient because currently all parameters of one scope have to use\n-    // the same tiling.\n-    auto first_lhs_parameter_it = lhs_dim_orders.cbegin();\n-    while (first_lhs_parameter_it != lhs_dim_orders.cend()) {\n-      if (auto it = old_to_new_mapping.find(first_lhs_parameter_it->first);\n-          it != old_to_new_mapping.cend() &&\n-          it->second->opcode() == HloOpcode::kParameter) {\n-        break;\n-      }\n-      ++first_lhs_parameter_it;\n-    }\n-    if (first_lhs_parameter_it != lhs_dim_orders.cend()) {\n-      const auto lhs_nc_iter_spec = DimensionOrderToTensorIterationSpec(\n-          first_lhs_parameter_it->second)[NonContractingDimensionIndex(dot, 0)];\n-      if (lhs_nc_iter_spec.size() > 1) {\n-        lhs_nc_split_major_part = lhs_nc_iter_spec.at(1).count;\n-      }\n-    }\n-  }\n+\n+  TF_ASSIGN_OR_RETURN(const FusionContext lhs_context, fuse_inputs(0));\n   if (auto result = fuse_inputs(1); !result.ok()) {\n     return result.status();\n   }\n@@ -1034,10 +1066,8 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n   // Fusion at dot's output.\n \n   // These describe _outputs_ of corresponding HLOs.\n-  DimOrderMap out_dim_orders;\n-  out_dim_orders.insert(\n-      {&dot, DimensionOrder::FromDotOutput(dot, /*split_k=*/1,\n-                                           lhs_nc_split_major_part)});\n+  auto context = FusionContext::FromDotOutput(\n+      dot, /*split_k=*/1, lhs_context.SplittableDimensionMajorPartSize());\n   HloInstruction* fusion_output = &dot;\n   bool output_changed = true;\n   while (output_changed) {\n@@ -1049,18 +1079,16 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n     if (!IsDistributiveOverAddition(*user)) {\n       break;\n     }\n-    auto result = AnalyzeForFusion(*user, /*as_input=*/false, out_dim_orders,\n-                                   old_to_new_mapping, gpu_version);\n-    if (!std::holds_alternative<DimOrderMap>(result)) {\n+    auto result = context.AnalyzeForFusion(*user, /*as_input=*/false,\n+                                           old_to_new_mapping, gpu_version);\n+    if (!std::holds_alternative<DimOrderUpdates>(result)) {\n       continue;\n     }\n-    TF_RET_CHECK(\n-        MergeDimOrderMapUpdates(out_dim_orders, std::get<DimOrderMap>(result)));\n+    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n     for (HloInstruction* operand : user->operands()) {\n       if (!old_to_new_mapping.contains(operand)) {\n-        TryToFuseWithInputsRecursively(*operand, out_dim_orders, gpu_version,\n-                                       old_to_new_mapping, fusion_inputs,\n-                                       builder);\n+        context.TryToFuseWithInputsRecursively(\n+            *operand, gpu_version, old_to_new_mapping, fusion_inputs, builder);\n       }\n     }\n     Fuse(*user, old_to_new_mapping, fusion_inputs, builder);\n@@ -1368,20 +1396,14 @@ Status MakeDotComputationSplitKBatch(\n   return OkStatus();\n }\n \n-// Propagate dimension orders in consumer->producer direction starting at\n-// `origin` with output `origin_dim_order` till parameters of the computation.\n-// Store the found parameters and their iteration specs.\n-Status PropagateDimensionOrdersToParameters(\n-    const HloInstruction& origin, DimensionOrder origin_dim_order,\n+Status FusionContext::PropagateDimensionOrdersToParameters(\n+    const HloInstruction& origin,\n     absl::flat_hash_set<const HloInstruction*>& parameters,\n     absl::flat_hash_map<const HloInstruction*, TensorIterationSpec>&\n         iter_specs) {\n   absl::flat_hash_set<const HloInstruction*> visited;\n   std::queue<const HloInstruction*> to_process;\n   // Dimension orders describing outputs of corresponding instructions.\n-  DimOrderMap dim_orders;\n-  TF_RET_CHECK(RequireTritonGemmSupportedDimOrder(origin_dim_order));\n-  dim_orders.insert({&origin, origin_dim_order});\n   visited.insert(&origin);\n   to_process.push(&origin);\n   while (!to_process.empty()) {\n@@ -1397,12 +1419,11 @@ Status PropagateDimensionOrdersToParameters(\n       VLOG(5) << hlo->ToString();\n     }\n     auto result =\n-        HandleInstruction(hlo, dim_orders, TransformDirection::kOutputToInput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderMap>(result));\n-    TF_RET_CHECK(\n-        MergeDimOrderMapUpdates(dim_orders, std::get<DimOrderMap>(result)));\n-    TF_RET_CHECK(\n-        RequireTritonGemmSupportedDimOrders(*hlo, dim_orders).CanFuse());\n+        HandleInstruction(hlo, dim_orders_, TransformDirection::kOutputToInput);\n+    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n+    TF_RET_CHECK(RequireTritonGemmSupportedDimOrders(\n+        *hlo, std::get<DimOrderUpdates>(result)));\n+    TF_RET_CHECK(MergeUpdates(std::get<DimOrderUpdates>(result)));\n     for (const HloInstruction* operand : hlo->operands()) {\n       if (!visited.insert(operand).second) {\n         continue;\n@@ -1417,10 +1438,10 @@ Status PropagateDimensionOrdersToParameters(\n   }\n   // For now all parameters of one scope have to use the same tiling.\n   for (const HloInstruction* parameter : parameters) {\n-    TF_RET_CHECK(dim_orders.at(parameter).IsPhysicallyEquivalent(\n-        dim_orders.at(*parameters.cbegin())));\n+    TF_RET_CHECK(dim_orders_.at(parameter).IsPhysicallyEquivalent(\n+        dim_orders_.at(*parameters.cbegin())));\n     iter_specs[parameter] =\n-        DimensionOrderToTensorIterationSpec(dim_orders.at(parameter));\n+        DimensionOrderToTensorIterationSpec(dim_orders_.at(parameter));\n   }\n   return OkStatus();\n }\n@@ -1575,49 +1596,40 @@ Status DotFusionAnalysis::ExecuteImpl(const HloComputation* computation,\n   const HloInstruction* dot =\n       hlo_query::GetFirstInstructionWithOpcode(*computation, HloOpcode::kDot);\n \n+  int64_t lhs_nc_split_major_part_size = -1;\n   for (const Scope scope : {Scope::LHS, Scope::RHS}) {\n     const int operand_number = static_cast<int>(scope);\n-    TF_RETURN_IF_ERROR(PropagateDimensionOrdersToParameters(\n-        *dot->operand(operand_number),\n-        DimensionOrder::FromDotOperand(*dot, operand_number, split_k),\n-        parameters_[scope], iter_specs_[scope]));\n-  }\n-\n-  int64_t lhs_nc_split_major_part_size = -1;\n-  if (!ScopeParameters(Scope::LHS).empty()) {\n-    const DimIterationSpec* lhs_nc_iter_spec =\n-        IterSpec(Scope::LHS, *ScopeParameters(Scope::LHS).cbegin(),\n-                 NonContractingDimensionIndex(*dot, 0));\n-    if (lhs_nc_iter_spec != nullptr && lhs_nc_iter_spec->size() > 1) {\n-      lhs_nc_split_major_part_size = lhs_nc_iter_spec->at(1).count;\n+    auto context = FusionContext::FromDotOperand(*dot, operand_number, split_k);\n+    TF_RETURN_IF_ERROR(context.PropagateDimensionOrdersToParameters(\n+        *dot->operand(operand_number), parameters_[scope], iter_specs_[scope]));\n+    if (scope == Scope::LHS && context.SplittableDimensionMajorPartSize() > 1) {\n+      lhs_nc_split_major_part_size = context.SplittableDimensionMajorPartSize();\n     }\n   }\n-  DimOrderMap dim_orders;\n-  dim_orders.insert({dot, DimensionOrder::FromDotOutput(\n-                              *dot, split_k, lhs_nc_split_major_part_size)});\n+\n+  auto context =\n+      FusionContext::FromDotOutput(*dot, split_k, lhs_nc_split_major_part_size);\n   const HloInstruction* output = dot;\n   // Currently supported is one fusion output and one path from dot to it.\n   // Propagate dimension order from dot to root.\n   while (!output->IsRoot()) {\n     TF_RET_CHECK(output->user_count() == 1);\n     output = output->users()[0];\n-    auto result = HandleInstruction(output, dim_orders,\n+    auto result = HandleInstruction(output, context.DimOrders(),\n                                     TransformDirection::kInputToOutput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderMap>(result));\n-    TF_RET_CHECK(RequireTritonGemmSupportedDimOrder(\n-        std::get<DimOrderMap>(result).at(output)));\n-    TF_RET_CHECK(\n-        MergeDimOrderMapUpdates(dim_orders, std::get<DimOrderMap>(result)));\n+    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n+    TF_RET_CHECK(context.RequireTritonGemmSupportedDimOrders(\n+        *output, std::get<DimOrderUpdates>(result)));\n+    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n   }\n   TF_RET_CHECK(iter_specs_[Scope::OUTPUT]\n                    .insert({output, DimensionOrderToTensorIterationSpec(\n-                                        dim_orders.at(output))})\n+                                        context.DimOrders().at(output))})\n                    .second);\n   if (output != dot) {\n     // Propagate back to parameters of the output fusion.\n-    TF_RETURN_IF_ERROR(PropagateDimensionOrdersToParameters(\n-        *output, dim_orders.at(output), parameters_[Scope::OUTPUT],\n-        iter_specs_[Scope::OUTPUT]));\n+    TF_RETURN_IF_ERROR(context.PropagateDimensionOrdersToParameters(\n+        *output, parameters_[Scope::OUTPUT], iter_specs_[Scope::OUTPUT]));\n   }\n   return OkStatus();\n }"
        }
    ]
},
{
    "Id": 355,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/3a8b4ffe6f1402b967709d2fc7120f808c5c198c",
    "date": "2023-07-13T15:59:53-07:00",
    "message": "PR #3980: [python:xla_extension] Handle unbounded recursion in cyclical PyTrees\n\nImported from GitHub PR https://github.com/openxla/xla/pull/3980\n\nProvides more graceful handling of an unbounded recursion error that can occur when attempting to flatten a PyTree with cyclical node references.\n\nExample from https://github.com/google/jax/issues/15711:\n```Python\nimport jax.tree_util as jtu\na = []\na.append(a)\njtu.tree_flatten(a)\n# \u201cpython\u201d terminated by signal SIGSEGV (Address boundary error)\n```\n\nWith this pull, the above snippet now returns the error message:\n```Python\nRecursionError: maximum recursion depth exceeded in flatten; PyTree may have cyclical node references.\n```\n\nThe maximum recursion depth before the error is throw, is controlled by the Python interpreter.\nCopybara import of the project:\n\n--\n73f9a28d3dd69025a9e90815183dc79e0dddcc0a by tttc3 <T.Coxon2@lboro.ac.uk>:\n\nHandle unbounded recursion in cyclical PyTrees\n\nMerging this change closes #3980\n\nPiperOrigin-RevId: 547950031",
    "label": "YES",
    "changes": [
        {
            "name": "pytree.cc",
            "path": "tensorflow/compiler/xla/python/pytree.cc",
            "patches": [
                {
                    "old_start": 185,
                    "old_length": 7,
                    "new_start": 185,
                    "new_length": 12,
                    "hunk": "@@ -185,7 +185,12 @@ void PyTreeDef::FlattenIntoImpl(\n   } else {\n     node.kind = GetKind(handle, &node.custom);\n     auto recurse = [this, &leaf_predicate, &leaves](py::handle child) {\n+      if (Py_EnterRecursiveCall(\n+              \" in flatten; PyTree may have cyclical node references.\")) {\n+        return;\n+      }\n       FlattenInto(child, leaves, leaf_predicate);\n+      Py_LeaveRecursiveCall();\n     };\n     switch (node.kind) {\n       case PyTreeKind::kNone:\n"
                },
                {
                    "old_start": 265,
                    "old_length": 6,
                    "new_start": 270,
                    "new_length": 11,
                    "hunk": "@@ -265,6 +270,11 @@ PyTreeDef::Flatten(py::handle x, std::optional<py::function> leaf_predicate) {\n   std::vector<py::object> leaves;\n   auto tree = std::make_unique<PyTreeDef>();\n   tree->FlattenInto(x, leaves, leaf_predicate);\n+  // Handle the unbounded recursion error for trees with cyclical node\n+  // references.\n+  if (PyErr_Occurred()) {\n+    throw py::error_already_set();\n+  }\n   return std::make_pair(std::move(leaves), std::move(tree));\n }\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      if (Py_EnterRecursiveCall(\n+              \" in flatten; PyTree may have cyclical node references.\")) {\n+        return;\n+      }\n+      Py_LeaveRecursiveCall();\n+  // Handle the unbounded recursion error for trees with cyclical node\n+  // references.\n+  if (PyErr_Occurred()) {\n+    throw py::error_already_set();\n+  }\n",
            "whole_hunk": "@@ -185,7 +185,12 @@ void PyTreeDef::FlattenIntoImpl(\n   } else {\n     node.kind = GetKind(handle, &node.custom);\n     auto recurse = [this, &leaf_predicate, &leaves](py::handle child) {\n+      if (Py_EnterRecursiveCall(\n+              \" in flatten; PyTree may have cyclical node references.\")) {\n+        return;\n+      }\n       FlattenInto(child, leaves, leaf_predicate);\n+      Py_LeaveRecursiveCall();\n     };\n     switch (node.kind) {\n       case PyTreeKind::kNone:\n@@ -265,6 +270,11 @@ PyTreeDef::Flatten(py::handle x, std::optional<py::function> leaf_predicate) {\n   std::vector<py::object> leaves;\n   auto tree = std::make_unique<PyTreeDef>();\n   tree->FlattenInto(x, leaves, leaf_predicate);\n+  // Handle the unbounded recursion error for trees with cyclical node\n+  // references.\n+  if (PyErr_Occurred()) {\n+    throw py::error_already_set();\n+  }\n   return std::make_pair(std::move(leaves), std::move(tree));\n }\n \n"
        },
        {
            "name": "xla_client.py",
            "path": "tensorflow/compiler/xla/python/xla_client.py",
            "patches": [
                {
                    "old_start": 44,
                    "old_length": 7,
                    "new_start": 44,
                    "new_length": 7,
                    "hunk": "@@ -44,7 +44,7 @@ profiler = _xla.profiler\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes.\n-_version = 166\n+_version = 167\n \n # Version number for MLIR:Python components.\n mlir_api_version = 52"
                }
            ],
            "whole_deleted": "-_version = 166\n",
            "whole_added": "+_version = 167\n",
            "whole_hunk": "@@ -44,7 +44,7 @@ profiler = _xla.profiler\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes.\n-_version = 166\n+_version = 167\n \n # Version number for MLIR:Python components.\n mlir_api_version = 52"
        }
    ]
},
{
    "Id": 604,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/662128e8ca3411286b234553a7efc1356353d0f5",
    "date": "2022-12-22T09:22:55+08:00",
    "message": "add rank checking for MEAN op\n\nThe MEAN op of NNAPI only supports a tensor with rank <= 4.\nCheck the rank of the input tensor before delegating the op.\n\n```\n...\n12-22 09:22:24.514  6130  6130 E ModelBuilder: Invalid Operation: NN_RET_CHECK failed (packages/modules/NeuralNetworks/common/types/operations/src/SimpleMath.cpp:30): inputRank <= 4u (inputRank = 5, 4u = 4) Unsupported input tensor rank for operation MEAN\n12-22 09:22:24.514  6130  6130 E tflite  : NN API returned error ANEURALNETWORKS_BAD_DATA at line 1131 while adding operation.\n12-22 09:22:24.515  6130  6130 E tflite  : Restored original execution plan after delegate application failure.\n...\n```",
    "label": "YES",
    "changes": [
        {
            "name": "nnapi_delegate.cc",
            "path": "tensorflow/lite/delegates/nnapi/nnapi_delegate.cc",
            "patches": [
                {
                    "old_start": 3075,
                    "old_length": 6,
                    "new_start": 3075,
                    "new_length": 10,
                    "hunk": "@@ -3075,6 +3075,10 @@ bool NNAPIDelegateKernel::Validate(\n              NNAPIValidationFailureType::kUnsupportedOutputType,\n              \"NNAPI does not support generating a scalar as output for MEAN.\",\n              &val_ctx);\n+      Expect(context->tensors[node->inputs->data[0]].dims->size <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandValue,\n+             \"NNAPI does not support mean of a tensor with rank > 4\",\n+             &val_ctx);\n     } break;\n     case kTfLiteBuiltinEmbeddingLookup: {\n       ExpectOpVersion(version, 1, &val_ctx);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      Expect(context->tensors[node->inputs->data[0]].dims->size <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandValue,\n+             \"NNAPI does not support mean of a tensor with rank > 4\",\n+             &val_ctx);\n",
            "whole_hunk": "@@ -3075,6 +3075,10 @@ bool NNAPIDelegateKernel::Validate(\n              NNAPIValidationFailureType::kUnsupportedOutputType,\n              \"NNAPI does not support generating a scalar as output for MEAN.\",\n              &val_ctx);\n+      Expect(context->tensors[node->inputs->data[0]].dims->size <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandValue,\n+             \"NNAPI does not support mean of a tensor with rank > 4\",\n+             &val_ctx);\n     } break;\n     case kTfLiteBuiltinEmbeddingLookup: {\n       ExpectOpVersion(version, 1, &val_ctx);"
        }
    ]
},
{
    "Id": 521,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1b0db88af3638acb44a96332802ab005c616e8a8",
    "date": "2023-02-17T20:20:22-08:00",
    "message": "Return invalid argument status when input validation fails.\n\nPiperOrigin-RevId: 510582268",
    "label": "NO",
    "changes": [
        {
            "name": "full_type_util.cc",
            "path": "tensorflow/core/framework/full_type_util.cc",
            "patches": [
                {
                    "old_start": 144,
                    "old_length": 7,
                    "new_start": 144,
                    "new_length": 12,
                    "hunk": "@@ -144,7 +144,12 @@ typedef absl::flat_hash_map<StringPiece, const AttrValue*> AttrMap;\n inline Status SubstituteFromAttrs(AttrMap& attrs, FullTypeDef& t);\n \n Status SubstituteVar(AttrMap& attrs, FullTypeDef& t) {\n-  DCHECK_EQ(t.args_size(), 0);\n+  if (t.args_size() != 0) {\n+    return Status(\n+        error::INVALID_ARGUMENT,\n+        absl::StrCat(\"Unexpected Var type, expected args_size 0, found \",\n+                     t.args_size()));\n+  }\n \n   StringPiece var_name = t.s();\n   if (!attrs.contains(var_name)) {\n"
                }
            ],
            "whole_deleted": "-  DCHECK_EQ(t.args_size(), 0);\n",
            "whole_added": "+  if (t.args_size() != 0) {\n+    return Status(\n+        error::INVALID_ARGUMENT,\n+        absl::StrCat(\"Unexpected Var type, expected args_size 0, found \",\n+                     t.args_size()));\n+  }\n",
            "whole_hunk": "@@ -144,7 +144,12 @@ typedef absl::flat_hash_map<StringPiece, const AttrValue*> AttrMap;\n inline Status SubstituteFromAttrs(AttrMap& attrs, FullTypeDef& t);\n \n Status SubstituteVar(AttrMap& attrs, FullTypeDef& t) {\n-  DCHECK_EQ(t.args_size(), 0);\n+  if (t.args_size() != 0) {\n+    return Status(\n+        error::INVALID_ARGUMENT,\n+        absl::StrCat(\"Unexpected Var type, expected args_size 0, found \",\n+                     t.args_size()));\n+  }\n \n   StringPiece var_name = t.s();\n   if (!attrs.contains(var_name)) {\n"
        },
        {
            "name": "full_type_util_test.cc",
            "path": "tensorflow/core/framework/full_type_util_test.cc",
            "patches": [
                {
                    "old_start": 560,
                    "old_length": 6,
                    "new_start": 560,
                    "new_length": 18,
                    "hunk": "@@ -560,6 +560,18 @@ TEST(SpecializeType, ForEachRejectsMalformedInput) {\n   EXPECT_FALSE(SpecializeType(attrs, op, ft).ok());\n }\n \n+TEST(SpecializeType, VarShouldHaveNoArgs) {\n+  OpDef op;\n+  FullTypeDef* t = op.add_output_arg()->mutable_experimental_full_type();\n+  t->set_type_id(TFT_VAR);\n+  t->add_args()->set_type_id(TFT_PRODUCT);\n+  NodeDef ndef;\n+  AttrSlice attrs(ndef);\n+\n+  FullTypeDef ft;\n+  EXPECT_FALSE(SpecializeType(attrs, op, ft).ok());\n+}\n+\n TEST(SpecializeType, RemovesLegacyVariant) {\n   OpDef op;\n   FullTypeDef* t = op.add_output_arg()->mutable_experimental_full_type();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST(SpecializeType, VarShouldHaveNoArgs) {\n+  OpDef op;\n+  FullTypeDef* t = op.add_output_arg()->mutable_experimental_full_type();\n+  t->set_type_id(TFT_VAR);\n+  t->add_args()->set_type_id(TFT_PRODUCT);\n+  NodeDef ndef;\n+  AttrSlice attrs(ndef);\n+\n+  FullTypeDef ft;\n+  EXPECT_FALSE(SpecializeType(attrs, op, ft).ok());\n+}\n+\n",
            "whole_hunk": "@@ -560,6 +560,18 @@ TEST(SpecializeType, ForEachRejectsMalformedInput) {\n   EXPECT_FALSE(SpecializeType(attrs, op, ft).ok());\n }\n \n+TEST(SpecializeType, VarShouldHaveNoArgs) {\n+  OpDef op;\n+  FullTypeDef* t = op.add_output_arg()->mutable_experimental_full_type();\n+  t->set_type_id(TFT_VAR);\n+  t->add_args()->set_type_id(TFT_PRODUCT);\n+  NodeDef ndef;\n+  AttrSlice attrs(ndef);\n+\n+  FullTypeDef ft;\n+  EXPECT_FALSE(SpecializeType(attrs, op, ft).ok());\n+}\n+\n TEST(SpecializeType, RemovesLegacyVariant) {\n   OpDef op;\n   FullTypeDef* t = op.add_output_arg()->mutable_experimental_full_type();"
        }
    ]
},
{
    "Id": 558,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2a052ebf19897e0dc91b8b18f8e23ecc35e7a5fe",
    "date": "2023-02-01T10:52:35-08:00",
    "message": "Modify LiteralTestUtil to ensure dynamic dimensions are equivalent when checking equality. Previously the LiteralTestUtil would consider two dynamic literals equal as long as they had identical elements (even if they had different dynamic dimensions).\n\nPiperOrigin-RevId: 506359222",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/BUILD",
            "patches": [
                {
                    "old_start": 618,
                    "old_length": 6,
                    "new_start": 618,
                    "new_length": 7,
                    "hunk": "@@ -618,6 +618,7 @@ cc_library(\n         \":error_spec\",\n         \":literal\",\n         \":literal_util\",\n+        \":shape_util\",\n         \":util\",\n         \"//tensorflow/tsl/platform:env\",\n         \"//tensorflow/tsl/platform:float8\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \":shape_util\",\n",
            "whole_hunk": "@@ -618,6 +618,7 @@ cc_library(\n         \":error_spec\",\n         \":literal\",\n         \":literal_util\",\n+        \":shape_util\",\n         \":util\",\n         \"//tensorflow/tsl/platform:env\",\n         \"//tensorflow/tsl/platform:float8\",\n"
        },
        {
            "name": "literal_comparison.cc",
            "path": "tensorflow/compiler/xla/literal_comparison.cc",
            "patches": [
                {
                    "old_start": 27,
                    "old_length": 6,
                    "new_start": 27,
                    "new_length": 7,
                    "hunk": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"tensorflow/compiler/xla/literal_util.h\"\n+#include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n #include \"tensorflow/tsl/platform/float8.h\"\n"
                },
                {
                    "old_start": 746,
                    "old_length": 7,
                    "new_start": 747,
                    "new_length": 11,
                    "hunk": "@@ -746,7 +747,11 @@ constexpr std::array<float, 5> NearComparator<NativeT>::kErrorBucketBounds;\n Status EqualHelper(const LiteralSlice& expected, const LiteralSlice& actual,\n                    const ShapeIndex& shape_index,\n                    const MiscompareCallback& miscompare_callback) {\n-  TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  if (expected.shape().is_static() && actual.shape().is_static()) {\n+    TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  } else {\n+    TF_RETURN_IF_ERROR(EqualDynamicShapesAndDimensions(expected, actual));\n+  }\n \n   Status result;\n   if (expected.shape().IsTuple()) {\n"
                },
                {
                    "old_start": 851,
                    "old_length": 7,
                    "new_start": 856,
                    "new_length": 11,
                    "hunk": "@@ -851,7 +856,11 @@ Status NearHelper(const LiteralSlice& expected, const LiteralSlice& actual,\n                   const ShapeIndex& shape_index, const ErrorSpec& error,\n                   std::optional<bool> detailed_message,\n                   const MiscompareCallback& miscompare_callback) {\n-  TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  if (expected.shape().is_static() && actual.shape().is_static()) {\n+    TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  } else {\n+    TF_RETURN_IF_ERROR(EqualDynamicShapesAndDimensions(expected, actual));\n+  }\n \n   if (expected.shape().IsTuple()) {\n     Status return_status;\n"
                },
                {
                    "old_start": 995,
                    "old_length": 6,
                    "new_start": 1004,
                    "new_length": 53,
                    "hunk": "@@ -995,6 +1004,53 @@ Status EqualShapes(const Shape& expected, const Shape& actual) {\n   return OkStatus();\n }\n \n+Status EqualDynamicShapesAndDimensions(const LiteralSlice& expected,\n+                                       const LiteralSlice& actual) {\n+  TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  return ShapeUtil::ForEachSubshapeWithStatus(\n+      expected.shape(), [&expected, &actual](const Shape& expected_shape,\n+                                             const ShapeIndex& index) {\n+        auto actual_shape = ShapeUtil::GetSubshape(actual.shape(), index);\n+        for (int i = 0; i < expected_shape.dimensions().size(); ++i) {\n+          if (!expected_shape.is_dynamic_dimension(i) &&\n+              !actual_shape.is_dynamic_dimension(i)) {\n+            // We're only interested in dynamic dimensions.\n+            continue;\n+          }\n+          if (expected_shape.is_dynamic_dimension(i) &&\n+              !actual_shape.is_dynamic_dimension(i)) {\n+            return InvalidArgument(\n+                \"mismatch at dimension %d. the expected shape %s is dynamic \"\n+                \"while \"\n+                \"the actual shape %s is not.\",\n+                i, ShapeUtil::HumanString(expected.shape()),\n+                ShapeUtil::HumanString(actual.shape()));\n+          }\n+          if (!expected_shape.is_dynamic_dimension(i) &&\n+              actual_shape.is_dynamic_dimension(i)) {\n+            return InvalidArgument(\n+                \"mismatch at dimension %d. the expected shape %s is not \"\n+                \"dynamic \"\n+                \"while the actual shape %s is dynamic.\",\n+                i, ShapeUtil::HumanString(expected.shape()),\n+                ShapeUtil::HumanString(actual.shape()));\n+          }\n+          // Both dimensions are dynamic. Check that they are equal.\n+          int64_t expected_dynamic_size = expected.GetDynamicSize(i, index);\n+          int64_t actual_dynamic_size = actual.GetDynamicSize(i, index);\n+          if (expected_dynamic_size != actual_dynamic_size) {\n+            return InvalidArgument(\n+                \"mismatch at dimension %d. The expected dynamic size does not \"\n+                \"match \"\n+                \"the actual dynamic size. %d vs. %d\",\n+                i, expected_dynamic_size, actual_dynamic_size);\n+          }\n+        }\n+\n+        return OkStatus();\n+      });\n+}\n+\n namespace {\n \n // If result is an error, extend the error message with the expected and actual\n"
                }
            ],
            "whole_deleted": "-  TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n-  TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n",
            "whole_added": "+#include \"tensorflow/compiler/xla/shape_util.h\"\n+  if (expected.shape().is_static() && actual.shape().is_static()) {\n+    TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  } else {\n+    TF_RETURN_IF_ERROR(EqualDynamicShapesAndDimensions(expected, actual));\n+  }\n+  if (expected.shape().is_static() && actual.shape().is_static()) {\n+    TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  } else {\n+    TF_RETURN_IF_ERROR(EqualDynamicShapesAndDimensions(expected, actual));\n+  }\n+Status EqualDynamicShapesAndDimensions(const LiteralSlice& expected,\n+                                       const LiteralSlice& actual) {\n+  TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  return ShapeUtil::ForEachSubshapeWithStatus(\n+      expected.shape(), [&expected, &actual](const Shape& expected_shape,\n+                                             const ShapeIndex& index) {\n+        auto actual_shape = ShapeUtil::GetSubshape(actual.shape(), index);\n+        for (int i = 0; i < expected_shape.dimensions().size(); ++i) {\n+          if (!expected_shape.is_dynamic_dimension(i) &&\n+              !actual_shape.is_dynamic_dimension(i)) {\n+            // We're only interested in dynamic dimensions.\n+            continue;\n+          }\n+          if (expected_shape.is_dynamic_dimension(i) &&\n+              !actual_shape.is_dynamic_dimension(i)) {\n+            return InvalidArgument(\n+                \"mismatch at dimension %d. the expected shape %s is dynamic \"\n+                \"while \"\n+                \"the actual shape %s is not.\",\n+                i, ShapeUtil::HumanString(expected.shape()),\n+                ShapeUtil::HumanString(actual.shape()));\n+          }\n+          if (!expected_shape.is_dynamic_dimension(i) &&\n+              actual_shape.is_dynamic_dimension(i)) {\n+            return InvalidArgument(\n+                \"mismatch at dimension %d. the expected shape %s is not \"\n+                \"dynamic \"\n+                \"while the actual shape %s is dynamic.\",\n+                i, ShapeUtil::HumanString(expected.shape()),\n+                ShapeUtil::HumanString(actual.shape()));\n+          }\n+          // Both dimensions are dynamic. Check that they are equal.\n+          int64_t expected_dynamic_size = expected.GetDynamicSize(i, index);\n+          int64_t actual_dynamic_size = actual.GetDynamicSize(i, index);\n+          if (expected_dynamic_size != actual_dynamic_size) {\n+            return InvalidArgument(\n+                \"mismatch at dimension %d. The expected dynamic size does not \"\n+                \"match \"\n+                \"the actual dynamic size. %d vs. %d\",\n+                i, expected_dynamic_size, actual_dynamic_size);\n+          }\n+        }\n+\n+        return OkStatus();\n+      });\n+}\n+\n",
            "whole_hunk": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"tensorflow/compiler/xla/literal_util.h\"\n+#include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n #include \"tensorflow/tsl/platform/float8.h\"\n@@ -746,7 +747,11 @@ constexpr std::array<float, 5> NearComparator<NativeT>::kErrorBucketBounds;\n Status EqualHelper(const LiteralSlice& expected, const LiteralSlice& actual,\n                    const ShapeIndex& shape_index,\n                    const MiscompareCallback& miscompare_callback) {\n-  TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  if (expected.shape().is_static() && actual.shape().is_static()) {\n+    TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  } else {\n+    TF_RETURN_IF_ERROR(EqualDynamicShapesAndDimensions(expected, actual));\n+  }\n \n   Status result;\n   if (expected.shape().IsTuple()) {\n@@ -851,7 +856,11 @@ Status NearHelper(const LiteralSlice& expected, const LiteralSlice& actual,\n                   const ShapeIndex& shape_index, const ErrorSpec& error,\n                   std::optional<bool> detailed_message,\n                   const MiscompareCallback& miscompare_callback) {\n-  TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  if (expected.shape().is_static() && actual.shape().is_static()) {\n+    TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  } else {\n+    TF_RETURN_IF_ERROR(EqualDynamicShapesAndDimensions(expected, actual));\n+  }\n \n   if (expected.shape().IsTuple()) {\n     Status return_status;\n@@ -995,6 +1004,53 @@ Status EqualShapes(const Shape& expected, const Shape& actual) {\n   return OkStatus();\n }\n \n+Status EqualDynamicShapesAndDimensions(const LiteralSlice& expected,\n+                                       const LiteralSlice& actual) {\n+  TF_RETURN_IF_ERROR(EqualShapes(expected.shape(), actual.shape()));\n+  return ShapeUtil::ForEachSubshapeWithStatus(\n+      expected.shape(), [&expected, &actual](const Shape& expected_shape,\n+                                             const ShapeIndex& index) {\n+        auto actual_shape = ShapeUtil::GetSubshape(actual.shape(), index);\n+        for (int i = 0; i < expected_shape.dimensions().size(); ++i) {\n+          if (!expected_shape.is_dynamic_dimension(i) &&\n+              !actual_shape.is_dynamic_dimension(i)) {\n+            // We're only interested in dynamic dimensions.\n+            continue;\n+          }\n+          if (expected_shape.is_dynamic_dimension(i) &&\n+              !actual_shape.is_dynamic_dimension(i)) {\n+            return InvalidArgument(\n+                \"mismatch at dimension %d. the expected shape %s is dynamic \"\n+                \"while \"\n+                \"the actual shape %s is not.\",\n+                i, ShapeUtil::HumanString(expected.shape()),\n+                ShapeUtil::HumanString(actual.shape()));\n+          }\n+          if (!expected_shape.is_dynamic_dimension(i) &&\n+              actual_shape.is_dynamic_dimension(i)) {\n+            return InvalidArgument(\n+                \"mismatch at dimension %d. the expected shape %s is not \"\n+                \"dynamic \"\n+                \"while the actual shape %s is dynamic.\",\n+                i, ShapeUtil::HumanString(expected.shape()),\n+                ShapeUtil::HumanString(actual.shape()));\n+          }\n+          // Both dimensions are dynamic. Check that they are equal.\n+          int64_t expected_dynamic_size = expected.GetDynamicSize(i, index);\n+          int64_t actual_dynamic_size = actual.GetDynamicSize(i, index);\n+          if (expected_dynamic_size != actual_dynamic_size) {\n+            return InvalidArgument(\n+                \"mismatch at dimension %d. The expected dynamic size does not \"\n+                \"match \"\n+                \"the actual dynamic size. %d vs. %d\",\n+                i, expected_dynamic_size, actual_dynamic_size);\n+          }\n+        }\n+\n+        return OkStatus();\n+      });\n+}\n+\n namespace {\n \n // If result is an error, extend the error message with the expected and actual\n"
        },
        {
            "name": "literal_comparison.h",
            "path": "tensorflow/compiler/xla/literal_comparison.h",
            "patches": [
                {
                    "old_start": 30,
                    "old_length": 6,
                    "new_start": 30,
                    "new_length": 11,
                    "hunk": "@@ -30,6 +30,11 @@ namespace literal_comparison {\n // primitive types.\n Status EqualShapes(const Shape& expected, const Shape& actual);\n \n+// Returns ok if the given literals share identical dynamic shapes and\n+// dimension sizes.\n+Status EqualDynamicShapesAndDimensions(const LiteralSlice& expected,\n+                                       const LiteralSlice& actual);\n+\n // Returns ok if the expected and actual literals are (bitwise) equal for all\n // elements in the literal. Also, asserts that the rank, dimensions sizes, and\n // primitive type are equal.\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// Returns ok if the given literals share identical dynamic shapes and\n+// dimension sizes.\n+Status EqualDynamicShapesAndDimensions(const LiteralSlice& expected,\n+                                       const LiteralSlice& actual);\n+\n",
            "whole_hunk": "@@ -30,6 +30,11 @@ namespace literal_comparison {\n // primitive types.\n Status EqualShapes(const Shape& expected, const Shape& actual);\n \n+// Returns ok if the given literals share identical dynamic shapes and\n+// dimension sizes.\n+Status EqualDynamicShapesAndDimensions(const LiteralSlice& expected,\n+                                       const LiteralSlice& actual);\n+\n // Returns ok if the expected and actual literals are (bitwise) equal for all\n // elements in the literal. Also, asserts that the rank, dimensions sizes, and\n // primitive type are equal.\n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/tests/BUILD",
            "patches": [
                {
                    "old_start": 2651,
                    "old_length": 8,
                    "new_start": 2651,
                    "new_length": 9,
                    "hunk": "@@ -2651,8 +2651,9 @@ xla_cc_test(\n     name = \"literal_test_util_test\",\n     srcs = [\"literal_test_util_test.cc\"],\n     deps = [\n+        \":literal_test_util\",\n+        \"//tensorflow/compiler/xla:literal\",\n         \"//tensorflow/compiler/xla:test_helpers\",\n-        \"//tensorflow/compiler/xla/tests:literal_test_util\",\n         \"//tensorflow/tsl/platform:env\",\n         \"//tensorflow/tsl/platform:logging\",\n         \"//tensorflow/tsl/platform:path\",\n"
                }
            ],
            "whole_deleted": "-        \"//tensorflow/compiler/xla/tests:literal_test_util\",\n",
            "whole_added": "+        \":literal_test_util\",\n+        \"//tensorflow/compiler/xla:literal\",\n",
            "whole_hunk": "@@ -2651,8 +2651,9 @@ xla_cc_test(\n     name = \"literal_test_util_test\",\n     srcs = [\"literal_test_util_test.cc\"],\n     deps = [\n+        \":literal_test_util\",\n+        \"//tensorflow/compiler/xla:literal\",\n         \"//tensorflow/compiler/xla:test_helpers\",\n-        \"//tensorflow/compiler/xla/tests:literal_test_util\",\n         \"//tensorflow/tsl/platform:env\",\n         \"//tensorflow/tsl/platform:logging\",\n         \"//tensorflow/tsl/platform:path\",\n"
        },
        {
            "name": "literal_test_util_test.cc",
            "path": "tensorflow/compiler/xla/tests/literal_test_util_test.cc",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 6,
                    "new_start": 21,
                    "new_length": 7,
                    "hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/strings/str_join.h\"\n+#include \"tensorflow/compiler/xla/literal.h\"\n #include \"tensorflow/compiler/xla/test_helpers.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n #include \"tensorflow/tsl/platform/logging.h\"\n"
                },
                {
                    "old_start": 357,
                    "old_length": 5,
                    "new_start": 358,
                    "new_length": 278,
                    "hunk": "@@ -357,5 +358,278 @@ TEST(LiteralTestUtilTest, DynamicNearEqualityR2Dim1) {\n   EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n }\n \n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal1.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal2.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 6);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 6);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, ExpectedIsDynamicActualIsNotR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal1.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal2.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  // Only literal1 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, ExpectedIsDynamicActualIsNotR1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  // Only literal1 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, ActualIsDynamicExpectedIsNotR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal1.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal2.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 5);\n+  // Only literal2 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, ActualIsDynamicExpectedIsNotR1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 5);\n+  // Only literal2 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim0) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal1.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(0, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal2.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim0_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(0, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal1.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal2.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(1, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(1, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2DifferentDimensions) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal1.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal2.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 2);\n+  // Different dimensions were set as dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2DifferentDimensions_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 2);\n+  // Different dimensions were set as dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreEqual) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 5);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreNear) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 5);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreEqualWithinDynamicBounds) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 3);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 99, 99});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 3);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreNearWithinDynamicBounds) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 3);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 99, 99});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 3);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesHaveDifferentDynamicSizes) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 4);\n+  // Dynamic sizes are not equal.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesHaveDifferentDynamicSizes_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 4);\n+  // Dynamic sizes are not equal.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, OneTupleDynamicOneIsNot) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  // Only one of the tuples is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, OneTupleDynamicOneIsNot_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  // Only one of the tuples is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n }  // namespace\n }  // namespace xla"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"tensorflow/compiler/xla/literal.h\"\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal1.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal2.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 6);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 6);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, ExpectedIsDynamicActualIsNotR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal1.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal2.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  // Only literal1 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, ExpectedIsDynamicActualIsNotR1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  // Only literal1 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, ActualIsDynamicExpectedIsNotR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal1.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal2.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 5);\n+  // Only literal2 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, ActualIsDynamicExpectedIsNotR1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 5);\n+  // Only literal2 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim0) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal1.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(0, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal2.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim0_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(0, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal1.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal2.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(1, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(1, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2DifferentDimensions) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal1.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal2.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 2);\n+  // Different dimensions were set as dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2DifferentDimensions_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 2);\n+  // Different dimensions were set as dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreEqual) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 5);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreNear) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 5);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreEqualWithinDynamicBounds) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 3);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 99, 99});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 3);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreNearWithinDynamicBounds) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 3);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 99, 99});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 3);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesHaveDifferentDynamicSizes) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 4);\n+  // Dynamic sizes are not equal.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesHaveDifferentDynamicSizes_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 4);\n+  // Dynamic sizes are not equal.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, OneTupleDynamicOneIsNot) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  // Only one of the tuples is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, OneTupleDynamicOneIsNot_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  // Only one of the tuples is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n",
            "whole_hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/strings/str_join.h\"\n+#include \"tensorflow/compiler/xla/literal.h\"\n #include \"tensorflow/compiler/xla/test_helpers.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n #include \"tensorflow/tsl/platform/logging.h\"\n@@ -357,5 +358,278 @@ TEST(LiteralTestUtilTest, DynamicNearEqualityR2Dim1) {\n   EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n }\n \n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal1.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal2.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 6);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 6);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, ExpectedIsDynamicActualIsNotR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal1.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal2.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  // Only literal1 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, ExpectedIsDynamicActualIsNotR1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  // Only literal1 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, ActualIsDynamicExpectedIsNotR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal1.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {10}));\n+  literal2.PopulateR1<uint32_t>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 5);\n+  // Only literal2 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, ActualIsDynamicExpectedIsNotR1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal2.SetDynamicSize(0, 5);\n+  // Only literal2 is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim0) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal1.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(0, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal2.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim0_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(0, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal1.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal2.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(1, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2Dim1_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(1, 3);\n+  // Dynamic sizes do not match.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2DifferentDimensions) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal1.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(U32, {3, 3}));\n+  literal2.PopulateR2<uint32_t>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 2);\n+  // Different dimensions were set as dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, UnequalDynamicDimensionsR2DifferentDimensions_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal2.SetDynamicSize(0, 2);\n+  // Different dimensions were set as dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreEqual) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 5);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreNear) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 5);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreEqualWithinDynamicBounds) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 3);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 99, 99});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 3);\n+  EXPECT_TRUE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesAreNearWithinDynamicBounds) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 3);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 99, 99});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 3);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesHaveDifferentDynamicSizes) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 4);\n+  // Dynamic sizes are not equal.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicTuplesHaveDifferentDynamicSizes_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal2.SetDynamicSize(0, {0}, 4);\n+  // Dynamic sizes are not equal.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n+\n+TEST(LiteralTestUtilTest, OneTupleDynamicOneIsNot) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(U32, {5}), ShapeUtil::MakeShape(U32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<uint32_t>({1, 2, 3, 4, 5});\n+  // Only one of the tuples is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Equal(literal1, literal2));\n+}\n+\n+TEST(LiteralTestUtilTest, OneTupleDynamicOneIsNot_F32) {\n+  auto literal1 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  auto literal2 = Literal(ShapeUtil::MakeTupleShape(\n+      {ShapeUtil::MakeShape(F32, {5}), ShapeUtil::MakeShape(F32, {5})}));\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal1, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  literal1.SetDynamicSize(0, {0}, 5);\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{0})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  MutableBorrowingLiteral(&literal2, /*view_root=*/{1})\n+      .PopulateR1<float>({1, 2, 3, 4, 5});\n+  // Only one of the tuples is dynamic.\n+  EXPECT_FALSE(LiteralTestUtil::Near(literal1, literal2, ErrorSpec{0.0001}));\n+}\n }  // namespace\n }  // namespace xla"
        }
    ]
},
{
    "Id": 132,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fdf0f5d3e67ccd6064ac09256ab78d7031d2d38d",
    "date": "2024-03-01T12:39:55+05:30",
    "message": "Fix checkfail in Conv2DBackpropFilter\n\nThe API tf.raw_ops.Conv2DBackpropFilter not have a check for strides size which may be causing checkfail as reported in #63076.",
    "label": "YES",
    "changes": [
        {
            "name": "conv_grad_filter_ops.cc",
            "path": "tensorflow/core/kernels/conv_grad_filter_ops.cc",
            "patches": [
                {
                    "old_start": 114,
                    "old_length": 6,
                    "new_start": 114,
                    "new_length": 9,
                    "hunk": "@@ -114,6 +114,9 @@ class Conv2DBackpropFilterOp : public OpKernel {\n     OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                 errors::InvalidArgument(\"Invalid data format\"));\n     OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides_));\n+    OP_REQUIRES(context, strides_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window strides field must \"\n+                                        \"specify 4 dimensions\"));\n     int stride_n = GetTensorDim(strides_, data_format_, 'N');\n     int stride_c = GetTensorDim(strides_, data_format_, 'C');\n     int stride_h = GetTensorDim(strides_, data_format_, 'H');"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    OP_REQUIRES(context, strides_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window strides field must \"\n+                                        \"specify 4 dimensions\"));\n",
            "whole_hunk": "@@ -114,6 +114,9 @@ class Conv2DBackpropFilterOp : public OpKernel {\n     OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                 errors::InvalidArgument(\"Invalid data format\"));\n     OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides_));\n+    OP_REQUIRES(context, strides_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window strides field must \"\n+                                        \"specify 4 dimensions\"));\n     int stride_n = GetTensorDim(strides_, data_format_, 'N');\n     int stride_c = GetTensorDim(strides_, data_format_, 'C');\n     int stride_h = GetTensorDim(strides_, data_format_, 'H');"
        }
    ]
},
{
    "Id": 131,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2b164c8cd6b5ae3dd6c664127ebdf1104836eeda",
    "date": "2024-03-04T20:34:41+05:30",
    "message": "Fix checkfail in DenseBincount\n\nThe API raw_ops.DenseBincount lacks validation of input to be vector. It does have checking for rank<=2 but not for rank>0.\r\nPassing a scalar value causes checkfail with debug build.\r\n\r\nReported at #63068",
    "label": "YES",
    "changes": [
        {
            "name": "math_ops.cc",
            "path": "tensorflow/core/ops/math_ops.cc",
            "patches": [
                {
                    "old_start": 1896,
                    "old_length": 6,
                    "new_start": 1896,
                    "new_length": 9,
                    "hunk": "@@ -1896,6 +1896,9 @@ REGISTER_OP(\"DenseBincount\")\n         c->set_output(0, c->MakeShape({size_val}));\n       } else if (c->Rank(c->input(0)) == 2) {\n         c->set_output(0, c->MakeShape({c->Dim(c->input(0), 0), size_val}));\n+      } else {\n+        return errors::InvalidArgument(\"input must not be a scalar. \"\n+            \"Recieved input of rank \", c->Rank(c->input(0)));\n       }\n       return absl::OkStatus();\n     });"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      } else {\n+        return errors::InvalidArgument(\"input must not be a scalar. \"\n+            \"Recieved input of rank \", c->Rank(c->input(0)));\n",
            "whole_hunk": "@@ -1896,6 +1896,9 @@ REGISTER_OP(\"DenseBincount\")\n         c->set_output(0, c->MakeShape({size_val}));\n       } else if (c->Rank(c->input(0)) == 2) {\n         c->set_output(0, c->MakeShape({c->Dim(c->input(0), 0), size_val}));\n+      } else {\n+        return errors::InvalidArgument(\"input must not be a scalar. \"\n+            \"Recieved input of rank \", c->Rank(c->input(0)));\n       }\n       return absl::OkStatus();\n     });"
        }
    ]
},
{
    "Id": 81,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a773acefff893ea3435c0c2eb763ddef21408db5",
    "date": "2024-04-07T19:58:17-07:00",
    "message": "[xla] hlo_computation: drop instruction_indices_\n\nIt's redundant given that we have HloInstruction.index_in_parent_.\nThere is only one caller (a TF_RET_CHECK) that needs it; I think\nwe can survive without that check.\n\nThis reduces the size of the struct from 176 to 152 bytes.\n\nWhile at it, move to_be_deleted_ up in the struct so that\ninstructions_, instruction_count_ and to_be_deleted_ are now\ncontiguous.\n\nPiperOrigin-RevId: 622709022",
    "label": "NO",
    "changes": [
        {
            "name": "hlo_computation.cc",
            "path": "third_party/xla/xla/hlo/ir/hlo_computation.cc",
            "patches": [
                {
                    "old_start": 136,
                    "old_length": 6,
                    "new_start": 136,
                    "new_length": 7,
                    "hunk": "@@ -136,6 +136,7 @@ HloComputation::HloComputation(\n     HloInstruction* root_instruction)\n     : unique_id_(-1),\n       root_instruction_(root_instruction),\n+      instruction_count_(0),\n       name_(NameUniquer::GetSanitizedName(name)) {\n   param_instructions_.resize(parameter_count, nullptr);\n   bool root_found = false;\n"
                },
                {
                    "old_start": 228,
                    "old_length": 7,
                    "new_start": 229,
                    "new_length": 7,
                    "hunk": "@@ -228,7 +229,7 @@ HloInstruction* HloComputation::AddInstructionInternal(\n   VLOG(2) << \"Adding instruction \" << pinst << \" \" << pinst->name()\n           << \" from computation \" << name() << \" opcode \" << info.opcode();\n   uint32_t index = instructions_.size();\n-  instruction_indices_[pinst] = index;\n+  instruction_count_++;\n   pinst->index_in_parent_ = index;\n   instructions_.push_back(info);\n   return pinst;\n"
                },
                {
                    "old_start": 453,
                    "old_length": 9,
                    "new_start": 454,
                    "new_length": 8,
                    "hunk": "@@ -453,9 +454,8 @@ Status HloComputation::RemoveInstructionImpl(HloInstruction* instruction,\n       << \"instruction \" << instruction->name()\n       << \" has control successors and cannot be removed\";\n \n-  auto inst_it = instruction_indices_.find(instruction);\n-  TF_RET_CHECK(inst_it != instruction_indices_.end());\n-  HloInstructionInfo* info = &instructions_[inst_it->second];\n+  HloInstructionInfo* info = &instructions_[instruction->index_in_parent_];\n+  DCHECK_EQ(info->inst(), instruction);\n   info->inst()->set_parent(nullptr);\n   to_be_deleted_.push_back(info->inst());  // Takes ownership\n   to_be_deleted_.back()->DetachFromOperandsAndUsers();\n"
                },
                {
                    "old_start": 471,
                    "old_length": 8,
                    "new_start": 471,
                    "new_length": 8,
                    "hunk": "@@ -471,8 +471,8 @@ Status HloComputation::RemoveInstructionImpl(HloInstruction* instruction,\n   // TODO(jeff): should we set info->opcode to something?\n   info->inst_ =\n       nullptr;  // Leave a hole: this is no longer part of \"instructions()\"\n-  instruction_indices_.erase(inst_it);\n   instruction->index_in_parent_ = ~0u;\n+  instruction_count_--;\n   return OkStatus();\n }\n \n"
                },
                {
                    "old_start": 647,
                    "old_length": 7,
                    "new_start": 647,
                    "new_length": 7,
                    "hunk": "@@ -647,7 +647,7 @@ std::vector<HloInstruction*> HloComputation::MakeInstructionPostOrder(\n                                   post_order, &dfs_stack_scratch);\n     }\n   }\n-  CHECK_EQ(instruction_indices_.size(), post_order.size())\n+  CHECK_EQ(instruction_count(), post_order.size())\n       << \"number of instructions does not match post order size\";\n   return post_order;\n }\n"
                }
            ],
            "whole_deleted": "-  instruction_indices_[pinst] = index;\n-  auto inst_it = instruction_indices_.find(instruction);\n-  TF_RET_CHECK(inst_it != instruction_indices_.end());\n-  HloInstructionInfo* info = &instructions_[inst_it->second];\n-  instruction_indices_.erase(inst_it);\n-  CHECK_EQ(instruction_indices_.size(), post_order.size())\n",
            "whole_added": "+      instruction_count_(0),\n+  instruction_count_++;\n+  HloInstructionInfo* info = &instructions_[instruction->index_in_parent_];\n+  DCHECK_EQ(info->inst(), instruction);\n+  instruction_count_--;\n+  CHECK_EQ(instruction_count(), post_order.size())\n",
            "whole_hunk": "@@ -136,6 +136,7 @@ HloComputation::HloComputation(\n     HloInstruction* root_instruction)\n     : unique_id_(-1),\n       root_instruction_(root_instruction),\n+      instruction_count_(0),\n       name_(NameUniquer::GetSanitizedName(name)) {\n   param_instructions_.resize(parameter_count, nullptr);\n   bool root_found = false;\n@@ -228,7 +229,7 @@ HloInstruction* HloComputation::AddInstructionInternal(\n   VLOG(2) << \"Adding instruction \" << pinst << \" \" << pinst->name()\n           << \" from computation \" << name() << \" opcode \" << info.opcode();\n   uint32_t index = instructions_.size();\n-  instruction_indices_[pinst] = index;\n+  instruction_count_++;\n   pinst->index_in_parent_ = index;\n   instructions_.push_back(info);\n   return pinst;\n@@ -453,9 +454,8 @@ Status HloComputation::RemoveInstructionImpl(HloInstruction* instruction,\n       << \"instruction \" << instruction->name()\n       << \" has control successors and cannot be removed\";\n \n-  auto inst_it = instruction_indices_.find(instruction);\n-  TF_RET_CHECK(inst_it != instruction_indices_.end());\n-  HloInstructionInfo* info = &instructions_[inst_it->second];\n+  HloInstructionInfo* info = &instructions_[instruction->index_in_parent_];\n+  DCHECK_EQ(info->inst(), instruction);\n   info->inst()->set_parent(nullptr);\n   to_be_deleted_.push_back(info->inst());  // Takes ownership\n   to_be_deleted_.back()->DetachFromOperandsAndUsers();\n@@ -471,8 +471,8 @@ Status HloComputation::RemoveInstructionImpl(HloInstruction* instruction,\n   // TODO(jeff): should we set info->opcode to something?\n   info->inst_ =\n       nullptr;  // Leave a hole: this is no longer part of \"instructions()\"\n-  instruction_indices_.erase(inst_it);\n   instruction->index_in_parent_ = ~0u;\n+  instruction_count_--;\n   return OkStatus();\n }\n \n@@ -647,7 +647,7 @@ std::vector<HloInstruction*> HloComputation::MakeInstructionPostOrder(\n                                   post_order, &dfs_stack_scratch);\n     }\n   }\n-  CHECK_EQ(instruction_indices_.size(), post_order.size())\n+  CHECK_EQ(instruction_count(), post_order.size())\n       << \"number of instructions does not match post order size\";\n   return post_order;\n }\n"
        },
        {
            "name": "hlo_computation.h",
            "path": "third_party/xla/xla/hlo/ir/hlo_computation.h",
            "patches": [
                {
                    "old_start": 439,
                    "old_length": 7,
                    "new_start": 439,
                    "new_length": 7,
                    "hunk": "@@ -439,7 +439,7 @@ class HloComputation {\n   void ForEachInstructionPostOrder(\n       absl::FunctionRef<void(HloInstruction*)> func) const;\n \n-  int64_t instruction_count() const { return instruction_indices_.size(); }\n+  int64_t instruction_count() const { return instruction_count_; }\n \n   // Creates and returns a list of the embedded computations called by this\n   // computation. This includes all embedded computations called directly or\n"
                },
                {
                    "old_start": 961,
                    "old_length": 18,
                    "new_start": 961,
                    "new_length": 20,
                    "hunk": "@@ -961,18 +961,20 @@ class HloComputation {\n   HloInstruction::InstructionVector param_instructions_;\n \n   // Store instructions in std::vector as they can be added and removed\n-  // arbitrarily and we want a stable iteration order. Keep a map from\n-  // instruction pointer to index in the vector for fast lookup.\n+  // arbitrarily and we want a stable iteration order.\n+  // For the reverse mapping we use HloInstruction::index_in_parent_.\n   HloInstructionList instructions_;\n-  absl::flat_hash_map<const HloInstruction*, int> instruction_indices_;\n \n-  // Execution thread of this computation. By default, it's main thread.\n-  std::string execution_thread_ = HloInstruction::kMainExecutionThread;\n+  // Number of not-marked-for-deletion entries in instructions_.\n+  int64_t instruction_count_;\n \n   // Removed instructions are moved into to_be_deleted_ first and then\n   // deallocated when Cleanup is called.\n   PtrVec<HloInstruction*> to_be_deleted_;\n \n+  // Execution thread of this computation. By default, it's main thread.\n+  std::string execution_thread_ = HloInstruction::kMainExecutionThread;\n+\n   std::string name_;\n \n   HloComputation(const HloComputation&) = delete;\n"
                },
                {
                    "old_start": 1010,
                    "old_length": 9,
                    "new_start": 1012,
                    "new_length": 6,
                    "hunk": "@@ -1010,9 +1012,6 @@ Status HloComputation::AcceptOrdered(\n   absl::flat_hash_set<const HloInstruction*> visited;\n   for (const HloInstruction* instruction : order) {\n     VLOG(3) << \"Visiting ordered: \" << instruction->ToString();\n-    TF_RET_CHECK(instruction_indices_.contains(instruction))\n-        << \"Instruction \" << instruction->name() << \" is not in computation \"\n-        << name();\n     TF_RET_CHECK(!visited.contains(instruction))\n         << \"Instruction \" << instruction->name()\n         << \" appears more than once in order\";"
                }
            ],
            "whole_deleted": "-  int64_t instruction_count() const { return instruction_indices_.size(); }\n-  // arbitrarily and we want a stable iteration order. Keep a map from\n-  // instruction pointer to index in the vector for fast lookup.\n-  absl::flat_hash_map<const HloInstruction*, int> instruction_indices_;\n-  // Execution thread of this computation. By default, it's main thread.\n-  std::string execution_thread_ = HloInstruction::kMainExecutionThread;\n-    TF_RET_CHECK(instruction_indices_.contains(instruction))\n-        << \"Instruction \" << instruction->name() << \" is not in computation \"\n-        << name();\n",
            "whole_added": "+  int64_t instruction_count() const { return instruction_count_; }\n+  // arbitrarily and we want a stable iteration order.\n+  // For the reverse mapping we use HloInstruction::index_in_parent_.\n+  // Number of not-marked-for-deletion entries in instructions_.\n+  int64_t instruction_count_;\n+  // Execution thread of this computation. By default, it's main thread.\n+  std::string execution_thread_ = HloInstruction::kMainExecutionThread;\n+\n",
            "whole_hunk": "@@ -439,7 +439,7 @@ class HloComputation {\n   void ForEachInstructionPostOrder(\n       absl::FunctionRef<void(HloInstruction*)> func) const;\n \n-  int64_t instruction_count() const { return instruction_indices_.size(); }\n+  int64_t instruction_count() const { return instruction_count_; }\n \n   // Creates and returns a list of the embedded computations called by this\n   // computation. This includes all embedded computations called directly or\n@@ -961,18 +961,20 @@ class HloComputation {\n   HloInstruction::InstructionVector param_instructions_;\n \n   // Store instructions in std::vector as they can be added and removed\n-  // arbitrarily and we want a stable iteration order. Keep a map from\n-  // instruction pointer to index in the vector for fast lookup.\n+  // arbitrarily and we want a stable iteration order.\n+  // For the reverse mapping we use HloInstruction::index_in_parent_.\n   HloInstructionList instructions_;\n-  absl::flat_hash_map<const HloInstruction*, int> instruction_indices_;\n \n-  // Execution thread of this computation. By default, it's main thread.\n-  std::string execution_thread_ = HloInstruction::kMainExecutionThread;\n+  // Number of not-marked-for-deletion entries in instructions_.\n+  int64_t instruction_count_;\n \n   // Removed instructions are moved into to_be_deleted_ first and then\n   // deallocated when Cleanup is called.\n   PtrVec<HloInstruction*> to_be_deleted_;\n \n+  // Execution thread of this computation. By default, it's main thread.\n+  std::string execution_thread_ = HloInstruction::kMainExecutionThread;\n+\n   std::string name_;\n \n   HloComputation(const HloComputation&) = delete;\n@@ -1010,9 +1012,6 @@ Status HloComputation::AcceptOrdered(\n   absl::flat_hash_set<const HloInstruction*> visited;\n   for (const HloInstruction* instruction : order) {\n     VLOG(3) << \"Visiting ordered: \" << instruction->ToString();\n-    TF_RET_CHECK(instruction_indices_.contains(instruction))\n-        << \"Instruction \" << instruction->name() << \" is not in computation \"\n-        << name();\n     TF_RET_CHECK(!visited.contains(instruction))\n         << \"Instruction \" << instruction->name()\n         << \" appears more than once in order\";"
        }
    ]
},
{
    "Id": 86,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fe32e229b996dd04abfc0c3d60018aa6f5c31bf2",
    "date": "2024-04-02T18:43:48-07:00",
    "message": "A couple minor bug fixes:\n1. When adding unknown shardings to input parameter tuples, ignore modules with no input parameters.\n2. Skip misaligned sharding checks for outfeed, send and send-done ops.\n\nPiperOrigin-RevId: 621345164",
    "label": "YES",
    "changes": [
        {
            "name": "auto_sharding.cc",
            "path": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding.cc",
            "patches": [
                {
                    "old_start": 3453,
                    "old_length": 6,
                    "new_start": 3453,
                    "new_length": 7,
                    "hunk": "@@ -3453,6 +3453,7 @@ AutoShardingImplementation::SaveAndRemoveShardingAnnotation(\n         spmd::SaveShardingForInstruction(inst,\n                                          /* save_for_copy_users */ false,\n                                          preserve_shardings);\n+        continue;\n       }\n       if (inst->has_sharding() &&\n           spmd::IsShardingMisaligned(inst->sharding(), inst->shape())) {\n"
                },
                {
                    "old_start": 4000,
                    "old_length": 15,
                    "new_start": 4001,
                    "new_length": 18,
                    "hunk": "@@ -4000,15 +4001,18 @@ absl::StatusOr<bool> AutoSharding::Run(\n     mesh_shapes.push_back(option_.device_mesh_shape);\n   }\n \n-  HloInstruction* parameter_instruction =\n-      module->entry_computation()->parameter_instruction(0);\n-  if (parameter_instruction->shape().IsTuple() &&\n-      parameter_instruction->has_sharding()) {\n-    CHECK_EQ(module->entry_computation()->num_parameters(), 1);\n-    parameter_instruction->set_sharding(\n-        spmd::ReplaceGivenShardingsWithUnknownForTuple(\n-            parameter_instruction->sharding(), parameter_instruction->shape(),\n-            module->config().allow_spmd_sharding_propagation_to_parameters()));\n+  if (module->entry_computation()->num_parameters() > 0) {\n+    HloInstruction* parameter_instruction =\n+        module->entry_computation()->parameter_instruction(0);\n+    if (parameter_instruction->shape().IsTuple() &&\n+        parameter_instruction->has_sharding()) {\n+      CHECK_EQ(module->entry_computation()->num_parameters(), 1);\n+      parameter_instruction->set_sharding(\n+          spmd::ReplaceGivenShardingsWithUnknownForTuple(\n+              parameter_instruction->sharding(), parameter_instruction->shape(),\n+              module->config()\n+                  .allow_spmd_sharding_propagation_to_parameters()));\n+    }\n   }\n \n   HloInstruction* root_instruction ="
                }
            ],
            "whole_deleted": "-  HloInstruction* parameter_instruction =\n-      module->entry_computation()->parameter_instruction(0);\n-  if (parameter_instruction->shape().IsTuple() &&\n-      parameter_instruction->has_sharding()) {\n-    CHECK_EQ(module->entry_computation()->num_parameters(), 1);\n-    parameter_instruction->set_sharding(\n-        spmd::ReplaceGivenShardingsWithUnknownForTuple(\n-            parameter_instruction->sharding(), parameter_instruction->shape(),\n-            module->config().allow_spmd_sharding_propagation_to_parameters()));\n",
            "whole_added": "+        continue;\n+  if (module->entry_computation()->num_parameters() > 0) {\n+    HloInstruction* parameter_instruction =\n+        module->entry_computation()->parameter_instruction(0);\n+    if (parameter_instruction->shape().IsTuple() &&\n+        parameter_instruction->has_sharding()) {\n+      CHECK_EQ(module->entry_computation()->num_parameters(), 1);\n+      parameter_instruction->set_sharding(\n+          spmd::ReplaceGivenShardingsWithUnknownForTuple(\n+              parameter_instruction->sharding(), parameter_instruction->shape(),\n+              module->config()\n+                  .allow_spmd_sharding_propagation_to_parameters()));\n+    }\n",
            "whole_hunk": "@@ -3453,6 +3453,7 @@ AutoShardingImplementation::SaveAndRemoveShardingAnnotation(\n         spmd::SaveShardingForInstruction(inst,\n                                          /* save_for_copy_users */ false,\n                                          preserve_shardings);\n+        continue;\n       }\n       if (inst->has_sharding() &&\n           spmd::IsShardingMisaligned(inst->sharding(), inst->shape())) {\n@@ -4000,15 +4001,18 @@ absl::StatusOr<bool> AutoSharding::Run(\n     mesh_shapes.push_back(option_.device_mesh_shape);\n   }\n \n-  HloInstruction* parameter_instruction =\n-      module->entry_computation()->parameter_instruction(0);\n-  if (parameter_instruction->shape().IsTuple() &&\n-      parameter_instruction->has_sharding()) {\n-    CHECK_EQ(module->entry_computation()->num_parameters(), 1);\n-    parameter_instruction->set_sharding(\n-        spmd::ReplaceGivenShardingsWithUnknownForTuple(\n-            parameter_instruction->sharding(), parameter_instruction->shape(),\n-            module->config().allow_spmd_sharding_propagation_to_parameters()));\n+  if (module->entry_computation()->num_parameters() > 0) {\n+    HloInstruction* parameter_instruction =\n+        module->entry_computation()->parameter_instruction(0);\n+    if (parameter_instruction->shape().IsTuple() &&\n+        parameter_instruction->has_sharding()) {\n+      CHECK_EQ(module->entry_computation()->num_parameters(), 1);\n+      parameter_instruction->set_sharding(\n+          spmd::ReplaceGivenShardingsWithUnknownForTuple(\n+              parameter_instruction->sharding(), parameter_instruction->shape(),\n+              module->config()\n+                  .allow_spmd_sharding_propagation_to_parameters()));\n+    }\n   }\n \n   HloInstruction* root_instruction ="
        }
    ]
},
{
    "Id": 652,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e3559b8dc1522c0f5bd8b044b788e2107f71cde3",
    "date": "2022-10-20T08:56:24-07:00",
    "message": "Fix the minibenchmark unit test.\nCurrently for custom validation case, accuracy validation is not supported, and the BenchmarkResult doesn't have meaning.\n\nPiperOrigin-RevId: 482501534",
    "label": "YES",
    "changes": [
        {
            "name": "c_api_test.cc",
            "path": "tensorflow/lite/experimental/acceleration/mini_benchmark/c/c_api_test.cc",
            "patches": [
                {
                    "old_start": 173,
                    "old_length": 7,
                    "new_start": 173,
                    "new_length": 6,
                    "hunk": "@@ -173,7 +173,6 @@ TEST_F(CApiTest, SucceedWithCustomValidation) {\n   EXPECT_THAT(events, testing::Not(testing::IsEmpty()));\n   for (auto& event : events) {\n     EXPECT_EQ(event->event_type(), tflite::BenchmarkEventType_END);\n-    EXPECT_FALSE(event->result()->ok());\n   }\n   TfLiteMiniBenchmarkResultFree(result);\n }"
                }
            ],
            "whole_deleted": "-    EXPECT_FALSE(event->result()->ok());\n",
            "whole_added": "",
            "whole_hunk": "@@ -173,7 +173,6 @@ TEST_F(CApiTest, SucceedWithCustomValidation) {\n   EXPECT_THAT(events, testing::Not(testing::IsEmpty()));\n   for (auto& event : events) {\n     EXPECT_EQ(event->event_type(), tflite::BenchmarkEventType_END);\n-    EXPECT_FALSE(event->result()->ok());\n   }\n   TfLiteMiniBenchmarkResultFree(result);\n }"
        }
    ]
},
{
    "Id": 453,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7ee6022b17404a7232c887002ff6eb43d008be6d",
    "date": "2023-04-11T15:58:05-07:00",
    "message": "Validate null pointer dereference",
    "label": "YES",
    "changes": [
        {
            "name": "conv_ops_fused_int8.cc",
            "path": "tensorflow/core/kernels/conv_ops_fused_int8.cc",
            "patches": [
                {
                    "old_start": 261,
                    "old_length": 7,
                    "new_start": 261,
                    "new_length": 7,
                    "hunk": "@@ -261,7 +261,7 @@ struct LaunchFusedConv2DOpCpuInt8Helper {\n         }\n \n         // (2) Side input.\n-        if (side_input_scale != 0.0f && side_input_ptr != nullptr) {\n+        if (side_input_scale != 0.0f && side_input_base != nullptr) {\n           const T* side_input_ptr = side_input_base + col * stride;\n           TempT* conv_output_ptr = conv_output.data();\n           for (int idx = 0; idx < num_rows; ++idx) {"
                }
            ],
            "whole_deleted": "-        if (side_input_scale != 0.0f && side_input_ptr != nullptr) {\n",
            "whole_added": "+        if (side_input_scale != 0.0f && side_input_base != nullptr) {\n",
            "whole_hunk": "@@ -261,7 +261,7 @@ struct LaunchFusedConv2DOpCpuInt8Helper {\n         }\n \n         // (2) Side input.\n-        if (side_input_scale != 0.0f && side_input_ptr != nullptr) {\n+        if (side_input_scale != 0.0f && side_input_base != nullptr) {\n           const T* side_input_ptr = side_input_base + col * stride;\n           TempT* conv_output_ptr = conv_output.data();\n           for (int idx = 0; idx < num_rows; ++idx) {"
        }
    ]
},
{
    "Id": 624,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ef070b94c1341986ec8f1469d0104f73c567de05",
    "date": "2022-11-30T03:02:33-08:00",
    "message": "ScatterNd, BroadcastTo, Reshape and ExpandDims call IsConstantOrPersistentTensor to check if the output tensor should be dynamic.\n\nThis removes unnecessary dynamic tensors from the graph allowing delegates to be applied and may reduce memory usage. It will also mean that Reshape and ExpandDims will always be in-place ops.\n\nPiperOrigin-RevId: 491864928",
    "label": "NO",
    "changes": [
        {
            "name": "broadcast_to.cc",
            "path": "tensorflow/lite/kernels/broadcast_to.cc",
            "patches": [
                {
                    "old_start": 101,
                    "old_length": 7,
                    "new_start": 101,
                    "new_length": 7,
                    "hunk": "@@ -101,7 +101,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // Not yet support string type due to the use of memcopy with fixed size.\n   TF_LITE_ENSURE(context, op_context.input->type != kTfLiteString);\n \n-  if (IsConstantTensor(op_context.shape)) {\n+  if (IsConstantOrPersistentTensor(op_context.shape)) {\n     return ResizeOutputTensor(context, &op_context);\n   }\n \n"
                }
            ],
            "whole_deleted": "-  if (IsConstantTensor(op_context.shape)) {\n",
            "whole_added": "+  if (IsConstantOrPersistentTensor(op_context.shape)) {\n",
            "whole_hunk": "@@ -101,7 +101,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   // Not yet support string type due to the use of memcopy with fixed size.\n   TF_LITE_ENSURE(context, op_context.input->type != kTfLiteString);\n \n-  if (IsConstantTensor(op_context.shape)) {\n+  if (IsConstantOrPersistentTensor(op_context.shape)) {\n     return ResizeOutputTensor(context, &op_context);\n   }\n \n"
        },
        {
            "name": "expand_dims.cc",
            "path": "tensorflow/lite/kernels/expand_dims.cc",
            "patches": [
                {
                    "old_start": 89,
                    "old_length": 7,
                    "new_start": 89,
                    "new_length": 7,
                    "hunk": "@@ -89,7 +89,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n   }\n \n-  if (IsConstantTensor(axis)) {\n+  if (IsConstantOrPersistentTensor(axis)) {\n     int axis_value;\n     TF_LITE_ENSURE_OK(context,\n                       GetAxisValueFromTensor(context, *axis, &axis_value));\n"
                }
            ],
            "whole_deleted": "-  if (IsConstantTensor(axis)) {\n",
            "whole_added": "+  if (IsConstantOrPersistentTensor(axis)) {\n",
            "whole_hunk": "@@ -89,7 +89,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n   }\n \n-  if (IsConstantTensor(axis)) {\n+  if (IsConstantOrPersistentTensor(axis)) {\n     int axis_value;\n     TF_LITE_ENSURE_OK(context,\n                       GetAxisValueFromTensor(context, *axis, &axis_value));\n"
        },
        {
            "name": "reshape.cc",
            "path": "tensorflow/lite/kernels/reshape.cc",
            "patches": [
                {
                    "old_start": 148,
                    "old_length": 7,
                    "new_start": 148,
                    "new_length": 7,
                    "hunk": "@@ -148,7 +148,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                     GetOutputSafe(context, node, kOutputTensor, &output));\n   if (output->type != kTfLiteString) {\n     if (NumInputs(node) == 1 ||\n-        IsConstantTensor(GetInput(context, node, kShapeTensor))) {\n+        IsConstantOrPersistentTensor(GetInput(context, node, kShapeTensor))) {\n       TF_LITE_ENSURE_OK(context, ResizeOutput(context, node));\n     } else {\n       SetTensorToDynamic(output);\n"
                }
            ],
            "whole_deleted": "-        IsConstantTensor(GetInput(context, node, kShapeTensor))) {\n",
            "whole_added": "+        IsConstantOrPersistentTensor(GetInput(context, node, kShapeTensor))) {\n",
            "whole_hunk": "@@ -148,7 +148,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                     GetOutputSafe(context, node, kOutputTensor, &output));\n   if (output->type != kTfLiteString) {\n     if (NumInputs(node) == 1 ||\n-        IsConstantTensor(GetInput(context, node, kShapeTensor))) {\n+        IsConstantOrPersistentTensor(GetInput(context, node, kShapeTensor))) {\n       TF_LITE_ENSURE_OK(context, ResizeOutput(context, node));\n     } else {\n       SetTensorToDynamic(output);\n"
        },
        {
            "name": "scatter_nd.cc",
            "path": "tensorflow/lite/kernels/scatter_nd.cc",
            "patches": [
                {
                    "old_start": 105,
                    "old_length": 7,
                    "new_start": 105,
                    "new_length": 7,
                    "hunk": "@@ -105,7 +105,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                     GetOutputSafe(context, node, kOutputTensor, &output));\n   output->type = updates->type;\n \n-  if (IsConstantTensor(shape)) {\n+  if (IsConstantOrPersistentTensor(shape)) {\n     switch (indices->type) {\n       case kTfLiteInt32:\n         TF_LITE_ENSURE_OK("
                }
            ],
            "whole_deleted": "-  if (IsConstantTensor(shape)) {\n",
            "whole_added": "+  if (IsConstantOrPersistentTensor(shape)) {\n",
            "whole_hunk": "@@ -105,7 +105,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                     GetOutputSafe(context, node, kOutputTensor, &output));\n   output->type = updates->type;\n \n-  if (IsConstantTensor(shape)) {\n+  if (IsConstantOrPersistentTensor(shape)) {\n     switch (indices->type) {\n       case kTfLiteInt32:\n         TF_LITE_ENSURE_OK("
        }
    ]
},
{
    "Id": 519,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/98293dec27f9f68f932ad658bff82c8fffda5406",
    "date": "2023-02-21T17:33:30-08:00",
    "message": "removing the check for dtypes",
    "label": "YES",
    "changes": [
        {
            "name": "cwise_ops_unary_test.py",
            "path": "tensorflow/python/kernel_tests/math_ops/cwise_ops_unary_test.py",
            "patches": [
                {
                    "old_start": 92,
                    "old_length": 8,
                    "new_start": 92,
                    "new_length": 7,
                    "hunk": "@@ -92,8 +92,7 @@ class UnaryOpTest(test.TestCase):\n       if x.dtype in (np.complex64, np.complex128) and tf_func == math_ops.sign:\n         return  # Return early\n \n-      if (x.dtype in (np.float16, dtypes_lib.bfloat16.as_numpy_dtype)\n-        and tf_func == math_ops.round):\n+      if tf_func == math_ops.round:\n         return  # Return early\n \n       if x.dtype in (np.float16, dtypes_lib.bfloat16.as_numpy_dtype):"
                }
            ],
            "whole_deleted": "-      if (x.dtype in (np.float16, dtypes_lib.bfloat16.as_numpy_dtype)\n-        and tf_func == math_ops.round):\n",
            "whole_added": "+      if tf_func == math_ops.round:\n",
            "whole_hunk": "@@ -92,8 +92,7 @@ class UnaryOpTest(test.TestCase):\n       if x.dtype in (np.complex64, np.complex128) and tf_func == math_ops.sign:\n         return  # Return early\n \n-      if (x.dtype in (np.float16, dtypes_lib.bfloat16.as_numpy_dtype)\n-        and tf_func == math_ops.round):\n+      if tf_func == math_ops.round:\n         return  # Return early\n \n       if x.dtype in (np.float16, dtypes_lib.bfloat16.as_numpy_dtype):"
        }
    ]
},
{
    "Id": 396,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9243f0660cc5368da2e1f0c925b2c41c486bca26",
    "date": "2023-06-19T02:01:00-07:00",
    "message": "Attempt to be less restrictive in FusionCanShareBufferHint().\n\nInitially I added a restriction that a fusion parameter should not be\n(transitively) used by more than one fusion output. I added this restriction\nbecause there was a test that was failing otherwise. After thinking about this\nagain, the real bug was just that I didn't check whether any of the users of the\nfusion output for which we want to see whether it can share the buffer with the\nfusion operand has some non-elementwise user.\n\nPiperOrigin-RevId: 541560719",
    "label": "YES",
    "changes": [
        {
            "name": "gpu_compiler.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_compiler.cc",
            "patches": [
                {
                    "old_start": 1722,
                    "old_length": 13,
                    "new_start": 1722,
                    "new_length": 12,
                    "hunk": "@@ -1722,13 +1722,12 @@ std::optional<bool> GpuCompiler::FusionCanShareBufferHint(\n   }\n \n   // We need to make sure that the fusion parameter is accessed in the same\n-  // iteration order as the fusion output. Also, there should not be two fusion\n-  // outputs that consume the fusion parameter, because we do not want to share\n-  // the same fusion operand with two different fusion outputs. To make sure\n+  // iteration order as the fusion output. Also, there should not be any other\n+  // fusion output that accesses it in a different iteration order. To make sure\n   // that the iteration order is the same, we only allow ops on the path from\n   // fusion parameter to fusion output which are elementwise (no copy) or\n-  // bitcast or a dynamic update slice with the first operand being on this\n-  // path.\n+  // bitcast or an elementwise dynamic update slice (i.e. with the first operand\n+  // being on this path).\n   HloInstruction* fusion_param =\n       user->fused_parameter(user->operand_index(operand));\n   HloInstruction* output = user->fused_expression_root();\n"
                },
                {
                    "old_start": 1745,
                    "old_length": 23,
                    "new_start": 1744,
                    "new_length": 17,
                    "hunk": "@@ -1745,23 +1744,17 @@ std::optional<bool> GpuCompiler::FusionCanShareBufferHint(\n     q.pop();\n     if (hlo_operand == output) {\n       found_path_to_output = true;\n-      // The output should have at most 1 user: the tuple op (in case of a\n-      // multi-output fusion)\n-      if (hlo_operand->user_count() > 1) {\n-        return false;\n-      }\n-      continue;\n+      // We still need to process the users of 'hlo_operand'. There can be other\n+      // users in addition to the tuple user.\n     }\n     for (HloInstruction* hlo : hlo_operand->users()) {\n       if (visited.contains(hlo)) {\n         continue;\n       }\n-      // This check also catches the case that we reach a different fusion\n-      // output, as that fusion output would have a tuple op as user, which we\n-      // do not allow here.\n       if ((!hlo->IsElementwiseOnOperand(hlo->operand_index(hlo_operand)) ||\n            hlo->opcode() == HloOpcode::kCopy) &&\n-          hlo->opcode() != HloOpcode::kBitcast) {\n+          hlo->opcode() != HloOpcode::kBitcast &&\n+          hlo->opcode() != HloOpcode::kTuple) {\n         return false;\n       }\n       visited.insert(hlo);\n"
                }
            ],
            "whole_deleted": "-  // iteration order as the fusion output. Also, there should not be two fusion\n-  // outputs that consume the fusion parameter, because we do not want to share\n-  // the same fusion operand with two different fusion outputs. To make sure\n-  // bitcast or a dynamic update slice with the first operand being on this\n-  // path.\n-      // The output should have at most 1 user: the tuple op (in case of a\n-      // multi-output fusion)\n-      if (hlo_operand->user_count() > 1) {\n-        return false;\n-      }\n-      continue;\n-      // This check also catches the case that we reach a different fusion\n-      // output, as that fusion output would have a tuple op as user, which we\n-      // do not allow here.\n-          hlo->opcode() != HloOpcode::kBitcast) {\n",
            "whole_added": "+  // iteration order as the fusion output. Also, there should not be any other\n+  // fusion output that accesses it in a different iteration order. To make sure\n+  // bitcast or an elementwise dynamic update slice (i.e. with the first operand\n+  // being on this path).\n+      // We still need to process the users of 'hlo_operand'. There can be other\n+      // users in addition to the tuple user.\n+          hlo->opcode() != HloOpcode::kBitcast &&\n+          hlo->opcode() != HloOpcode::kTuple) {\n",
            "whole_hunk": "@@ -1722,13 +1722,12 @@ std::optional<bool> GpuCompiler::FusionCanShareBufferHint(\n   }\n \n   // We need to make sure that the fusion parameter is accessed in the same\n-  // iteration order as the fusion output. Also, there should not be two fusion\n-  // outputs that consume the fusion parameter, because we do not want to share\n-  // the same fusion operand with two different fusion outputs. To make sure\n+  // iteration order as the fusion output. Also, there should not be any other\n+  // fusion output that accesses it in a different iteration order. To make sure\n   // that the iteration order is the same, we only allow ops on the path from\n   // fusion parameter to fusion output which are elementwise (no copy) or\n-  // bitcast or a dynamic update slice with the first operand being on this\n-  // path.\n+  // bitcast or an elementwise dynamic update slice (i.e. with the first operand\n+  // being on this path).\n   HloInstruction* fusion_param =\n       user->fused_parameter(user->operand_index(operand));\n   HloInstruction* output = user->fused_expression_root();\n@@ -1745,23 +1744,17 @@ std::optional<bool> GpuCompiler::FusionCanShareBufferHint(\n     q.pop();\n     if (hlo_operand == output) {\n       found_path_to_output = true;\n-      // The output should have at most 1 user: the tuple op (in case of a\n-      // multi-output fusion)\n-      if (hlo_operand->user_count() > 1) {\n-        return false;\n-      }\n-      continue;\n+      // We still need to process the users of 'hlo_operand'. There can be other\n+      // users in addition to the tuple user.\n     }\n     for (HloInstruction* hlo : hlo_operand->users()) {\n       if (visited.contains(hlo)) {\n         continue;\n       }\n-      // This check also catches the case that we reach a different fusion\n-      // output, as that fusion output would have a tuple op as user, which we\n-      // do not allow here.\n       if ((!hlo->IsElementwiseOnOperand(hlo->operand_index(hlo_operand)) ||\n            hlo->opcode() == HloOpcode::kCopy) &&\n-          hlo->opcode() != HloOpcode::kBitcast) {\n+          hlo->opcode() != HloOpcode::kBitcast &&\n+          hlo->opcode() != HloOpcode::kTuple) {\n         return false;\n       }\n       visited.insert(hlo);\n"
        },
        {
            "name": "gpu_copy_insertion_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_copy_insertion_test.cc",
            "patches": [
                {
                    "old_start": 204,
                    "old_length": 13,
                    "new_start": 204,
                    "new_length": 14,
                    "hunk": "@@ -204,13 +204,14 @@ fused_computation {\n   param_1.1 = f32[2,3]{1,0} parameter(1)\n   neg = f32[2,3]{1,0} negate(param_1.1)\n   mul = f32[2,3]{1,0} multiply(param_0.1, neg)\n-  ROOT tuple = (f32[2,3]{1,0}, f32[2,3]{1,0}) tuple(mul, neg)\n+  transpose = f32[3,2]{1,0} transpose(neg), dimensions={1,0}\n+  ROOT tuple = (f32[2,3]{1,0}, f32[2,3]{1,0}, f32[3,2]{1,0}) tuple(mul, neg, transpose)\n }\n \n ENTRY main {\n   param_0 = f32[2,3]{1,0} parameter(0)\n   param_1 = f32[2,3]{1,0} parameter(1)\n-  ROOT fusion = (f32[2,3]{1,0}, f32[2,3]{1,0}) fusion(param_0, param_1), kind=kLoop, calls=fused_computation\n+  ROOT fusion = (f32[2,3]{1,0}, f32[2,3]{1,0}, f32[3,2]{1,0}) fusion(param_0, param_1), kind=kLoop, calls=fused_computation\n }\n )\";\n \n"
                },
                {
                    "old_start": 220,
                    "old_length": 7,
                    "new_start": 221,
                    "new_length": 7,
                    "hunk": "@@ -220,7 +221,7 @@ ENTRY main {\n   ExpectOptionalTrue(\n       GpuCompiler::FusionCanShareBufferHint(fusion, fusion->operand(0), {0}));\n   // The second operand cannot share the buffer with the second fusion output,\n-  // because the 'neg' op is also used on the path to the first fusion output.\n+  // because the 'neg' op is also used by a non-elementwise op.\n   ExpectOptionalFalse(\n       GpuCompiler::FusionCanShareBufferHint(fusion, fusion->operand(1), {1}));\n   // The first operand cannot share the buffer with the second fusion output,"
                }
            ],
            "whole_deleted": "-  ROOT tuple = (f32[2,3]{1,0}, f32[2,3]{1,0}) tuple(mul, neg)\n-  ROOT fusion = (f32[2,3]{1,0}, f32[2,3]{1,0}) fusion(param_0, param_1), kind=kLoop, calls=fused_computation\n-  // because the 'neg' op is also used on the path to the first fusion output.\n",
            "whole_added": "+  transpose = f32[3,2]{1,0} transpose(neg), dimensions={1,0}\n+  ROOT tuple = (f32[2,3]{1,0}, f32[2,3]{1,0}, f32[3,2]{1,0}) tuple(mul, neg, transpose)\n+  ROOT fusion = (f32[2,3]{1,0}, f32[2,3]{1,0}, f32[3,2]{1,0}) fusion(param_0, param_1), kind=kLoop, calls=fused_computation\n+  // because the 'neg' op is also used by a non-elementwise op.\n",
            "whole_hunk": "@@ -204,13 +204,14 @@ fused_computation {\n   param_1.1 = f32[2,3]{1,0} parameter(1)\n   neg = f32[2,3]{1,0} negate(param_1.1)\n   mul = f32[2,3]{1,0} multiply(param_0.1, neg)\n-  ROOT tuple = (f32[2,3]{1,0}, f32[2,3]{1,0}) tuple(mul, neg)\n+  transpose = f32[3,2]{1,0} transpose(neg), dimensions={1,0}\n+  ROOT tuple = (f32[2,3]{1,0}, f32[2,3]{1,0}, f32[3,2]{1,0}) tuple(mul, neg, transpose)\n }\n \n ENTRY main {\n   param_0 = f32[2,3]{1,0} parameter(0)\n   param_1 = f32[2,3]{1,0} parameter(1)\n-  ROOT fusion = (f32[2,3]{1,0}, f32[2,3]{1,0}) fusion(param_0, param_1), kind=kLoop, calls=fused_computation\n+  ROOT fusion = (f32[2,3]{1,0}, f32[2,3]{1,0}, f32[3,2]{1,0}) fusion(param_0, param_1), kind=kLoop, calls=fused_computation\n }\n )\";\n \n@@ -220,7 +221,7 @@ ENTRY main {\n   ExpectOptionalTrue(\n       GpuCompiler::FusionCanShareBufferHint(fusion, fusion->operand(0), {0}));\n   // The second operand cannot share the buffer with the second fusion output,\n-  // because the 'neg' op is also used on the path to the first fusion output.\n+  // because the 'neg' op is also used by a non-elementwise op.\n   ExpectOptionalFalse(\n       GpuCompiler::FusionCanShareBufferHint(fusion, fusion->operand(1), {1}));\n   // The first operand cannot share the buffer with the second fusion output,"
        }
    ]
},
{
    "Id": 644,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/860538f62384fc4d9427aeac6f20ff7a87bcd21b",
    "date": "2022-11-02T12:01:29-07:00",
    "message": "Test dynamic_arg_shardings only for '==' equality not also the default pointer equality. Also add tests which checks this behavior and makes sure that we don't fallback to python\n\nPiperOrigin-RevId: 485656967",
    "label": "YES",
    "changes": [
        {
            "name": "jax_jit.cc",
            "path": "tensorflow/compiler/xla/python/jax_jit.cc",
            "patches": [
                {
                    "old_start": 183,
                    "old_length": 11,
                    "new_start": 183,
                    "new_length": 11,
                    "hunk": "@@ -183,11 +183,11 @@ bool CallSignature::operator==(const CallSignature& other) const {\n   // TODO(chky): Consider implementing hashing and equality for sharding in cpp\n   // instead of hashing and checking sharding's pointer values.\n   return std::tie(dynamic_arg_treedefs, dynamic_arg_names,\n-                  dynamic_arg_signatures, dynamic_arg_shardings, device,\n-                  jax_enable_x64, jax_array, static_arg_names) ==\n+                  dynamic_arg_signatures, device, jax_enable_x64, jax_array,\n+                  static_arg_names) ==\n              std::tie(other.dynamic_arg_treedefs, other.dynamic_arg_names,\n-                      other.dynamic_arg_signatures, other.dynamic_arg_shardings,\n-                      other.device, other.jax_enable_x64, other.jax_array,\n+                      other.dynamic_arg_signatures, other.device,\n+                      other.jax_enable_x64, other.jax_array,\n                       other.static_arg_names) &&\n          // `==` on py:objects is the Python `is`. We need equal.\n          std::equal(dynamic_arg_shardings.begin(), dynamic_arg_shardings.end(),\n"
                }
            ],
            "whole_deleted": "-                  dynamic_arg_signatures, dynamic_arg_shardings, device,\n-                  jax_enable_x64, jax_array, static_arg_names) ==\n-                      other.dynamic_arg_signatures, other.dynamic_arg_shardings,\n-                      other.device, other.jax_enable_x64, other.jax_array,\n",
            "whole_added": "+                  dynamic_arg_signatures, device, jax_enable_x64, jax_array,\n+                  static_arg_names) ==\n+                      other.dynamic_arg_signatures, other.device,\n+                      other.jax_enable_x64, other.jax_array,\n",
            "whole_hunk": "@@ -183,11 +183,11 @@ bool CallSignature::operator==(const CallSignature& other) const {\n   // TODO(chky): Consider implementing hashing and equality for sharding in cpp\n   // instead of hashing and checking sharding's pointer values.\n   return std::tie(dynamic_arg_treedefs, dynamic_arg_names,\n-                  dynamic_arg_signatures, dynamic_arg_shardings, device,\n-                  jax_enable_x64, jax_array, static_arg_names) ==\n+                  dynamic_arg_signatures, device, jax_enable_x64, jax_array,\n+                  static_arg_names) ==\n              std::tie(other.dynamic_arg_treedefs, other.dynamic_arg_names,\n-                      other.dynamic_arg_signatures, other.dynamic_arg_shardings,\n-                      other.device, other.jax_enable_x64, other.jax_array,\n+                      other.dynamic_arg_signatures, other.device,\n+                      other.jax_enable_x64, other.jax_array,\n                       other.static_arg_names) &&\n          // `==` on py:objects is the Python `is`. We need equal.\n          std::equal(dynamic_arg_shardings.begin(), dynamic_arg_shardings.end(),\n"
        },
        {
            "name": "xla_client.py",
            "path": "tensorflow/compiler/xla/python/xla_client.py",
            "patches": [
                {
                    "old_start": 43,
                    "old_length": 7,
                    "new_start": 43,
                    "new_length": 7,
                    "hunk": "@@ -43,7 +43,7 @@ profiler = _xla.profiler\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes.\n-_version = 102\n+_version = 103\n \n # Version number for MLIR:Python components.\n mlir_api_version = 36"
                }
            ],
            "whole_deleted": "-_version = 102\n",
            "whole_added": "+_version = 103\n",
            "whole_hunk": "@@ -43,7 +43,7 @@ profiler = _xla.profiler\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes.\n-_version = 102\n+_version = 103\n \n # Version number for MLIR:Python components.\n mlir_api_version = 36"
        }
    ]
},
{
    "Id": 77,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/aba50056eff560a068b29f13f34c5c3f7ab8e3c4",
    "date": "2024-04-11T16:01:43-07:00",
    "message": "#tf-data-service Explicitly checks if snapshot metadata exists.\n\nThe `GFile` documentation does not mention what exceptions it raises:\nhttps://github.com/tensorflow/tensorflow/blob/38b17d708344a91234dad879794735e79f9af42a/tensorflow/python/platform/gfile.py#L37.\n\nIt would be safer to explicitly check for file existence and return the\nappropriate result.\n\nPiperOrigin-RevId: 623961044",
    "label": "YES",
    "changes": [
        {
            "name": "load_op.py",
            "path": "tensorflow/python/data/ops/load_op.py",
            "patches": [
                {
                    "old_start": 106,
                    "old_length": 10,
                    "new_start": 106,
                    "new_length": 12,
                    "hunk": "@@ -106,10 +106,12 @@ def _load_distributed_snapshot_metadata(\n     DistributedSnapshotMetadata if the snapshot is a distributed snapshot.\n     Returns None if it is a non-distributed snapshot.\n   \"\"\"\n+  metadata_file = _pywrap_snapshot_utils.TF_DATA_SnapshotMetadataFilePath(path)\n+  if not gfile.Exists(metadata_file):\n+    return None\n+\n   try:\n-    with gfile.GFile(\n-        _pywrap_snapshot_utils.TF_DATA_SnapshotMetadataFilePath(path), \"r\"\n-    ) as f:\n+    with gfile.GFile(metadata_file, \"r\") as f:\n       return text_format.ParseLines(\n           f, snapshot_pb2.DistributedSnapshotMetadata())\n   except (\n"
                },
                {
                    "old_start": 149,
                    "old_length": 6,
                    "new_start": 151,
                    "new_length": 12,
                    "hunk": "@@ -149,6 +151,12 @@ def _load_element_spec(path: str) -> Any:\n     NotFoundError if the element spec file does not exist or cannot be decoded.\n   \"\"\"\n   dataset_spec_filename = os.path.join(path, dataset_ops.DATASET_SPEC_FILENAME)\n+  if not gfile.Exists(dataset_spec_filename):\n+    raise errors.NotFoundError(\n+        node_def=None, op=None,\n+        message=\"tf.data snapshot element_spec file not found: \"\n+                f\"{dataset_spec_filename}.\")\n+\n   with gfile.GFile(dataset_spec_filename, \"rb\") as f:\n     encoded_spec = f.read()\n   try:"
                }
            ],
            "whole_deleted": "-    with gfile.GFile(\n-        _pywrap_snapshot_utils.TF_DATA_SnapshotMetadataFilePath(path), \"r\"\n-    ) as f:\n",
            "whole_added": "+  metadata_file = _pywrap_snapshot_utils.TF_DATA_SnapshotMetadataFilePath(path)\n+  if not gfile.Exists(metadata_file):\n+    return None\n+\n+    with gfile.GFile(metadata_file, \"r\") as f:\n+  if not gfile.Exists(dataset_spec_filename):\n+    raise errors.NotFoundError(\n+        node_def=None, op=None,\n+        message=\"tf.data snapshot element_spec file not found: \"\n+                f\"{dataset_spec_filename}.\")\n+\n",
            "whole_hunk": "@@ -106,10 +106,12 @@ def _load_distributed_snapshot_metadata(\n     DistributedSnapshotMetadata if the snapshot is a distributed snapshot.\n     Returns None if it is a non-distributed snapshot.\n   \"\"\"\n+  metadata_file = _pywrap_snapshot_utils.TF_DATA_SnapshotMetadataFilePath(path)\n+  if not gfile.Exists(metadata_file):\n+    return None\n+\n   try:\n-    with gfile.GFile(\n-        _pywrap_snapshot_utils.TF_DATA_SnapshotMetadataFilePath(path), \"r\"\n-    ) as f:\n+    with gfile.GFile(metadata_file, \"r\") as f:\n       return text_format.ParseLines(\n           f, snapshot_pb2.DistributedSnapshotMetadata())\n   except (\n@@ -149,6 +151,12 @@ def _load_element_spec(path: str) -> Any:\n     NotFoundError if the element spec file does not exist or cannot be decoded.\n   \"\"\"\n   dataset_spec_filename = os.path.join(path, dataset_ops.DATASET_SPEC_FILENAME)\n+  if not gfile.Exists(dataset_spec_filename):\n+    raise errors.NotFoundError(\n+        node_def=None, op=None,\n+        message=\"tf.data snapshot element_spec file not found: \"\n+                f\"{dataset_spec_filename}.\")\n+\n   with gfile.GFile(dataset_spec_filename, \"rb\") as f:\n     encoded_spec = f.read()\n   try:"
        }
    ]
},
{
    "Id": 477,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a9aa884e1eae90bc473a8c78372d4ce3750af947",
    "date": "2023-03-22T19:29:19-07:00",
    "message": "[PJRT] Add send/recv callback API where impl does host/device layout rewrites.\n\nThe new API is ExecuteOptions::use_major_to_minor_data_layout_for_callbacks. It\ndefaults to false (the behavior prior to this change), meaning the caller is\nresponsible for rewriting data using PjRtTransferMetadata::device_shape and\nPjRtHostMemoryForDeviceManager. When set to true, the implementation assumes the\nhost is using major-to-minor layout and handles the rewrite internally.\n\nThis change also adds a new method `PjRtClient::SupportsSendRecvCallbacks`.\nPrior to this change, callers would check if\n`PjRtClient::GetPjRtHostMemoryForDeviceManager` returned nullptr. Since callers\nno longer necessarily need the manager to use host callbacks, the more explicit\nmethod is necessary.\n\nFinally, this change makes jax always use the new option.\n\nMost of the complexity in this change is plumbing the option everywhere that\nneeds it, since even though jax always set\nuse_major_to_minor_data_layout_for_callbacks = true now, we still need to\nsupport other callers.\n\nThe motivation for this change is avoiding passing xla::Shapes across the PJRT\nAPI for the sake of the C API. C API users will not have the option of setting\nuse_major_to_minor_data_layout_for_callbacks = false.\nPiperOrigin-RevId: 518733871",
    "label": "NO",
    "changes": [
        {
            "name": "host_callback.cc",
            "path": "tensorflow/compiler/xla/pjrt/host_callback.cc",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 22,
                    "new_start": 23,
                    "new_length": 26,
                    "hunk": "@@ -23,22 +23,26 @@ namespace xla {\n Status HostCallbackContext::OnSend(int arg_num,\n                                    const PjRtTransferMetadata& metadata,\n                                    PjRtChunk data) {\n-  const auto& arg_info = host_callback_.operands.at(arg_num);\n-  const auto& host_shape = arg_info.shape;\n-  const auto& device_shape = metadata.device_shape;\n+  if (!use_major_to_minor_data_layout_for_callbacks_) {\n+    const auto& arg_info = host_callback_.operands.at(arg_num);\n+    const auto& host_shape = arg_info.shape;\n+    const auto& device_shape = metadata.device_shape;\n \n-  size_t host_size = ShapeUtil::ByteSizeOf(host_shape);\n-  DCHECK_GE(data.size(), host_size);\n+    size_t host_size = ShapeUtil::ByteSizeOf(host_shape);\n+    DCHECK_GE(data.size(), host_size);\n+\n+    auto delinearized = PjRtChunk::AllocateDefault(host_size);\n+    TF_CHECK_OK(host_memory_for_device_manager_->ToHostLayout(\n+        data.data(), data.size(), device_shape, delinearized.data(),\n+        delinearized.size(), host_shape));\n \n-  auto delinearized = PjRtChunk::AllocateDefault(host_size);\n-  TF_CHECK_OK(host_memory_for_device_manager_->ToHostLayout(\n-      data.data(), data.size(), device_shape, delinearized.data(),\n-      delinearized.size(), host_shape));\n+    data = std::move(delinearized);\n+  }\n \n   // This assignment to update `args_` will not race with the assignments in\n   // future send ops for this `arg_num` because send callbacks are supposed to\n   // be invoked sequentially.\n-  args_.at(arg_num) = std::move(delinearized);\n+  args_.at(arg_num) = std::move(data);\n \n   DCHECK_GE(ready_count_.load(), 1);\n   if (ready_count_.fetch_sub(1) != 1) {\n"
                },
                {
                    "old_start": 92,
                    "old_length": 14,
                    "new_start": 96,
                    "new_length": 15,
                    "hunk": "@@ -92,14 +96,15 @@ void HostCallbackContext::Receive(int res_num,\n   auto& result_channel = result_channels_.at(res_num);\n   PjRtChunk chunk = result_channel->Pop();\n \n-  const auto& host_shape = host_callback_.results.at(res_num).shape;\n-  const auto& device_shape = metadata.device_shape;\n+  if (!use_major_to_minor_data_layout_for_callbacks_) {\n+    const auto& host_shape = host_callback_.results.at(res_num).shape;\n+    const auto& device_shape = metadata.device_shape;\n+    auto statusor_linearized = host_memory_for_device_manager_->ToDeviceLayout(\n+        chunk.data(), chunk.size(), host_shape, device_shape);\n+    chunk = std::move(statusor_linearized.value());\n+  }\n \n-  auto statusor_linearized = host_memory_for_device_manager_->ToDeviceLayout(\n-      chunk.data(), chunk.size(), host_shape, device_shape);\n-  stream.AddChunk(std::move(statusor_linearized).value()).OnReady([](Status s) {\n-    TF_CHECK_OK(s);\n-  });\n+  stream.AddChunk(std::move(chunk)).OnReady([](Status s) { TF_CHECK_OK(s); });\n }\n \n std::unique_ptr<HostCallbackContext>\n"
                },
                {
                    "old_start": 107,
                    "old_length": 9,
                    "new_start": 112,
                    "new_length": 11,
                    "hunk": "@@ -107,9 +112,11 @@ CreateHostCallbackStateAndAppendSendRecvCallbacks(\n     HostCallback host_callback,\n     PjRtHostMemoryForDeviceManager* host_memory_for_device_manager,\n     std::vector<SendCallback>& send_callbacks,\n-    std::vector<RecvCallback>& recv_callbacks) {\n+    std::vector<RecvCallback>& recv_callbacks,\n+    bool use_major_to_minor_data_layout_for_callbacks) {\n   auto context = std::make_unique<HostCallbackContext>(\n-      std::move(host_callback), host_memory_for_device_manager);\n+      std::move(host_callback), use_major_to_minor_data_layout_for_callbacks,\n+      host_memory_for_device_manager);\n \n   const auto& hb = context->host_callback();\n   for (int arg_num = 0; arg_num < hb.operands.size(); ++arg_num) {\n"
                }
            ],
            "whole_deleted": "-  const auto& arg_info = host_callback_.operands.at(arg_num);\n-  const auto& host_shape = arg_info.shape;\n-  const auto& device_shape = metadata.device_shape;\n-  size_t host_size = ShapeUtil::ByteSizeOf(host_shape);\n-  DCHECK_GE(data.size(), host_size);\n-  auto delinearized = PjRtChunk::AllocateDefault(host_size);\n-  TF_CHECK_OK(host_memory_for_device_manager_->ToHostLayout(\n-      data.data(), data.size(), device_shape, delinearized.data(),\n-      delinearized.size(), host_shape));\n-  args_.at(arg_num) = std::move(delinearized);\n-  const auto& host_shape = host_callback_.results.at(res_num).shape;\n-  const auto& device_shape = metadata.device_shape;\n-  auto statusor_linearized = host_memory_for_device_manager_->ToDeviceLayout(\n-      chunk.data(), chunk.size(), host_shape, device_shape);\n-  stream.AddChunk(std::move(statusor_linearized).value()).OnReady([](Status s) {\n-    TF_CHECK_OK(s);\n-  });\n-    std::vector<RecvCallback>& recv_callbacks) {\n-      std::move(host_callback), host_memory_for_device_manager);\n",
            "whole_added": "+  if (!use_major_to_minor_data_layout_for_callbacks_) {\n+    const auto& arg_info = host_callback_.operands.at(arg_num);\n+    const auto& host_shape = arg_info.shape;\n+    const auto& device_shape = metadata.device_shape;\n+    size_t host_size = ShapeUtil::ByteSizeOf(host_shape);\n+    DCHECK_GE(data.size(), host_size);\n+\n+    auto delinearized = PjRtChunk::AllocateDefault(host_size);\n+    TF_CHECK_OK(host_memory_for_device_manager_->ToHostLayout(\n+        data.data(), data.size(), device_shape, delinearized.data(),\n+        delinearized.size(), host_shape));\n+    data = std::move(delinearized);\n+  }\n+  args_.at(arg_num) = std::move(data);\n+  if (!use_major_to_minor_data_layout_for_callbacks_) {\n+    const auto& host_shape = host_callback_.results.at(res_num).shape;\n+    const auto& device_shape = metadata.device_shape;\n+    auto statusor_linearized = host_memory_for_device_manager_->ToDeviceLayout(\n+        chunk.data(), chunk.size(), host_shape, device_shape);\n+    chunk = std::move(statusor_linearized.value());\n+  }\n+  stream.AddChunk(std::move(chunk)).OnReady([](Status s) { TF_CHECK_OK(s); });\n+    std::vector<RecvCallback>& recv_callbacks,\n+    bool use_major_to_minor_data_layout_for_callbacks) {\n+      std::move(host_callback), use_major_to_minor_data_layout_for_callbacks,\n+      host_memory_for_device_manager);\n",
            "whole_hunk": "@@ -23,22 +23,26 @@ namespace xla {\n Status HostCallbackContext::OnSend(int arg_num,\n                                    const PjRtTransferMetadata& metadata,\n                                    PjRtChunk data) {\n-  const auto& arg_info = host_callback_.operands.at(arg_num);\n-  const auto& host_shape = arg_info.shape;\n-  const auto& device_shape = metadata.device_shape;\n+  if (!use_major_to_minor_data_layout_for_callbacks_) {\n+    const auto& arg_info = host_callback_.operands.at(arg_num);\n+    const auto& host_shape = arg_info.shape;\n+    const auto& device_shape = metadata.device_shape;\n \n-  size_t host_size = ShapeUtil::ByteSizeOf(host_shape);\n-  DCHECK_GE(data.size(), host_size);\n+    size_t host_size = ShapeUtil::ByteSizeOf(host_shape);\n+    DCHECK_GE(data.size(), host_size);\n+\n+    auto delinearized = PjRtChunk::AllocateDefault(host_size);\n+    TF_CHECK_OK(host_memory_for_device_manager_->ToHostLayout(\n+        data.data(), data.size(), device_shape, delinearized.data(),\n+        delinearized.size(), host_shape));\n \n-  auto delinearized = PjRtChunk::AllocateDefault(host_size);\n-  TF_CHECK_OK(host_memory_for_device_manager_->ToHostLayout(\n-      data.data(), data.size(), device_shape, delinearized.data(),\n-      delinearized.size(), host_shape));\n+    data = std::move(delinearized);\n+  }\n \n   // This assignment to update `args_` will not race with the assignments in\n   // future send ops for this `arg_num` because send callbacks are supposed to\n   // be invoked sequentially.\n-  args_.at(arg_num) = std::move(delinearized);\n+  args_.at(arg_num) = std::move(data);\n \n   DCHECK_GE(ready_count_.load(), 1);\n   if (ready_count_.fetch_sub(1) != 1) {\n@@ -92,14 +96,15 @@ void HostCallbackContext::Receive(int res_num,\n   auto& result_channel = result_channels_.at(res_num);\n   PjRtChunk chunk = result_channel->Pop();\n \n-  const auto& host_shape = host_callback_.results.at(res_num).shape;\n-  const auto& device_shape = metadata.device_shape;\n+  if (!use_major_to_minor_data_layout_for_callbacks_) {\n+    const auto& host_shape = host_callback_.results.at(res_num).shape;\n+    const auto& device_shape = metadata.device_shape;\n+    auto statusor_linearized = host_memory_for_device_manager_->ToDeviceLayout(\n+        chunk.data(), chunk.size(), host_shape, device_shape);\n+    chunk = std::move(statusor_linearized.value());\n+  }\n \n-  auto statusor_linearized = host_memory_for_device_manager_->ToDeviceLayout(\n-      chunk.data(), chunk.size(), host_shape, device_shape);\n-  stream.AddChunk(std::move(statusor_linearized).value()).OnReady([](Status s) {\n-    TF_CHECK_OK(s);\n-  });\n+  stream.AddChunk(std::move(chunk)).OnReady([](Status s) { TF_CHECK_OK(s); });\n }\n \n std::unique_ptr<HostCallbackContext>\n@@ -107,9 +112,11 @@ CreateHostCallbackStateAndAppendSendRecvCallbacks(\n     HostCallback host_callback,\n     PjRtHostMemoryForDeviceManager* host_memory_for_device_manager,\n     std::vector<SendCallback>& send_callbacks,\n-    std::vector<RecvCallback>& recv_callbacks) {\n+    std::vector<RecvCallback>& recv_callbacks,\n+    bool use_major_to_minor_data_layout_for_callbacks) {\n   auto context = std::make_unique<HostCallbackContext>(\n-      std::move(host_callback), host_memory_for_device_manager);\n+      std::move(host_callback), use_major_to_minor_data_layout_for_callbacks,\n+      host_memory_for_device_manager);\n \n   const auto& hb = context->host_callback();\n   for (int arg_num = 0; arg_num < hb.operands.size(); ++arg_num) {\n"
        },
        {
            "name": "host_callback.h",
            "path": "tensorflow/compiler/xla/pjrt/host_callback.h",
            "patches": [
                {
                    "old_start": 83,
                    "old_length": 20,
                    "new_start": 83,
                    "new_length": 20,
                    "hunk": "@@ -83,20 +83,20 @@ struct HostCallback {\n // A helper class that maintains the send/recv states for a host callback.\n class HostCallbackContext {\n  public:\n-  HostCallbackContext(HostCallback host_callback, PjRtClient* client)\n-      : HostCallbackContext(std::move(host_callback),\n-                            client->GetPjRtHostMemoryForDeviceManager()) {}\n-\n   HostCallbackContext(\n       HostCallback host_callback,\n+      bool use_major_to_minor_data_layout_for_callbacks,\n       PjRtHostMemoryForDeviceManager* host_memory_for_device_manager)\n       : host_callback_(std::move(host_callback)),\n+        use_major_to_minor_data_layout_for_callbacks_(\n+            use_major_to_minor_data_layout_for_callbacks),\n         host_memory_for_device_manager_(host_memory_for_device_manager),\n         args_(host_callback_.operands.size()),\n         result_channels_(host_callback_.results.size()),\n         ready_count_(args_.size()) {\n-    CHECK(host_memory_for_device_manager_);\n-\n+    if (!use_major_to_minor_data_layout_for_callbacks_) {\n+      CHECK(host_memory_for_device_manager_);\n+    }\n     for (auto& channel : result_channels_) {\n       channel = std::make_unique<ThreadSafePjRtChunkQueue>();\n     }\n"
                },
                {
                    "old_start": 112,
                    "old_length": 6,
                    "new_start": 112,
                    "new_length": 7,
                    "hunk": "@@ -112,6 +112,7 @@ class HostCallbackContext {\n \n  private:\n   HostCallback host_callback_;\n+  bool use_major_to_minor_data_layout_for_callbacks_;\n   PjRtHostMemoryForDeviceManager* host_memory_for_device_manager_ = nullptr;\n   std::vector<PjRtChunk> args_;\n   std::vector<std::unique_ptr<ThreadSafePjRtChunkQueue>> result_channels_;\n"
                },
                {
                    "old_start": 128,
                    "old_length": 13,
                    "new_start": 129,
                    "new_length": 20,
                    "hunk": "@@ -128,13 +129,20 @@ struct HostCallbackStates {\n   std::vector<std::vector<RecvCallback>> recv_callbacks;\n };\n \n-// Creates the execution context for the `host_callback` for one replica.\n+// Creates the execution context for the `host_callback` for one\n+// replica.\n+//\n+// `use_major_to_minor_data_layout_for_callbacks` should match the value set in\n+// the corresponding ExecuteOptions; see the comment there for more\n+// info. `host_memory_for_device_manager` may be nullptr if\n+// `use_major_to_minor_data_layout_for_callbacks` is true.\n std::unique_ptr<HostCallbackContext>\n CreateHostCallbackStateAndAppendSendRecvCallbacks(\n     HostCallback host_callback,\n     PjRtHostMemoryForDeviceManager* host_memory_for_device_manager,\n     std::vector<SendCallback>& send_callbacks,\n-    std::vector<RecvCallback>& recv_callbacks);\n+    std::vector<RecvCallback>& recv_callbacks,\n+    bool use_major_to_minor_data_layout_for_callbacks);\n \n }  // namespace xla\n \n"
                }
            ],
            "whole_deleted": "-  HostCallbackContext(HostCallback host_callback, PjRtClient* client)\n-      : HostCallbackContext(std::move(host_callback),\n-                            client->GetPjRtHostMemoryForDeviceManager()) {}\n-\n-    CHECK(host_memory_for_device_manager_);\n-\n-// Creates the execution context for the `host_callback` for one replica.\n-    std::vector<RecvCallback>& recv_callbacks);\n",
            "whole_added": "+      bool use_major_to_minor_data_layout_for_callbacks,\n+        use_major_to_minor_data_layout_for_callbacks_(\n+            use_major_to_minor_data_layout_for_callbacks),\n+    if (!use_major_to_minor_data_layout_for_callbacks_) {\n+      CHECK(host_memory_for_device_manager_);\n+    }\n+  bool use_major_to_minor_data_layout_for_callbacks_;\n+// Creates the execution context for the `host_callback` for one\n+// replica.\n+//\n+// `use_major_to_minor_data_layout_for_callbacks` should match the value set in\n+// the corresponding ExecuteOptions; see the comment there for more\n+// info. `host_memory_for_device_manager` may be nullptr if\n+// `use_major_to_minor_data_layout_for_callbacks` is true.\n+    std::vector<RecvCallback>& recv_callbacks,\n+    bool use_major_to_minor_data_layout_for_callbacks);\n",
            "whole_hunk": "@@ -83,20 +83,20 @@ struct HostCallback {\n // A helper class that maintains the send/recv states for a host callback.\n class HostCallbackContext {\n  public:\n-  HostCallbackContext(HostCallback host_callback, PjRtClient* client)\n-      : HostCallbackContext(std::move(host_callback),\n-                            client->GetPjRtHostMemoryForDeviceManager()) {}\n-\n   HostCallbackContext(\n       HostCallback host_callback,\n+      bool use_major_to_minor_data_layout_for_callbacks,\n       PjRtHostMemoryForDeviceManager* host_memory_for_device_manager)\n       : host_callback_(std::move(host_callback)),\n+        use_major_to_minor_data_layout_for_callbacks_(\n+            use_major_to_minor_data_layout_for_callbacks),\n         host_memory_for_device_manager_(host_memory_for_device_manager),\n         args_(host_callback_.operands.size()),\n         result_channels_(host_callback_.results.size()),\n         ready_count_(args_.size()) {\n-    CHECK(host_memory_for_device_manager_);\n-\n+    if (!use_major_to_minor_data_layout_for_callbacks_) {\n+      CHECK(host_memory_for_device_manager_);\n+    }\n     for (auto& channel : result_channels_) {\n       channel = std::make_unique<ThreadSafePjRtChunkQueue>();\n     }\n@@ -112,6 +112,7 @@ class HostCallbackContext {\n \n  private:\n   HostCallback host_callback_;\n+  bool use_major_to_minor_data_layout_for_callbacks_;\n   PjRtHostMemoryForDeviceManager* host_memory_for_device_manager_ = nullptr;\n   std::vector<PjRtChunk> args_;\n   std::vector<std::unique_ptr<ThreadSafePjRtChunkQueue>> result_channels_;\n@@ -128,13 +129,20 @@ struct HostCallbackStates {\n   std::vector<std::vector<RecvCallback>> recv_callbacks;\n };\n \n-// Creates the execution context for the `host_callback` for one replica.\n+// Creates the execution context for the `host_callback` for one\n+// replica.\n+//\n+// `use_major_to_minor_data_layout_for_callbacks` should match the value set in\n+// the corresponding ExecuteOptions; see the comment there for more\n+// info. `host_memory_for_device_manager` may be nullptr if\n+// `use_major_to_minor_data_layout_for_callbacks` is true.\n std::unique_ptr<HostCallbackContext>\n CreateHostCallbackStateAndAppendSendRecvCallbacks(\n     HostCallback host_callback,\n     PjRtHostMemoryForDeviceManager* host_memory_for_device_manager,\n     std::vector<SendCallback>& send_callbacks,\n-    std::vector<RecvCallback>& recv_callbacks);\n+    std::vector<RecvCallback>& recv_callbacks,\n+    bool use_major_to_minor_data_layout_for_callbacks);\n \n }  // namespace xla\n \n"
        },
        {
            "name": "host_callback_test.cc",
            "path": "tensorflow/compiler/xla/pjrt/host_callback_test.cc",
            "patches": [
                {
                    "old_start": 91,
                    "old_length": 7,
                    "new_start": 91,
                    "new_length": 8,
                    "hunk": "@@ -91,7 +91,8 @@ TEST(HostCallbackTest, Basic) {\n \n   auto context = CreateHostCallbackStateAndAppendSendRecvCallbacks(\n       std::move(host_callback), &test_host_memory_for_device_manager,\n-      send_callbacks, recv_callbacks);\n+      send_callbacks, recv_callbacks,\n+      /*use_major_to_minor_data_layout_for_callback=*/false);\n \n   PjRtTransferMetadata metadata;\n   metadata.device_shape = shape;\n"
                }
            ],
            "whole_deleted": "-      send_callbacks, recv_callbacks);\n",
            "whole_added": "+      send_callbacks, recv_callbacks,\n+      /*use_major_to_minor_data_layout_for_callback=*/false);\n",
            "whole_hunk": "@@ -91,7 +91,8 @@ TEST(HostCallbackTest, Basic) {\n \n   auto context = CreateHostCallbackStateAndAppendSendRecvCallbacks(\n       std::move(host_callback), &test_host_memory_for_device_manager,\n-      send_callbacks, recv_callbacks);\n+      send_callbacks, recv_callbacks,\n+      /*use_major_to_minor_data_layout_for_callback=*/false);\n \n   PjRtTransferMetadata metadata;\n   metadata.device_shape = shape;\n"
        },
        {
            "name": "pjrt_client.h",
            "path": "tensorflow/compiler/xla/pjrt/pjrt_client.h",
            "patches": [
                {
                    "old_start": 755,
                    "old_length": 6,
                    "new_start": 755,
                    "new_length": 11,
                    "hunk": "@@ -755,6 +755,11 @@ class PjRtClient {\n   // Defragment device memory.\n   virtual Status Defragment() = 0;\n \n+  // If false, this client does not support send/recv host callbacks, and\n+  // callers should not set the `send_callbacks` and `recv_callbacks` arguments\n+  // in ExecuteOptions.\n+  virtual bool SupportsSendRecvCallbacks() const { return false; }\n+\n   // Return the PjRtHostMemoryForDeviceManager for this client. It can be\n   // nullptr if the implementation does not provide one.\n   virtual PjRtHostMemoryForDeviceManager* GetPjRtHostMemoryForDeviceManager()\n"
                },
                {
                    "old_start": 1088,
                    "old_length": 6,
                    "new_start": 1093,
                    "new_length": 9,
                    "hunk": "@@ -1088,6 +1093,9 @@ class ExecuteContext {\n };\n \n struct PjRtTransferMetadata {\n+  // May be invalid if\n+  // ExecuteOptions::use_major_to_minor_data_layout_for_callbacks is true for\n+  // this execution.\n   Shape device_shape;\n };\n \n"
                },
                {
                    "old_start": 1159,
                    "old_length": 6,
                    "new_start": 1167,
                    "new_length": 15,
                    "hunk": "@@ -1159,6 +1167,15 @@ struct ExecuteOptions {\n   absl::Span<const std::vector<SendCallback>> send_callbacks;\n   absl::Span<const std::vector<RecvCallback>> recv_callbacks;\n \n+  // If true, send callbacks are passed PjRtChunks in major-to-minor layout, and\n+  // recv functions should pass major-to-minor chunks to\n+  // CopyToDeviceStream::AddChunk.\n+  //\n+  // If false, send callbacks are passed PjRtChunks in the on-device layout\n+  // specified in the PjRtTransferMetadata, and recv functions should similarly\n+  // pass device-layout chunks to CopyToDeviceStream::AddChunk.\n+  bool use_major_to_minor_data_layout_for_callbacks = false;\n+\n   // The `execution_mode` decides whether the execution will be invoked in the\n   // caller thread or launched to a separate thread. By default, the\n   // implementation may choose either strategy or use a heuristic to decide.\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // If false, this client does not support send/recv host callbacks, and\n+  // callers should not set the `send_callbacks` and `recv_callbacks` arguments\n+  // in ExecuteOptions.\n+  virtual bool SupportsSendRecvCallbacks() const { return false; }\n+\n+  // May be invalid if\n+  // ExecuteOptions::use_major_to_minor_data_layout_for_callbacks is true for\n+  // this execution.\n+  // If true, send callbacks are passed PjRtChunks in major-to-minor layout, and\n+  // recv functions should pass major-to-minor chunks to\n+  // CopyToDeviceStream::AddChunk.\n+  //\n+  // If false, send callbacks are passed PjRtChunks in the on-device layout\n+  // specified in the PjRtTransferMetadata, and recv functions should similarly\n+  // pass device-layout chunks to CopyToDeviceStream::AddChunk.\n+  bool use_major_to_minor_data_layout_for_callbacks = false;\n+\n",
            "whole_hunk": "@@ -755,6 +755,11 @@ class PjRtClient {\n   // Defragment device memory.\n   virtual Status Defragment() = 0;\n \n+  // If false, this client does not support send/recv host callbacks, and\n+  // callers should not set the `send_callbacks` and `recv_callbacks` arguments\n+  // in ExecuteOptions.\n+  virtual bool SupportsSendRecvCallbacks() const { return false; }\n+\n   // Return the PjRtHostMemoryForDeviceManager for this client. It can be\n   // nullptr if the implementation does not provide one.\n   virtual PjRtHostMemoryForDeviceManager* GetPjRtHostMemoryForDeviceManager()\n@@ -1088,6 +1093,9 @@ class ExecuteContext {\n };\n \n struct PjRtTransferMetadata {\n+  // May be invalid if\n+  // ExecuteOptions::use_major_to_minor_data_layout_for_callbacks is true for\n+  // this execution.\n   Shape device_shape;\n };\n \n@@ -1159,6 +1167,15 @@ struct ExecuteOptions {\n   absl::Span<const std::vector<SendCallback>> send_callbacks;\n   absl::Span<const std::vector<RecvCallback>> recv_callbacks;\n \n+  // If true, send callbacks are passed PjRtChunks in major-to-minor layout, and\n+  // recv functions should pass major-to-minor chunks to\n+  // CopyToDeviceStream::AddChunk.\n+  //\n+  // If false, send callbacks are passed PjRtChunks in the on-device layout\n+  // specified in the PjRtTransferMetadata, and recv functions should similarly\n+  // pass device-layout chunks to CopyToDeviceStream::AddChunk.\n+  bool use_major_to_minor_data_layout_for_callbacks = false;\n+\n   // The `execution_mode` decides whether the execution will be invoked in the\n   // caller thread or launched to a separate thread. By default, the\n   // implementation may choose either strategy or use a heuristic to decide.\n"
        },
        {
            "name": "py_executable.cc",
            "path": "tensorflow/compiler/xla/python/py_executable.cc",
            "patches": [
                {
                    "old_start": 76,
                    "old_length": 6,
                    "new_start": 76,
                    "new_length": 7,
                    "hunk": "@@ -76,6 +76,7 @@ PyLoadedExecutable::PyLoadedExecutable(\n     VLOG(1) << \"Fingerprint for executable \" << ifrt_loaded_executable_->name()\n             << \": \" << *fingerprint_;\n   }\n+  options_.use_major_to_minor_data_layout_for_callbacks = true;\n }\n \n PyLoadedExecutable::~PyLoadedExecutable() {\n"
                },
                {
                    "old_start": 186,
                    "old_length": 9,
                    "new_start": 187,
                    "new_length": 7,
                    "hunk": "@@ -186,9 +187,7 @@ StatusOr<PyExecuteResults> ExecuteShardedOnLocalDevicesInternal(\n     auto opts = options;\n     std::shared_ptr<HostCallbackStates> host_callback_states;\n     if (!host_callbacks.empty()) {\n-      auto* host_memory_for_device_manager =\n-          client->pjrt_client()->GetPjRtHostMemoryForDeviceManager();\n-      if (host_memory_for_device_manager == nullptr) {\n+      if (!client->pjrt_client()->SupportsSendRecvCallbacks()) {\n         return InternalError(\"Host callback not supported for runtime type: %s\",\n                              client->runtime_type());\n       }\n"
                },
                {
                    "old_start": 206,
                    "old_length": 7,
                    "new_start": 204,
                    "new_length": 9,
                    "hunk": "@@ -206,7 +204,9 @@ StatusOr<PyExecuteResults> ExecuteShardedOnLocalDevicesInternal(\n         for (const py::capsule& host_callback : host_callbacks) {\n           contexts.push_back(CreateHostCallbackStateAndAppendSendRecvCallbacks(\n               *host_callback.get_pointer<HostCallback>(),\n-              host_memory_for_device_manager, send_callbacks, recv_callbacks));\n+              /*host_memory_for_device_manager=*/nullptr, send_callbacks,\n+              recv_callbacks,\n+              /*use_major_to_minor_data_layout_for_callbacks=*/true));\n         }\n       }\n       opts.send_callbacks = host_callback_states->send_callbacks;"
                }
            ],
            "whole_deleted": "-      auto* host_memory_for_device_manager =\n-          client->pjrt_client()->GetPjRtHostMemoryForDeviceManager();\n-      if (host_memory_for_device_manager == nullptr) {\n-              host_memory_for_device_manager, send_callbacks, recv_callbacks));\n",
            "whole_added": "+  options_.use_major_to_minor_data_layout_for_callbacks = true;\n+      if (!client->pjrt_client()->SupportsSendRecvCallbacks()) {\n+              /*host_memory_for_device_manager=*/nullptr, send_callbacks,\n+              recv_callbacks,\n+              /*use_major_to_minor_data_layout_for_callbacks=*/true));\n",
            "whole_hunk": "@@ -76,6 +76,7 @@ PyLoadedExecutable::PyLoadedExecutable(\n     VLOG(1) << \"Fingerprint for executable \" << ifrt_loaded_executable_->name()\n             << \": \" << *fingerprint_;\n   }\n+  options_.use_major_to_minor_data_layout_for_callbacks = true;\n }\n \n PyLoadedExecutable::~PyLoadedExecutable() {\n@@ -186,9 +187,7 @@ StatusOr<PyExecuteResults> ExecuteShardedOnLocalDevicesInternal(\n     auto opts = options;\n     std::shared_ptr<HostCallbackStates> host_callback_states;\n     if (!host_callbacks.empty()) {\n-      auto* host_memory_for_device_manager =\n-          client->pjrt_client()->GetPjRtHostMemoryForDeviceManager();\n-      if (host_memory_for_device_manager == nullptr) {\n+      if (!client->pjrt_client()->SupportsSendRecvCallbacks()) {\n         return InternalError(\"Host callback not supported for runtime type: %s\",\n                              client->runtime_type());\n       }\n@@ -206,7 +204,9 @@ StatusOr<PyExecuteResults> ExecuteShardedOnLocalDevicesInternal(\n         for (const py::capsule& host_callback : host_callbacks) {\n           contexts.push_back(CreateHostCallbackStateAndAppendSendRecvCallbacks(\n               *host_callback.get_pointer<HostCallback>(),\n-              host_memory_for_device_manager, send_callbacks, recv_callbacks));\n+              /*host_memory_for_device_manager=*/nullptr, send_callbacks,\n+              recv_callbacks,\n+              /*use_major_to_minor_data_layout_for_callbacks=*/true));\n         }\n       }\n       opts.send_callbacks = host_callback_states->send_callbacks;"
        }
    ]
},
{
    "Id": 275,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/63feaf321165e1e2795f43e3834c007364921df6",
    "date": "2023-09-29T14:53:21+03:00",
    "message": "Add check for raster bits.",
    "label": "YES",
    "changes": [
        {
            "name": "gif_io.cc",
            "path": "tensorflow/core/lib/gif/gif_io.cc",
            "patches": [
                {
                    "old_start": 78,
                    "old_length": 6,
                    "new_start": 78,
                    "new_length": 12,
                    "hunk": "@@ -78,6 +78,12 @@ uint8* Decode(const void* srcdata, int datasize,\n   if (DGifSlurp(gif_file) != GIF_OK) {\n     *error_string = absl::StrCat(\"failed to slurp gif file: \",\n                                  GifErrorStringNonNull(gif_file->Error));\n+    // Stop load if no images are detected or the allocation of the last image\n+    // buffer was failed.\n+    if (gif_file->ImageCount <= 0 ||\n+        gif_file->SavedImages[gif_file->ImageCount - 1].RasterBits == NULL) {\n+    }\n+\n     LOG(ERROR) << *error_string;\n     return nullptr;\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    // Stop load if no images are detected or the allocation of the last image\n+    // buffer was failed.\n+    if (gif_file->ImageCount <= 0 ||\n+        gif_file->SavedImages[gif_file->ImageCount - 1].RasterBits == NULL) {\n+    }\n+\n",
            "whole_hunk": "@@ -78,6 +78,12 @@ uint8* Decode(const void* srcdata, int datasize,\n   if (DGifSlurp(gif_file) != GIF_OK) {\n     *error_string = absl::StrCat(\"failed to slurp gif file: \",\n                                  GifErrorStringNonNull(gif_file->Error));\n+    // Stop load if no images are detected or the allocation of the last image\n+    // buffer was failed.\n+    if (gif_file->ImageCount <= 0 ||\n+        gif_file->SavedImages[gif_file->ImageCount - 1].RasterBits == NULL) {\n+    }\n+\n     LOG(ERROR) << *error_string;\n     return nullptr;\n   }"
        }
    ]
},
{
    "Id": 665,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a34c51c30ceb98a02124da2c18301abd1aa76e60",
    "date": "2022-10-06T22:14:59-07:00",
    "message": "Only unfollow when numbers of strategies are different. Do not check output sharding consistency anymore because when the input and output shardings are consistent, they can be different if input and outputs have different number of dimensions.\n\nPiperOrigin-RevId: 479492433",
    "label": "YES",
    "changes": [
        {
            "name": "auto_sharding.cc",
            "path": "tensorflow/compiler/xla/experimental/auto_sharding/auto_sharding.cc",
            "patches": [
                {
                    "old_start": 1043,
                    "old_length": 19,
                    "new_start": 1043,
                    "new_length": 6,
                    "hunk": "@@ -1043,19 +1043,6 @@ bool LeafVectorsAreConsistent(const std::vector<ShardingStrategy>& one,\n   if (one.size() != two.size()) {\n     return false;\n   }\n-  if (is_reshape) {\n-    // Checks only the sizes for reshapes.\n-    return true;\n-  }\n-  for (size_t i = 0; i < one.size(); i++) {\n-    // Note: this will always unfollow for instructions with operands of\n-    // different shapes. Need to re-evaluate whether we should follow in this\n-    // case.\n-    if (one.at(i).output_sharding.ToString() !=\n-        two.at(i).output_sharding.ToString()) {\n-      return false;\n-    }\n-  }\n   return true;\n }\n "
                }
            ],
            "whole_deleted": "-  if (is_reshape) {\n-    // Checks only the sizes for reshapes.\n-    return true;\n-  }\n-  for (size_t i = 0; i < one.size(); i++) {\n-    // Note: this will always unfollow for instructions with operands of\n-    // different shapes. Need to re-evaluate whether we should follow in this\n-    // case.\n-    if (one.at(i).output_sharding.ToString() !=\n-        two.at(i).output_sharding.ToString()) {\n-      return false;\n-    }\n-  }\n",
            "whole_added": "",
            "whole_hunk": "@@ -1043,19 +1043,6 @@ bool LeafVectorsAreConsistent(const std::vector<ShardingStrategy>& one,\n   if (one.size() != two.size()) {\n     return false;\n   }\n-  if (is_reshape) {\n-    // Checks only the sizes for reshapes.\n-    return true;\n-  }\n-  for (size_t i = 0; i < one.size(); i++) {\n-    // Note: this will always unfollow for instructions with operands of\n-    // different shapes. Need to re-evaluate whether we should follow in this\n-    // case.\n-    if (one.at(i).output_sharding.ToString() !=\n-        two.at(i).output_sharding.ToString()) {\n-      return false;\n-    }\n-  }\n   return true;\n }\n "
        }
    ]
},
{
    "Id": 338,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/91b04be7fdac04467d537b81826057f8fc26dea0",
    "date": "2023-07-31T17:58:32-07:00",
    "message": "[XLA:GPU] Clean up cudnn runtime fusion, but don't enable it.\n\nIn a previous CL I disabled cudnn runtime fusion because we found some convs\nthat weren't working -- cudnn returned 0 algorithms, so you couldn't run the\nconv.  It was unclear at the time why this was happening.\n\nAfter reading the code today, I noticed that kElu convs have a check that was\nmissing in kRelu6 and kLeakyRelu convs.  Namely, kElu fusion skips convs with\nan odd number of input/output features.\n\nWhen I look at the failing testcases we had for kRelu6 and kLeakyRelu, they\nboth had an odd number of features.  Bingo!\n\nUnfortunately, I cannot enable this by default, because it's very slow to\ncompile these convs on Ampere -- 7s with runtime fusion, as opposed to ~50ms\nwithout.  This causes a 100x slowdown in compilation time.\n\nPiperOrigin-RevId: 552642750",
    "label": "YES",
    "changes": [
        {
            "name": "debug_options_flags.cc",
            "path": "tensorflow/compiler/xla/debug_options_flags.cc",
            "patches": [
                {
                    "old_start": 43,
                    "old_length": 27,
                    "new_start": 43,
                    "new_length": 12,
                    "hunk": "@@ -43,27 +43,12 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_cuda_data_dir(\"./cuda_sdk_lib\");\n   opts.set_xla_gpu_asm_extra_flags(\"\");\n \n-  // As of cudnn 8.9.0, enabling cudnn runtime fusion sometimes causes a\n-  // situation where cudnn returns 0 algorithms for an otherwise-valid conv,\n-  // causing compilation to fail.  Examples of failing convs:\n-  //\n-  // // failing kLeakyRelu, b/290967578\n-  // (f16[2,256,768,16]{3,2,1,0}, u8[0]{0})\n-  //   custom-call(f16[2,256,768,3]{3,2,1,0} %a, f16[16,3,3,3]{3,2,1,0} %b,\n-  //   f16[16]{0} %c), window={size=3x3 pad=1_1x1_1},\n-  //   dim_labels=b01f_o01i->b01f, operand_precision={highest,highest},\n-  //   custom_call_target=\"__cudnn$convBiasActivationForward\",\n-  //   backend_config={\"activation_mode\":\"kLeakyRelu\",\"conv_result_scale\":1,\n-  //                   \"side_input_scale\":0,\"leakyrelu_alpha\":0.199951171875}\n-  //\n-  // // failing kRelu6, b/291011396\n-  // (f16[1,384,1024,32]{3,2,1,0}, u8[0]{0})\n-  //   custom-call(f16[1,769,2049,3]{3,2,1,0} %a, f16[32,3,3,3]{3,2,1,0} %b,\n-  //   f16[32]{0} %c), window={size=3x3 stride=2x2}, dim_labels=b01f_o01i->b01f,\n-  //   operand_precision={highest,highest},\n-  //   custom_call_target=\"__cudnn$convBiasActivationForward\",\n-  //   backend_config={\"activation_mode\":\"kRelu6\",\"conv_result_scale\":1,\n-  //                   \"side_input_scale\":0,\"leakyrelu_alpha\":0}\n+  // As of cudnn 8.9.0, runtime fusion creates convolutions that take about 7s\n+  // seconds to run the first time we call them, at least on Ampere.  In\n+  // contrast, non-runtime-fusion convs usually run in about 50ms.  Thus runtime\n+  // fusion can cause a 100x slowdown when compiling models that have convs that\n+  // use runtime fusion.  We therefore can't enable this by default today.\n+  // Additional details in b/237009940#comment46.\n   opts.set_xla_gpu_use_runtime_fusion(false);\n \n   opts.set_xla_eliminate_hlo_implicit_broadcast(true);\n"
                }
            ],
            "whole_deleted": "-  // As of cudnn 8.9.0, enabling cudnn runtime fusion sometimes causes a\n-  // situation where cudnn returns 0 algorithms for an otherwise-valid conv,\n-  // causing compilation to fail.  Examples of failing convs:\n-  //\n-  // // failing kLeakyRelu, b/290967578\n-  // (f16[2,256,768,16]{3,2,1,0}, u8[0]{0})\n-  //   custom-call(f16[2,256,768,3]{3,2,1,0} %a, f16[16,3,3,3]{3,2,1,0} %b,\n-  //   f16[16]{0} %c), window={size=3x3 pad=1_1x1_1},\n-  //   dim_labels=b01f_o01i->b01f, operand_precision={highest,highest},\n-  //   custom_call_target=\"__cudnn$convBiasActivationForward\",\n-  //   backend_config={\"activation_mode\":\"kLeakyRelu\",\"conv_result_scale\":1,\n-  //                   \"side_input_scale\":0,\"leakyrelu_alpha\":0.199951171875}\n-  //\n-  // // failing kRelu6, b/291011396\n-  // (f16[1,384,1024,32]{3,2,1,0}, u8[0]{0})\n-  //   custom-call(f16[1,769,2049,3]{3,2,1,0} %a, f16[32,3,3,3]{3,2,1,0} %b,\n-  //   f16[32]{0} %c), window={size=3x3 stride=2x2}, dim_labels=b01f_o01i->b01f,\n-  //   operand_precision={highest,highest},\n-  //   custom_call_target=\"__cudnn$convBiasActivationForward\",\n-  //   backend_config={\"activation_mode\":\"kRelu6\",\"conv_result_scale\":1,\n-  //                   \"side_input_scale\":0,\"leakyrelu_alpha\":0}\n",
            "whole_added": "+  // As of cudnn 8.9.0, runtime fusion creates convolutions that take about 7s\n+  // seconds to run the first time we call them, at least on Ampere.  In\n+  // contrast, non-runtime-fusion convs usually run in about 50ms.  Thus runtime\n+  // fusion can cause a 100x slowdown when compiling models that have convs that\n+  // use runtime fusion.  We therefore can't enable this by default today.\n+  // Additional details in b/237009940#comment46.\n",
            "whole_hunk": "@@ -43,27 +43,12 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n   opts.set_xla_gpu_cuda_data_dir(\"./cuda_sdk_lib\");\n   opts.set_xla_gpu_asm_extra_flags(\"\");\n \n-  // As of cudnn 8.9.0, enabling cudnn runtime fusion sometimes causes a\n-  // situation where cudnn returns 0 algorithms for an otherwise-valid conv,\n-  // causing compilation to fail.  Examples of failing convs:\n-  //\n-  // // failing kLeakyRelu, b/290967578\n-  // (f16[2,256,768,16]{3,2,1,0}, u8[0]{0})\n-  //   custom-call(f16[2,256,768,3]{3,2,1,0} %a, f16[16,3,3,3]{3,2,1,0} %b,\n-  //   f16[16]{0} %c), window={size=3x3 pad=1_1x1_1},\n-  //   dim_labels=b01f_o01i->b01f, operand_precision={highest,highest},\n-  //   custom_call_target=\"__cudnn$convBiasActivationForward\",\n-  //   backend_config={\"activation_mode\":\"kLeakyRelu\",\"conv_result_scale\":1,\n-  //                   \"side_input_scale\":0,\"leakyrelu_alpha\":0.199951171875}\n-  //\n-  // // failing kRelu6, b/291011396\n-  // (f16[1,384,1024,32]{3,2,1,0}, u8[0]{0})\n-  //   custom-call(f16[1,769,2049,3]{3,2,1,0} %a, f16[32,3,3,3]{3,2,1,0} %b,\n-  //   f16[32]{0} %c), window={size=3x3 stride=2x2}, dim_labels=b01f_o01i->b01f,\n-  //   operand_precision={highest,highest},\n-  //   custom_call_target=\"__cudnn$convBiasActivationForward\",\n-  //   backend_config={\"activation_mode\":\"kRelu6\",\"conv_result_scale\":1,\n-  //                   \"side_input_scale\":0,\"leakyrelu_alpha\":0}\n+  // As of cudnn 8.9.0, runtime fusion creates convolutions that take about 7s\n+  // seconds to run the first time we call them, at least on Ampere.  In\n+  // contrast, non-runtime-fusion convs usually run in about 50ms.  Thus runtime\n+  // fusion can cause a 100x slowdown when compiling models that have convs that\n+  // use runtime fusion.  We therefore can't enable this by default today.\n+  // Additional details in b/237009940#comment46.\n   opts.set_xla_gpu_use_runtime_fusion(false);\n \n   opts.set_xla_eliminate_hlo_implicit_broadcast(true);\n"
        },
        {
            "name": "cudnn_fused_conv_rewriter.cc",
            "path": "tensorflow/compiler/xla/service/gpu/cudnn_fused_conv_rewriter.cc",
            "patches": [
                {
                    "old_start": 86,
                    "old_length": 6,
                    "new_start": 86,
                    "new_length": 31,
                    "hunk": "@@ -86,6 +86,31 @@ bool ShouldUseCudnnRuntimeFusion(const DebugOptions& debug_opts,\n   return debug_opts.xla_gpu_use_runtime_fusion() && cc.IsAtLeast(7, 5);\n }\n \n+bool IsSuitableForCudnnRuntimeFusion(HloInstruction* conv) {\n+  // cudnn runtime fusion is pathologically slow on convs with side-inputs.\n+  // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n+  if (conv->operands().size() > 3) {\n+    return false;\n+  }\n+\n+  // cuDNN runtime funsion kernels require 32-bit aligned data access, which\n+  // means that the number of in/out channels must be divisible by 2 for fp16.\n+  // (We don't currently do runtime fusion for int8.)\n+  if (conv->operand(0)->shape().element_type() != F16) {\n+    return false;\n+  }\n+  const Shape& shape = conv->operand(1)->shape();\n+  int64_t num_input_features = shape.dimensions(\n+      conv->convolution_dimension_numbers().kernel_input_feature_dimension());\n+  int64_t num_output_features = shape.dimensions(\n+      conv->convolution_dimension_numbers().kernel_output_feature_dimension());\n+  if (num_input_features % 2 != 0 || num_output_features % 2 != 0) {\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n // Can instr be converted to type `dst_ty` without losing any precision?  For\n // our purposes, this is true if:\n //\n"
                },
                {
                    "old_start": 821,
                    "old_length": 26,
                    "new_start": 846,
                    "new_length": 7,
                    "hunk": "@@ -821,26 +846,7 @@ StatusOr<bool> FuseElu(HloComputation* comp, se::CudaComputeCapability cc) {\n       continue;\n     }\n \n-    // In some cases, the XLA optimizes the inputs of the convolution by\n-    // moving and broadcasting the bias to the side input, e.g., when the input\n-    // spatial dimensions are all ones and filter spatial dimentsions are all\n-    // non-ones. However, there is a known issue that the side input is not well\n-    // supported in the cuDNN runtime fusion. Therefore, we skip these cases.\n-    // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n-    if (conv->operands().size() > 3) {\n-      continue;\n-    }\n-\n-    // cuDNN runtime funsion kernels require 32-bit aligned data access. Since\n-    // we only allow fp16 datatype, we need to check if the in and out channels\n-    // of filter are even numbers.\n-    const Shape& shape = conv->operand(1)->shape();\n-    int64_t num_input_features = shape.dimensions(\n-        conv->convolution_dimension_numbers().kernel_input_feature_dimension());\n-    int64_t num_output_features =\n-        shape.dimensions(conv->convolution_dimension_numbers()\n-                             .kernel_output_feature_dimension());\n-    if (num_input_features % 2 != 0 || num_output_features % 2 != 0) {\n+    if (!IsSuitableForCudnnRuntimeFusion(conv)) {\n       continue;\n     }\n \n"
                },
                {
                    "old_start": 927,
                    "old_length": 9,
                    "new_start": 933,
                    "new_length": 7,
                    "hunk": "@@ -927,9 +933,7 @@ StatusOr<bool> FuseRelu6(HloComputation* comp, se::CudaComputeCapability cc) {\n       continue;\n     }\n \n-    // cudnn runtime fusions seem to be very slow when a side input is present.\n-    // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n-    if (conv->operands().size() > 3) {\n+    if (!IsSuitableForCudnnRuntimeFusion(conv)) {\n       continue;\n     }\n \n"
                },
                {
                    "old_start": 986,
                    "old_length": 9,
                    "new_start": 990,
                    "new_length": 7,
                    "hunk": "@@ -986,9 +990,7 @@ StatusOr<bool> FuseLeakyRelu(HloComputation* comp,\n       continue;\n     }\n \n-    // cudnn runtime fusions seem to be very slow when a side input is present.\n-    // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n-    if (conv->operands().size() > 3) {\n+    if (!IsSuitableForCudnnRuntimeFusion(conv)) {\n       continue;\n     }\n \n"
                }
            ],
            "whole_deleted": "-    // In some cases, the XLA optimizes the inputs of the convolution by\n-    // moving and broadcasting the bias to the side input, e.g., when the input\n-    // spatial dimensions are all ones and filter spatial dimentsions are all\n-    // non-ones. However, there is a known issue that the side input is not well\n-    // supported in the cuDNN runtime fusion. Therefore, we skip these cases.\n-    // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n-    if (conv->operands().size() > 3) {\n-      continue;\n-    }\n-\n-    // cuDNN runtime funsion kernels require 32-bit aligned data access. Since\n-    // we only allow fp16 datatype, we need to check if the in and out channels\n-    // of filter are even numbers.\n-    const Shape& shape = conv->operand(1)->shape();\n-    int64_t num_input_features = shape.dimensions(\n-        conv->convolution_dimension_numbers().kernel_input_feature_dimension());\n-    int64_t num_output_features =\n-        shape.dimensions(conv->convolution_dimension_numbers()\n-                             .kernel_output_feature_dimension());\n-    if (num_input_features % 2 != 0 || num_output_features % 2 != 0) {\n-    // cudnn runtime fusions seem to be very slow when a side input is present.\n-    // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n-    if (conv->operands().size() > 3) {\n-    // cudnn runtime fusions seem to be very slow when a side input is present.\n-    // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n-    if (conv->operands().size() > 3) {\n",
            "whole_added": "+bool IsSuitableForCudnnRuntimeFusion(HloInstruction* conv) {\n+  // cudnn runtime fusion is pathologically slow on convs with side-inputs.\n+  // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n+  if (conv->operands().size() > 3) {\n+    return false;\n+  }\n+\n+  // cuDNN runtime funsion kernels require 32-bit aligned data access, which\n+  // means that the number of in/out channels must be divisible by 2 for fp16.\n+  // (We don't currently do runtime fusion for int8.)\n+  if (conv->operand(0)->shape().element_type() != F16) {\n+    return false;\n+  }\n+  const Shape& shape = conv->operand(1)->shape();\n+  int64_t num_input_features = shape.dimensions(\n+      conv->convolution_dimension_numbers().kernel_input_feature_dimension());\n+  int64_t num_output_features = shape.dimensions(\n+      conv->convolution_dimension_numbers().kernel_output_feature_dimension());\n+  if (num_input_features % 2 != 0 || num_output_features % 2 != 0) {\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+    if (!IsSuitableForCudnnRuntimeFusion(conv)) {\n+    if (!IsSuitableForCudnnRuntimeFusion(conv)) {\n+    if (!IsSuitableForCudnnRuntimeFusion(conv)) {\n",
            "whole_hunk": "@@ -86,6 +86,31 @@ bool ShouldUseCudnnRuntimeFusion(const DebugOptions& debug_opts,\n   return debug_opts.xla_gpu_use_runtime_fusion() && cc.IsAtLeast(7, 5);\n }\n \n+bool IsSuitableForCudnnRuntimeFusion(HloInstruction* conv) {\n+  // cudnn runtime fusion is pathologically slow on convs with side-inputs.\n+  // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n+  if (conv->operands().size() > 3) {\n+    return false;\n+  }\n+\n+  // cuDNN runtime funsion kernels require 32-bit aligned data access, which\n+  // means that the number of in/out channels must be divisible by 2 for fp16.\n+  // (We don't currently do runtime fusion for int8.)\n+  if (conv->operand(0)->shape().element_type() != F16) {\n+    return false;\n+  }\n+  const Shape& shape = conv->operand(1)->shape();\n+  int64_t num_input_features = shape.dimensions(\n+      conv->convolution_dimension_numbers().kernel_input_feature_dimension());\n+  int64_t num_output_features = shape.dimensions(\n+      conv->convolution_dimension_numbers().kernel_output_feature_dimension());\n+  if (num_input_features % 2 != 0 || num_output_features % 2 != 0) {\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n // Can instr be converted to type `dst_ty` without losing any precision?  For\n // our purposes, this is true if:\n //\n@@ -821,26 +846,7 @@ StatusOr<bool> FuseElu(HloComputation* comp, se::CudaComputeCapability cc) {\n       continue;\n     }\n \n-    // In some cases, the XLA optimizes the inputs of the convolution by\n-    // moving and broadcasting the bias to the side input, e.g., when the input\n-    // spatial dimensions are all ones and filter spatial dimentsions are all\n-    // non-ones. However, there is a known issue that the side input is not well\n-    // supported in the cuDNN runtime fusion. Therefore, we skip these cases.\n-    // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n-    if (conv->operands().size() > 3) {\n-      continue;\n-    }\n-\n-    // cuDNN runtime funsion kernels require 32-bit aligned data access. Since\n-    // we only allow fp16 datatype, we need to check if the in and out channels\n-    // of filter are even numbers.\n-    const Shape& shape = conv->operand(1)->shape();\n-    int64_t num_input_features = shape.dimensions(\n-        conv->convolution_dimension_numbers().kernel_input_feature_dimension());\n-    int64_t num_output_features =\n-        shape.dimensions(conv->convolution_dimension_numbers()\n-                             .kernel_output_feature_dimension());\n-    if (num_input_features % 2 != 0 || num_output_features % 2 != 0) {\n+    if (!IsSuitableForCudnnRuntimeFusion(conv)) {\n       continue;\n     }\n \n@@ -927,9 +933,7 @@ StatusOr<bool> FuseRelu6(HloComputation* comp, se::CudaComputeCapability cc) {\n       continue;\n     }\n \n-    // cudnn runtime fusions seem to be very slow when a side input is present.\n-    // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n-    if (conv->operands().size() > 3) {\n+    if (!IsSuitableForCudnnRuntimeFusion(conv)) {\n       continue;\n     }\n \n@@ -986,9 +990,7 @@ StatusOr<bool> FuseLeakyRelu(HloComputation* comp,\n       continue;\n     }\n \n-    // cudnn runtime fusions seem to be very slow when a side input is present.\n-    // TODO(kaixih@nvidia): remove this check when cuDNN fixes it.\n-    if (conv->operands().size() > 3) {\n+    if (!IsSuitableForCudnnRuntimeFusion(conv)) {\n       continue;\n     }\n \n"
        },
        {
            "name": "cudnn_fused_conv_rewriter_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/cudnn_fused_conv_rewriter_test.cc",
            "patches": [
                {
                    "old_start": 113,
                    "old_length": 7,
                    "new_start": 113,
                    "new_length": 13,
                    "hunk": "@@ -113,7 +113,13 @@ class CudnnFusedConvRewriterTest : public GpuCodegenTest {\n           << optimized_hlo_string;\n       EXPECT_THAT(optimized_hlo_string,\n                   HasSubstr(kCudnnConvBiasActivationForwardCallTarget));\n-      EXPECT_TRUE(RunAndCompare(hlo_with_new_type, ErrorSpec{0.01}))\n+\n+      TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                              ParseAndReturnVerifiedModule(hlo_with_new_type));\n+      DebugOptions debug_opts = module->config().debug_options();\n+      debug_opts.set_xla_gpu_use_runtime_fusion(true);\n+      module->config().set_debug_options(debug_opts);\n+      EXPECT_TRUE(RunAndCompare(std::move(module), ErrorSpec{0.01}))\n           << optimized_hlo_string;\n     }\n   }\n"
                },
                {
                    "old_start": 368,
                    "old_length": 6,
                    "new_start": 374,
                    "new_length": 31,
                    "hunk": "@@ -368,6 +374,31 @@ TEST_F(CudnnFusedConvRewriterTest, TestRelu6) {\n     })\");\n }\n \n+// At time of writing, cudnn runtime fusion cannot handle f16 convs with an odd\n+// number of input/output channels.  Check that we don't try to run this conv\n+// with runtime fusion (or, if we do, that it works!).\n+TEST_F(CudnnFusedConvRewriterTest, TestRelu6OddChannels) {\n+  if (!GetCudaComputeCapability().IsAtLeast(\n+          se::CudaComputeCapability::AMPERE)) {\n+    GTEST_SKIP() << \"Conv-Bias-Relu6 fusion is supported and recommended with \"\n+                    \"the Nvidia Ampere+ GPUs.\";\n+  }\n+\n+  TestMatchWithAllTypes(R\"(\n+    HloModule Test\n+    ENTRY Test {\n+      zeros = TYPE[1,384,1024,32] broadcast(TYPE[] constant(0)), dimensions={}\n+      sixes = TYPE[1,384,1024,32] broadcast(TYPE[] constant(6)), dimensions={}\n+      input = TYPE[1,769,2049,3] parameter(0)\n+      filter = TYPE[32,3,3,3] parameter(1)\n+      bias = TYPE[32] parameter(2)\n+      conv = TYPE[1,384,1024,32] convolution(input, filter), window={size=3x3 stride=2x2}, dim_labels=b01f_o01i->b01f\n+      broadcasted_bias = TYPE[1,384,1024,32] broadcast(bias), dimensions={3}\n+      sum = add(conv, broadcasted_bias)\n+      ROOT relu6 = clamp(zeros, sum, sixes)\n+    })\");\n+}\n+\n TEST_F(CudnnFusedConvRewriterTest, TestLeakyRelu) {\n   if (!GetCudaComputeCapability().IsAtLeast(\n           se::CudaComputeCapability::AMPERE)) {\n"
                },
                {
                    "old_start": 1381,
                    "old_length": 8,
                    "new_start": 1412,
                    "new_length": 8,
                    "hunk": "@@ -1381,8 +1412,8 @@ TEST_F(CudnnFusedConvRewriterHloTest, FuseRelu6) {\n   const std::string module_str = R\"(\n     HloModule Test\n     ENTRY Test {\n-      inputs = f16[1,17,9,9] parameter(0)\n-      filters = f16[3,3,17,32] parameter(1)\n+      inputs = f16[1,18,9,9] parameter(0)\n+      filters = f16[3,3,18,32] parameter(1)\n       bias = f16[32] parameter(2)\n       bias_broadcast = f16[1,32,9,9] broadcast(bias), dimensions={1}\n       zero = f16[] constant(0)\n"
                },
                {
                    "old_start": 1423,
                    "old_length": 8,
                    "new_start": 1454,
                    "new_length": 8,
                    "hunk": "@@ -1423,8 +1454,8 @@ TEST_F(CudnnFusedConvRewriterHloTest, DontFuseRelu6IfMultipleUses) {\n   const std::string module_str = R\"(\n     HloModule Test\n     ENTRY Test {\n-      inputs = f16[1,17,9,9] parameter(0)\n-      filters = f16[3,3,17,32] parameter(1)\n+      inputs = f16[1,18,9,9] parameter(0)\n+      filters = f16[3,3,18,32] parameter(1)\n       bias = f16[1,32,9,9] broadcast(f16[32] parameter(2)), dimensions={1}\n       zeros = f16[1,32,9,9] broadcast(f16[] constant(0)), dimensions={}\n       sixes = f16[1,32,9,9] broadcast(f16[] constant(6)), dimensions={}"
                }
            ],
            "whole_deleted": "-      EXPECT_TRUE(RunAndCompare(hlo_with_new_type, ErrorSpec{0.01}))\n-      inputs = f16[1,17,9,9] parameter(0)\n-      filters = f16[3,3,17,32] parameter(1)\n-      inputs = f16[1,17,9,9] parameter(0)\n-      filters = f16[3,3,17,32] parameter(1)\n",
            "whole_added": "+\n+      TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                              ParseAndReturnVerifiedModule(hlo_with_new_type));\n+      DebugOptions debug_opts = module->config().debug_options();\n+      debug_opts.set_xla_gpu_use_runtime_fusion(true);\n+      module->config().set_debug_options(debug_opts);\n+      EXPECT_TRUE(RunAndCompare(std::move(module), ErrorSpec{0.01}))\n+// At time of writing, cudnn runtime fusion cannot handle f16 convs with an odd\n+// number of input/output channels.  Check that we don't try to run this conv\n+// with runtime fusion (or, if we do, that it works!).\n+TEST_F(CudnnFusedConvRewriterTest, TestRelu6OddChannels) {\n+  if (!GetCudaComputeCapability().IsAtLeast(\n+          se::CudaComputeCapability::AMPERE)) {\n+    GTEST_SKIP() << \"Conv-Bias-Relu6 fusion is supported and recommended with \"\n+                    \"the Nvidia Ampere+ GPUs.\";\n+  }\n+\n+  TestMatchWithAllTypes(R\"(\n+    HloModule Test\n+    ENTRY Test {\n+      zeros = TYPE[1,384,1024,32] broadcast(TYPE[] constant(0)), dimensions={}\n+      sixes = TYPE[1,384,1024,32] broadcast(TYPE[] constant(6)), dimensions={}\n+      input = TYPE[1,769,2049,3] parameter(0)\n+      filter = TYPE[32,3,3,3] parameter(1)\n+      bias = TYPE[32] parameter(2)\n+      conv = TYPE[1,384,1024,32] convolution(input, filter), window={size=3x3 stride=2x2}, dim_labels=b01f_o01i->b01f\n+      broadcasted_bias = TYPE[1,384,1024,32] broadcast(bias), dimensions={3}\n+      sum = add(conv, broadcasted_bias)\n+      ROOT relu6 = clamp(zeros, sum, sixes)\n+    })\");\n+}\n+\n+      inputs = f16[1,18,9,9] parameter(0)\n+      filters = f16[3,3,18,32] parameter(1)\n+      inputs = f16[1,18,9,9] parameter(0)\n+      filters = f16[3,3,18,32] parameter(1)\n",
            "whole_hunk": "@@ -113,7 +113,13 @@ class CudnnFusedConvRewriterTest : public GpuCodegenTest {\n           << optimized_hlo_string;\n       EXPECT_THAT(optimized_hlo_string,\n                   HasSubstr(kCudnnConvBiasActivationForwardCallTarget));\n-      EXPECT_TRUE(RunAndCompare(hlo_with_new_type, ErrorSpec{0.01}))\n+\n+      TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                              ParseAndReturnVerifiedModule(hlo_with_new_type));\n+      DebugOptions debug_opts = module->config().debug_options();\n+      debug_opts.set_xla_gpu_use_runtime_fusion(true);\n+      module->config().set_debug_options(debug_opts);\n+      EXPECT_TRUE(RunAndCompare(std::move(module), ErrorSpec{0.01}))\n           << optimized_hlo_string;\n     }\n   }\n@@ -368,6 +374,31 @@ TEST_F(CudnnFusedConvRewriterTest, TestRelu6) {\n     })\");\n }\n \n+// At time of writing, cudnn runtime fusion cannot handle f16 convs with an odd\n+// number of input/output channels.  Check that we don't try to run this conv\n+// with runtime fusion (or, if we do, that it works!).\n+TEST_F(CudnnFusedConvRewriterTest, TestRelu6OddChannels) {\n+  if (!GetCudaComputeCapability().IsAtLeast(\n+          se::CudaComputeCapability::AMPERE)) {\n+    GTEST_SKIP() << \"Conv-Bias-Relu6 fusion is supported and recommended with \"\n+                    \"the Nvidia Ampere+ GPUs.\";\n+  }\n+\n+  TestMatchWithAllTypes(R\"(\n+    HloModule Test\n+    ENTRY Test {\n+      zeros = TYPE[1,384,1024,32] broadcast(TYPE[] constant(0)), dimensions={}\n+      sixes = TYPE[1,384,1024,32] broadcast(TYPE[] constant(6)), dimensions={}\n+      input = TYPE[1,769,2049,3] parameter(0)\n+      filter = TYPE[32,3,3,3] parameter(1)\n+      bias = TYPE[32] parameter(2)\n+      conv = TYPE[1,384,1024,32] convolution(input, filter), window={size=3x3 stride=2x2}, dim_labels=b01f_o01i->b01f\n+      broadcasted_bias = TYPE[1,384,1024,32] broadcast(bias), dimensions={3}\n+      sum = add(conv, broadcasted_bias)\n+      ROOT relu6 = clamp(zeros, sum, sixes)\n+    })\");\n+}\n+\n TEST_F(CudnnFusedConvRewriterTest, TestLeakyRelu) {\n   if (!GetCudaComputeCapability().IsAtLeast(\n           se::CudaComputeCapability::AMPERE)) {\n@@ -1381,8 +1412,8 @@ TEST_F(CudnnFusedConvRewriterHloTest, FuseRelu6) {\n   const std::string module_str = R\"(\n     HloModule Test\n     ENTRY Test {\n-      inputs = f16[1,17,9,9] parameter(0)\n-      filters = f16[3,3,17,32] parameter(1)\n+      inputs = f16[1,18,9,9] parameter(0)\n+      filters = f16[3,3,18,32] parameter(1)\n       bias = f16[32] parameter(2)\n       bias_broadcast = f16[1,32,9,9] broadcast(bias), dimensions={1}\n       zero = f16[] constant(0)\n@@ -1423,8 +1454,8 @@ TEST_F(CudnnFusedConvRewriterHloTest, DontFuseRelu6IfMultipleUses) {\n   const std::string module_str = R\"(\n     HloModule Test\n     ENTRY Test {\n-      inputs = f16[1,17,9,9] parameter(0)\n-      filters = f16[3,3,17,32] parameter(1)\n+      inputs = f16[1,18,9,9] parameter(0)\n+      filters = f16[3,3,18,32] parameter(1)\n       bias = f16[1,32,9,9] broadcast(f16[32] parameter(2)), dimensions={1}\n       zeros = f16[1,32,9,9] broadcast(f16[] constant(0)), dimensions={}\n       sixes = f16[1,32,9,9] broadcast(f16[] constant(6)), dimensions={}"
        }
    ]
},
{
    "Id": 324,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/cf5c14e10a465a9e6ede1640d08289e7e68be324",
    "date": "2023-08-10T06:13:29-07:00",
    "message": "Return error on invalid input in `tfl.atan2`\n\nPiperOrigin-RevId: 555461404",
    "label": "YES",
    "changes": [
        {
            "name": "atan2.cc",
            "path": "tensorflow/lite/kernels/atan2.cc",
            "patches": [
                {
                    "old_start": 82,
                    "old_length": 11,
                    "new_start": 82,
                    "new_length": 13,
                    "hunk": "@@ -82,11 +82,13 @@ TfLiteStatus Atan2Eval(TfLiteContext* context, TfLiteNode* node) {\n     case kTfLiteFloat64:\n       TF_LITE_ENSURE_OK(context, Atan2<double>(input_y, input_x, output));\n       break;\n-    default:\n+    default: {\n       TF_LITE_KERNEL_LOG(\n           context,\n           \"Unsupported datatype for atan2 output: %s\",\n           TfLiteTypeGetName(output->type));\n+      return TfLiteStatus::kTfLiteError;\n+    }\n   }\n \n   return TfLiteStatus::kTfLiteOk;"
                }
            ],
            "whole_deleted": "-    default:\n",
            "whole_added": "+    default: {\n+      return TfLiteStatus::kTfLiteError;\n+    }\n",
            "whole_hunk": "@@ -82,11 +82,13 @@ TfLiteStatus Atan2Eval(TfLiteContext* context, TfLiteNode* node) {\n     case kTfLiteFloat64:\n       TF_LITE_ENSURE_OK(context, Atan2<double>(input_y, input_x, output));\n       break;\n-    default:\n+    default: {\n       TF_LITE_KERNEL_LOG(\n           context,\n           \"Unsupported datatype for atan2 output: %s\",\n           TfLiteTypeGetName(output->type));\n+      return TfLiteStatus::kTfLiteError;\n+    }\n   }\n \n   return TfLiteStatus::kTfLiteOk;"
        }
    ]
},
{
    "Id": 136,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1668cc3c7371287fdd17988eb0e218c8231fc10f",
    "date": "2024-02-29T20:37:10+05:30",
    "message": "Fix checkfail in tf.raw_ops.Substr \n\nThe API tf.raw_ops.Substr  currently validates whether the input args pos and len are of same shape or not.Its not checking whether these tensors are empty or not and trying to access the Tensor values directly without validating.If a user passes empty tensors it will lead to assertion failure causing core dumped error.\r\n\r\nMay fixes #63036",
    "label": "YES",
    "changes": [
        {
            "name": "substr_op.cc",
            "path": "tensorflow/core/kernels/substr_op.cc",
            "patches": [
                {
                    "old_start": 56,
                    "old_length": 7,
                    "new_start": 56,
                    "new_length": 12,
                    "hunk": "@@ -56,7 +56,12 @@ class SubstrOp : public OpKernel {\n                 errors::InvalidArgument(\n                     \"pos and len should have the same shape, got: \",\n                     pos_shape.DebugString(), \" vs. \", len_shape.DebugString()));\n-\n+    OP_REQUIRES(context, pos_tensor.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor pos_tensor: \",\n+                                        pos_tensor.DebugString()));\n+    OP_REQUIRES(context, len_tensor.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor len_tensor: \",\n+                                        len_tensor.DebugString()));\n     bool is_scalar = TensorShapeUtils::IsScalar(pos_shape);\n \n     if (is_scalar || input_shape == pos_shape) {"
                }
            ],
            "whole_deleted": "-\n",
            "whole_added": "+    OP_REQUIRES(context, pos_tensor.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor pos_tensor: \",\n+                                        pos_tensor.DebugString()));\n+    OP_REQUIRES(context, len_tensor.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor len_tensor: \",\n+                                        len_tensor.DebugString()));\n",
            "whole_hunk": "@@ -56,7 +56,12 @@ class SubstrOp : public OpKernel {\n                 errors::InvalidArgument(\n                     \"pos and len should have the same shape, got: \",\n                     pos_shape.DebugString(), \" vs. \", len_shape.DebugString()));\n-\n+    OP_REQUIRES(context, pos_tensor.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor pos_tensor: \",\n+                                        pos_tensor.DebugString()));\n+    OP_REQUIRES(context, len_tensor.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor len_tensor: \",\n+                                        len_tensor.DebugString()));\n     bool is_scalar = TensorShapeUtils::IsScalar(pos_shape);\n \n     if (is_scalar || input_shape == pos_shape) {"
        }
    ]
},
{
    "Id": 72,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1b1d9407422ef2a8b6a04c6918d75459de9b6355",
    "date": "2024-04-16T15:58:14-07:00",
    "message": "#tf-data Add a range check for `index_flat_map`.\n\nWithout this check, it could crash if `index_map_fn` returns an\nout-of-bound index due to:\nhttps://github.com/tensorflow/tensorflow/blob/69a908420c5c5b90027f23905cd842c76ca3955c/tensorflow/core/framework/tensor.cc#L1104\n\nPiperOrigin-RevId: 625476301",
    "label": "YES",
    "changes": [
        {
            "name": "index_flat_map_dataset_op.cc",
            "path": "tensorflow/core/kernels/data/experimental/index_flat_map_dataset_op.cc",
            "patches": [
                {
                    "old_start": 89,
                    "old_length": 15,
                    "new_start": 89,
                    "new_length": 21,
                    "hunk": "@@ -89,15 +89,21 @@ absl::StatusOr<size_t> GetValue(const Tensor& tensor) {\n }\n \n // Returns the `offset`-th element from `tensors`.\n-std::vector<Tensor> GetSlice(const std::vector<Tensor>& tensors,\n-                             size_t offset) {\n+absl::StatusOr<std::vector<Tensor>> GetSlice(const std::vector<Tensor>& tensors,\n+                                             size_t offset) {\n   std::vector<Tensor> result;\n   for (size_t i = 0; i < tensors.size(); ++i) {\n     if (tensors[i].dims() == 0) {  // Scalar.\n       result.push_back(tensors[i]);\n-    } else {\n-      result.push_back(MaybeCopySubSlice(tensors[i], offset));\n+      continue;\n+    }\n+    if (offset > tensors[i].dim_size(0)) {\n+      return absl::InvalidArgumentError(absl::StrCat(\n+          \"`index_flat_map` got invalid `index_map_fn` which returns offset \",\n+          offset, \", but the input element has \", tensors[i].dim_size(0),\n+          \" elements: \", tensors[i].DebugString()));\n     }\n+    result.push_back(MaybeCopySubSlice(tensors[i], offset));\n   }\n   return result;\n }\n"
                },
                {
                    "old_start": 264,
                    "old_length": 7,
                    "new_start": 270,
                    "new_length": 8,
                    "hunk": "@@ -264,7 +270,8 @@ class IndexFlatMapDatasetOp::Dataset::Iterator\n         }\n         input_element_count_ = next_input_index;\n       }\n-      *out_tensors = GetSlice(input_unflattened_tensors_, offset);\n+      TF_ASSIGN_OR_RETURN(*out_tensors,\n+                          GetSlice(input_unflattened_tensors_, offset));\n       ++element_count_;\n     } else {\n       // TODO(b/325112575): Make it easier to return multiple values from\n"
                },
                {
                    "old_start": 279,
                    "old_length": 7,
                    "new_start": 286,
                    "new_length": 7,
                    "hunk": "@@ -279,7 +286,7 @@ class IndexFlatMapDatasetOp::Dataset::Iterator\n       if (*end_of_sequence) {\n         return absl::OkStatus();\n       }\n-      *out_tensors = GetSlice(mapped_tensors, offset);\n+      TF_ASSIGN_OR_RETURN(*out_tensors, GetSlice(mapped_tensors, offset));\n     }\n     return absl::OkStatus();\n   }\n"
                }
            ],
            "whole_deleted": "-std::vector<Tensor> GetSlice(const std::vector<Tensor>& tensors,\n-                             size_t offset) {\n-    } else {\n-      result.push_back(MaybeCopySubSlice(tensors[i], offset));\n-      *out_tensors = GetSlice(input_unflattened_tensors_, offset);\n-      *out_tensors = GetSlice(mapped_tensors, offset);\n",
            "whole_added": "+absl::StatusOr<std::vector<Tensor>> GetSlice(const std::vector<Tensor>& tensors,\n+                                             size_t offset) {\n+      continue;\n+    }\n+    if (offset > tensors[i].dim_size(0)) {\n+      return absl::InvalidArgumentError(absl::StrCat(\n+          \"`index_flat_map` got invalid `index_map_fn` which returns offset \",\n+          offset, \", but the input element has \", tensors[i].dim_size(0),\n+          \" elements: \", tensors[i].DebugString()));\n+    result.push_back(MaybeCopySubSlice(tensors[i], offset));\n+      TF_ASSIGN_OR_RETURN(*out_tensors,\n+                          GetSlice(input_unflattened_tensors_, offset));\n+      TF_ASSIGN_OR_RETURN(*out_tensors, GetSlice(mapped_tensors, offset));\n",
            "whole_hunk": "@@ -89,15 +89,21 @@ absl::StatusOr<size_t> GetValue(const Tensor& tensor) {\n }\n \n // Returns the `offset`-th element from `tensors`.\n-std::vector<Tensor> GetSlice(const std::vector<Tensor>& tensors,\n-                             size_t offset) {\n+absl::StatusOr<std::vector<Tensor>> GetSlice(const std::vector<Tensor>& tensors,\n+                                             size_t offset) {\n   std::vector<Tensor> result;\n   for (size_t i = 0; i < tensors.size(); ++i) {\n     if (tensors[i].dims() == 0) {  // Scalar.\n       result.push_back(tensors[i]);\n-    } else {\n-      result.push_back(MaybeCopySubSlice(tensors[i], offset));\n+      continue;\n+    }\n+    if (offset > tensors[i].dim_size(0)) {\n+      return absl::InvalidArgumentError(absl::StrCat(\n+          \"`index_flat_map` got invalid `index_map_fn` which returns offset \",\n+          offset, \", but the input element has \", tensors[i].dim_size(0),\n+          \" elements: \", tensors[i].DebugString()));\n     }\n+    result.push_back(MaybeCopySubSlice(tensors[i], offset));\n   }\n   return result;\n }\n@@ -264,7 +270,8 @@ class IndexFlatMapDatasetOp::Dataset::Iterator\n         }\n         input_element_count_ = next_input_index;\n       }\n-      *out_tensors = GetSlice(input_unflattened_tensors_, offset);\n+      TF_ASSIGN_OR_RETURN(*out_tensors,\n+                          GetSlice(input_unflattened_tensors_, offset));\n       ++element_count_;\n     } else {\n       // TODO(b/325112575): Make it easier to return multiple values from\n@@ -279,7 +286,7 @@ class IndexFlatMapDatasetOp::Dataset::Iterator\n       if (*end_of_sequence) {\n         return absl::OkStatus();\n       }\n-      *out_tensors = GetSlice(mapped_tensors, offset);\n+      TF_ASSIGN_OR_RETURN(*out_tensors, GetSlice(mapped_tensors, offset));\n     }\n     return absl::OkStatus();\n   }\n"
        },
        {
            "name": "index_flat_map_test.py",
            "path": "tensorflow/python/data/experimental/kernel_tests/index_flat_map_test.py",
            "patches": [
                {
                    "old_start": 112,
                    "old_length": 6,
                    "new_start": 112,
                    "new_length": 20,
                    "hunk": "@@ -112,6 +112,20 @@ class IndexFlatMapTest(test_base.DatasetTestBase, parameterized.TestCase):\n         dataset, _map_func, _index_map_func)\n     self.assertDatasetProduces(dataset, list(range(dataset_range)))\n \n+  @combinations.generate(test_base.default_test_combinations())\n+  def test_offset_out_of_range(self):\n+\n+    def _index_map_func(_) -> tuple[int, int]:\n+      return (0, 1000)\n+\n+    input_data = [\"0 1\", \"2 3 4 5\", \"6 7\", \"8\"]\n+    dataset = dataset_ops.Dataset.from_tensor_slices(input_data)\n+    dataset = index_flat_map_op.index_flat_map(dataset, _split, _index_map_func)\n+    with self.assertRaisesRegex(\n+        errors.InvalidArgumentError,\n+        \"invalid `index_map_fn` which returns offset 1000\"):\n+      self.getDatasetOutput(dataset)\n+\n   @combinations.generate(test_base.default_test_combinations())\n   def test_invalid_map_fn(self):\n \n"
                },
                {
                    "old_start": 121,
                    "old_length": 8,
                    "new_start": 135,
                    "new_length": 7,
                    "hunk": "@@ -121,8 +135,7 @@ class IndexFlatMapTest(test_base.DatasetTestBase, parameterized.TestCase):\n \n     input_data = [\"0 1\", \"2 3 4 5\", \"6 7\", \"8\"]\n     dataset = dataset_ops.Dataset.from_tensor_slices(input_data)\n-    dataset = index_flat_map_op.index_flat_map(\n-        dataset, _split, _index_map_func)\n+    dataset = index_flat_map_op.index_flat_map(dataset, _split, _index_map_func)\n     with self.assertRaisesRegex(\n         errors.InvalidArgumentError,\n         \"expected to return two int values\"):"
                }
            ],
            "whole_deleted": "-    dataset = index_flat_map_op.index_flat_map(\n-        dataset, _split, _index_map_func)\n",
            "whole_added": "+  @combinations.generate(test_base.default_test_combinations())\n+  def test_offset_out_of_range(self):\n+\n+    def _index_map_func(_) -> tuple[int, int]:\n+      return (0, 1000)\n+\n+    input_data = [\"0 1\", \"2 3 4 5\", \"6 7\", \"8\"]\n+    dataset = dataset_ops.Dataset.from_tensor_slices(input_data)\n+    dataset = index_flat_map_op.index_flat_map(dataset, _split, _index_map_func)\n+    with self.assertRaisesRegex(\n+        errors.InvalidArgumentError,\n+        \"invalid `index_map_fn` which returns offset 1000\"):\n+      self.getDatasetOutput(dataset)\n+\n+    dataset = index_flat_map_op.index_flat_map(dataset, _split, _index_map_func)\n",
            "whole_hunk": "@@ -112,6 +112,20 @@ class IndexFlatMapTest(test_base.DatasetTestBase, parameterized.TestCase):\n         dataset, _map_func, _index_map_func)\n     self.assertDatasetProduces(dataset, list(range(dataset_range)))\n \n+  @combinations.generate(test_base.default_test_combinations())\n+  def test_offset_out_of_range(self):\n+\n+    def _index_map_func(_) -> tuple[int, int]:\n+      return (0, 1000)\n+\n+    input_data = [\"0 1\", \"2 3 4 5\", \"6 7\", \"8\"]\n+    dataset = dataset_ops.Dataset.from_tensor_slices(input_data)\n+    dataset = index_flat_map_op.index_flat_map(dataset, _split, _index_map_func)\n+    with self.assertRaisesRegex(\n+        errors.InvalidArgumentError,\n+        \"invalid `index_map_fn` which returns offset 1000\"):\n+      self.getDatasetOutput(dataset)\n+\n   @combinations.generate(test_base.default_test_combinations())\n   def test_invalid_map_fn(self):\n \n@@ -121,8 +135,7 @@ class IndexFlatMapTest(test_base.DatasetTestBase, parameterized.TestCase):\n \n     input_data = [\"0 1\", \"2 3 4 5\", \"6 7\", \"8\"]\n     dataset = dataset_ops.Dataset.from_tensor_slices(input_data)\n-    dataset = index_flat_map_op.index_flat_map(\n-        dataset, _split, _index_map_func)\n+    dataset = index_flat_map_op.index_flat_map(dataset, _split, _index_map_func)\n     with self.assertRaisesRegex(\n         errors.InvalidArgumentError,\n         \"expected to return two int values\"):"
        }
    ]
},
{
    "Id": 490,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/85c7ae4332f54fb6259186e37e774c9b4a9bd6b0",
    "date": "2023-03-15T10:53:55-07:00",
    "message": "Reenable tsan for distributed snapshots fault tolerance tests.\n\n- Keep track of dead workers in the snapshot manager to ensure stream ownership isn't retained after timeout.\n- Change the orphan test to check for the dead worker's stream having been assigned to any remaining worker (not just the newest one, as tsan causes workers to intermittently timeout).\n- To prevent timeouts, decrease the size of the test datasets and increase the number of shards.\n- Use zero workers in on-disk state recovery tests.  The worker is irrelevant since it's just a test of the dispatcher reading the on-disk state.  And with a worker, there is some bad condition under tsan ~1% of the time, but presumably only when the on-disk state is bad anyway -- will figure this out later.\n\nPiperOrigin-RevId: 516868639",
    "label": "YES",
    "changes": [
        {
            "name": "snapshot_manager.cc",
            "path": "tensorflow/core/data/service/snapshot/snapshot_manager.cc",
            "patches": [
                {
                    "old_start": 365,
                    "old_length": 6,
                    "new_start": 365,
                    "new_length": 8,
                    "hunk": "@@ -365,6 +365,8 @@ SnapshotManager::MaybeGetOrCreateStreamAssignment(\n \n Status SnapshotManager::WorkerHeartbeat(const WorkerHeartbeatRequest& request,\n                                         WorkerHeartbeatResponse& response) {\n+  dead_workers_.erase(request.worker_address());\n+\n   if (mode_ == Mode::kDone || mode_ == Mode::kError) {\n     // When the snapshot manager is done or in an error state, it returns an\n     // empty response to inform the workers to cancel the ongoing tasks.\n"
                },
                {
                    "old_start": 401,
                    "old_length": 6,
                    "new_start": 403,
                    "new_length": 12,
                    "hunk": "@@ -401,6 +403,12 @@ Status SnapshotManager::GetSnapshotSplit(const GetSnapshotSplitRequest& request,\n           \" has no known assignment and its desired stream, \",\n           request.stream_index(), \", is unavailable\");\n     }\n+    if (dead_workers_.contains(request.worker_address())) {\n+      return errors::FailedPrecondition(\n+          \"worker \", request.worker_address(),\n+          \" is considered to have timed out and must heartbeat to retain its \"\n+          \"stream assignment before requesting more splits\");\n+    }\n     ReassignPreviouslyAssignedStream(request.stream_index(),\n                                      request.worker_address());\n   } else if (it->second != request.stream_index()) {\n"
                },
                {
                    "old_start": 457,
                    "old_length": 12,
                    "new_start": 465,
                    "new_length": 13,
                    "hunk": "@@ -457,12 +465,13 @@ Status SnapshotManager::GetSnapshotStreams(\n   return OkStatus();\n }\n \n-void SnapshotManager::HandleMissingWorker(absl::string_view worker_address) {\n+void SnapshotManager::HandleMissingWorker(const std::string& worker_address) {\n   if (auto it = assignments_.find(worker_address); it != assignments_.end()) {\n     LOG(INFO) << \"deleting assignment for stream \" << it->second\n               << \" due to lost worker \" << worker_address;\n     orphans_.insert(it->second);\n     assignments_.erase(it);\n+    dead_workers_.insert(worker_address);\n   }\n }\n \n"
                }
            ],
            "whole_deleted": "-void SnapshotManager::HandleMissingWorker(absl::string_view worker_address) {\n",
            "whole_added": "+  dead_workers_.erase(request.worker_address());\n+\n+    if (dead_workers_.contains(request.worker_address())) {\n+      return errors::FailedPrecondition(\n+          \"worker \", request.worker_address(),\n+          \" is considered to have timed out and must heartbeat to retain its \"\n+          \"stream assignment before requesting more splits\");\n+    }\n+void SnapshotManager::HandleMissingWorker(const std::string& worker_address) {\n+    dead_workers_.insert(worker_address);\n",
            "whole_hunk": "@@ -365,6 +365,8 @@ SnapshotManager::MaybeGetOrCreateStreamAssignment(\n \n Status SnapshotManager::WorkerHeartbeat(const WorkerHeartbeatRequest& request,\n                                         WorkerHeartbeatResponse& response) {\n+  dead_workers_.erase(request.worker_address());\n+\n   if (mode_ == Mode::kDone || mode_ == Mode::kError) {\n     // When the snapshot manager is done or in an error state, it returns an\n     // empty response to inform the workers to cancel the ongoing tasks.\n@@ -401,6 +403,12 @@ Status SnapshotManager::GetSnapshotSplit(const GetSnapshotSplitRequest& request,\n           \" has no known assignment and its desired stream, \",\n           request.stream_index(), \", is unavailable\");\n     }\n+    if (dead_workers_.contains(request.worker_address())) {\n+      return errors::FailedPrecondition(\n+          \"worker \", request.worker_address(),\n+          \" is considered to have timed out and must heartbeat to retain its \"\n+          \"stream assignment before requesting more splits\");\n+    }\n     ReassignPreviouslyAssignedStream(request.stream_index(),\n                                      request.worker_address());\n   } else if (it->second != request.stream_index()) {\n@@ -457,12 +465,13 @@ Status SnapshotManager::GetSnapshotStreams(\n   return OkStatus();\n }\n \n-void SnapshotManager::HandleMissingWorker(absl::string_view worker_address) {\n+void SnapshotManager::HandleMissingWorker(const std::string& worker_address) {\n   if (auto it = assignments_.find(worker_address); it != assignments_.end()) {\n     LOG(INFO) << \"deleting assignment for stream \" << it->second\n               << \" due to lost worker \" << worker_address;\n     orphans_.insert(it->second);\n     assignments_.erase(it);\n+    dead_workers_.insert(worker_address);\n   }\n }\n \n"
        },
        {
            "name": "snapshot_manager.h",
            "path": "tensorflow/core/data/service/snapshot/snapshot_manager.h",
            "patches": [
                {
                    "old_start": 82,
                    "old_length": 7,
                    "new_start": 82,
                    "new_length": 7,
                    "hunk": "@@ -82,7 +82,7 @@ class SnapshotManager {\n \n   // Checks for a stream that should move from `assignments_` to `orphans_` due\n   // to its assigned worker having stopped heartbeating.\n-  void HandleMissingWorker(absl::string_view worker_address);\n+  void HandleMissingWorker(const std::string& worker_address);\n   // Checks for streams that should move from `unknowns_` to `orphans_` due to\n   // the dispatcher not having gotten a heartbeat from an assigned worker.\n   void UpdateStreams();\n"
                },
                {
                    "old_start": 132,
                    "old_length": 6,
                    "new_start": 132,
                    "new_length": 10,
                    "hunk": "@@ -132,6 +132,10 @@ class SnapshotManager {\n   // If `Resume`d, the timestamp of the resumption of the snapshot.\n   std::optional<absl::Duration> resume_time_micros_;\n \n+  // The addresses of all workers considered to be dead based on heartbeat\n+  // timeout.\n+  absl::flat_hash_set<std::string> dead_workers_;\n+\n   // A split provider for each input source of the dataset being snapshotted.\n   std::vector<std::unique_ptr<SplitProvider>> split_providers_;\n   int64_t num_sources() const { return split_providers_.size(); }\n"
                }
            ],
            "whole_deleted": "-  void HandleMissingWorker(absl::string_view worker_address);\n",
            "whole_added": "+  void HandleMissingWorker(const std::string& worker_address);\n+  // The addresses of all workers considered to be dead based on heartbeat\n+  // timeout.\n+  absl::flat_hash_set<std::string> dead_workers_;\n+\n",
            "whole_hunk": "@@ -82,7 +82,7 @@ class SnapshotManager {\n \n   // Checks for a stream that should move from `assignments_` to `orphans_` due\n   // to its assigned worker having stopped heartbeating.\n-  void HandleMissingWorker(absl::string_view worker_address);\n+  void HandleMissingWorker(const std::string& worker_address);\n   // Checks for streams that should move from `unknowns_` to `orphans_` due to\n   // the dispatcher not having gotten a heartbeat from an assigned worker.\n   void UpdateStreams();\n@@ -132,6 +132,10 @@ class SnapshotManager {\n   // If `Resume`d, the timestamp of the resumption of the snapshot.\n   std::optional<absl::Duration> resume_time_micros_;\n \n+  // The addresses of all workers considered to be dead based on heartbeat\n+  // timeout.\n+  absl::flat_hash_set<std::string> dead_workers_;\n+\n   // A split provider for each input source of the dataset being snapshotted.\n   std::vector<std::unique_ptr<SplitProvider>> split_providers_;\n   int64_t num_sources() const { return split_providers_.size(); }\n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/python/data/experimental/kernel_tests/service/BUILD",
            "patches": [
                {
                    "old_start": 232,
                    "old_length": 12,
                    "new_start": 232,
                    "new_length": 11,
                    "hunk": "@@ -232,12 +232,11 @@ tf_py_test(\n     name = \"distributed_save_ft_test\",\n     size = \"medium\",\n     srcs = [\"distributed_save_ft_test.py\"],\n-    shard_count = 8,\n+    shard_count = 17,\n     # TODO(b/250921378): Fix sanitizers.\n     tags = [\n         \"noasan\",\n         \"nomsan\",\n-        \"notsan\",\n     ],\n     deps = [\n         \":test_base\",\n"
                }
            ],
            "whole_deleted": "-    shard_count = 8,\n-        \"notsan\",\n",
            "whole_added": "+    shard_count = 17,\n",
            "whole_hunk": "@@ -232,12 +232,11 @@ tf_py_test(\n     name = \"distributed_save_ft_test\",\n     size = \"medium\",\n     srcs = [\"distributed_save_ft_test.py\"],\n-    shard_count = 8,\n+    shard_count = 17,\n     # TODO(b/250921378): Fix sanitizers.\n     tags = [\n         \"noasan\",\n         \"nomsan\",\n-        \"notsan\",\n     ],\n     deps = [\n         \":test_base\",\n"
        },
        {
            "name": "distributed_save_ft_test.py",
            "path": "tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.py",
            "patches": [
                {
                    "old_start": 107,
                    "old_length": 6,
                    "new_start": 107,
                    "new_length": 7,
                    "hunk": "@@ -107,6 +107,7 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n           ds, self._path, cluster.dispatcher_address()\n       )\n \n+  # TODO(b/250921378): Figure out why tsan times out when there is a worker.\n   @combinations.generate(\n       combinations.times(\n           test_base.eager_only_combinations(),\n"
                },
                {
                    "old_start": 116,
                    "old_length": 7,
                    "new_start": 117,
                    "new_length": 7,
                    "hunk": "@@ -116,7 +117,7 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n       )\n   )\n   def testSnapshotRecoveryFailsWithBadStreamName(self, bad_stream_dir_name):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     os.makedirs(os.path.join(self._path, \"streams\", bad_stream_dir_name))\n     with self.assertRaisesRegex(ValueError, \"can't parse\"):\n       cluster.restart_dispatcher()\n"
                },
                {
                    "old_start": 130,
                    "old_length": 14,
                    "new_start": 131,
                    "new_length": 14,
                    "hunk": "@@ -130,14 +131,14 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n       )\n   )\n   def testSnapshotRecoveryFailsWithBadSourceName(self, bad_source_dir_name):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     os.makedirs(os.path.join(self.splits_dir(), bad_source_dir_name))\n     with self.assertRaisesRegex(ValueError, \"can't parse\"):\n       cluster.restart_dispatcher()\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithOutOfBoundsSourceName(self):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     os.makedirs(os.path.join(self.splits_dir(), \"source_1\"))\n     with self.assertRaisesRegex(ValueError, \"found conflict\"):\n       cluster.restart_dispatcher()\n"
                },
                {
                    "old_start": 157,
                    "old_length": 7,
                    "new_start": 158,
                    "new_length": 7,
                    "hunk": "@@ -157,7 +158,7 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n       )\n   )\n   def testSnapshotRecoveryFailsWithBadSplitNames(self, bad_split_filename):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     write_file(os.path.join(self.source_dir(), bad_split_filename))\n     with self.assertRaisesRegex(\n         ValueError, \"Expected split_<local_split_index>_<global_split_index>\"):\n"
                },
                {
                    "old_start": 165,
                    "old_length": 7,
                    "new_start": 166,
                    "new_length": 7,
                    "hunk": "@@ -165,7 +166,7 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithOutOfOrderSplitName(self):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     write_file(os.path.join(self.source_dir(), \"split_1_0\"))\n     with self.assertRaisesRegex(\n         ValueError, \"The local split index 1 exceeds the global split index 0\"):\n"
                },
                {
                    "old_start": 173,
                    "old_length": 21,
                    "new_start": 174,
                    "new_length": 21,
                    "hunk": "@@ -173,21 +174,21 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithOutOfBoundsSplitName(self):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     write_file(os.path.join(self.source_dir(), \"split_1_1\"))\n     with self.assertRaisesRegex(ValueError, \"found conflict\"):\n       cluster.restart_dispatcher()\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithMissingGlobalIndexInSplitNames(self):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     write_file(os.path.join(self.source_dir(), \"split_0_1\"))\n     with self.assertRaisesRegex(ValueError, \"found missing global\"):\n       cluster.restart_dispatcher()\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithDuplicateGlobalIndexInSplitName(self):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     write_file(os.path.join(self.source_dir(stream_idx=0), \"split_0_1\"))\n     write_file(os.path.join(self.source_dir(stream_idx=1), \"split_0_1\"))\n     with self.assertRaisesRegex(ValueError, \"found duplicate global\"):\n"
                },
                {
                    "old_start": 213,
                    "old_length": 12,
                    "new_start": 214,
                    "new_length": 15,
                    "hunk": "@@ -213,12 +214,15 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n     while cluster.snapshot_streams(self._path)[assignments[0]].state != _ORPHAN:\n       time.sleep(0.1)\n     cluster.add_worker(start=True)\n-    self.assertEqual(get_stream_assignment(cluster, n), assignments[0])\n+    self.assertCountEqual(\n+        [get_stream_assignment(cluster, i) for i in range(1, n + 1)],\n+        range(n),\n+    )\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testLargeMultiSourceSnapshotRecoversAndCompletes(self):\n     n = 5\n-    cluster, _ = self.setup(num_workers=n, ds_size=10000, num_sources=3)\n+    cluster, _ = self.setup(num_workers=n, ds_size=1000, num_sources=3)\n     cluster.restart_dispatcher()\n     self._wait_for_snapshot(cluster)\n     self.assertTrue(os.path.exists(os.path.join(self._path, \"DONE\")))"
                }
            ],
            "whole_deleted": "-    cluster, _ = self.setup()\n-    cluster, _ = self.setup()\n-    cluster, _ = self.setup()\n-    cluster, _ = self.setup()\n-    cluster, _ = self.setup()\n-    cluster, _ = self.setup()\n-    cluster, _ = self.setup()\n-    cluster, _ = self.setup()\n-    self.assertEqual(get_stream_assignment(cluster, n), assignments[0])\n-    cluster, _ = self.setup(num_workers=n, ds_size=10000, num_sources=3)\n",
            "whole_added": "+  # TODO(b/250921378): Figure out why tsan times out when there is a worker.\n+    cluster, _ = self.setup(num_workers=0)\n+    cluster, _ = self.setup(num_workers=0)\n+    cluster, _ = self.setup(num_workers=0)\n+    cluster, _ = self.setup(num_workers=0)\n+    cluster, _ = self.setup(num_workers=0)\n+    cluster, _ = self.setup(num_workers=0)\n+    cluster, _ = self.setup(num_workers=0)\n+    cluster, _ = self.setup(num_workers=0)\n+    self.assertCountEqual(\n+        [get_stream_assignment(cluster, i) for i in range(1, n + 1)],\n+        range(n),\n+    )\n+    cluster, _ = self.setup(num_workers=n, ds_size=1000, num_sources=3)\n",
            "whole_hunk": "@@ -107,6 +107,7 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n           ds, self._path, cluster.dispatcher_address()\n       )\n \n+  # TODO(b/250921378): Figure out why tsan times out when there is a worker.\n   @combinations.generate(\n       combinations.times(\n           test_base.eager_only_combinations(),\n@@ -116,7 +117,7 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n       )\n   )\n   def testSnapshotRecoveryFailsWithBadStreamName(self, bad_stream_dir_name):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     os.makedirs(os.path.join(self._path, \"streams\", bad_stream_dir_name))\n     with self.assertRaisesRegex(ValueError, \"can't parse\"):\n       cluster.restart_dispatcher()\n@@ -130,14 +131,14 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n       )\n   )\n   def testSnapshotRecoveryFailsWithBadSourceName(self, bad_source_dir_name):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     os.makedirs(os.path.join(self.splits_dir(), bad_source_dir_name))\n     with self.assertRaisesRegex(ValueError, \"can't parse\"):\n       cluster.restart_dispatcher()\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithOutOfBoundsSourceName(self):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     os.makedirs(os.path.join(self.splits_dir(), \"source_1\"))\n     with self.assertRaisesRegex(ValueError, \"found conflict\"):\n       cluster.restart_dispatcher()\n@@ -157,7 +158,7 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n       )\n   )\n   def testSnapshotRecoveryFailsWithBadSplitNames(self, bad_split_filename):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     write_file(os.path.join(self.source_dir(), bad_split_filename))\n     with self.assertRaisesRegex(\n         ValueError, \"Expected split_<local_split_index>_<global_split_index>\"):\n@@ -165,7 +166,7 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithOutOfOrderSplitName(self):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     write_file(os.path.join(self.source_dir(), \"split_1_0\"))\n     with self.assertRaisesRegex(\n         ValueError, \"The local split index 1 exceeds the global split index 0\"):\n@@ -173,21 +174,21 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithOutOfBoundsSplitName(self):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     write_file(os.path.join(self.source_dir(), \"split_1_1\"))\n     with self.assertRaisesRegex(ValueError, \"found conflict\"):\n       cluster.restart_dispatcher()\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithMissingGlobalIndexInSplitNames(self):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     write_file(os.path.join(self.source_dir(), \"split_0_1\"))\n     with self.assertRaisesRegex(ValueError, \"found missing global\"):\n       cluster.restart_dispatcher()\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithDuplicateGlobalIndexInSplitName(self):\n-    cluster, _ = self.setup()\n+    cluster, _ = self.setup(num_workers=0)\n     write_file(os.path.join(self.source_dir(stream_idx=0), \"split_0_1\"))\n     write_file(os.path.join(self.source_dir(stream_idx=1), \"split_0_1\"))\n     with self.assertRaisesRegex(ValueError, \"found duplicate global\"):\n@@ -213,12 +214,15 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n     while cluster.snapshot_streams(self._path)[assignments[0]].state != _ORPHAN:\n       time.sleep(0.1)\n     cluster.add_worker(start=True)\n-    self.assertEqual(get_stream_assignment(cluster, n), assignments[0])\n+    self.assertCountEqual(\n+        [get_stream_assignment(cluster, i) for i in range(1, n + 1)],\n+        range(n),\n+    )\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testLargeMultiSourceSnapshotRecoversAndCompletes(self):\n     n = 5\n-    cluster, _ = self.setup(num_workers=n, ds_size=10000, num_sources=3)\n+    cluster, _ = self.setup(num_workers=n, ds_size=1000, num_sources=3)\n     cluster.restart_dispatcher()\n     self._wait_for_snapshot(cluster)\n     self.assertTrue(os.path.exists(os.path.join(self._path, \"DONE\")))"
        }
    ]
},
{
    "Id": 231,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/20c05898b344c0fe17fab13cc355b1e94e3864ec",
    "date": "2023-11-30T13:33:31-08:00",
    "message": "[XLA:GPU] Fix fusion parameter limit again and remove hard check\n\nI removed the hard check, because this seems to break frequently, but it's not really an error.\n\nAlso had to fix some tests, because the fix changes the order of parameters in the fusion.\n\nAlso fixed TensorIterationSpec::IterationSpecFragment::ToString() during the debugging.\n\nPiperOrigin-RevId: 586770496",
    "label": "YES",
    "changes": [
        {
            "name": "gemm_rewriter_triton.cc",
            "path": "third_party/xla/xla/service/gpu/gemm_rewriter_triton.cc",
            "patches": [
                {
                    "old_start": 134,
                    "old_length": 8,
                    "new_start": 134,
                    "new_length": 9,
                    "hunk": "@@ -134,8 +134,9 @@ void FuseDotOnly(HloInstruction& hlo, OldToNewHloMap& output_old_to_new_map,\n // an input.\n int64_t NumAddedParameters(const HloInstruction& hlo) {\n   // Non-scalar constant is equivalent to a parameter: one input, one output.\n-  if (hlo.opcode() == HloOpcode::kConstant &&\n-      !ShapeUtil::IsScalar(hlo.shape())) {\n+  if (hlo.opcode() == HloOpcode::kParameter ||\n+      (hlo.opcode() == HloOpcode::kConstant &&\n+       !ShapeUtil::IsScalar(hlo.shape()))) {\n     return 0;\n   }\n   // All other instructions add all own inputs and remove own single output.\n"
                },
                {
                    "old_start": 153,
                    "old_length": 7,
                    "new_start": 154,
                    "new_length": 7,
                    "hunk": "@@ -153,7 +154,7 @@ void TryToFuseWithInputsRecursively(HloInstruction& root,\n                                     HloComputation::Builder& builder) {\n   // Instructions at the fusion edge that can either get fused too or\n   // become parameters of the fusion. Used to track the number of parameters.\n-  absl::flat_hash_set<const HloInstruction*> inputs;\n+  absl::flat_hash_set<const HloInstruction*> inputs = {&root};\n   // Traverse all connected instructions that could be fused, analyze them and\n   // collect ones that will be fused.\n   absl::flat_hash_set<const HloInstruction*> to_fuse_set;\n"
                },
                {
                    "old_start": 251,
                    "old_length": 10,
                    "new_start": 252,
                    "new_length": 10,
                    "hunk": "@@ -251,10 +252,10 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n                                    gpu_version, context, old_to_new_map,\n                                    fusion_inputs, builder);\n     const int new_parameters = fusion_inputs.size() - operand_count_before;\n-    TF_RET_CHECK(new_parameters <=\n-                 TritonFusionAnalysis::kMaxParameterPerDotScope)\n-        << \"Too many new parameters: \" << new_parameters << \" > \"\n-        << TritonFusionAnalysis::kMaxParameterPerDotScope;\n+    if (new_parameters > TritonFusionAnalysis::kMaxParameterPerDotScope) {\n+      LOG(WARNING) << \"Too many new parameters fused: \" << new_parameters\n+                   << \" > \" << TritonFusionAnalysis::kMaxParameterPerDotScope;\n+    }\n     return context;\n   };\n \n"
                }
            ],
            "whole_deleted": "-  if (hlo.opcode() == HloOpcode::kConstant &&\n-      !ShapeUtil::IsScalar(hlo.shape())) {\n-  absl::flat_hash_set<const HloInstruction*> inputs;\n-    TF_RET_CHECK(new_parameters <=\n-                 TritonFusionAnalysis::kMaxParameterPerDotScope)\n-        << \"Too many new parameters: \" << new_parameters << \" > \"\n-        << TritonFusionAnalysis::kMaxParameterPerDotScope;\n",
            "whole_added": "+  if (hlo.opcode() == HloOpcode::kParameter ||\n+      (hlo.opcode() == HloOpcode::kConstant &&\n+       !ShapeUtil::IsScalar(hlo.shape()))) {\n+  absl::flat_hash_set<const HloInstruction*> inputs = {&root};\n+    if (new_parameters > TritonFusionAnalysis::kMaxParameterPerDotScope) {\n+      LOG(WARNING) << \"Too many new parameters fused: \" << new_parameters\n+                   << \" > \" << TritonFusionAnalysis::kMaxParameterPerDotScope;\n+    }\n",
            "whole_hunk": "@@ -134,8 +134,9 @@ void FuseDotOnly(HloInstruction& hlo, OldToNewHloMap& output_old_to_new_map,\n // an input.\n int64_t NumAddedParameters(const HloInstruction& hlo) {\n   // Non-scalar constant is equivalent to a parameter: one input, one output.\n-  if (hlo.opcode() == HloOpcode::kConstant &&\n-      !ShapeUtil::IsScalar(hlo.shape())) {\n+  if (hlo.opcode() == HloOpcode::kParameter ||\n+      (hlo.opcode() == HloOpcode::kConstant &&\n+       !ShapeUtil::IsScalar(hlo.shape()))) {\n     return 0;\n   }\n   // All other instructions add all own inputs and remove own single output.\n@@ -153,7 +154,7 @@ void TryToFuseWithInputsRecursively(HloInstruction& root,\n                                     HloComputation::Builder& builder) {\n   // Instructions at the fusion edge that can either get fused too or\n   // become parameters of the fusion. Used to track the number of parameters.\n-  absl::flat_hash_set<const HloInstruction*> inputs;\n+  absl::flat_hash_set<const HloInstruction*> inputs = {&root};\n   // Traverse all connected instructions that could be fused, analyze them and\n   // collect ones that will be fused.\n   absl::flat_hash_set<const HloInstruction*> to_fuse_set;\n@@ -251,10 +252,10 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n                                    gpu_version, context, old_to_new_map,\n                                    fusion_inputs, builder);\n     const int new_parameters = fusion_inputs.size() - operand_count_before;\n-    TF_RET_CHECK(new_parameters <=\n-                 TritonFusionAnalysis::kMaxParameterPerDotScope)\n-        << \"Too many new parameters: \" << new_parameters << \" > \"\n-        << TritonFusionAnalysis::kMaxParameterPerDotScope;\n+    if (new_parameters > TritonFusionAnalysis::kMaxParameterPerDotScope) {\n+      LOG(WARNING) << \"Too many new parameters fused: \" << new_parameters\n+                   << \" > \" << TritonFusionAnalysis::kMaxParameterPerDotScope;\n+    }\n     return context;\n   };\n \n"
        },
        {
            "name": "gemm_rewriter_triton_test.cc",
            "path": "third_party/xla/xla/service/gpu/gemm_rewriter_triton_test.cc",
            "patches": [
                {
                    "old_start": 547,
                    "old_length": 6,
                    "new_start": 547,
                    "new_length": 33,
                    "hunk": "@@ -547,6 +547,33 @@ ENTRY e {\n             TritonFusionAnalysis::kMaxParameterPerDotScope + 1);\n }\n \n+TEST_F(GemmRewriterTritonLevel2Test, DoNotFuseTooManyParametersForConcat) {\n+  static_assert(TritonFusionAnalysis::kMaxParameterPerDotScope == 4,\n+                \"We have to update this test.\");\n+  // The concat shouldn't overgo the allowed parameter limit.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+ENTRY e {\n+  a = f32[3,3]{1,0} parameter(0)\n+  b = f32[3,3]{1,0} parameter(1)\n+  c = f32[3,3]{1,0} parameter(2)\n+  d = f32[3,3]{1,0} parameter(3)\n+  e = f32[3,3]{1,0} parameter(4)\n+  f = f16[3,3]{1,0} parameter(5)\n+  concat = f32[15,3]{1,0} concatenate(a, b, c, d, e), dimensions={0}\n+  convert = f32[3,3]{1,0} convert(f)\n+  ROOT dot = f32[15,3]{1,0} dot(concat, convert), lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+})\"));\n+\n+  EXPECT_TRUE(GemmRewriterTriton(gpu_version_).Run(module.get()).value());\n+  EXPECT_EQ(module->entry_computation()->root_instruction()->opcode(),\n+            HloOpcode::kFusion);\n+  EXPECT_EQ(module->entry_computation()->root_instruction()->fusion_kind(),\n+            HloInstruction::FusionKind::kCustom);\n+  EXPECT_LE(module->entry_computation()->root_instruction()->operand_count(),\n+            TritonFusionAnalysis::kMaxParameterPerDotScope + 1);\n+}\n+\n TEST_F(GemmRewriterTritonLevel2Test,\n        InstructionsReachableFromMultipleOperandsAreHandledCorrectly) {\n   static_assert(TritonFusionAnalysis::kMaxParameterPerDotScope == 4,\n"
                },
                {
                    "old_start": 827,
                    "old_length": 34,
                    "new_start": 854,
                    "new_length": 34,
                    "hunk": "@@ -827,34 +854,34 @@ e {\n                           TritonFusionAnalysis::Execute(*computation));\n \n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(0), 0),\n+                                 computation->parameter_instruction(1), 0),\n               ElementsAre(FieldsAre(/*stride=*/1536, /*count=*/153,\n                                     /*slice_start=*/0, /*sliced_count=*/153,\n                                     /*subfragments=*/ElementsAre(153))));\n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(0), 1),\n+                                 computation->parameter_instruction(1), 1),\n               ElementsAre(FieldsAre(/*stride=*/1, /*count=*/1536,\n                                     /*slice_start=*/0, /*sliced_count=*/1536,\n                                     /*subfragments=*/ElementsAre(1536))));\n \n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(1), 0),\n+                                 computation->parameter_instruction(2), 0),\n               ElementsAre(FieldsAre(/*stride=*/128, /*count=*/153,\n                                     /*slice_start=*/0, /*sliced_count=*/153,\n                                     /*subfragments=*/ElementsAre(153))));\n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(1), 1),\n+                                 computation->parameter_instruction(2), 1),\n               ElementsAre(FieldsAre(/*stride=*/1, /*count=*/128,\n                                     /*slice_start=*/-1536, /*sliced_count=*/128,\n                                     /*subfragments=*/ElementsAre(128))));\n \n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(2), 0),\n+                                 computation->parameter_instruction(3), 0),\n               ElementsAre(FieldsAre(/*stride=*/256, /*count=*/153,\n                                     /*slice_start=*/0, /*sliced_count=*/153,\n                                     /*subfragments=*/ElementsAre(153))));\n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(2), 1),\n+                                 computation->parameter_instruction(3), 1),\n               ElementsAre(FieldsAre(/*stride=*/1, /*count=*/256,\n                                     /*slice_start=*/-1536 - 128,\n                                     /*sliced_count=*/256,\n"
                },
                {
                    "old_start": 945,
                    "old_length": 7,
                    "new_start": 972,
                    "new_length": 7,
                    "hunk": "@@ -945,7 +972,7 @@ e {\n                   .Run(module.get())\n                   .value());\n   EXPECT_THAT(module->entry_computation()->root_instruction(),\n-              GmockMatch((m::Fusion(m::Concatenate(), m::Parameter(),\n+              GmockMatch((m::Fusion(m::Parameter(), m::Concatenate(),\n                                     m::Parameter(), m::Parameter()))));\n }\n \n"
                }
            ],
            "whole_deleted": "-                                 computation->parameter_instruction(0), 0),\n-                                 computation->parameter_instruction(0), 1),\n-                                 computation->parameter_instruction(1), 0),\n-                                 computation->parameter_instruction(1), 1),\n-                                 computation->parameter_instruction(2), 0),\n-                                 computation->parameter_instruction(2), 1),\n-              GmockMatch((m::Fusion(m::Concatenate(), m::Parameter(),\n",
            "whole_added": "+TEST_F(GemmRewriterTritonLevel2Test, DoNotFuseTooManyParametersForConcat) {\n+  static_assert(TritonFusionAnalysis::kMaxParameterPerDotScope == 4,\n+                \"We have to update this test.\");\n+  // The concat shouldn't overgo the allowed parameter limit.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+ENTRY e {\n+  a = f32[3,3]{1,0} parameter(0)\n+  b = f32[3,3]{1,0} parameter(1)\n+  c = f32[3,3]{1,0} parameter(2)\n+  d = f32[3,3]{1,0} parameter(3)\n+  e = f32[3,3]{1,0} parameter(4)\n+  f = f16[3,3]{1,0} parameter(5)\n+  concat = f32[15,3]{1,0} concatenate(a, b, c, d, e), dimensions={0}\n+  convert = f32[3,3]{1,0} convert(f)\n+  ROOT dot = f32[15,3]{1,0} dot(concat, convert), lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+})\"));\n+\n+  EXPECT_TRUE(GemmRewriterTriton(gpu_version_).Run(module.get()).value());\n+  EXPECT_EQ(module->entry_computation()->root_instruction()->opcode(),\n+            HloOpcode::kFusion);\n+  EXPECT_EQ(module->entry_computation()->root_instruction()->fusion_kind(),\n+            HloInstruction::FusionKind::kCustom);\n+  EXPECT_LE(module->entry_computation()->root_instruction()->operand_count(),\n+            TritonFusionAnalysis::kMaxParameterPerDotScope + 1);\n+}\n+\n+                                 computation->parameter_instruction(1), 0),\n+                                 computation->parameter_instruction(1), 1),\n+                                 computation->parameter_instruction(2), 0),\n+                                 computation->parameter_instruction(2), 1),\n+                                 computation->parameter_instruction(3), 0),\n+                                 computation->parameter_instruction(3), 1),\n+              GmockMatch((m::Fusion(m::Parameter(), m::Concatenate(),\n",
            "whole_hunk": "@@ -547,6 +547,33 @@ ENTRY e {\n             TritonFusionAnalysis::kMaxParameterPerDotScope + 1);\n }\n \n+TEST_F(GemmRewriterTritonLevel2Test, DoNotFuseTooManyParametersForConcat) {\n+  static_assert(TritonFusionAnalysis::kMaxParameterPerDotScope == 4,\n+                \"We have to update this test.\");\n+  // The concat shouldn't overgo the allowed parameter limit.\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<VerifiedHloModule> module,\n+                          ParseAndReturnVerifiedModule(R\"(\n+ENTRY e {\n+  a = f32[3,3]{1,0} parameter(0)\n+  b = f32[3,3]{1,0} parameter(1)\n+  c = f32[3,3]{1,0} parameter(2)\n+  d = f32[3,3]{1,0} parameter(3)\n+  e = f32[3,3]{1,0} parameter(4)\n+  f = f16[3,3]{1,0} parameter(5)\n+  concat = f32[15,3]{1,0} concatenate(a, b, c, d, e), dimensions={0}\n+  convert = f32[3,3]{1,0} convert(f)\n+  ROOT dot = f32[15,3]{1,0} dot(concat, convert), lhs_contracting_dims={1}, rhs_contracting_dims={1}\n+})\"));\n+\n+  EXPECT_TRUE(GemmRewriterTriton(gpu_version_).Run(module.get()).value());\n+  EXPECT_EQ(module->entry_computation()->root_instruction()->opcode(),\n+            HloOpcode::kFusion);\n+  EXPECT_EQ(module->entry_computation()->root_instruction()->fusion_kind(),\n+            HloInstruction::FusionKind::kCustom);\n+  EXPECT_LE(module->entry_computation()->root_instruction()->operand_count(),\n+            TritonFusionAnalysis::kMaxParameterPerDotScope + 1);\n+}\n+\n TEST_F(GemmRewriterTritonLevel2Test,\n        InstructionsReachableFromMultipleOperandsAreHandledCorrectly) {\n   static_assert(TritonFusionAnalysis::kMaxParameterPerDotScope == 4,\n@@ -827,34 +854,34 @@ e {\n                           TritonFusionAnalysis::Execute(*computation));\n \n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(0), 0),\n+                                 computation->parameter_instruction(1), 0),\n               ElementsAre(FieldsAre(/*stride=*/1536, /*count=*/153,\n                                     /*slice_start=*/0, /*sliced_count=*/153,\n                                     /*subfragments=*/ElementsAre(153))));\n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(0), 1),\n+                                 computation->parameter_instruction(1), 1),\n               ElementsAre(FieldsAre(/*stride=*/1, /*count=*/1536,\n                                     /*slice_start=*/0, /*sliced_count=*/1536,\n                                     /*subfragments=*/ElementsAre(1536))));\n \n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(1), 0),\n+                                 computation->parameter_instruction(2), 0),\n               ElementsAre(FieldsAre(/*stride=*/128, /*count=*/153,\n                                     /*slice_start=*/0, /*sliced_count=*/153,\n                                     /*subfragments=*/ElementsAre(153))));\n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(1), 1),\n+                                 computation->parameter_instruction(2), 1),\n               ElementsAre(FieldsAre(/*stride=*/1, /*count=*/128,\n                                     /*slice_start=*/-1536, /*sliced_count=*/128,\n                                     /*subfragments=*/ElementsAre(128))));\n \n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(2), 0),\n+                                 computation->parameter_instruction(3), 0),\n               ElementsAre(FieldsAre(/*stride=*/256, /*count=*/153,\n                                     /*slice_start=*/0, /*sliced_count=*/153,\n                                     /*subfragments=*/ElementsAre(153))));\n   EXPECT_THAT(*analysis.IterSpec(TritonFusionAnalysis::Scope::RHS,\n-                                 computation->parameter_instruction(2), 1),\n+                                 computation->parameter_instruction(3), 1),\n               ElementsAre(FieldsAre(/*stride=*/1, /*count=*/256,\n                                     /*slice_start=*/-1536 - 128,\n                                     /*sliced_count=*/256,\n@@ -945,7 +972,7 @@ e {\n                   .Run(module.get())\n                   .value());\n   EXPECT_THAT(module->entry_computation()->root_instruction(),\n-              GmockMatch((m::Fusion(m::Concatenate(), m::Parameter(),\n+              GmockMatch((m::Fusion(m::Parameter(), m::Concatenate(),\n                                     m::Parameter(), m::Parameter()))));\n }\n \n"
        },
        {
            "name": "triton_tiling_propagation.cc",
            "path": "third_party/xla/xla/service/gpu/triton_tiling_propagation.cc",
            "patches": [
                {
                    "old_start": 72,
                    "old_length": 7,
                    "new_start": 72,
                    "new_length": 8,
                    "hunk": "@@ -72,7 +72,8 @@ bool TensorIterationSpec::operator==(const TensorIterationSpec& other) const {\n \n std::string TensorIterationSpec::IterationSpecFragment::ToString() const {\n   return absl::StrCat(\"{stride=\", stride, \", count=\", count,\n-                      \", slice_start=\", slice_start, \", subfragments=[\",\n+                      \", slice_start=\", slice_start,\n+                      \", sliced_count=\", sliced_count, \", subfragments=[\",\n                       absl::StrJoin(subfragments, \", \"), \"]}\");\n }\n "
                }
            ],
            "whole_deleted": "-                      \", slice_start=\", slice_start, \", subfragments=[\",\n",
            "whole_added": "+                      \", slice_start=\", slice_start,\n+                      \", sliced_count=\", sliced_count, \", subfragments=[\",\n",
            "whole_hunk": "@@ -72,7 +72,8 @@ bool TensorIterationSpec::operator==(const TensorIterationSpec& other) const {\n \n std::string TensorIterationSpec::IterationSpecFragment::ToString() const {\n   return absl::StrCat(\"{stride=\", stride, \", count=\", count,\n-                      \", slice_start=\", slice_start, \", subfragments=[\",\n+                      \", slice_start=\", slice_start,\n+                      \", sliced_count=\", sliced_count, \", subfragments=[\",\n                       absl::StrJoin(subfragments, \", \"), \"]}\");\n }\n "
        }
    ]
},
{
    "Id": 118,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/255d8a4c173e7946885abaac9f12d3b1059ca9b5",
    "date": "2024-03-12T11:32:15-07:00",
    "message": "Fixes strict subclass check for checkpoint callback field.\n\nPiperOrigin-RevId: 615096147",
    "label": "YES",
    "changes": [
        {
            "name": "restore.py",
            "path": "tensorflow/python/checkpoint/restore.py",
            "patches": [
                {
                    "old_start": 113,
                    "old_length": 16,
                    "new_start": 113,
                    "new_length": 16,
                    "hunk": "@@ -113,16 +113,16 @@ class CheckpointPosition(object):\n      callback: Reshard callback for resharding this checkpoint position. Maybe\n        None.\n     \"\"\"\n-    if not isinstance(\n-        self.callback, checkpoint_adapter.ReshardCallback\n-    ):\n-      raise TypeError(\"Cannot override resharding callback.\")\n+    if not issubclass(checkpoint_adapter.ReshardCallback, type(self.callback)):\n+      raise TypeError(\n+          \"Cannot override resharding callback, already set to non trivial.\"\n+      )\n     self.callback = callback\n \n   def has_non_trivial_reshard_callback(self) -> bool:\n     \"\"\"Determine whether this value has a non-trivial resharding callback.\"\"\"\n-    return not isinstance(\n-        self.callback, checkpoint_adapter.ReshardCallback\n+    return not issubclass(\n+        checkpoint_adapter.ReshardCallback, type(self.callback)\n     )\n \n   def is_simple_variable(self) -> bool:\n"
                },
                {
                    "old_start": 747,
                    "old_length": 7,
                    "new_start": 747,
                    "new_length": 7,
                    "hunk": "@@ -747,7 +747,7 @@ def _queue_slot_variables(checkpoint_position, visit_queue):\n             # If the corresponding variable has a non trivial resharding\n             # attached, the the slot variable should be resharded in the same\n             # way.\n-            checkpoint_position.reshard_callback\n+            checkpoint_position.callback\n             if checkpoint_position.has_non_trivial_reshard_callback()\n             else None,\n         )\n"
                },
                {
                    "old_start": 782,
                    "old_length": 7,
                    "new_start": 782,
                    "new_length": 7,
                    "hunk": "@@ -782,7 +782,7 @@ def _queue_slot_variables(checkpoint_position, visit_queue):\n               # If the corresponding variable has a non trivial resharding\n               # attached, the the slot variable should be resharded in the same\n               # way.\n-              checkpoint_position.reshard_callback\n+              checkpoint_position.callback\n               if checkpoint_position.has_non_trivial_reshard_callback()\n               else None,\n           )"
                }
            ],
            "whole_deleted": "-    if not isinstance(\n-        self.callback, checkpoint_adapter.ReshardCallback\n-    ):\n-      raise TypeError(\"Cannot override resharding callback.\")\n-    return not isinstance(\n-        self.callback, checkpoint_adapter.ReshardCallback\n-            checkpoint_position.reshard_callback\n-              checkpoint_position.reshard_callback\n",
            "whole_added": "+    if not issubclass(checkpoint_adapter.ReshardCallback, type(self.callback)):\n+      raise TypeError(\n+          \"Cannot override resharding callback, already set to non trivial.\"\n+      )\n+    return not issubclass(\n+        checkpoint_adapter.ReshardCallback, type(self.callback)\n+            checkpoint_position.callback\n+              checkpoint_position.callback\n",
            "whole_hunk": "@@ -113,16 +113,16 @@ class CheckpointPosition(object):\n      callback: Reshard callback for resharding this checkpoint position. Maybe\n        None.\n     \"\"\"\n-    if not isinstance(\n-        self.callback, checkpoint_adapter.ReshardCallback\n-    ):\n-      raise TypeError(\"Cannot override resharding callback.\")\n+    if not issubclass(checkpoint_adapter.ReshardCallback, type(self.callback)):\n+      raise TypeError(\n+          \"Cannot override resharding callback, already set to non trivial.\"\n+      )\n     self.callback = callback\n \n   def has_non_trivial_reshard_callback(self) -> bool:\n     \"\"\"Determine whether this value has a non-trivial resharding callback.\"\"\"\n-    return not isinstance(\n-        self.callback, checkpoint_adapter.ReshardCallback\n+    return not issubclass(\n+        checkpoint_adapter.ReshardCallback, type(self.callback)\n     )\n \n   def is_simple_variable(self) -> bool:\n@@ -747,7 +747,7 @@ def _queue_slot_variables(checkpoint_position, visit_queue):\n             # If the corresponding variable has a non trivial resharding\n             # attached, the the slot variable should be resharded in the same\n             # way.\n-            checkpoint_position.reshard_callback\n+            checkpoint_position.callback\n             if checkpoint_position.has_non_trivial_reshard_callback()\n             else None,\n         )\n@@ -782,7 +782,7 @@ def _queue_slot_variables(checkpoint_position, visit_queue):\n               # If the corresponding variable has a non trivial resharding\n               # attached, the the slot variable should be resharded in the same\n               # way.\n-              checkpoint_position.reshard_callback\n+              checkpoint_position.callback\n               if checkpoint_position.has_non_trivial_reshard_callback()\n               else None,\n           )"
        }
    ]
},
{
    "Id": 621,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/4edf32dad2f84cb824a3fe186fc3fa3fd53929be",
    "date": "2022-12-02T18:46:52-08:00",
    "message": "Add tests to check if pjit handles deleted array inputs gracefully and consistently\n\npjit dispatch paths should check deleted array inputs when attempting to use\nthem. These new tests ensure that various pjit dispatch paths detect and handle\nthem gracefully and consistently.\n\nAdd a check to the PyArray argument handling to make the tests pass.\n\nPiperOrigin-RevId: 492605524",
    "label": "NO",
    "changes": [
        {
            "name": "py_values.cc",
            "path": "tensorflow/compiler/xla/python/py_values.cc",
            "patches": [
                {
                    "old_start": 563,
                    "old_length": 6,
                    "new_start": 563,
                    "new_length": 9,
                    "hunk": "@@ -563,6 +563,9 @@ StatusOr<PyArgSignature> PyArgSignatureOfValue(py::handle arg,\n   if (arg.get_type() == PyArray::type()) {\n     auto array = py::reinterpret_borrow<PyArray>(arg);\n     if (array.fastpath_enabled()) {\n+      if (array.IsDeleted()) {\n+        return xla::InvalidArgument(\"Array has been deleted.\");\n+      }\n       auto dtype = array.GetBuffer(0)->on_device_shape().element_type();\n       return PyArgSignature(dtype, array.shape(), array.weak_type());\n     }\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      if (array.IsDeleted()) {\n+        return xla::InvalidArgument(\"Array has been deleted.\");\n+      }\n",
            "whole_hunk": "@@ -563,6 +563,9 @@ StatusOr<PyArgSignature> PyArgSignatureOfValue(py::handle arg,\n   if (arg.get_type() == PyArray::type()) {\n     auto array = py::reinterpret_borrow<PyArray>(arg);\n     if (array.fastpath_enabled()) {\n+      if (array.IsDeleted()) {\n+        return xla::InvalidArgument(\"Array has been deleted.\");\n+      }\n       auto dtype = array.GetBuffer(0)->on_device_shape().element_type();\n       return PyArgSignature(dtype, array.shape(), array.weak_type());\n     }\n"
        },
        {
            "name": "xla_client.py",
            "path": "tensorflow/compiler/xla/python/xla_client.py",
            "patches": [
                {
                    "old_start": 43,
                    "old_length": 7,
                    "new_start": 43,
                    "new_length": 7,
                    "hunk": "@@ -43,7 +43,7 @@ profiler = _xla.profiler\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes.\n-_version = 108\n+_version = 109\n \n # Version number for MLIR:Python components.\n mlir_api_version = 39"
                }
            ],
            "whole_deleted": "-_version = 108\n",
            "whole_added": "+_version = 109\n",
            "whole_hunk": "@@ -43,7 +43,7 @@ profiler = _xla.profiler\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes.\n-_version = 108\n+_version = 109\n \n # Version number for MLIR:Python components.\n mlir_api_version = 39"
        }
    ]
},
{
    "Id": 542,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/5ed3c7881f1f039b1bb502eb68c65250de3bbac8",
    "date": "2023-02-08T08:56:20-08:00",
    "message": "Fix ThreadPoolHandle 0 nthreads argument.\n\nIt was reported that a value of 0 leads to a check failure.  Using 0 to indicate\n`port::MaxParallelism`, for consistency with `Dataset`.\n\nFixes #59162\n\nPiperOrigin-RevId: 508092599",
    "label": "YES",
    "changes": [
        {
            "name": "threadpool_dataset_op.cc",
            "path": "tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc",
            "patches": [
                {
                    "old_start": 100,
                    "old_length": 6,
                    "new_start": 100,
                    "new_length": 12,
                    "hunk": "@@ -100,6 +100,12 @@ class ThreadPoolHandleOp : public OpKernel {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_intra_op_parallelism\",\n                                      &max_intra_op_parallelism_));\n     OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads_));\n+\n+    // For consistency with Dataset, use MaxParallelism if 0 threads are\n+    // specified.\n+    if (num_threads_ == 0) {\n+      num_threads_ = port::MaxParallelism();\n+    }\n   }\n \n   // The resource is deleted from the resource manager only when it is private"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+    // For consistency with Dataset, use MaxParallelism if 0 threads are\n+    // specified.\n+    if (num_threads_ == 0) {\n+      num_threads_ = port::MaxParallelism();\n+    }\n",
            "whole_hunk": "@@ -100,6 +100,12 @@ class ThreadPoolHandleOp : public OpKernel {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_intra_op_parallelism\",\n                                      &max_intra_op_parallelism_));\n     OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads_));\n+\n+    // For consistency with Dataset, use MaxParallelism if 0 threads are\n+    // specified.\n+    if (num_threads_ == 0) {\n+      num_threads_ = port::MaxParallelism();\n+    }\n   }\n \n   // The resource is deleted from the resource manager only when it is private"
        }
    ]
},
{
    "Id": 601,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a0c74d196a21b9d1230820d31e15492ec84d07a6",
    "date": "2022-12-22T15:56:04-08:00",
    "message": "[PJRT:C] Check PJRT_Api struct_size to detect version incompatibilities.\n\nI just ran into this, and it's very confusing without this check.\n\nThis also moves `CheckMatchingStructSizes` from the wrapper impl to\nthe helper functions file.\n\nPiperOrigin-RevId: 497254775",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/pjrt/c/BUILD",
            "patches": [
                {
                    "old_start": 68,
                    "old_length": 6,
                    "new_start": 68,
                    "new_length": 7,
                    "hunk": "@@ -68,6 +68,7 @@ cc_library(\n     visibility = [\"//visibility:public\"],\n     deps = [\n         \":pjrt_c_api_hdrs\",\n+        \":pjrt_c_api_helpers\",\n         \":pjrt_c_api_wrapper_impl\",\n         \"//tensorflow/compiler/xla/pjrt:tfrt_cpu_pjrt_client\",\n     ],\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \":pjrt_c_api_helpers\",\n",
            "whole_hunk": "@@ -68,6 +68,7 @@ cc_library(\n     visibility = [\"//visibility:public\"],\n     deps = [\n         \":pjrt_c_api_hdrs\",\n+        \":pjrt_c_api_helpers\",\n         \":pjrt_c_api_wrapper_impl\",\n         \"//tensorflow/compiler/xla/pjrt:tfrt_cpu_pjrt_client\",\n     ],\n"
        },
        {
            "name": "pjrt_c_api_cpu.cc",
            "path": "tensorflow/compiler/xla/pjrt/c/pjrt_c_api_cpu.cc",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 6,
                    "new_start": 21,
                    "new_length": 7,
                    "hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <vector>\n \n #include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api.h\"\n+#include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api_helpers.h\"\n #include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api_wrapper_impl.h\"\n #include \"tensorflow/compiler/xla/pjrt/tfrt_cpu_pjrt_client.h\"\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api_helpers.h\"\n",
            "whole_hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <vector>\n \n #include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api.h\"\n+#include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api_helpers.h\"\n #include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api_wrapper_impl.h\"\n #include \"tensorflow/compiler/xla/pjrt/tfrt_cpu_pjrt_client.h\"\n \n"
        },
        {
            "name": "pjrt_c_api_helpers.cc",
            "path": "tensorflow/compiler/xla/pjrt/c/pjrt_c_api_helpers.cc",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 6,
                    "new_start": 17,
                    "new_length": 7,
                    "hunk": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <functional>\n #include <memory>\n+#include <string>\n #include <utility>\n \n #include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api.h\"\n"
                },
                {
                    "old_start": 380,
                    "old_length": 4,
                    "new_start": 381,
                    "new_length": 21,
                    "hunk": "@@ -380,4 +381,21 @@ PJRT_SerializedExecutableDeleter MakeSerializedExecutableDeleter(\n   };\n }\n \n+static std::string StructSizeErrorMsg(absl::string_view struct_name,\n+                                      size_t expected_size,\n+                                      size_t actual_size) {\n+  return absl::StrCat(\"Unexpected \", struct_name, \" size: expected \",\n+                      expected_size, \", got \", actual_size,\n+                      \". Check installed software versions.\");\n+}\n+\n+xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n+                                     size_t expected_size, size_t actual_size) {\n+  if (expected_size != actual_size) {\n+    return tsl::errors::InvalidArgument(\n+        StructSizeErrorMsg(struct_name, expected_size, actual_size));\n+  }\n+  return tsl::OkStatus();\n+}\n+\n }  // namespace pjrt\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <string>\n+static std::string StructSizeErrorMsg(absl::string_view struct_name,\n+                                      size_t expected_size,\n+                                      size_t actual_size) {\n+  return absl::StrCat(\"Unexpected \", struct_name, \" size: expected \",\n+                      expected_size, \", got \", actual_size,\n+                      \". Check installed software versions.\");\n+}\n+\n+xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n+                                     size_t expected_size, size_t actual_size) {\n+  if (expected_size != actual_size) {\n+    return tsl::errors::InvalidArgument(\n+        StructSizeErrorMsg(struct_name, expected_size, actual_size));\n+  }\n+  return tsl::OkStatus();\n+}\n+\n",
            "whole_hunk": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <functional>\n #include <memory>\n+#include <string>\n #include <utility>\n \n #include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api.h\"\n@@ -380,4 +381,21 @@ PJRT_SerializedExecutableDeleter MakeSerializedExecutableDeleter(\n   };\n }\n \n+static std::string StructSizeErrorMsg(absl::string_view struct_name,\n+                                      size_t expected_size,\n+                                      size_t actual_size) {\n+  return absl::StrCat(\"Unexpected \", struct_name, \" size: expected \",\n+                      expected_size, \", got \", actual_size,\n+                      \". Check installed software versions.\");\n+}\n+\n+xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n+                                     size_t expected_size, size_t actual_size) {\n+  if (expected_size != actual_size) {\n+    return tsl::errors::InvalidArgument(\n+        StructSizeErrorMsg(struct_name, expected_size, actual_size));\n+  }\n+  return tsl::OkStatus();\n+}\n+\n }  // namespace pjrt\n"
        },
        {
            "name": "pjrt_c_api_helpers.h",
            "path": "tensorflow/compiler/xla/pjrt/c/pjrt_c_api_helpers.h",
            "patches": [
                {
                    "old_start": 106,
                    "old_length": 6,
                    "new_start": 106,
                    "new_length": 12,
                    "hunk": "@@ -106,6 +106,12 @@ xla::PjRtClient::HostBufferSemantics ConvertFromPjRtHostBufferSemantics(\n xla::PjRtFuture<xla::Status> ConvertCEventToCppFuture(PJRT_Event* c_event,\n                                                       const PJRT_Api* c_api);\n \n+// Helper function for checking C API argument struct sizes. Returns a non-OK\n+// status if the expected and actual sizes aren't equal (i.e. no ABI\n+// compatibility guarantees).\n+xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n+                                     size_t expected_size, size_t actual_size);\n+\n }  // namespace pjrt\n \n #endif  // TENSORFLOW_COMPILER_XLA_PJRT_C_PJRT_C_API_HELPERS_H_\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// Helper function for checking C API argument struct sizes. Returns a non-OK\n+// status if the expected and actual sizes aren't equal (i.e. no ABI\n+// compatibility guarantees).\n+xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n+                                     size_t expected_size, size_t actual_size);\n+\n",
            "whole_hunk": "@@ -106,6 +106,12 @@ xla::PjRtClient::HostBufferSemantics ConvertFromPjRtHostBufferSemantics(\n xla::PjRtFuture<xla::Status> ConvertCEventToCppFuture(PJRT_Event* c_event,\n                                                       const PJRT_Api* c_api);\n \n+// Helper function for checking C API argument struct sizes. Returns a non-OK\n+// status if the expected and actual sizes aren't equal (i.e. no ABI\n+// compatibility guarantees).\n+xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n+                                     size_t expected_size, size_t actual_size);\n+\n }  // namespace pjrt\n \n #endif  // TENSORFLOW_COMPILER_XLA_PJRT_C_PJRT_C_API_HELPERS_H_\n"
        },
        {
            "name": "pjrt_c_api_wrapper_impl.cc",
            "path": "tensorflow/compiler/xla/pjrt/c/pjrt_c_api_wrapper_impl.cc",
            "patches": [
                {
                    "old_start": 47,
                    "old_length": 22,
                    "new_start": 47,
                    "new_length": 6,
                    "hunk": "@@ -47,22 +47,6 @@ limitations under the License.\n \n namespace pjrt {\n \n-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size) {\n-  if (expected_size != actual_size) {\n-    return tsl::errors::InvalidArgument(\n-        StructSizeErrorMsg(struct_name, expected_size, actual_size));\n-  }\n-  return tsl::OkStatus();\n-}\n-\n-std::string StructSizeErrorMsg(absl::string_view struct_name,\n-                               size_t expected_size, size_t actual_size) {\n-  return absl::StrCat(\"Unexpected \", struct_name, \" size: expected \",\n-                      expected_size, \", got \", actual_size,\n-                      \". Check installed software versions.\");\n-}\n-\n std::string ProgramFormatErrorMsg(absl::string_view program_format) {\n   return absl::StrCat(\"Unknown program format '\", program_format, \"'.\");\n }\n"
                }
            ],
            "whole_deleted": "-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size) {\n-  if (expected_size != actual_size) {\n-    return tsl::errors::InvalidArgument(\n-        StructSizeErrorMsg(struct_name, expected_size, actual_size));\n-  }\n-  return tsl::OkStatus();\n-}\n-\n-std::string StructSizeErrorMsg(absl::string_view struct_name,\n-                               size_t expected_size, size_t actual_size) {\n-  return absl::StrCat(\"Unexpected \", struct_name, \" size: expected \",\n-                      expected_size, \", got \", actual_size,\n-                      \". Check installed software versions.\");\n-}\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -47,22 +47,6 @@ limitations under the License.\n \n namespace pjrt {\n \n-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size) {\n-  if (expected_size != actual_size) {\n-    return tsl::errors::InvalidArgument(\n-        StructSizeErrorMsg(struct_name, expected_size, actual_size));\n-  }\n-  return tsl::OkStatus();\n-}\n-\n-std::string StructSizeErrorMsg(absl::string_view struct_name,\n-                               size_t expected_size, size_t actual_size) {\n-  return absl::StrCat(\"Unexpected \", struct_name, \" size: expected \",\n-                      expected_size, \", got \", actual_size,\n-                      \". Check installed software versions.\");\n-}\n-\n std::string ProgramFormatErrorMsg(absl::string_view program_format) {\n   return absl::StrCat(\"Unknown program format '\", program_format, \"'.\");\n }\n"
        },
        {
            "name": "pjrt_c_api_wrapper_impl.h",
            "path": "tensorflow/compiler/xla/pjrt/c/pjrt_c_api_wrapper_impl.h",
            "patches": [
                {
                    "old_start": 190,
                    "old_length": 16,
                    "new_start": 190,
                    "new_length": 6,
                    "hunk": "@@ -190,16 +190,6 @@ PJRT_Error* PJRT_Buffer_UnsafePointer(PJRT_Buffer_UnsafePointer_Args* args);\n #define _PJRT_CONCAT(x, y) _PJRT_CONCAT_IMPL(x, y)\n #define _PJRT_CONCAT_IMPL(x, y) x##y\n \n-// Helper function for checking C API argument struct sizes. Returns a non-OK\n-// status if the expected and actual sizes aren't equal (i.e. no ABI\n-// compatibility guarantees).\n-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size);\n-\n-// Helper function\n-std::string StructSizeErrorMsg(absl::string_view struct_name,\n-                               size_t expected_size, size_t actual_size);\n-\n // Returns a specific error message when the program format is unknown.\n // Does not check the program format itself.\n std::string ProgramFormatErrorMsg(absl::string_view program_format);\n"
                }
            ],
            "whole_deleted": "-// Helper function for checking C API argument struct sizes. Returns a non-OK\n-// status if the expected and actual sizes aren't equal (i.e. no ABI\n-// compatibility guarantees).\n-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size);\n-\n-// Helper function\n-std::string StructSizeErrorMsg(absl::string_view struct_name,\n-                               size_t expected_size, size_t actual_size);\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -190,16 +190,6 @@ PJRT_Error* PJRT_Buffer_UnsafePointer(PJRT_Buffer_UnsafePointer_Args* args);\n #define _PJRT_CONCAT(x, y) _PJRT_CONCAT_IMPL(x, y)\n #define _PJRT_CONCAT_IMPL(x, y) x##y\n \n-// Helper function for checking C API argument struct sizes. Returns a non-OK\n-// status if the expected and actual sizes aren't equal (i.e. no ABI\n-// compatibility guarantees).\n-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size);\n-\n-// Helper function\n-std::string StructSizeErrorMsg(absl::string_view struct_name,\n-                               size_t expected_size, size_t actual_size);\n-\n // Returns a specific error message when the program format is unknown.\n // Does not check the program format itself.\n std::string ProgramFormatErrorMsg(absl::string_view program_format);\n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/stream_executor/tpu/BUILD",
            "patches": [
                {
                    "old_start": 31,
                    "old_length": 6,
                    "new_start": 31,
                    "new_length": 7,
                    "hunk": "@@ -31,6 +31,7 @@ cc_library(\n         \"//tensorflow/compiler/xla:status\",\n         \"//tensorflow/compiler/xla:statusor\",\n         \"//tensorflow/compiler/xla/pjrt/c:pjrt_c_api_hdrs\",\n+        \"//tensorflow/compiler/xla/pjrt/c:pjrt_c_api_helpers\",\n         \"//tensorflow/tsl/platform:errors\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/strings\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/compiler/xla/pjrt/c:pjrt_c_api_helpers\",\n",
            "whole_hunk": "@@ -31,6 +31,7 @@ cc_library(\n         \"//tensorflow/compiler/xla:status\",\n         \"//tensorflow/compiler/xla:statusor\",\n         \"//tensorflow/compiler/xla/pjrt/c:pjrt_c_api_hdrs\",\n+        \"//tensorflow/compiler/xla/pjrt/c:pjrt_c_api_helpers\",\n         \"//tensorflow/tsl/platform:errors\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/strings\",\n"
        },
        {
            "name": "pjrt_api.cc",
            "path": "tensorflow/compiler/xla/stream_executor/tpu/pjrt_api.cc",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 6,
                    "new_start": 21,
                    "new_length": 7,
                    "hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api_helpers.h\"\n #include \"tensorflow/compiler/xla/status.h\"\n #include \"tensorflow/compiler/xla/statusor.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n"
                },
                {
                    "old_start": 81,
                    "old_length": 7,
                    "new_start": 82,
                    "new_length": 11,
                    "hunk": "@@ -81,7 +82,11 @@ xla::Status LoadPjrtPlugin(absl::string_view device_type,\n   }\n   LOG(INFO) << \"GetPjrtApi was found for \" << device_type << \" at \"\n             << library_path;\n-  TF_RETURN_IF_ERROR(stream_executor::tpu::SetPjrtApi(device_type, fptr()));\n+\n+  const PJRT_Api* pjrt_api = fptr();\n+  TF_RETURN_IF_ERROR(pjrt::CheckMatchingStructSizes(\n+      \"PJRT_Api\", PJRT_Api_STRUCT_SIZE, pjrt_api->struct_size));\n+  TF_RETURN_IF_ERROR(stream_executor::tpu::SetPjrtApi(device_type, pjrt_api));\n   return tsl::OkStatus();\n }\n "
                }
            ],
            "whole_deleted": "-  TF_RETURN_IF_ERROR(stream_executor::tpu::SetPjrtApi(device_type, fptr()));\n",
            "whole_added": "+#include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api_helpers.h\"\n+\n+  const PJRT_Api* pjrt_api = fptr();\n+  TF_RETURN_IF_ERROR(pjrt::CheckMatchingStructSizes(\n+      \"PJRT_Api\", PJRT_Api_STRUCT_SIZE, pjrt_api->struct_size));\n+  TF_RETURN_IF_ERROR(stream_executor::tpu::SetPjrtApi(device_type, pjrt_api));\n",
            "whole_hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n \n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"tensorflow/compiler/xla/pjrt/c/pjrt_c_api_helpers.h\"\n #include \"tensorflow/compiler/xla/status.h\"\n #include \"tensorflow/compiler/xla/statusor.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n@@ -81,7 +82,11 @@ xla::Status LoadPjrtPlugin(absl::string_view device_type,\n   }\n   LOG(INFO) << \"GetPjrtApi was found for \" << device_type << \" at \"\n             << library_path;\n-  TF_RETURN_IF_ERROR(stream_executor::tpu::SetPjrtApi(device_type, fptr()));\n+\n+  const PJRT_Api* pjrt_api = fptr();\n+  TF_RETURN_IF_ERROR(pjrt::CheckMatchingStructSizes(\n+      \"PJRT_Api\", PJRT_Api_STRUCT_SIZE, pjrt_api->struct_size));\n+  TF_RETURN_IF_ERROR(stream_executor::tpu::SetPjrtApi(device_type, pjrt_api));\n   return tsl::OkStatus();\n }\n "
        }
    ]
},
{
    "Id": 634,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/974f0a2279317cc3a4190559951f7a2cafdde197",
    "date": "2022-11-16T09:17:02-08:00",
    "message": "[XLA:GPU] Cleanup fusion checks.\n\nCreatesHeavyComputation() check is not needed by fusion merger anymore (calls IsProducerConsumerFusible()) and multi-output fusion (calls IsProducerConsumerMultiOutputFusible()) - now covered by the cost analysis. It might still make sense in instruction fusion which calls IsProducerConsumerFusible() too, so the call to CreatesHeavyComputation() moved there.\n\nPiperOrigin-RevId: 488953965",
    "label": "NO",
    "changes": [
        {
            "name": "gpu_fusible.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible.cc",
            "patches": [
                {
                    "old_start": 279,
                    "old_length": 10,
                    "new_start": 279,
                    "new_length": 6,
                    "hunk": "@@ -279,10 +279,6 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n     return \"the producer is not fusible as it is a multi-output fusion\";\n   }\n \n-  if (CreatesHeavyComputation(producer, consumer)) {\n-    return \"the fusion would create a heavy computation\";\n-  }\n-\n   // Fuse scalar constants into loop fusion nodes. This reduces the number of\n   // parameters and makes matching scalar broadcasts easier.\n   //\n"
                },
                {
                    "old_start": 340,
                    "old_length": 8,
                    "new_start": 336,
                    "new_length": 6,
                    "hunk": "@@ -340,8 +336,6 @@ FusionDecision IsProducerConsumerMultiOutputFusible(\n     return \"producer is not loop-fusible\";\n   } else if (!IsFusibleAsMultiOutputFusionRoot(consumer)) {\n     return \"consumer is not fusible as multi-output-fusion-root\";\n-  } else if (CreatesHeavyComputation(producer, consumer)) {\n-    return \"fusion creates heavy computation\";\n   } else if (NoFusionPossible fusible =\n                  !ShapesCompatibleForMultiOutputFusion(producer, consumer)) {\n     return !fusible;\n"
                }
            ],
            "whole_deleted": "-  if (CreatesHeavyComputation(producer, consumer)) {\n-    return \"the fusion would create a heavy computation\";\n-  }\n-\n-  } else if (CreatesHeavyComputation(producer, consumer)) {\n-    return \"fusion creates heavy computation\";\n",
            "whole_added": "",
            "whole_hunk": "@@ -279,10 +279,6 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n     return \"the producer is not fusible as it is a multi-output fusion\";\n   }\n \n-  if (CreatesHeavyComputation(producer, consumer)) {\n-    return \"the fusion would create a heavy computation\";\n-  }\n-\n   // Fuse scalar constants into loop fusion nodes. This reduces the number of\n   // parameters and makes matching scalar broadcasts easier.\n   //\n@@ -340,8 +336,6 @@ FusionDecision IsProducerConsumerMultiOutputFusible(\n     return \"producer is not loop-fusible\";\n   } else if (!IsFusibleAsMultiOutputFusionRoot(consumer)) {\n     return \"consumer is not fusible as multi-output-fusion-root\";\n-  } else if (CreatesHeavyComputation(producer, consumer)) {\n-    return \"fusion creates heavy computation\";\n   } else if (NoFusionPossible fusible =\n                  !ShapesCompatibleForMultiOutputFusion(producer, consumer)) {\n     return !fusible;\n"
        },
        {
            "name": "instruction_fusion.cc",
            "path": "tensorflow/compiler/xla/service/gpu/instruction_fusion.cc",
            "patches": [
                {
                    "old_start": 78,
                    "old_length": 6,
                    "new_start": 78,
                    "new_length": 11,
                    "hunk": "@@ -78,6 +78,11 @@ FusionDecision GpuInstructionFusion::ShouldFuseInexpensiveChecks(\n           !IsProducerConsumerFusible(*producer, *consumer)) {\n     return !fusible;\n   }\n+\n+  if (CreatesHeavyComputation(*producer, *consumer)) {\n+    return \"the fusion would create a heavy computation\";\n+  }\n+\n   if (NoFusionPossible fusible =\n           !InstructionFusion::ShouldFuse(consumer, operand_index)) {\n     return !fusible;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+  if (CreatesHeavyComputation(*producer, *consumer)) {\n+    return \"the fusion would create a heavy computation\";\n+  }\n+\n",
            "whole_hunk": "@@ -78,6 +78,11 @@ FusionDecision GpuInstructionFusion::ShouldFuseInexpensiveChecks(\n           !IsProducerConsumerFusible(*producer, *consumer)) {\n     return !fusible;\n   }\n+\n+  if (CreatesHeavyComputation(*producer, *consumer)) {\n+    return \"the fusion would create a heavy computation\";\n+  }\n+\n   if (NoFusionPossible fusible =\n           !InstructionFusion::ShouldFuse(consumer, operand_index)) {\n     return !fusible;"
        }
    ]
},
{
    "Id": 311,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e56206755ff0c98269eb3e50c98fccbaadb6884d",
    "date": "2023-08-21T18:50:57-07:00",
    "message": "Support quantized i64 during flatbuffer import\n\nSometimes tflite flatbuffer represent `i32` quantized values as `i64`s. In these cases we should truncate down to a lower bit width to avoid creating illegal types.\n\nWe check the bitwidth of the type at load time and pick a power-of-2 bit\nwidth where the value can be safely truncated.\n\nPiperOrigin-RevId: 558958449",
    "label": "NO",
    "changes": [
        {
            "name": "flatbuffer_import.cc",
            "path": "tensorflow/compiler/mlir/lite/flatbuffer_import.cc",
            "patches": [
                {
                    "old_start": 162,
                    "old_length": 18,
                    "new_start": 162,
                    "new_length": 20,
                    "hunk": "@@ -162,18 +162,20 @@ Location OpLoc(const OperatorT& op,\n // Returns the correct type for a quantized tensor\n // We have a special case for constants since they have a higher minimum value.\n StatusOr<QuantizedType> GetQuantizedType(const TensorT& tensor, Builder builder,\n-                                         bool is_constant = false) {\n+                                         bool is_constant = false,\n+                                         mlir::Type storage_type = {}) {\n   tflite::QuantizationParametersT& quant_params = *tensor.quantization;\n   if (quant_params.details.AsCustomQuantization()) {\n     return errors::Unimplemented(\"Cannot handle experimental quantization\");\n   }\n \n   bool is_signed = true;\n-  mlir::IntegerType storage_type;\n   if (tensor.type == tflite::TensorType_UINT8) {\n     is_signed = false;\n-    storage_type = builder.getIntegerType(8);\n-  } else {\n+    storage_type = mlir::IntegerType::get(builder.getContext(), 8);\n+  }\n+\n+  if (!storage_type) {\n     auto raw_elem_type = ConvertElementType(tensor.type, builder);\n     if (!raw_elem_type.isa<mlir::IntegerType>()) {\n       return errors::InvalidArgument(\n"
                },
                {
                    "old_start": 186,
                    "old_length": 13,
                    "new_start": 188,
                    "new_length": 14,
                    "hunk": "@@ -186,13 +188,14 @@ StatusOr<QuantizedType> GetQuantizedType(const TensorT& tensor, Builder builder,\n   // Since we don't know which ones are weights, we represent this optimization\n   // as a change in the storage bounds for the type for all constants of this\n   // type.\n-  bool is_weight_buffer = is_constant && (storage_type.getWidth() == 8);\n-\n-  int64_t storage_min = QuantizedType::getDefaultMinimumForInteger(\n-                            is_signed, storage_type.getWidth()) +\n-                        static_cast<int>(is_weight_buffer);\n-  int64_t storage_max = QuantizedType::getDefaultMaximumForInteger(\n-      is_signed, storage_type.getWidth());\n+  int bitwidth = storage_type.getIntOrFloatBitWidth();\n+  bool is_weight_buffer = is_constant && (bitwidth == 8);\n+\n+  int64_t storage_min =\n+      QuantizedType::getDefaultMinimumForInteger(is_signed, bitwidth) +\n+      static_cast<int>(is_weight_buffer);\n+  int64_t storage_max =\n+      QuantizedType::getDefaultMaximumForInteger(is_signed, bitwidth);\n   uint32_t flags =\n       is_signed ? mlir::quant::QuantizationFlags::FlagValue::Signed : 0;\n \n"
                },
                {
                    "old_start": 232,
                    "old_length": 7,
                    "new_start": 235,
                    "new_length": 8,
                    "hunk": "@@ -232,7 +235,8 @@ StatusOr<QuantizedType> GetCalibratedQuantizedType(const TensorT& tensor,\n \n StatusOr<mlir::TensorType> GetTensorType(const TensorT& tensor, Builder builder,\n                                          bool is_constant = false,\n-                                         bool is_intermediate = false) {\n+                                         bool is_intermediate = false,\n+                                         bool get_storage = false) {\n   mlir::Type elem_type = ConvertElementType(tensor.type, builder);\n   if (tensor.type == tflite::TensorType_VARIANT) {\n     llvm::SmallVector<mlir::TensorType> tensor_types;\n"
                },
                {
                    "old_start": 254,
                    "old_length": 9,
                    "new_start": 258,
                    "new_length": 13,
                    "hunk": "@@ -254,9 +258,13 @@ StatusOr<mlir::TensorType> GetTensorType(const TensorT& tensor, Builder builder,\n     }\n     elem_type = mlir::TF::VariantType::get(tensor_types, builder.getContext());\n   }\n-  if (IsQuantized(tensor)) {\n+  if (IsQuantized(tensor) && !get_storage) {\n     TF_ASSIGN_OR_RETURN(elem_type,\n                         GetQuantizedType(tensor, builder, is_constant));\n+  } else if (IsQuantized(tensor) && get_storage) {\n+    // If the type is quantized we strip the signedness from the storage type.\n+    elem_type = mlir::IntegerType::get(elem_type.getContext(),\n+                                       elem_type.getIntOrFloatBitWidth());\n   }\n \n   // Intermediate tensors with calibration value (but not scale and zero points)\n"
                },
                {
                    "old_start": 355,
                    "old_length": 8,
                    "new_start": 363,
                    "new_length": 8,
                    "hunk": "@@ -355,8 +363,8 @@ std::string GetMlirOpName(const tflite::OperatorT& op,\n // The read_size parameter is present to allow reading both float16 and float32s\n // without a case split.\n template <typename T>\n-std::vector<T> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n-  std::vector<T> ret;\n+llvm::SmallVector<mlir::APInt> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n+  llvm::SmallVector<mlir::APInt> ret;\n   size_t read_size = sizeof(T);\n   int bytes_len = bytes.size();\n   assert(bytes_len % read_size == 0);\n"
                },
                {
                    "old_start": 366,
                    "old_length": 9,
                    "new_start": 374,
                    "new_length": 10,
                    "hunk": "@@ -366,9 +374,10 @@ std::vector<T> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n \n   const char* data_ptr = reinterpret_cast<const char*>(bytes.data());\n   for (int i = 0; i < elem_count; i++) {\n-    ret.push_back(llvm::support::endian::readNext<\n-                  T, llvm::support::endian::system_endianness(),\n-                  llvm::support::unaligned>(data_ptr));\n+    T val = llvm::support::endian::readNext<\n+        T, llvm::support::endian::system_endianness(),\n+        llvm::support::unaligned>(data_ptr);\n+    ret.push_back(mlir::APInt(sizeof(T) * 8, val));\n   }\n   return ret;\n }\n"
                },
                {
                    "old_start": 398,
                    "old_length": 12,
                    "new_start": 407,
                    "new_length": 12,
                    "hunk": "@@ -398,12 +407,12 @@ tensorflow::TensorProto ConvertTfliteConstTensor(\n }\n \n StatusOr<mlir::ElementsAttr> ConvertFloatBuffer(\n-    mlir::RankedTensorType shaped_type, mlir::FloatType elem_type,\n-    const std::vector<uint8_t>& buffer) {\n+    mlir::RankedTensorType shaped_type, const std::vector<uint8_t>& buffer) {\n   size_t bytes_len = buffer.size();\n+  mlir::Type elem_type = shaped_type.getElementType();\n \n   // The bytes of floats are stored little-endian.\n-  switch (elem_type.getWidth()) {\n+  switch (elem_type.getIntOrFloatBitWidth()) {\n     case 16: {\n       assert(bytes_len % 2 == 0);\n       assert(elem_type.isF16());\n"
                },
                {
                    "old_start": 458,
                    "old_length": 16,
                    "new_start": 467,
                    "new_length": 43,
                    "hunk": "@@ -458,16 +467,43 @@ StatusOr<mlir::ElementsAttr> ConvertFloatBuffer(\n           DenseElementsAttr::get(shaped_type, ArrayRef<double>(values)));\n     }\n   }\n-  return errors::InvalidArgument(\"unsupported bit width\", elem_type.getWidth());\n+  return errors::InvalidArgument(\"unsupported bit width\",\n+                                 elem_type.getIntOrFloatBitWidth());\n+}\n+\n+// If the values in the buffer can be clamped to a bitwidth, truncate\n+// and return the new clamped integer width.\n+void truncateLimitedIntegerAPInt(llvm::SmallVector<mlir::APInt>& values) {\n+  mlir::APInt min = values[0];\n+  mlir::APInt max = values[0];\n+  for (auto& val : values) {\n+    min = llvm::APIntOps::smin(val, min);\n+    max = llvm::APIntOps::smax(val, max);\n+  }\n+\n+  for (int64_t bw = 8; bw < min.getBitWidth(); bw += bw) {\n+    auto limitMin = mlir::APInt::getSignedMinValue(bw).sext(min.getBitWidth());\n+    auto limitMax = mlir::APInt::getSignedMaxValue(bw).sext(min.getBitWidth());\n+    if (min.sle(limitMin) || max.sle(limitMin) || min.sge(limitMax) ||\n+        max.sge(limitMax)) {\n+      continue;\n+    }\n+\n+    for (int i = 0; i < values.size(); i++) {\n+      values[i] = values[i].trunc(bw);\n+    }\n+    break;\n+  }\n }\n \n StatusOr<mlir::ElementsAttr> ConvertIntBuffer(\n-    mlir::RankedTensorType shaped_type, mlir::Type elem_type,\n-    const std::vector<uint8_t>& buffer) {\n+    mlir::RankedTensorType shaped_type, const std::vector<uint8_t>& buffer,\n+    bool truncate = false) {\n+  mlir::Type elem_type = shaped_type.getElementType();\n   unsigned bit_width;\n   if (auto itype = elem_type.dyn_cast<mlir::IntegerType>()) {\n     bit_width = itype.getWidth();\n-  } else if (auto qtype = elem_type.dyn_cast<QuantizedType>()) {\n+  } else if (auto qtype = elem_type.dyn_cast<mlir::quant::QuantizedType>()) {\n     bit_width = qtype.getStorageTypeIntegralWidth();\n     shaped_type = tensorflow::GetTypeFromTFTensorShape(shaped_type.getShape(),\n                                                        qtype.getStorageType());\n"
                },
                {
                    "old_start": 475,
                    "old_length": 47,
                    "new_start": 511,
                    "new_length": 57,
                    "hunk": "@@ -475,47 +511,57 @@ StatusOr<mlir::ElementsAttr> ConvertIntBuffer(\n     return errors::InvalidArgument(\"unsupported integer constant type\");\n   }\n \n+  llvm::SmallVector<mlir::APInt> values;\n   switch (bit_width) {\n     case 1: {\n       // vector<bool> doesn't convert to an ArrayRef\n-      llvm::SmallVector<bool, 8> values;\n-      values.reserve(buffer.size());\n+      llvm::SmallVector<bool, 8> boolValues;\n+      boolValues.reserve(buffer.size());\n       for (auto b : buffer) {\n-        values.emplace_back(b != 0);\n+        boolValues.emplace_back(b != 0);\n       }\n       return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(values)));\n+          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(boolValues)));\n     }\n     case 4: {\n-      auto values =\n+      auto i4Values =\n           tflite::UnpackDenseInt4IntoInt8(buffer, shaped_type.getNumElements());\n       // Use `getFromRawBuffer()` instead of `get()` to bypass a templated size\n       // check which doesn't work with int4 because int4_t doesn't exist.\n       return mlir::ElementsAttr(DenseElementsAttr::getFromRawBuffer(\n-          shaped_type, ArrayRef<char>(values)));\n+          shaped_type, ArrayRef<char>(i4Values)));\n     }\n     case 8: {\n       return mlir::ElementsAttr(\n           DenseElementsAttr::get(shaped_type, ArrayRef<uint8_t>(buffer)));\n     }\n     case 16: {\n-      auto values = ReadAsHostEndian<uint16_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint16_t>(values)));\n+      values = ReadAsHostEndian<uint16_t>(buffer);\n+      break;\n     }\n     case 32: {\n-      auto values = ReadAsHostEndian<uint32_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint32_t>(values)));\n+      values = ReadAsHostEndian<uint32_t>(buffer);\n+      break;\n     }\n     case 64: {\n-      auto values = ReadAsHostEndian<uint64_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint64_t>(values)));\n+      values = ReadAsHostEndian<uint64_t>(buffer);\n+      break;\n     }\n     default:\n       return errors::Unimplemented(\"Cannot handle bit width \", bit_width);\n   }\n+\n+  if (truncate) {\n+    truncateLimitedIntegerAPInt(values);\n+    auto sign = mlir::cast<mlir::IntegerType>(shaped_type.getElementType())\n+                    .getSignedness();\n+    auto ety = mlir::IntegerType::get(shaped_type.getContext(),\n+                                      values[0].getBitWidth(), sign);\n+    shaped_type =\n+        tensorflow::GetTypeFromTFTensorShape(shaped_type.getShape(), ety);\n+  }\n+\n+  return mlir::ElementsAttr(DenseElementsAttr::get(shaped_type, values));\n }\n \n StatusOr<Operation*> BuildExternalConstOp(const tflite::TensorT& tensor,\n"
                },
                {
                    "old_start": 561,
                    "old_length": 9,
                    "new_start": 607,
                    "new_length": 15,
                    "hunk": "@@ -561,9 +607,15 @@ static mlir::ElementsAttr GetSplat(RankedTensorType type, int unique_index,\n // variable `stateful_variable_idx` is used as a unique value for each constant\n // to avoid CSEed. `tensor` is the data structure of flatbuffer. `shaped_type`\n // is the ShapedType for the const op.\n-Operation* BuildVariableOp(const tflite::TensorT& tensor,\n-                           mlir::RankedTensorType shaped_type,\n-                           OpBuilder builder, Location loc) {\n+StatusOr<Operation*> BuildVariableOp(const tflite::TensorT& tensor,\n+                                     OpBuilder builder, Location loc) {\n+  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n+                                               /*is_constant=*/true));\n+  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n+  if (!shaped_type) {\n+    return errors::Internal(\"Constant doesn't have a shape\");\n+  }\n+\n   static int stateful_variable_idx = 0;\n   mlir::ElementsAttr value =\n       GetSplat(shaped_type, stateful_variable_idx++, builder);\n"
                },
                {
                    "old_start": 607,
                    "old_length": 8,
                    "new_start": 659,
                    "new_length": 20,
                    "hunk": "@@ -607,8 +659,20 @@ static StatusOr<std::vector<int32_t>> ConvertSparseIndexVector(\n \n static StatusOr<Operation*> BuildSparseConstOp(\n     const tflite::TensorT& tensor, const std::vector<uint8_t>& buffer,\n-    const mlir::RankedTensorType shaped_type, OpBuilder& builder,\n-    Location loc) {\n+    OpBuilder& builder, Location loc) {\n+  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n+                                               /*is_constant=*/true));\n+  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n+  if (!shaped_type) {\n+    return errors::Internal(\"Constant doesn't have a shape\");\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(type, GetTensorType(tensor, builder,\n+                                          /*is_constant=*/true,\n+                                          /*is_intermediate=*/false,\n+                                          /*get_storage=*/true));\n+  auto value_type = mlir::dyn_cast<mlir::RankedTensorType>(type);\n+\n   tensorflow::TensorProto repr = ConvertTfliteConstTensor(tensor, buffer);\n   repr.clear_tensor_shape();\n   if (IsQuantized(tensor)) {\n"
                },
                {
                    "old_start": 652,
                    "old_length": 13,
                    "new_start": 716,
                    "new_length": 6,
                    "hunk": "@@ -652,13 +716,6 @@ static StatusOr<Operation*> BuildSparseConstOp(\n       builder.getContext(), tensor.sparsity->traversal_order,\n       tensor.sparsity->block_map, dim_metadata);\n \n-  auto value_type = shaped_type;\n-  if (IsQuantized(tensor)) {\n-    value_type = tensorflow::GetTypeFromTFTensorShape(\n-        shaped_type.getShape(), shaped_type.getElementType()\n-                                    .dyn_cast<mlir::quant::QuantizedType>()\n-                                    .getStorageType());\n-  }\n   std::vector<char> dense_buffer(\n       value_type.getElementType().getIntOrFloatBitWidth() / CHAR_BIT);\n   mlir::TypedAttr dummy_value =\n"
                },
                {
                    "old_start": 680,
                    "old_length": 28,
                    "new_start": 737,
                    "new_length": 41,
                    "hunk": "@@ -680,28 +737,41 @@ StatusOr<Operation*> BuildConstOp(const tflite::TensorT& tensor,\n                                   const std::vector<uint8_t>& buffer,\n                                   bool is_variable, OpBuilder builder,\n                                   Location loc, bool use_stablehlo_constant) {\n+  if (tensor.sparsity != nullptr) {\n+    return BuildSparseConstOp(tensor, buffer, builder, loc);\n+  }\n+\n+  if (is_variable) {\n+    return BuildVariableOp(tensor, builder, loc);\n+  }\n+\n   TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n-                                               /*is_constant=*/true));\n+                                               /*is_constant=*/true,\n+                                               /*is_intermediate=*/false,\n+                                               /*get_storage=*/true));\n   auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n   if (!shaped_type) {\n     return errors::Internal(\"Constant doesn't have a shape\");\n   }\n \n-  if (tensor.sparsity != nullptr) {\n-    return BuildSparseConstOp(tensor, buffer, shaped_type, builder, loc);\n+  mlir::ElementsAttr value;\n+  if (IsQuantized(tensor)) {\n+    bool truncate = shaped_type.getElementType().getIntOrFloatBitWidth() == 64;\n+    TF_ASSIGN_OR_RETURN(value, ConvertIntBuffer(shaped_type, buffer, truncate));\n+    TF_ASSIGN_OR_RETURN(\n+        auto type, GetQuantizedType(tensor, builder, /*is_constant=*/true,\n+                                    /*storage_type=*/value.getElementType()));\n+    shaped_type = shaped_type.clone(type);\n+    auto op = builder.create<tfl::QConstOp>(\n+        loc, mlir::TypeAttr::get(shaped_type), value);\n+    return op.getOperation();\n   }\n \n   auto elem_type = shaped_type.getElementType();\n-\n-  mlir::ElementsAttr value;\n-  if (is_variable) {\n-    return BuildVariableOp(tensor, shaped_type, builder, loc);\n-  } else if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n-    TF_ASSIGN_OR_RETURN(value,\n-                        ConvertFloatBuffer(shaped_type, float_type, buffer));\n-  } else if (elem_type.isa<mlir::IntegerType, QuantizedType>()) {\n-    TF_ASSIGN_OR_RETURN(value,\n-                        ConvertIntBuffer(shaped_type, elem_type, buffer));\n+  if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n+    TF_ASSIGN_OR_RETURN(value, ConvertFloatBuffer(shaped_type, buffer));\n+  } else if (elem_type.isa<mlir::IntegerType>()) {\n+    TF_ASSIGN_OR_RETURN(value, ConvertIntBuffer(shaped_type, buffer));\n   } else if (elem_type.isa<mlir::TF::StringType>()) {\n     tensorflow::TensorProto repr = ConvertTfliteConstTensor(tensor, buffer);\n     std::vector<llvm::StringRef> refs;\n"
                },
                {
                    "old_start": 720,
                    "old_length": 11,
                    "new_start": 790,
                    "new_length": 6,
                    "hunk": "@@ -720,11 +790,6 @@ StatusOr<Operation*> BuildConstOp(const tflite::TensorT& tensor,\n     return errors::Unimplemented(\"Constant of unsupported type\");\n   }\n \n-  if (IsQuantized(tensor)) {\n-    auto op = builder.create<tfl::QConstOp>(\n-        loc, mlir::TypeAttr::get(shaped_type), value);\n-    return op.getOperation();\n-  }\n   if (use_stablehlo_constant) {\n     auto op = builder.create<mlir::stablehlo::ConstantOp>(loc, value);\n     return op.getOperation();"
                }
            ],
            "whole_deleted": "-                                         bool is_constant = false) {\n-  mlir::IntegerType storage_type;\n-    storage_type = builder.getIntegerType(8);\n-  } else {\n-  bool is_weight_buffer = is_constant && (storage_type.getWidth() == 8);\n-\n-  int64_t storage_min = QuantizedType::getDefaultMinimumForInteger(\n-                            is_signed, storage_type.getWidth()) +\n-                        static_cast<int>(is_weight_buffer);\n-  int64_t storage_max = QuantizedType::getDefaultMaximumForInteger(\n-      is_signed, storage_type.getWidth());\n-                                         bool is_intermediate = false) {\n-  if (IsQuantized(tensor)) {\n-std::vector<T> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n-  std::vector<T> ret;\n-    ret.push_back(llvm::support::endian::readNext<\n-                  T, llvm::support::endian::system_endianness(),\n-                  llvm::support::unaligned>(data_ptr));\n-    mlir::RankedTensorType shaped_type, mlir::FloatType elem_type,\n-    const std::vector<uint8_t>& buffer) {\n-  switch (elem_type.getWidth()) {\n-  return errors::InvalidArgument(\"unsupported bit width\", elem_type.getWidth());\n-    mlir::RankedTensorType shaped_type, mlir::Type elem_type,\n-    const std::vector<uint8_t>& buffer) {\n-  } else if (auto qtype = elem_type.dyn_cast<QuantizedType>()) {\n-      llvm::SmallVector<bool, 8> values;\n-      values.reserve(buffer.size());\n-        values.emplace_back(b != 0);\n-          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(values)));\n-      auto values =\n-          shaped_type, ArrayRef<char>(values)));\n-      auto values = ReadAsHostEndian<uint16_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint16_t>(values)));\n-      auto values = ReadAsHostEndian<uint32_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint32_t>(values)));\n-      auto values = ReadAsHostEndian<uint64_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint64_t>(values)));\n-Operation* BuildVariableOp(const tflite::TensorT& tensor,\n-                           mlir::RankedTensorType shaped_type,\n-                           OpBuilder builder, Location loc) {\n-    const mlir::RankedTensorType shaped_type, OpBuilder& builder,\n-    Location loc) {\n-  auto value_type = shaped_type;\n-  if (IsQuantized(tensor)) {\n-    value_type = tensorflow::GetTypeFromTFTensorShape(\n-        shaped_type.getShape(), shaped_type.getElementType()\n-                                    .dyn_cast<mlir::quant::QuantizedType>()\n-                                    .getStorageType());\n-  }\n-                                               /*is_constant=*/true));\n-  if (tensor.sparsity != nullptr) {\n-    return BuildSparseConstOp(tensor, buffer, shaped_type, builder, loc);\n-\n-  mlir::ElementsAttr value;\n-  if (is_variable) {\n-    return BuildVariableOp(tensor, shaped_type, builder, loc);\n-  } else if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n-    TF_ASSIGN_OR_RETURN(value,\n-                        ConvertFloatBuffer(shaped_type, float_type, buffer));\n-  } else if (elem_type.isa<mlir::IntegerType, QuantizedType>()) {\n-    TF_ASSIGN_OR_RETURN(value,\n-                        ConvertIntBuffer(shaped_type, elem_type, buffer));\n-  if (IsQuantized(tensor)) {\n-    auto op = builder.create<tfl::QConstOp>(\n-        loc, mlir::TypeAttr::get(shaped_type), value);\n-    return op.getOperation();\n-  }\n",
            "whole_added": "+                                         bool is_constant = false,\n+                                         mlir::Type storage_type = {}) {\n+    storage_type = mlir::IntegerType::get(builder.getContext(), 8);\n+  }\n+\n+  if (!storage_type) {\n+  int bitwidth = storage_type.getIntOrFloatBitWidth();\n+  bool is_weight_buffer = is_constant && (bitwidth == 8);\n+\n+  int64_t storage_min =\n+      QuantizedType::getDefaultMinimumForInteger(is_signed, bitwidth) +\n+      static_cast<int>(is_weight_buffer);\n+  int64_t storage_max =\n+      QuantizedType::getDefaultMaximumForInteger(is_signed, bitwidth);\n+                                         bool is_intermediate = false,\n+                                         bool get_storage = false) {\n+  if (IsQuantized(tensor) && !get_storage) {\n+  } else if (IsQuantized(tensor) && get_storage) {\n+    // If the type is quantized we strip the signedness from the storage type.\n+    elem_type = mlir::IntegerType::get(elem_type.getContext(),\n+                                       elem_type.getIntOrFloatBitWidth());\n+llvm::SmallVector<mlir::APInt> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n+  llvm::SmallVector<mlir::APInt> ret;\n+    T val = llvm::support::endian::readNext<\n+        T, llvm::support::endian::system_endianness(),\n+        llvm::support::unaligned>(data_ptr);\n+    ret.push_back(mlir::APInt(sizeof(T) * 8, val));\n+    mlir::RankedTensorType shaped_type, const std::vector<uint8_t>& buffer) {\n+  mlir::Type elem_type = shaped_type.getElementType();\n+  switch (elem_type.getIntOrFloatBitWidth()) {\n+  return errors::InvalidArgument(\"unsupported bit width\",\n+                                 elem_type.getIntOrFloatBitWidth());\n+}\n+\n+// If the values in the buffer can be clamped to a bitwidth, truncate\n+// and return the new clamped integer width.\n+void truncateLimitedIntegerAPInt(llvm::SmallVector<mlir::APInt>& values) {\n+  mlir::APInt min = values[0];\n+  mlir::APInt max = values[0];\n+  for (auto& val : values) {\n+    min = llvm::APIntOps::smin(val, min);\n+    max = llvm::APIntOps::smax(val, max);\n+  }\n+\n+  for (int64_t bw = 8; bw < min.getBitWidth(); bw += bw) {\n+    auto limitMin = mlir::APInt::getSignedMinValue(bw).sext(min.getBitWidth());\n+    auto limitMax = mlir::APInt::getSignedMaxValue(bw).sext(min.getBitWidth());\n+    if (min.sle(limitMin) || max.sle(limitMin) || min.sge(limitMax) ||\n+        max.sge(limitMax)) {\n+      continue;\n+    }\n+\n+    for (int i = 0; i < values.size(); i++) {\n+      values[i] = values[i].trunc(bw);\n+    }\n+    break;\n+  }\n+    mlir::RankedTensorType shaped_type, const std::vector<uint8_t>& buffer,\n+    bool truncate = false) {\n+  mlir::Type elem_type = shaped_type.getElementType();\n+  } else if (auto qtype = elem_type.dyn_cast<mlir::quant::QuantizedType>()) {\n+  llvm::SmallVector<mlir::APInt> values;\n+      llvm::SmallVector<bool, 8> boolValues;\n+      boolValues.reserve(buffer.size());\n+        boolValues.emplace_back(b != 0);\n+          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(boolValues)));\n+      auto i4Values =\n+          shaped_type, ArrayRef<char>(i4Values)));\n+      values = ReadAsHostEndian<uint16_t>(buffer);\n+      break;\n+      values = ReadAsHostEndian<uint32_t>(buffer);\n+      break;\n+      values = ReadAsHostEndian<uint64_t>(buffer);\n+      break;\n+\n+  if (truncate) {\n+    truncateLimitedIntegerAPInt(values);\n+    auto sign = mlir::cast<mlir::IntegerType>(shaped_type.getElementType())\n+                    .getSignedness();\n+    auto ety = mlir::IntegerType::get(shaped_type.getContext(),\n+                                      values[0].getBitWidth(), sign);\n+    shaped_type =\n+        tensorflow::GetTypeFromTFTensorShape(shaped_type.getShape(), ety);\n+  }\n+\n+  return mlir::ElementsAttr(DenseElementsAttr::get(shaped_type, values));\n+StatusOr<Operation*> BuildVariableOp(const tflite::TensorT& tensor,\n+                                     OpBuilder builder, Location loc) {\n+  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n+                                               /*is_constant=*/true));\n+  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n+  if (!shaped_type) {\n+    return errors::Internal(\"Constant doesn't have a shape\");\n+  }\n+\n+    OpBuilder& builder, Location loc) {\n+  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n+                                               /*is_constant=*/true));\n+  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n+  if (!shaped_type) {\n+    return errors::Internal(\"Constant doesn't have a shape\");\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(type, GetTensorType(tensor, builder,\n+                                          /*is_constant=*/true,\n+                                          /*is_intermediate=*/false,\n+                                          /*get_storage=*/true));\n+  auto value_type = mlir::dyn_cast<mlir::RankedTensorType>(type);\n+\n+  if (tensor.sparsity != nullptr) {\n+    return BuildSparseConstOp(tensor, buffer, builder, loc);\n+  }\n+\n+  if (is_variable) {\n+    return BuildVariableOp(tensor, builder, loc);\n+  }\n+\n+                                               /*is_constant=*/true,\n+                                               /*is_intermediate=*/false,\n+                                               /*get_storage=*/true));\n+  mlir::ElementsAttr value;\n+  if (IsQuantized(tensor)) {\n+    bool truncate = shaped_type.getElementType().getIntOrFloatBitWidth() == 64;\n+    TF_ASSIGN_OR_RETURN(value, ConvertIntBuffer(shaped_type, buffer, truncate));\n+    TF_ASSIGN_OR_RETURN(\n+        auto type, GetQuantizedType(tensor, builder, /*is_constant=*/true,\n+                                    /*storage_type=*/value.getElementType()));\n+    shaped_type = shaped_type.clone(type);\n+    auto op = builder.create<tfl::QConstOp>(\n+        loc, mlir::TypeAttr::get(shaped_type), value);\n+    return op.getOperation();\n+  if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n+    TF_ASSIGN_OR_RETURN(value, ConvertFloatBuffer(shaped_type, buffer));\n+  } else if (elem_type.isa<mlir::IntegerType>()) {\n+    TF_ASSIGN_OR_RETURN(value, ConvertIntBuffer(shaped_type, buffer));\n",
            "whole_hunk": "@@ -162,18 +162,20 @@ Location OpLoc(const OperatorT& op,\n // Returns the correct type for a quantized tensor\n // We have a special case for constants since they have a higher minimum value.\n StatusOr<QuantizedType> GetQuantizedType(const TensorT& tensor, Builder builder,\n-                                         bool is_constant = false) {\n+                                         bool is_constant = false,\n+                                         mlir::Type storage_type = {}) {\n   tflite::QuantizationParametersT& quant_params = *tensor.quantization;\n   if (quant_params.details.AsCustomQuantization()) {\n     return errors::Unimplemented(\"Cannot handle experimental quantization\");\n   }\n \n   bool is_signed = true;\n-  mlir::IntegerType storage_type;\n   if (tensor.type == tflite::TensorType_UINT8) {\n     is_signed = false;\n-    storage_type = builder.getIntegerType(8);\n-  } else {\n+    storage_type = mlir::IntegerType::get(builder.getContext(), 8);\n+  }\n+\n+  if (!storage_type) {\n     auto raw_elem_type = ConvertElementType(tensor.type, builder);\n     if (!raw_elem_type.isa<mlir::IntegerType>()) {\n       return errors::InvalidArgument(\n@@ -186,13 +188,14 @@ StatusOr<QuantizedType> GetQuantizedType(const TensorT& tensor, Builder builder,\n   // Since we don't know which ones are weights, we represent this optimization\n   // as a change in the storage bounds for the type for all constants of this\n   // type.\n-  bool is_weight_buffer = is_constant && (storage_type.getWidth() == 8);\n-\n-  int64_t storage_min = QuantizedType::getDefaultMinimumForInteger(\n-                            is_signed, storage_type.getWidth()) +\n-                        static_cast<int>(is_weight_buffer);\n-  int64_t storage_max = QuantizedType::getDefaultMaximumForInteger(\n-      is_signed, storage_type.getWidth());\n+  int bitwidth = storage_type.getIntOrFloatBitWidth();\n+  bool is_weight_buffer = is_constant && (bitwidth == 8);\n+\n+  int64_t storage_min =\n+      QuantizedType::getDefaultMinimumForInteger(is_signed, bitwidth) +\n+      static_cast<int>(is_weight_buffer);\n+  int64_t storage_max =\n+      QuantizedType::getDefaultMaximumForInteger(is_signed, bitwidth);\n   uint32_t flags =\n       is_signed ? mlir::quant::QuantizationFlags::FlagValue::Signed : 0;\n \n@@ -232,7 +235,8 @@ StatusOr<QuantizedType> GetCalibratedQuantizedType(const TensorT& tensor,\n \n StatusOr<mlir::TensorType> GetTensorType(const TensorT& tensor, Builder builder,\n                                          bool is_constant = false,\n-                                         bool is_intermediate = false) {\n+                                         bool is_intermediate = false,\n+                                         bool get_storage = false) {\n   mlir::Type elem_type = ConvertElementType(tensor.type, builder);\n   if (tensor.type == tflite::TensorType_VARIANT) {\n     llvm::SmallVector<mlir::TensorType> tensor_types;\n@@ -254,9 +258,13 @@ StatusOr<mlir::TensorType> GetTensorType(const TensorT& tensor, Builder builder,\n     }\n     elem_type = mlir::TF::VariantType::get(tensor_types, builder.getContext());\n   }\n-  if (IsQuantized(tensor)) {\n+  if (IsQuantized(tensor) && !get_storage) {\n     TF_ASSIGN_OR_RETURN(elem_type,\n                         GetQuantizedType(tensor, builder, is_constant));\n+  } else if (IsQuantized(tensor) && get_storage) {\n+    // If the type is quantized we strip the signedness from the storage type.\n+    elem_type = mlir::IntegerType::get(elem_type.getContext(),\n+                                       elem_type.getIntOrFloatBitWidth());\n   }\n \n   // Intermediate tensors with calibration value (but not scale and zero points)\n@@ -355,8 +363,8 @@ std::string GetMlirOpName(const tflite::OperatorT& op,\n // The read_size parameter is present to allow reading both float16 and float32s\n // without a case split.\n template <typename T>\n-std::vector<T> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n-  std::vector<T> ret;\n+llvm::SmallVector<mlir::APInt> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n+  llvm::SmallVector<mlir::APInt> ret;\n   size_t read_size = sizeof(T);\n   int bytes_len = bytes.size();\n   assert(bytes_len % read_size == 0);\n@@ -366,9 +374,10 @@ std::vector<T> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n \n   const char* data_ptr = reinterpret_cast<const char*>(bytes.data());\n   for (int i = 0; i < elem_count; i++) {\n-    ret.push_back(llvm::support::endian::readNext<\n-                  T, llvm::support::endian::system_endianness(),\n-                  llvm::support::unaligned>(data_ptr));\n+    T val = llvm::support::endian::readNext<\n+        T, llvm::support::endian::system_endianness(),\n+        llvm::support::unaligned>(data_ptr);\n+    ret.push_back(mlir::APInt(sizeof(T) * 8, val));\n   }\n   return ret;\n }\n@@ -398,12 +407,12 @@ tensorflow::TensorProto ConvertTfliteConstTensor(\n }\n \n StatusOr<mlir::ElementsAttr> ConvertFloatBuffer(\n-    mlir::RankedTensorType shaped_type, mlir::FloatType elem_type,\n-    const std::vector<uint8_t>& buffer) {\n+    mlir::RankedTensorType shaped_type, const std::vector<uint8_t>& buffer) {\n   size_t bytes_len = buffer.size();\n+  mlir::Type elem_type = shaped_type.getElementType();\n \n   // The bytes of floats are stored little-endian.\n-  switch (elem_type.getWidth()) {\n+  switch (elem_type.getIntOrFloatBitWidth()) {\n     case 16: {\n       assert(bytes_len % 2 == 0);\n       assert(elem_type.isF16());\n@@ -458,16 +467,43 @@ StatusOr<mlir::ElementsAttr> ConvertFloatBuffer(\n           DenseElementsAttr::get(shaped_type, ArrayRef<double>(values)));\n     }\n   }\n-  return errors::InvalidArgument(\"unsupported bit width\", elem_type.getWidth());\n+  return errors::InvalidArgument(\"unsupported bit width\",\n+                                 elem_type.getIntOrFloatBitWidth());\n+}\n+\n+// If the values in the buffer can be clamped to a bitwidth, truncate\n+// and return the new clamped integer width.\n+void truncateLimitedIntegerAPInt(llvm::SmallVector<mlir::APInt>& values) {\n+  mlir::APInt min = values[0];\n+  mlir::APInt max = values[0];\n+  for (auto& val : values) {\n+    min = llvm::APIntOps::smin(val, min);\n+    max = llvm::APIntOps::smax(val, max);\n+  }\n+\n+  for (int64_t bw = 8; bw < min.getBitWidth(); bw += bw) {\n+    auto limitMin = mlir::APInt::getSignedMinValue(bw).sext(min.getBitWidth());\n+    auto limitMax = mlir::APInt::getSignedMaxValue(bw).sext(min.getBitWidth());\n+    if (min.sle(limitMin) || max.sle(limitMin) || min.sge(limitMax) ||\n+        max.sge(limitMax)) {\n+      continue;\n+    }\n+\n+    for (int i = 0; i < values.size(); i++) {\n+      values[i] = values[i].trunc(bw);\n+    }\n+    break;\n+  }\n }\n \n StatusOr<mlir::ElementsAttr> ConvertIntBuffer(\n-    mlir::RankedTensorType shaped_type, mlir::Type elem_type,\n-    const std::vector<uint8_t>& buffer) {\n+    mlir::RankedTensorType shaped_type, const std::vector<uint8_t>& buffer,\n+    bool truncate = false) {\n+  mlir::Type elem_type = shaped_type.getElementType();\n   unsigned bit_width;\n   if (auto itype = elem_type.dyn_cast<mlir::IntegerType>()) {\n     bit_width = itype.getWidth();\n-  } else if (auto qtype = elem_type.dyn_cast<QuantizedType>()) {\n+  } else if (auto qtype = elem_type.dyn_cast<mlir::quant::QuantizedType>()) {\n     bit_width = qtype.getStorageTypeIntegralWidth();\n     shaped_type = tensorflow::GetTypeFromTFTensorShape(shaped_type.getShape(),\n                                                        qtype.getStorageType());\n@@ -475,47 +511,57 @@ StatusOr<mlir::ElementsAttr> ConvertIntBuffer(\n     return errors::InvalidArgument(\"unsupported integer constant type\");\n   }\n \n+  llvm::SmallVector<mlir::APInt> values;\n   switch (bit_width) {\n     case 1: {\n       // vector<bool> doesn't convert to an ArrayRef\n-      llvm::SmallVector<bool, 8> values;\n-      values.reserve(buffer.size());\n+      llvm::SmallVector<bool, 8> boolValues;\n+      boolValues.reserve(buffer.size());\n       for (auto b : buffer) {\n-        values.emplace_back(b != 0);\n+        boolValues.emplace_back(b != 0);\n       }\n       return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(values)));\n+          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(boolValues)));\n     }\n     case 4: {\n-      auto values =\n+      auto i4Values =\n           tflite::UnpackDenseInt4IntoInt8(buffer, shaped_type.getNumElements());\n       // Use `getFromRawBuffer()` instead of `get()` to bypass a templated size\n       // check which doesn't work with int4 because int4_t doesn't exist.\n       return mlir::ElementsAttr(DenseElementsAttr::getFromRawBuffer(\n-          shaped_type, ArrayRef<char>(values)));\n+          shaped_type, ArrayRef<char>(i4Values)));\n     }\n     case 8: {\n       return mlir::ElementsAttr(\n           DenseElementsAttr::get(shaped_type, ArrayRef<uint8_t>(buffer)));\n     }\n     case 16: {\n-      auto values = ReadAsHostEndian<uint16_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint16_t>(values)));\n+      values = ReadAsHostEndian<uint16_t>(buffer);\n+      break;\n     }\n     case 32: {\n-      auto values = ReadAsHostEndian<uint32_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint32_t>(values)));\n+      values = ReadAsHostEndian<uint32_t>(buffer);\n+      break;\n     }\n     case 64: {\n-      auto values = ReadAsHostEndian<uint64_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint64_t>(values)));\n+      values = ReadAsHostEndian<uint64_t>(buffer);\n+      break;\n     }\n     default:\n       return errors::Unimplemented(\"Cannot handle bit width \", bit_width);\n   }\n+\n+  if (truncate) {\n+    truncateLimitedIntegerAPInt(values);\n+    auto sign = mlir::cast<mlir::IntegerType>(shaped_type.getElementType())\n+                    .getSignedness();\n+    auto ety = mlir::IntegerType::get(shaped_type.getContext(),\n+                                      values[0].getBitWidth(), sign);\n+    shaped_type =\n+        tensorflow::GetTypeFromTFTensorShape(shaped_type.getShape(), ety);\n+  }\n+\n+  return mlir::ElementsAttr(DenseElementsAttr::get(shaped_type, values));\n }\n \n StatusOr<Operation*> BuildExternalConstOp(const tflite::TensorT& tensor,\n@@ -561,9 +607,15 @@ static mlir::ElementsAttr GetSplat(RankedTensorType type, int unique_index,\n // variable `stateful_variable_idx` is used as a unique value for each constant\n // to avoid CSEed. `tensor` is the data structure of flatbuffer. `shaped_type`\n // is the ShapedType for the const op.\n-Operation* BuildVariableOp(const tflite::TensorT& tensor,\n-                           mlir::RankedTensorType shaped_type,\n-                           OpBuilder builder, Location loc) {\n+StatusOr<Operation*> BuildVariableOp(const tflite::TensorT& tensor,\n+                                     OpBuilder builder, Location loc) {\n+  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n+                                               /*is_constant=*/true));\n+  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n+  if (!shaped_type) {\n+    return errors::Internal(\"Constant doesn't have a shape\");\n+  }\n+\n   static int stateful_variable_idx = 0;\n   mlir::ElementsAttr value =\n       GetSplat(shaped_type, stateful_variable_idx++, builder);\n@@ -607,8 +659,20 @@ static StatusOr<std::vector<int32_t>> ConvertSparseIndexVector(\n \n static StatusOr<Operation*> BuildSparseConstOp(\n     const tflite::TensorT& tensor, const std::vector<uint8_t>& buffer,\n-    const mlir::RankedTensorType shaped_type, OpBuilder& builder,\n-    Location loc) {\n+    OpBuilder& builder, Location loc) {\n+  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n+                                               /*is_constant=*/true));\n+  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n+  if (!shaped_type) {\n+    return errors::Internal(\"Constant doesn't have a shape\");\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(type, GetTensorType(tensor, builder,\n+                                          /*is_constant=*/true,\n+                                          /*is_intermediate=*/false,\n+                                          /*get_storage=*/true));\n+  auto value_type = mlir::dyn_cast<mlir::RankedTensorType>(type);\n+\n   tensorflow::TensorProto repr = ConvertTfliteConstTensor(tensor, buffer);\n   repr.clear_tensor_shape();\n   if (IsQuantized(tensor)) {\n@@ -652,13 +716,6 @@ static StatusOr<Operation*> BuildSparseConstOp(\n       builder.getContext(), tensor.sparsity->traversal_order,\n       tensor.sparsity->block_map, dim_metadata);\n \n-  auto value_type = shaped_type;\n-  if (IsQuantized(tensor)) {\n-    value_type = tensorflow::GetTypeFromTFTensorShape(\n-        shaped_type.getShape(), shaped_type.getElementType()\n-                                    .dyn_cast<mlir::quant::QuantizedType>()\n-                                    .getStorageType());\n-  }\n   std::vector<char> dense_buffer(\n       value_type.getElementType().getIntOrFloatBitWidth() / CHAR_BIT);\n   mlir::TypedAttr dummy_value =\n@@ -680,28 +737,41 @@ StatusOr<Operation*> BuildConstOp(const tflite::TensorT& tensor,\n                                   const std::vector<uint8_t>& buffer,\n                                   bool is_variable, OpBuilder builder,\n                                   Location loc, bool use_stablehlo_constant) {\n+  if (tensor.sparsity != nullptr) {\n+    return BuildSparseConstOp(tensor, buffer, builder, loc);\n+  }\n+\n+  if (is_variable) {\n+    return BuildVariableOp(tensor, builder, loc);\n+  }\n+\n   TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n-                                               /*is_constant=*/true));\n+                                               /*is_constant=*/true,\n+                                               /*is_intermediate=*/false,\n+                                               /*get_storage=*/true));\n   auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n   if (!shaped_type) {\n     return errors::Internal(\"Constant doesn't have a shape\");\n   }\n \n-  if (tensor.sparsity != nullptr) {\n-    return BuildSparseConstOp(tensor, buffer, shaped_type, builder, loc);\n+  mlir::ElementsAttr value;\n+  if (IsQuantized(tensor)) {\n+    bool truncate = shaped_type.getElementType().getIntOrFloatBitWidth() == 64;\n+    TF_ASSIGN_OR_RETURN(value, ConvertIntBuffer(shaped_type, buffer, truncate));\n+    TF_ASSIGN_OR_RETURN(\n+        auto type, GetQuantizedType(tensor, builder, /*is_constant=*/true,\n+                                    /*storage_type=*/value.getElementType()));\n+    shaped_type = shaped_type.clone(type);\n+    auto op = builder.create<tfl::QConstOp>(\n+        loc, mlir::TypeAttr::get(shaped_type), value);\n+    return op.getOperation();\n   }\n \n   auto elem_type = shaped_type.getElementType();\n-\n-  mlir::ElementsAttr value;\n-  if (is_variable) {\n-    return BuildVariableOp(tensor, shaped_type, builder, loc);\n-  } else if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n-    TF_ASSIGN_OR_RETURN(value,\n-                        ConvertFloatBuffer(shaped_type, float_type, buffer));\n-  } else if (elem_type.isa<mlir::IntegerType, QuantizedType>()) {\n-    TF_ASSIGN_OR_RETURN(value,\n-                        ConvertIntBuffer(shaped_type, elem_type, buffer));\n+  if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n+    TF_ASSIGN_OR_RETURN(value, ConvertFloatBuffer(shaped_type, buffer));\n+  } else if (elem_type.isa<mlir::IntegerType>()) {\n+    TF_ASSIGN_OR_RETURN(value, ConvertIntBuffer(shaped_type, buffer));\n   } else if (elem_type.isa<mlir::TF::StringType>()) {\n     tensorflow::TensorProto repr = ConvertTfliteConstTensor(tensor, buffer);\n     std::vector<llvm::StringRef> refs;\n@@ -720,11 +790,6 @@ StatusOr<Operation*> BuildConstOp(const tflite::TensorT& tensor,\n     return errors::Unimplemented(\"Constant of unsupported type\");\n   }\n \n-  if (IsQuantized(tensor)) {\n-    auto op = builder.create<tfl::QConstOp>(\n-        loc, mlir::TypeAttr::get(shaped_type), value);\n-    return op.getOperation();\n-  }\n   if (use_stablehlo_constant) {\n     auto op = builder.create<mlir::stablehlo::ConstantOp>(loc, value);\n     return op.getOperation();"
        }
    ]
},
{
    "Id": 430,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b7d44fb5b5aeb132bfa6ae700981b0e1ca0609b1",
    "date": "2023-05-08T16:06:11-07:00",
    "message": "Relax the isolated process check in NNAPI delegate for Android 14 (U) and above\n\nPiperOrigin-RevId: 530432149",
    "label": "NO",
    "changes": [
        {
            "name": "nnapi_implementation.cc",
            "path": "tensorflow/lite/nnapi/nnapi_implementation.cc",
            "patches": [
                {
                    "old_start": 185,
                    "old_length": 7,
                    "new_start": 185,
                    "new_length": 8,
                    "hunk": "@@ -185,7 +185,8 @@ const NnApi LoadNnApi() {\n     return nnapi;\n   }\n \n-  if (IsIsolatedProcess()) {\n+  // Disable NNAPI for Android 13 and earlier if running in isolated process.\n+  if (nnapi.android_sdk_version <= 33 && IsIsolatedProcess()) {\n     NNAPI_LOG(\"NNAPI is disabled in an isolated process\");\n     nnapi.nnapi_exists = false;\n     return nnapi;"
                }
            ],
            "whole_deleted": "-  if (IsIsolatedProcess()) {\n",
            "whole_added": "+  // Disable NNAPI for Android 13 and earlier if running in isolated process.\n+  if (nnapi.android_sdk_version <= 33 && IsIsolatedProcess()) {\n",
            "whole_hunk": "@@ -185,7 +185,8 @@ const NnApi LoadNnApi() {\n     return nnapi;\n   }\n \n-  if (IsIsolatedProcess()) {\n+  // Disable NNAPI for Android 13 and earlier if running in isolated process.\n+  if (nnapi.android_sdk_version <= 33 && IsIsolatedProcess()) {\n     NNAPI_LOG(\"NNAPI is disabled in an isolated process\");\n     nnapi.nnapi_exists = false;\n     return nnapi;"
        }
    ]
},
{
    "Id": 658,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1b1ebd2b295ae199bddda268d579f4a4cbfccefd",
    "date": "2022-10-15T12:36:48-07:00",
    "message": "Merge pull request #55677 from PatriceVignola:add-get-input-tensor-from-variable-validation\n\nPiperOrigin-RevId: 481375400",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/c/BUILD",
            "patches": [
                {
                    "old_start": 703,
                    "old_length": 6,
                    "new_start": 703,
                    "new_length": 7,
                    "hunk": "@@ -703,6 +703,7 @@ tf_cuda_library(\n         \":tf_tensor_internal\",\n         \"//tensorflow/core:framework\",\n         \"//tensorflow/core:framework_internal_impl\",\n+        \"//tensorflow/core/lib/gtl:cleanup\",\n         \"//tensorflow/core/platform:errors\",\n         \"//tensorflow/core/platform:mutex\",\n         \"//tensorflow/core/platform:refcount\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/core/lib/gtl:cleanup\",\n",
            "whole_hunk": "@@ -703,6 +703,7 @@ tf_cuda_library(\n         \":tf_tensor_internal\",\n         \"//tensorflow/core:framework\",\n         \"//tensorflow/core:framework_internal_impl\",\n+        \"//tensorflow/core/lib/gtl:cleanup\",\n         \"//tensorflow/core/platform:errors\",\n         \"//tensorflow/core/platform:mutex\",\n         \"//tensorflow/core/platform:refcount\",\n"
        },
        {
            "name": "kernels_experimental.cc",
            "path": "tensorflow/c/kernels_experimental.cc",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 6,
                    "new_start": 26,
                    "new_length": 7,
                    "hunk": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/resource_mgr.h\"\n #include \"tensorflow/core/framework/resource_var.h\"\n #include \"tensorflow/core/framework/variant.h\"\n+#include \"tensorflow/core/lib/gtl/cleanup.h\"\n \n #ifndef IS_MOBILE_PLATFORM\n #include \"tensorflow/core/kernels/data/optional_ops_util.h\"\n"
                },
                {
                    "old_start": 352,
                    "old_length": 6,
                    "new_start": 353,
                    "new_length": 11,
                    "hunk": "@@ -352,6 +353,11 @@ void TF_GetInputTensorFromVariable(TF_OpKernelContext* ctx, int input,\n                                                     TF_Tensor* dest),\n                                    TF_Tensor** out, TF_Status* status) {\n   auto* cc_ctx = reinterpret_cast<::tensorflow::OpKernelContext*>(ctx);\n+\n+  auto status_setter = ::tensorflow::gtl::MakeCleanup([cc_ctx, status]() {\n+    ::tensorflow::Set_TF_Status_from_Status(status, cc_ctx->status());\n+  });\n+\n   tensorflow::Status s;\n   if (cc_ctx->input_dtype(input) == tensorflow::DT_RESOURCE) {\n     tensorflow::core::RefCountPtr<tensorflow::Var> var;\n"
                },
                {
                    "old_start": 361,
                    "old_length": 19,
                    "new_start": 367,
                    "new_length": 19,
                    "hunk": "@@ -361,19 +367,19 @@ void TF_GetInputTensorFromVariable(TF_OpKernelContext* ctx, int input,\n       OP_REQUIRES_OK(cc_ctx, EnsureSparseVariableAccess(ctx, isVariantType,\n                                                         copyFunc, var.get()));\n       *out = ::tensorflow::TF_TensorFromTensor(*var->tensor(), &s);\n-      ::tensorflow::Set_TF_Status_from_Status(status, s);\n+      OP_REQUIRES_OK(cc_ctx, s);\n       return;\n     }\n     OP_REQUIRES_OK(cc_ctx, PrepareToUpdateVariable(\n                                ctx, var->tensor(),\n                                var->copy_on_read_mode.load(), false, copyFunc));\n     *out = ::tensorflow::TF_TensorFromTensor(*var->tensor(), &s);\n-    ::tensorflow::Set_TF_Status_from_Status(status, s);\n+    OP_REQUIRES_OK(cc_ctx, s);\n     return;\n   }\n   *out = ::tensorflow::TF_TensorFromTensor(\n       cc_ctx->mutable_input(input, lock_held), &s);\n-  ::tensorflow::Set_TF_Status_from_Status(status, s);\n+  OP_REQUIRES_OK(cc_ctx, s);\n }\n \n void TF_OpKernelContext_ForwardRefInputToRefOutput(TF_OpKernelContext* ctx,\n"
                }
            ],
            "whole_deleted": "-      ::tensorflow::Set_TF_Status_from_Status(status, s);\n-    ::tensorflow::Set_TF_Status_from_Status(status, s);\n-  ::tensorflow::Set_TF_Status_from_Status(status, s);\n",
            "whole_added": "+#include \"tensorflow/core/lib/gtl/cleanup.h\"\n+\n+  auto status_setter = ::tensorflow::gtl::MakeCleanup([cc_ctx, status]() {\n+    ::tensorflow::Set_TF_Status_from_Status(status, cc_ctx->status());\n+  });\n+\n+      OP_REQUIRES_OK(cc_ctx, s);\n+    OP_REQUIRES_OK(cc_ctx, s);\n+  OP_REQUIRES_OK(cc_ctx, s);\n",
            "whole_hunk": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/resource_mgr.h\"\n #include \"tensorflow/core/framework/resource_var.h\"\n #include \"tensorflow/core/framework/variant.h\"\n+#include \"tensorflow/core/lib/gtl/cleanup.h\"\n \n #ifndef IS_MOBILE_PLATFORM\n #include \"tensorflow/core/kernels/data/optional_ops_util.h\"\n@@ -352,6 +353,11 @@ void TF_GetInputTensorFromVariable(TF_OpKernelContext* ctx, int input,\n                                                     TF_Tensor* dest),\n                                    TF_Tensor** out, TF_Status* status) {\n   auto* cc_ctx = reinterpret_cast<::tensorflow::OpKernelContext*>(ctx);\n+\n+  auto status_setter = ::tensorflow::gtl::MakeCleanup([cc_ctx, status]() {\n+    ::tensorflow::Set_TF_Status_from_Status(status, cc_ctx->status());\n+  });\n+\n   tensorflow::Status s;\n   if (cc_ctx->input_dtype(input) == tensorflow::DT_RESOURCE) {\n     tensorflow::core::RefCountPtr<tensorflow::Var> var;\n@@ -361,19 +367,19 @@ void TF_GetInputTensorFromVariable(TF_OpKernelContext* ctx, int input,\n       OP_REQUIRES_OK(cc_ctx, EnsureSparseVariableAccess(ctx, isVariantType,\n                                                         copyFunc, var.get()));\n       *out = ::tensorflow::TF_TensorFromTensor(*var->tensor(), &s);\n-      ::tensorflow::Set_TF_Status_from_Status(status, s);\n+      OP_REQUIRES_OK(cc_ctx, s);\n       return;\n     }\n     OP_REQUIRES_OK(cc_ctx, PrepareToUpdateVariable(\n                                ctx, var->tensor(),\n                                var->copy_on_read_mode.load(), false, copyFunc));\n     *out = ::tensorflow::TF_TensorFromTensor(*var->tensor(), &s);\n-    ::tensorflow::Set_TF_Status_from_Status(status, s);\n+    OP_REQUIRES_OK(cc_ctx, s);\n     return;\n   }\n   *out = ::tensorflow::TF_TensorFromTensor(\n       cc_ctx->mutable_input(input, lock_held), &s);\n-  ::tensorflow::Set_TF_Status_from_Status(status, s);\n+  OP_REQUIRES_OK(cc_ctx, s);\n }\n \n void TF_OpKernelContext_ForwardRefInputToRefOutput(TF_OpKernelContext* ctx,\n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/core/lib/gtl/BUILD",
            "patches": [
                {
                    "old_start": 43,
                    "old_length": 6,
                    "new_start": 43,
                    "new_length": 7,
                    "hunk": "@@ -43,6 +43,7 @@ cc_library(\n cc_library(\n     name = \"cleanup\",\n     hdrs = [\"cleanup.h\"],\n+    visibility = [\"//visibility:public\"],\n     deps = [\"//tensorflow/core/platform:macros\"],\n )\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    visibility = [\"//visibility:public\"],\n",
            "whole_hunk": "@@ -43,6 +43,7 @@ cc_library(\n cc_library(\n     name = \"cleanup\",\n     hdrs = [\"cleanup.h\"],\n+    visibility = [\"//visibility:public\"],\n     deps = [\"//tensorflow/core/platform:macros\"],\n )\n "
        }
    ]
},
{
    "Id": 78,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/dce9eeac57d02fd34bc3b146a55c583c38cb490c",
    "date": "2024-04-09T16:43:03-07:00",
    "message": "Adding null pointer checks.\n\nPiperOrigin-RevId: 623312795",
    "label": "YES",
    "changes": [
        {
            "name": "hlo_verifier.cc",
            "path": "third_party/xla/xla/service/hlo_verifier.cc",
            "patches": [
                {
                    "old_start": 2010,
                    "old_length": 17,
                    "new_start": 2010,
                    "new_length": 21,
                    "hunk": "@@ -2010,17 +2010,21 @@ std::string ComputationsToString(\n \n // Verifies various invariants about the structure of the HLO:\n //\n-// (1) each instruction has a non-null parent() set to the HloComputation\n-// which\n-//     contains it.\n+// (1) each instruction is non-null and has a non-null parent() set to the\n+// HloComputation which contains it.\n //\n-// (2) each computation has a non-null parent() set to the HloModule which\n-//     contains it.\n+// (2) each computation is non-null and has a non-null parent() set to the\n+// HloModule which contains it.\n //\n-// (3) the operands of each instruction are in the same computation as the\n-//     instruction.\n+// (3) the operands of each instruction are non-null and are in the same\n+// computation as the instruction.\n Status VerifyHloStructure(HloModule* module) {\n   for (const HloComputation* computation : module->computations()) {\n+    if (computation == nullptr) {\n+      return Internal(\"Computation in module %s is a null pointer\",\n+                      module->name());\n+    }\n+\n     if (computation->parent() == nullptr) {\n       return Internal(\"Computation %s has a null parent pointer\",\n                       computation->name());\n"
                },
                {
                    "old_start": 2031,
                    "old_length": 6,
                    "new_start": 2035,
                    "new_length": 10,
                    "hunk": "@@ -2031,6 +2035,10 @@ Status VerifyHloStructure(HloModule* module) {\n     }\n \n     for (const HloInstruction* instruction : computation->instructions()) {\n+      if (instruction == nullptr) {\n+        return Internal(\"Instruction in computation %s is a null pointer\",\n+                        computation->name());\n+      }\n       if (instruction->parent() == nullptr) {\n         return Internal(\"Instruction %s has a null parent pointer\",\n                         instruction->name());\n"
                },
                {
                    "old_start": 2051,
                    "old_length": 6,
                    "new_start": 2059,
                    "new_length": 17,
                    "hunk": "@@ -2051,6 +2059,17 @@ Status VerifyHloStructure(HloModule* module) {\n     for (const HloInstruction* instruction : computation->instructions()) {\n       for (int i = 0; i < instruction->operand_count(); ++i) {\n         const HloInstruction* operand = instruction->operand(i);\n+        if (operand == nullptr) {\n+          return Internal(\n+              \"Operand %d (out of %d) of instruction: %s is a null pointer\", i,\n+              instruction->operand_count(), instruction->name());\n+        }\n+        if (operand->parent() == nullptr) {\n+          return Internal(\n+              \"Operand %d (out of %d) of instruction: %s has a null pointer \"\n+              \"parent\",\n+              i, instruction->operand_count(), instruction->name());\n+        }\n         if (operand->parent() != instruction->parent()) {\n           return Internal(\n               \"Operand %d (%s) of instruction %s is in a different \""
                }
            ],
            "whole_deleted": "-// (1) each instruction has a non-null parent() set to the HloComputation\n-// which\n-//     contains it.\n-// (2) each computation has a non-null parent() set to the HloModule which\n-//     contains it.\n-// (3) the operands of each instruction are in the same computation as the\n-//     instruction.\n",
            "whole_added": "+// (1) each instruction is non-null and has a non-null parent() set to the\n+// HloComputation which contains it.\n+// (2) each computation is non-null and has a non-null parent() set to the\n+// HloModule which contains it.\n+// (3) the operands of each instruction are non-null and are in the same\n+// computation as the instruction.\n+    if (computation == nullptr) {\n+      return Internal(\"Computation in module %s is a null pointer\",\n+                      module->name());\n+    }\n+\n+      if (instruction == nullptr) {\n+        return Internal(\"Instruction in computation %s is a null pointer\",\n+                        computation->name());\n+      }\n+        if (operand == nullptr) {\n+          return Internal(\n+              \"Operand %d (out of %d) of instruction: %s is a null pointer\", i,\n+              instruction->operand_count(), instruction->name());\n+        }\n+        if (operand->parent() == nullptr) {\n+          return Internal(\n+              \"Operand %d (out of %d) of instruction: %s has a null pointer \"\n+              \"parent\",\n+              i, instruction->operand_count(), instruction->name());\n+        }\n",
            "whole_hunk": "@@ -2010,17 +2010,21 @@ std::string ComputationsToString(\n \n // Verifies various invariants about the structure of the HLO:\n //\n-// (1) each instruction has a non-null parent() set to the HloComputation\n-// which\n-//     contains it.\n+// (1) each instruction is non-null and has a non-null parent() set to the\n+// HloComputation which contains it.\n //\n-// (2) each computation has a non-null parent() set to the HloModule which\n-//     contains it.\n+// (2) each computation is non-null and has a non-null parent() set to the\n+// HloModule which contains it.\n //\n-// (3) the operands of each instruction are in the same computation as the\n-//     instruction.\n+// (3) the operands of each instruction are non-null and are in the same\n+// computation as the instruction.\n Status VerifyHloStructure(HloModule* module) {\n   for (const HloComputation* computation : module->computations()) {\n+    if (computation == nullptr) {\n+      return Internal(\"Computation in module %s is a null pointer\",\n+                      module->name());\n+    }\n+\n     if (computation->parent() == nullptr) {\n       return Internal(\"Computation %s has a null parent pointer\",\n                       computation->name());\n@@ -2031,6 +2035,10 @@ Status VerifyHloStructure(HloModule* module) {\n     }\n \n     for (const HloInstruction* instruction : computation->instructions()) {\n+      if (instruction == nullptr) {\n+        return Internal(\"Instruction in computation %s is a null pointer\",\n+                        computation->name());\n+      }\n       if (instruction->parent() == nullptr) {\n         return Internal(\"Instruction %s has a null parent pointer\",\n                         instruction->name());\n@@ -2051,6 +2059,17 @@ Status VerifyHloStructure(HloModule* module) {\n     for (const HloInstruction* instruction : computation->instructions()) {\n       for (int i = 0; i < instruction->operand_count(); ++i) {\n         const HloInstruction* operand = instruction->operand(i);\n+        if (operand == nullptr) {\n+          return Internal(\n+              \"Operand %d (out of %d) of instruction: %s is a null pointer\", i,\n+              instruction->operand_count(), instruction->name());\n+        }\n+        if (operand->parent() == nullptr) {\n+          return Internal(\n+              \"Operand %d (out of %d) of instruction: %s has a null pointer \"\n+              \"parent\",\n+              i, instruction->operand_count(), instruction->name());\n+        }\n         if (operand->parent() != instruction->parent()) {\n           return Internal(\n               \"Operand %d (%s) of instruction %s is in a different \""
        }
    ]
},
{
    "Id": 650,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fcc97a32a8671814935e735492258089976c4457",
    "date": "2022-10-27T01:05:19-07:00",
    "message": "Fix RaggedTensor slicing (`__getitem__`) for additional supported value types\n\nWe now check if the data is a RaggedTensor or not, rather than checking if the data is a Tensor. This properly dispatches `__getitem__` to the underlying type rather than sometimes treating it as a RaggedTensor. This previously led to `Ellipsis` not being supported.\n\nPiperOrigin-RevId: 484179745",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/python/ops/ragged/BUILD",
            "patches": [
                {
                    "old_start": 1443,
                    "old_length": 6,
                    "new_start": 1443,
                    "new_length": 7,
                    "hunk": "@@ -1443,6 +1443,7 @@ py_test(\n     name = \"ragged_tensor_supported_values_test\",\n     srcs = [\"ragged_tensor_supported_values_test.py\"],\n     python_version = \"PY3\",\n+    shard_count = 10,\n     srcs_version = \"PY3\",\n     deps = [\n         \":ragged_factory_ops\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    shard_count = 10,\n",
            "whole_hunk": "@@ -1443,6 +1443,7 @@ py_test(\n     name = \"ragged_tensor_supported_values_test\",\n     srcs = [\"ragged_tensor_supported_values_test.py\"],\n     python_version = \"PY3\",\n+    shard_count = 10,\n     srcs_version = \"PY3\",\n     deps = [\n         \":ragged_factory_ops\",\n"
        },
        {
            "name": "ragged_getitem.py",
            "path": "tensorflow/python/ops/ragged/ragged_getitem.py",
            "patches": [
                {
                    "old_start": 246,
                    "old_length": 7,
                    "new_start": 246,
                    "new_length": 7,
                    "hunk": "@@ -246,7 +246,7 @@ def _ragged_getitem_inner_dimensions(rt_input, key_list):\n   if not key_list:\n     return rt_input\n \n-  if isinstance(rt_input, ops.Tensor):\n+  if not isinstance(rt_input, ragged_tensor.RaggedTensor):\n     return rt_input.__getitem__([slice(None, None, None)] + key_list)\n \n   column_key = key_list[0]\n"
                }
            ],
            "whole_deleted": "-  if isinstance(rt_input, ops.Tensor):\n",
            "whole_added": "+  if not isinstance(rt_input, ragged_tensor.RaggedTensor):\n",
            "whole_hunk": "@@ -246,7 +246,7 @@ def _ragged_getitem_inner_dimensions(rt_input, key_list):\n   if not key_list:\n     return rt_input\n \n-  if isinstance(rt_input, ops.Tensor):\n+  if not isinstance(rt_input, ragged_tensor.RaggedTensor):\n     return rt_input.__getitem__([slice(None, None, None)] + key_list)\n \n   column_key = key_list[0]\n"
        },
        {
            "name": "ragged_tensor_supported_values_test.py",
            "path": "tensorflow/python/ops/ragged/ragged_tensor_supported_values_test.py",
            "patches": [
                {
                    "old_start": 295,
                    "old_length": 6,
                    "new_start": 295,
                    "new_length": 36,
                    "hunk": "@@ -295,6 +295,36 @@ class RaggedTensorSupportedValuesTest(test_util.TensorFlowTestCase,\n     self.assertAllTensorsEqual(wrapped_res.nested_row_splits,\n                                res.nested_row_splits)\n \n+  @parameterized.parameters(\n+      (lambda x: x[1:], True),\n+      (lambda x: x[0], False),\n+      (lambda x: x[:], True),\n+      (lambda x: x[0, :], False),\n+      (lambda x: x[...], True),\n+      (lambda x: x[1:2, ...], True),\n+      (lambda x: x[..., 1:2], True),\n+      (lambda x: x[0:2, ::2], True),\n+  )\n+  def testSlicing(self, slice_fn, is_ragged_output):\n+    tensor_values = constant_op.constant([[1.0, 2], [3, 4], [5, 6], [7, 8]])\n+    row_splits = constant_op.constant([0, 2, 3, 4], dtypes.int32)\n+    raw_rt = RaggedTensor.from_row_splits(tensor_values, row_splits)\n+\n+    values = WrappedTensor(tensor_values)\n+    rt = RaggedTensor.from_row_splits(values, row_splits)\n+\n+    res = slice_fn(rt)\n+    raw_res = slice_fn(raw_rt)\n+    if is_ragged_output:\n+      self.assertIsInstance(res, RaggedTensor)\n+      self.assertIsInstance(res.flat_values, WrappedTensor)\n+      self.assertAllEqual(res.flat_values.value, raw_res.flat_values)\n+      self.assertAllTensorsEqual(res.nested_row_splits,\n+                                 raw_res.nested_row_splits)\n+    else:\n+      self.assertIsInstance(res, WrappedTensor)\n+      self.assertAllEqual(res.value, raw_res)\n+\n \n @test_util.run_all_in_graph_and_eager_modes\n class RaggedTensorSpecSupportedValuesTest(test_util.TensorFlowTestCase,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  @parameterized.parameters(\n+      (lambda x: x[1:], True),\n+      (lambda x: x[0], False),\n+      (lambda x: x[:], True),\n+      (lambda x: x[0, :], False),\n+      (lambda x: x[...], True),\n+      (lambda x: x[1:2, ...], True),\n+      (lambda x: x[..., 1:2], True),\n+      (lambda x: x[0:2, ::2], True),\n+  )\n+  def testSlicing(self, slice_fn, is_ragged_output):\n+    tensor_values = constant_op.constant([[1.0, 2], [3, 4], [5, 6], [7, 8]])\n+    row_splits = constant_op.constant([0, 2, 3, 4], dtypes.int32)\n+    raw_rt = RaggedTensor.from_row_splits(tensor_values, row_splits)\n+\n+    values = WrappedTensor(tensor_values)\n+    rt = RaggedTensor.from_row_splits(values, row_splits)\n+\n+    res = slice_fn(rt)\n+    raw_res = slice_fn(raw_rt)\n+    if is_ragged_output:\n+      self.assertIsInstance(res, RaggedTensor)\n+      self.assertIsInstance(res.flat_values, WrappedTensor)\n+      self.assertAllEqual(res.flat_values.value, raw_res.flat_values)\n+      self.assertAllTensorsEqual(res.nested_row_splits,\n+                                 raw_res.nested_row_splits)\n+    else:\n+      self.assertIsInstance(res, WrappedTensor)\n+      self.assertAllEqual(res.value, raw_res)\n+\n",
            "whole_hunk": "@@ -295,6 +295,36 @@ class RaggedTensorSupportedValuesTest(test_util.TensorFlowTestCase,\n     self.assertAllTensorsEqual(wrapped_res.nested_row_splits,\n                                res.nested_row_splits)\n \n+  @parameterized.parameters(\n+      (lambda x: x[1:], True),\n+      (lambda x: x[0], False),\n+      (lambda x: x[:], True),\n+      (lambda x: x[0, :], False),\n+      (lambda x: x[...], True),\n+      (lambda x: x[1:2, ...], True),\n+      (lambda x: x[..., 1:2], True),\n+      (lambda x: x[0:2, ::2], True),\n+  )\n+  def testSlicing(self, slice_fn, is_ragged_output):\n+    tensor_values = constant_op.constant([[1.0, 2], [3, 4], [5, 6], [7, 8]])\n+    row_splits = constant_op.constant([0, 2, 3, 4], dtypes.int32)\n+    raw_rt = RaggedTensor.from_row_splits(tensor_values, row_splits)\n+\n+    values = WrappedTensor(tensor_values)\n+    rt = RaggedTensor.from_row_splits(values, row_splits)\n+\n+    res = slice_fn(rt)\n+    raw_res = slice_fn(raw_rt)\n+    if is_ragged_output:\n+      self.assertIsInstance(res, RaggedTensor)\n+      self.assertIsInstance(res.flat_values, WrappedTensor)\n+      self.assertAllEqual(res.flat_values.value, raw_res.flat_values)\n+      self.assertAllTensorsEqual(res.nested_row_splits,\n+                                 raw_res.nested_row_splits)\n+    else:\n+      self.assertIsInstance(res, WrappedTensor)\n+      self.assertAllEqual(res.value, raw_res)\n+\n \n @test_util.run_all_in_graph_and_eager_modes\n class RaggedTensorSpecSupportedValuesTest(test_util.TensorFlowTestCase,"
        }
    ]
},
{
    "Id": 514,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2547314a01549add2dde112279badee2c3c860ae",
    "date": "2023-02-27T07:46:28-08:00",
    "message": "Add NumElements == 1 check for each input_mins/maxes tensors for QuantizedConcatOp.\nReplace silent OP_REQUIRES in void utility functions to return error Status.\n\nPiperOrigin-RevId: 512623798",
    "label": "NO",
    "changes": [
        {
            "name": "quantized_concat_op.cc",
            "path": "tensorflow/core/kernels/quantized_concat_op.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 8,
                    "hunk": "@@ -15,6 +15,8 @@ limitations under the License.\n \n #define EIGEN_USE_THREADS\n \n+#include <limits>\n+#include <utility>\n #include <vector>\n \n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n"
                },
                {
                    "old_start": 24,
                    "old_length": 6,
                    "new_start": 26,
                    "new_length": 7,
                    "hunk": "@@ -24,6 +26,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/concat_lib_cpu.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n+#include \"tensorflow/core/platform/status.h\"\n \n namespace tensorflow {\n \n"
                },
                {
                    "old_start": 75,
                    "old_length": 7,
                    "new_start": 78,
                    "new_length": 7,
                    "hunk": "@@ -75,7 +78,7 @@ class QuantizedConcatOp : public OpKernel {\n \n   explicit QuantizedConcatOp(OpKernelConstruction* c) : OpKernel(c) {}\n \n-  void CalculateInputAndOutputRange(\n+  Status CalculateInputAndOutputRange(\n       const OpInputList& input_mins, const OpInputList& input_maxes,\n       const size_t N,\n       std::vector<std::pair<float, float>>* input_mins_and_maxes,\n"
                },
                {
                    "old_start": 84,
                    "old_length": 6,
                    "new_start": 87,
                    "new_length": 18,
                    "hunk": "@@ -84,6 +87,18 @@ class QuantizedConcatOp : public OpKernel {\n     float overall_min = std::numeric_limits<float>::max();\n     float overall_max = std::numeric_limits<float>::lowest();\n     for (int i = 0; i < N; ++i) {\n+      if (input_mins[i].NumElements() != 1) {\n+        return errors::InvalidArgument(\n+            \"input_mins each tensor's num elements must be 1, given num \"\n+            \"elements \",\n+            input_mins[i].NumElements(), \" in index \", i);\n+      }\n+      if (input_maxes[i].NumElements() != 1) {\n+        return errors::InvalidArgument(\n+            \"input_maxes each tensor's num elements must be 1, given num \"\n+            \"elements \",\n+            input_maxes[i].NumElements(), \" in index \", i);\n+      }\n       const float input_min = input_mins[i].flat<float>()(0);\n       const float input_max = input_maxes[i].flat<float>()(0);\n       input_mins_and_maxes->emplace_back(input_min, input_max);\n"
                },
                {
                    "old_start": 103,
                    "old_length": 6,
                    "new_start": 118,
                    "new_length": 7,
                    "hunk": "@@ -103,6 +118,7 @@ class QuantizedConcatOp : public OpKernel {\n       *output_min = overall_min;\n       *output_max = overall_max;\n     }\n+    return OkStatus();\n   }\n \n   int64_t CalculateInputsDim(const TensorShape& input_shape,\n"
                },
                {
                    "old_start": 114,
                    "old_length": 12,
                    "new_start": 130,
                    "new_length": 12,
                    "hunk": "@@ -114,12 +130,12 @@ class QuantizedConcatOp : public OpKernel {\n     return inputs_flat_dim0;\n   }\n \n-  void CalculateConcatDims(const size_t N, const TensorShape& input_shape,\n-                           int input_dims, const OpInputList& values,\n-                           OpKernelContext* context, const int32_t concat_dim,\n-                           const int64_t inputs_flat_dim0,\n-                           ConstMatrixVector* inputs_flat,\n-                           int* output_concat_dim) {\n+  Status CalculateConcatDims(const size_t N, const TensorShape& input_shape,\n+                             int input_dims, const OpInputList& values,\n+                             const int32_t concat_dim,\n+                             const int64_t inputs_flat_dim0,\n+                             ConstMatrixVector* inputs_flat,\n+                             int* output_concat_dim) {\n     // Note that we reduce the concat of n-dimensional tensors into a two\n     // dimensional concat. Assuming the dimensions of any input/output\n     // tensor are {x0, x1,...,xn-1, y0, y1,...,ym-1}, where the concat is along\n"
                },
                {
                    "old_start": 131,
                    "old_length": 22,
                    "new_start": 147,
                    "new_length": 22,
                    "hunk": "@@ -131,22 +147,22 @@ class QuantizedConcatOp : public OpKernel {\n     for (int i = 0; i < N; ++i) {\n       const auto in = values[i];\n       const bool in_is_scalar = TensorShapeUtils::IsScalar(in.shape());\n-      OP_REQUIRES(\n-          context, in.dims() == input_dims || (input_is_scalar && in_is_scalar),\n-          errors::InvalidArgument(\n-              \"ConcatOp : Ranks of all input tensors should match: shape[0] = \",\n-              input_shape.DebugString(), \" vs. shape[\", i,\n-              \"] = \", in.shape().DebugString()));\n+      if (!(in.dims() == input_dims || (input_is_scalar && in_is_scalar))) {\n+        return errors::InvalidArgument(\n+            \"ConcatOp : Ranks of all input tensors should match: shape[0] = \",\n+            input_shape.DebugString(), \" vs. shape[\", i,\n+            \"] = \", in.shape().DebugString());\n+      }\n       for (int j = 0; j < input_dims; ++j) {\n         if (j == concat_dim) {\n           continue;\n         }\n-        OP_REQUIRES(\n-            context, in.dim_size(j) == input_shape.dim_size(j),\n-            errors::InvalidArgument(\n-                \"ConcatOp : Dimensions of inputs should match: shape[0] = \",\n-                input_shape.DebugString(), \" vs. shape[\", i,\n-                \"] = \", in.shape().DebugString()));\n+        if (in.dim_size(j) != input_shape.dim_size(j)) {\n+          return errors::InvalidArgument(\n+              \"ConcatOp : Dimensions of inputs should match: shape[0] = \",\n+              input_shape.DebugString(), \" vs. shape[\", i,\n+              \"] = \", in.shape().DebugString());\n+        }\n       }\n       if (in.NumElements() > 0) {\n         int64_t inputs_flat_dim1 = in.NumElements() / inputs_flat_dim0;\n"
                },
                {
                    "old_start": 155,
                    "old_length": 6,
                    "new_start": 171,
                    "new_length": 7,
                    "hunk": "@@ -155,6 +171,7 @@ class QuantizedConcatOp : public OpKernel {\n       }\n       *output_concat_dim += in.dims() > 0 ? in.dim_size(concat_dim) : 1;\n     }\n+    return OkStatus();\n   }\n \n   void Compute(OpKernelContext* context) override {\n"
                },
                {
                    "old_start": 192,
                    "old_length": 15,
                    "new_start": 209,
                    "new_length": 18,
                    "hunk": "@@ -192,15 +209,18 @@ class QuantizedConcatOp : public OpKernel {\n     float output_min = std::numeric_limits<float>::max();\n     float output_max = std::numeric_limits<float>::lowest();\n     std::vector<std::pair<float, float>> input_mins_and_maxes;\n-    CalculateInputAndOutputRange(input_mins, input_maxes, N,\n-                                 &input_mins_and_maxes, &output_min,\n-                                 &output_max);\n+    OP_REQUIRES_OK(context,\n+                   CalculateInputAndOutputRange(input_mins, input_maxes, N,\n+                                                &input_mins_and_maxes,\n+                                                &output_min, &output_max));\n     const int64_t inputs_flat_dim0 =\n         CalculateInputsDim(input_shape, concat_dim);\n     ConstMatrixVector inputs_flat;\n     int output_concat_dim;\n-    CalculateConcatDims(N, input_shape, input_dims, values, context, concat_dim,\n-                        inputs_flat_dim0, &inputs_flat, &output_concat_dim);\n+    OP_REQUIRES_OK(\n+        context, CalculateConcatDims(N, input_shape, input_dims, values,\n+                                     concat_dim, inputs_flat_dim0, &inputs_flat,\n+                                     &output_concat_dim));\n \n     TensorShape output_shape(input_shape);\n     // TODO(irving): Remove rank 0 case once !kAllowLegacyScalars\n"
                }
            ],
            "whole_deleted": "-  void CalculateInputAndOutputRange(\n-  void CalculateConcatDims(const size_t N, const TensorShape& input_shape,\n-                           int input_dims, const OpInputList& values,\n-                           OpKernelContext* context, const int32_t concat_dim,\n-                           const int64_t inputs_flat_dim0,\n-                           ConstMatrixVector* inputs_flat,\n-                           int* output_concat_dim) {\n-      OP_REQUIRES(\n-          context, in.dims() == input_dims || (input_is_scalar && in_is_scalar),\n-          errors::InvalidArgument(\n-              \"ConcatOp : Ranks of all input tensors should match: shape[0] = \",\n-              input_shape.DebugString(), \" vs. shape[\", i,\n-              \"] = \", in.shape().DebugString()));\n-        OP_REQUIRES(\n-            context, in.dim_size(j) == input_shape.dim_size(j),\n-            errors::InvalidArgument(\n-                \"ConcatOp : Dimensions of inputs should match: shape[0] = \",\n-                input_shape.DebugString(), \" vs. shape[\", i,\n-                \"] = \", in.shape().DebugString()));\n-    CalculateInputAndOutputRange(input_mins, input_maxes, N,\n-                                 &input_mins_and_maxes, &output_min,\n-                                 &output_max);\n-    CalculateConcatDims(N, input_shape, input_dims, values, context, concat_dim,\n-                        inputs_flat_dim0, &inputs_flat, &output_concat_dim);\n",
            "whole_added": "+#include <limits>\n+#include <utility>\n+#include \"tensorflow/core/platform/status.h\"\n+  Status CalculateInputAndOutputRange(\n+      if (input_mins[i].NumElements() != 1) {\n+        return errors::InvalidArgument(\n+            \"input_mins each tensor's num elements must be 1, given num \"\n+            \"elements \",\n+            input_mins[i].NumElements(), \" in index \", i);\n+      }\n+      if (input_maxes[i].NumElements() != 1) {\n+        return errors::InvalidArgument(\n+            \"input_maxes each tensor's num elements must be 1, given num \"\n+            \"elements \",\n+            input_maxes[i].NumElements(), \" in index \", i);\n+      }\n+    return OkStatus();\n+  Status CalculateConcatDims(const size_t N, const TensorShape& input_shape,\n+                             int input_dims, const OpInputList& values,\n+                             const int32_t concat_dim,\n+                             const int64_t inputs_flat_dim0,\n+                             ConstMatrixVector* inputs_flat,\n+                             int* output_concat_dim) {\n+      if (!(in.dims() == input_dims || (input_is_scalar && in_is_scalar))) {\n+        return errors::InvalidArgument(\n+            \"ConcatOp : Ranks of all input tensors should match: shape[0] = \",\n+            input_shape.DebugString(), \" vs. shape[\", i,\n+            \"] = \", in.shape().DebugString());\n+      }\n+        if (in.dim_size(j) != input_shape.dim_size(j)) {\n+          return errors::InvalidArgument(\n+              \"ConcatOp : Dimensions of inputs should match: shape[0] = \",\n+              input_shape.DebugString(), \" vs. shape[\", i,\n+              \"] = \", in.shape().DebugString());\n+        }\n+    return OkStatus();\n+    OP_REQUIRES_OK(context,\n+                   CalculateInputAndOutputRange(input_mins, input_maxes, N,\n+                                                &input_mins_and_maxes,\n+                                                &output_min, &output_max));\n+    OP_REQUIRES_OK(\n+        context, CalculateConcatDims(N, input_shape, input_dims, values,\n+                                     concat_dim, inputs_flat_dim0, &inputs_flat,\n+                                     &output_concat_dim));\n",
            "whole_hunk": "@@ -15,6 +15,8 @@ limitations under the License.\n \n #define EIGEN_USE_THREADS\n \n+#include <limits>\n+#include <utility>\n #include <vector>\n \n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n@@ -24,6 +26,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/concat_lib_cpu.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n+#include \"tensorflow/core/platform/status.h\"\n \n namespace tensorflow {\n \n@@ -75,7 +78,7 @@ class QuantizedConcatOp : public OpKernel {\n \n   explicit QuantizedConcatOp(OpKernelConstruction* c) : OpKernel(c) {}\n \n-  void CalculateInputAndOutputRange(\n+  Status CalculateInputAndOutputRange(\n       const OpInputList& input_mins, const OpInputList& input_maxes,\n       const size_t N,\n       std::vector<std::pair<float, float>>* input_mins_and_maxes,\n@@ -84,6 +87,18 @@ class QuantizedConcatOp : public OpKernel {\n     float overall_min = std::numeric_limits<float>::max();\n     float overall_max = std::numeric_limits<float>::lowest();\n     for (int i = 0; i < N; ++i) {\n+      if (input_mins[i].NumElements() != 1) {\n+        return errors::InvalidArgument(\n+            \"input_mins each tensor's num elements must be 1, given num \"\n+            \"elements \",\n+            input_mins[i].NumElements(), \" in index \", i);\n+      }\n+      if (input_maxes[i].NumElements() != 1) {\n+        return errors::InvalidArgument(\n+            \"input_maxes each tensor's num elements must be 1, given num \"\n+            \"elements \",\n+            input_maxes[i].NumElements(), \" in index \", i);\n+      }\n       const float input_min = input_mins[i].flat<float>()(0);\n       const float input_max = input_maxes[i].flat<float>()(0);\n       input_mins_and_maxes->emplace_back(input_min, input_max);\n@@ -103,6 +118,7 @@ class QuantizedConcatOp : public OpKernel {\n       *output_min = overall_min;\n       *output_max = overall_max;\n     }\n+    return OkStatus();\n   }\n \n   int64_t CalculateInputsDim(const TensorShape& input_shape,\n@@ -114,12 +130,12 @@ class QuantizedConcatOp : public OpKernel {\n     return inputs_flat_dim0;\n   }\n \n-  void CalculateConcatDims(const size_t N, const TensorShape& input_shape,\n-                           int input_dims, const OpInputList& values,\n-                           OpKernelContext* context, const int32_t concat_dim,\n-                           const int64_t inputs_flat_dim0,\n-                           ConstMatrixVector* inputs_flat,\n-                           int* output_concat_dim) {\n+  Status CalculateConcatDims(const size_t N, const TensorShape& input_shape,\n+                             int input_dims, const OpInputList& values,\n+                             const int32_t concat_dim,\n+                             const int64_t inputs_flat_dim0,\n+                             ConstMatrixVector* inputs_flat,\n+                             int* output_concat_dim) {\n     // Note that we reduce the concat of n-dimensional tensors into a two\n     // dimensional concat. Assuming the dimensions of any input/output\n     // tensor are {x0, x1,...,xn-1, y0, y1,...,ym-1}, where the concat is along\n@@ -131,22 +147,22 @@ class QuantizedConcatOp : public OpKernel {\n     for (int i = 0; i < N; ++i) {\n       const auto in = values[i];\n       const bool in_is_scalar = TensorShapeUtils::IsScalar(in.shape());\n-      OP_REQUIRES(\n-          context, in.dims() == input_dims || (input_is_scalar && in_is_scalar),\n-          errors::InvalidArgument(\n-              \"ConcatOp : Ranks of all input tensors should match: shape[0] = \",\n-              input_shape.DebugString(), \" vs. shape[\", i,\n-              \"] = \", in.shape().DebugString()));\n+      if (!(in.dims() == input_dims || (input_is_scalar && in_is_scalar))) {\n+        return errors::InvalidArgument(\n+            \"ConcatOp : Ranks of all input tensors should match: shape[0] = \",\n+            input_shape.DebugString(), \" vs. shape[\", i,\n+            \"] = \", in.shape().DebugString());\n+      }\n       for (int j = 0; j < input_dims; ++j) {\n         if (j == concat_dim) {\n           continue;\n         }\n-        OP_REQUIRES(\n-            context, in.dim_size(j) == input_shape.dim_size(j),\n-            errors::InvalidArgument(\n-                \"ConcatOp : Dimensions of inputs should match: shape[0] = \",\n-                input_shape.DebugString(), \" vs. shape[\", i,\n-                \"] = \", in.shape().DebugString()));\n+        if (in.dim_size(j) != input_shape.dim_size(j)) {\n+          return errors::InvalidArgument(\n+              \"ConcatOp : Dimensions of inputs should match: shape[0] = \",\n+              input_shape.DebugString(), \" vs. shape[\", i,\n+              \"] = \", in.shape().DebugString());\n+        }\n       }\n       if (in.NumElements() > 0) {\n         int64_t inputs_flat_dim1 = in.NumElements() / inputs_flat_dim0;\n@@ -155,6 +171,7 @@ class QuantizedConcatOp : public OpKernel {\n       }\n       *output_concat_dim += in.dims() > 0 ? in.dim_size(concat_dim) : 1;\n     }\n+    return OkStatus();\n   }\n \n   void Compute(OpKernelContext* context) override {\n@@ -192,15 +209,18 @@ class QuantizedConcatOp : public OpKernel {\n     float output_min = std::numeric_limits<float>::max();\n     float output_max = std::numeric_limits<float>::lowest();\n     std::vector<std::pair<float, float>> input_mins_and_maxes;\n-    CalculateInputAndOutputRange(input_mins, input_maxes, N,\n-                                 &input_mins_and_maxes, &output_min,\n-                                 &output_max);\n+    OP_REQUIRES_OK(context,\n+                   CalculateInputAndOutputRange(input_mins, input_maxes, N,\n+                                                &input_mins_and_maxes,\n+                                                &output_min, &output_max));\n     const int64_t inputs_flat_dim0 =\n         CalculateInputsDim(input_shape, concat_dim);\n     ConstMatrixVector inputs_flat;\n     int output_concat_dim;\n-    CalculateConcatDims(N, input_shape, input_dims, values, context, concat_dim,\n-                        inputs_flat_dim0, &inputs_flat, &output_concat_dim);\n+    OP_REQUIRES_OK(\n+        context, CalculateConcatDims(N, input_shape, input_dims, values,\n+                                     concat_dim, inputs_flat_dim0, &inputs_flat,\n+                                     &output_concat_dim));\n \n     TensorShape output_shape(input_shape);\n     // TODO(irving): Remove rank 0 case once !kAllowLegacyScalars\n"
        },
        {
            "name": "quantized_concat_op_test.cc",
            "path": "tensorflow/core/kernels/quantized_concat_op_test.cc",
            "patches": [
                {
                    "old_start": 34,
                    "old_length": 6,
                    "new_start": 34,
                    "new_length": 7,
                    "hunk": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n #include \"tensorflow/core/lib/core/status.h\"\n #include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/test.h\"\n #include \"tensorflow/core/platform/test_benchmark.h\"\n \n"
                },
                {
                    "old_start": 51,
                    "old_length": 8,
                    "new_start": 52,
                    "new_length": 54,
                    "hunk": "@@ -51,8 +52,54 @@ class QuantizedConcatTest : public OpsTestBase {\n                       float second_max);\n   void TestSecondDim8Bit(float first_min, float first_max, float second_min,\n                          float second_max);\n+  void TestInvalidMinMax(const Tensor& first_min, const Tensor& first_max);\n };\n \n+TEST_F(QuantizedConcatTest, InvalidMin) {\n+  // first_min NumElements == 3.\n+  Tensor first_min(DT_FLOAT, {3});\n+  test::FillValues<float>(&first_min, {0.0, 0.0, 0.0});\n+  Tensor first_max(DT_FLOAT, {});\n+  test::FillValues<float>(&first_max, {0.0});\n+  TestInvalidMinMax(first_min, first_max);\n+}\n+\n+TEST_F(QuantizedConcatTest, InvalidMax) {\n+  Tensor first_min(DT_FLOAT, {});\n+  test::FillValues<float>(&first_min, {0.0});\n+  // first_max NumElements == 0.\n+  Tensor first_max(DT_FLOAT, {3, 0, 2});\n+  TestInvalidMinMax(first_min, first_max);\n+}\n+\n+void QuantizedConcatTest::TestInvalidMinMax(const Tensor& first_min,\n+                                            const Tensor& first_max) {\n+  TF_ASSERT_OK(NodeDefBuilder(\"quantized_concat_op\", \"QuantizedConcat\")\n+                   .Input(FakeInput(DT_INT32))\n+                   .Input(FakeInput(2, DT_QUINT8))\n+                   .Input(FakeInput(2, DT_FLOAT))\n+                   .Input(FakeInput(2, DT_FLOAT))\n+                   .Attr(\"N\", 2)\n+                   .Attr(\"T\", DataTypeToEnum<quint8>::v())\n+                   .Finalize(node_def()));\n+  TF_ASSERT_OK(InitOp());\n+  Tensor first_quantized(DT_QUINT8, {1});\n+  test::FillValues<quint8>(&first_quantized, {1});\n+  Tensor second_quantized(DT_QUINT8, {1});\n+  test::FillValues<quint8>(&second_quantized, {1});\n+\n+  AddInputFromArray<int32>(TensorShape({}), {0});\n+  AddInputFromArray<quint8>(first_quantized.shape(),\n+                            first_quantized.flat<quint8>());\n+  AddInputFromArray<quint8>(second_quantized.shape(),\n+                            second_quantized.flat<quint8>());\n+  AddInputFromArray<float>(first_min.shape(), first_min.flat<float>());\n+  AddInputFromArray<float>(TensorShape({}), {1.0});\n+  AddInputFromArray<float>(first_max.shape(), first_max.flat<float>());\n+  AddInputFromArray<float>(TensorShape({}), {2.0});\n+  EXPECT_TRUE(errors::IsInvalidArgument(RunOpKernel()));\n+}\n+\n TEST_F(QuantizedConcatTest, Small8Bit) {\n   TestSmall8Bit(0.0f, 255.0f, 0.0f, 25.0f);\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"tensorflow/core/platform/errors.h\"\n+  void TestInvalidMinMax(const Tensor& first_min, const Tensor& first_max);\n+TEST_F(QuantizedConcatTest, InvalidMin) {\n+  // first_min NumElements == 3.\n+  Tensor first_min(DT_FLOAT, {3});\n+  test::FillValues<float>(&first_min, {0.0, 0.0, 0.0});\n+  Tensor first_max(DT_FLOAT, {});\n+  test::FillValues<float>(&first_max, {0.0});\n+  TestInvalidMinMax(first_min, first_max);\n+}\n+\n+TEST_F(QuantizedConcatTest, InvalidMax) {\n+  Tensor first_min(DT_FLOAT, {});\n+  test::FillValues<float>(&first_min, {0.0});\n+  // first_max NumElements == 0.\n+  Tensor first_max(DT_FLOAT, {3, 0, 2});\n+  TestInvalidMinMax(first_min, first_max);\n+}\n+\n+void QuantizedConcatTest::TestInvalidMinMax(const Tensor& first_min,\n+                                            const Tensor& first_max) {\n+  TF_ASSERT_OK(NodeDefBuilder(\"quantized_concat_op\", \"QuantizedConcat\")\n+                   .Input(FakeInput(DT_INT32))\n+                   .Input(FakeInput(2, DT_QUINT8))\n+                   .Input(FakeInput(2, DT_FLOAT))\n+                   .Input(FakeInput(2, DT_FLOAT))\n+                   .Attr(\"N\", 2)\n+                   .Attr(\"T\", DataTypeToEnum<quint8>::v())\n+                   .Finalize(node_def()));\n+  TF_ASSERT_OK(InitOp());\n+  Tensor first_quantized(DT_QUINT8, {1});\n+  test::FillValues<quint8>(&first_quantized, {1});\n+  Tensor second_quantized(DT_QUINT8, {1});\n+  test::FillValues<quint8>(&second_quantized, {1});\n+\n+  AddInputFromArray<int32>(TensorShape({}), {0});\n+  AddInputFromArray<quint8>(first_quantized.shape(),\n+                            first_quantized.flat<quint8>());\n+  AddInputFromArray<quint8>(second_quantized.shape(),\n+                            second_quantized.flat<quint8>());\n+  AddInputFromArray<float>(first_min.shape(), first_min.flat<float>());\n+  AddInputFromArray<float>(TensorShape({}), {1.0});\n+  AddInputFromArray<float>(first_max.shape(), first_max.flat<float>());\n+  AddInputFromArray<float>(TensorShape({}), {2.0});\n+  EXPECT_TRUE(errors::IsInvalidArgument(RunOpKernel()));\n+}\n+\n",
            "whole_hunk": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n #include \"tensorflow/core/lib/core/status.h\"\n #include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/test.h\"\n #include \"tensorflow/core/platform/test_benchmark.h\"\n \n@@ -51,8 +52,54 @@ class QuantizedConcatTest : public OpsTestBase {\n                       float second_max);\n   void TestSecondDim8Bit(float first_min, float first_max, float second_min,\n                          float second_max);\n+  void TestInvalidMinMax(const Tensor& first_min, const Tensor& first_max);\n };\n \n+TEST_F(QuantizedConcatTest, InvalidMin) {\n+  // first_min NumElements == 3.\n+  Tensor first_min(DT_FLOAT, {3});\n+  test::FillValues<float>(&first_min, {0.0, 0.0, 0.0});\n+  Tensor first_max(DT_FLOAT, {});\n+  test::FillValues<float>(&first_max, {0.0});\n+  TestInvalidMinMax(first_min, first_max);\n+}\n+\n+TEST_F(QuantizedConcatTest, InvalidMax) {\n+  Tensor first_min(DT_FLOAT, {});\n+  test::FillValues<float>(&first_min, {0.0});\n+  // first_max NumElements == 0.\n+  Tensor first_max(DT_FLOAT, {3, 0, 2});\n+  TestInvalidMinMax(first_min, first_max);\n+}\n+\n+void QuantizedConcatTest::TestInvalidMinMax(const Tensor& first_min,\n+                                            const Tensor& first_max) {\n+  TF_ASSERT_OK(NodeDefBuilder(\"quantized_concat_op\", \"QuantizedConcat\")\n+                   .Input(FakeInput(DT_INT32))\n+                   .Input(FakeInput(2, DT_QUINT8))\n+                   .Input(FakeInput(2, DT_FLOAT))\n+                   .Input(FakeInput(2, DT_FLOAT))\n+                   .Attr(\"N\", 2)\n+                   .Attr(\"T\", DataTypeToEnum<quint8>::v())\n+                   .Finalize(node_def()));\n+  TF_ASSERT_OK(InitOp());\n+  Tensor first_quantized(DT_QUINT8, {1});\n+  test::FillValues<quint8>(&first_quantized, {1});\n+  Tensor second_quantized(DT_QUINT8, {1});\n+  test::FillValues<quint8>(&second_quantized, {1});\n+\n+  AddInputFromArray<int32>(TensorShape({}), {0});\n+  AddInputFromArray<quint8>(first_quantized.shape(),\n+                            first_quantized.flat<quint8>());\n+  AddInputFromArray<quint8>(second_quantized.shape(),\n+                            second_quantized.flat<quint8>());\n+  AddInputFromArray<float>(first_min.shape(), first_min.flat<float>());\n+  AddInputFromArray<float>(TensorShape({}), {1.0});\n+  AddInputFromArray<float>(first_max.shape(), first_max.flat<float>());\n+  AddInputFromArray<float>(TensorShape({}), {2.0});\n+  EXPECT_TRUE(errors::IsInvalidArgument(RunOpKernel()));\n+}\n+\n TEST_F(QuantizedConcatTest, Small8Bit) {\n   TestSmall8Bit(0.0f, 255.0f, 0.0f, 25.0f);\n }"
        }
    ]
},
{
    "Id": 328,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/00517642a356c5e04f009ea61c74638d89746392",
    "date": "2023-08-09T06:07:32-07:00",
    "message": "Return error on invalid input in `tfl.splitv`\n\nPiperOrigin-RevId: 555138718",
    "label": "YES",
    "changes": [
        {
            "name": "split_v.cc",
            "path": "tensorflow/lite/kernels/split_v.cc",
            "patches": [
                {
                    "old_start": 106,
                    "old_length": 6,
                    "new_start": 106,
                    "new_length": 7,
                    "hunk": "@@ -106,6 +106,7 @@ TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n       TF_LITE_KERNEL_LOG(\n           context,\n           \"The sum of size_splits must be less than the dimension of value.\");\n+      return kTfLiteError;\n     } else {\n       size_splits_vector[minus_one_index] = input_size - size_splits_sum;\n     }\n"
                },
                {
                    "old_start": 113,
                    "old_length": 6,
                    "new_start": 114,
                    "new_length": 7,
                    "hunk": "@@ -113,6 +114,7 @@ TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n     TF_LITE_KERNEL_LOG(\n         context,\n         \"The size_splits must sum to the dimension of value along axis.\");\n+    return kTfLiteError;\n   }\n \n   for (int i = 0; i < NumOutputs(node); ++i) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      return kTfLiteError;\n+    return kTfLiteError;\n",
            "whole_hunk": "@@ -106,6 +106,7 @@ TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n       TF_LITE_KERNEL_LOG(\n           context,\n           \"The sum of size_splits must be less than the dimension of value.\");\n+      return kTfLiteError;\n     } else {\n       size_splits_vector[minus_one_index] = input_size - size_splits_sum;\n     }\n@@ -113,6 +114,7 @@ TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n     TF_LITE_KERNEL_LOG(\n         context,\n         \"The size_splits must sum to the dimension of value along axis.\");\n+    return kTfLiteError;\n   }\n \n   for (int i = 0; i < NumOutputs(node); ++i) {"
        }
    ]
},
{
    "Id": 581,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/232fffd51f6a8ac6b7ce14944d264827155adcea",
    "date": "2023-01-20T10:22:05-08:00",
    "message": "Enhanced the future test by checking error strings.\n\nPiperOrigin-RevId: 503470419",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/python/ifrt/BUILD",
            "patches": [
                {
                    "old_start": 90,
                    "old_length": 6,
                    "new_start": 90,
                    "new_length": 7,
                    "hunk": "@@ -90,6 +90,7 @@ xla_cc_test(\n     deps = [\n         \":ifrt\",\n         \"//tensorflow/tsl/lib/core:status_test_util\",\n+        \"//tensorflow/tsl/platform:status_matchers\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/tsl/platform:status_matchers\",\n",
            "whole_hunk": "@@ -90,6 +90,7 @@ xla_cc_test(\n     deps = [\n         \":ifrt\",\n         \"//tensorflow/tsl/lib/core:status_test_util\",\n+        \"//tensorflow/tsl/platform:status_matchers\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n"
        },
        {
            "name": "future_test.cc",
            "path": "tensorflow/compiler/xla/python/ifrt/future_test.cc",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 11,
                    "new_start": 20,
                    "new_length": 15,
                    "hunk": "@@ -20,11 +20,15 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/types/span.h\"\n #include \"tensorflow/tsl/lib/core/status_test_util.h\"\n+#include \"tensorflow/tsl/platform/status_matchers.h\"\n \n namespace xla {\n namespace ifrt {\n namespace {\n \n+using ::testing::HasSubstr;\n+using ::tsl::testing::StatusIs;\n+\n TEST(FutureTest, JoinZeroFuture) {\n   Future<Status> future = JoinFutures({});\n \n"
                },
                {
                    "old_start": 51,
                    "old_length": 8,
                    "new_start": 55,
                    "new_length": 9,
                    "hunk": "@@ -51,8 +55,9 @@ TEST(FutureTest, JoinOneFailingFuture) {\n   Future<Status> future = JoinFutures(absl::MakeSpan(futures));\n \n   ASSERT_FALSE(future.IsReady());\n-  promise.Set(InvalidArgument(\"\"));\n-  EXPECT_EQ(future.Await().code(), tensorflow::error::INVALID_ARGUMENT);\n+  promise.Set(InvalidArgument(\"Some error\"));\n+  EXPECT_THAT(future.Await(), StatusIs(tensorflow::error::INVALID_ARGUMENT,\n+                                       HasSubstr(\"Some error\")));\n }\n \n TEST(FutureTest, JoinAllOkFutures) {\n"
                },
                {
                    "old_start": 90,
                    "old_length": 9,
                    "new_start": 95,
                    "new_length": 10,
                    "hunk": "@@ -90,9 +95,10 @@ TEST(FutureTest, JoinAllFailingFutures) {\n \n   ASSERT_FALSE(future.IsReady());\n   for (Promise<Status>& promise : promises) {\n-    promise.Set(InvalidArgument(\"\"));\n+    promise.Set(InvalidArgument(\"Some error\"));\n   }\n-  EXPECT_EQ(future.Await().code(), tensorflow::error::INVALID_ARGUMENT);\n+  EXPECT_THAT(future.Await(), StatusIs(tensorflow::error::INVALID_ARGUMENT,\n+                                       HasSubstr(\"Some error\")));\n }\n \n class JoinAllOkFuturesExceptForOneTest : public testing::TestWithParam<int> {};\n"
                },
                {
                    "old_start": 114,
                    "old_length": 12,
                    "new_start": 120,
                    "new_length": 13,
                    "hunk": "@@ -114,12 +120,13 @@ TEST_P(JoinAllOkFuturesExceptForOneTest, JoinAllOkFuturesExceptForOne) {\n   ASSERT_FALSE(future.IsReady());\n   for (int i = 0; i < kNumFutures; ++i) {\n     if (i == failing_future_idx) {\n-      promises[i].Set(InvalidArgument(\"\"));\n+      promises[i].Set(InvalidArgument(\"Some error\"));\n     } else {\n       promises[i].Set(OkStatus());\n     }\n   }\n-  EXPECT_EQ(future.Await().code(), tensorflow::error::INVALID_ARGUMENT);\n+  EXPECT_THAT(future.Await(), StatusIs(tensorflow::error::INVALID_ARGUMENT,\n+                                       HasSubstr(\"Some error\")));\n }\n \n INSTANTIATE_TEST_SUITE_P(FutureTest, JoinAllOkFuturesExceptForOneTest,"
                }
            ],
            "whole_deleted": "-  promise.Set(InvalidArgument(\"\"));\n-  EXPECT_EQ(future.Await().code(), tensorflow::error::INVALID_ARGUMENT);\n-    promise.Set(InvalidArgument(\"\"));\n-  EXPECT_EQ(future.Await().code(), tensorflow::error::INVALID_ARGUMENT);\n-      promises[i].Set(InvalidArgument(\"\"));\n-  EXPECT_EQ(future.Await().code(), tensorflow::error::INVALID_ARGUMENT);\n",
            "whole_added": "+#include \"tensorflow/tsl/platform/status_matchers.h\"\n+using ::testing::HasSubstr;\n+using ::tsl::testing::StatusIs;\n+\n+  promise.Set(InvalidArgument(\"Some error\"));\n+  EXPECT_THAT(future.Await(), StatusIs(tensorflow::error::INVALID_ARGUMENT,\n+                                       HasSubstr(\"Some error\")));\n+    promise.Set(InvalidArgument(\"Some error\"));\n+  EXPECT_THAT(future.Await(), StatusIs(tensorflow::error::INVALID_ARGUMENT,\n+                                       HasSubstr(\"Some error\")));\n+      promises[i].Set(InvalidArgument(\"Some error\"));\n+  EXPECT_THAT(future.Await(), StatusIs(tensorflow::error::INVALID_ARGUMENT,\n+                                       HasSubstr(\"Some error\")));\n",
            "whole_hunk": "@@ -20,11 +20,15 @@ limitations under the License.\n #include <gtest/gtest.h>\n #include \"absl/types/span.h\"\n #include \"tensorflow/tsl/lib/core/status_test_util.h\"\n+#include \"tensorflow/tsl/platform/status_matchers.h\"\n \n namespace xla {\n namespace ifrt {\n namespace {\n \n+using ::testing::HasSubstr;\n+using ::tsl::testing::StatusIs;\n+\n TEST(FutureTest, JoinZeroFuture) {\n   Future<Status> future = JoinFutures({});\n \n@@ -51,8 +55,9 @@ TEST(FutureTest, JoinOneFailingFuture) {\n   Future<Status> future = JoinFutures(absl::MakeSpan(futures));\n \n   ASSERT_FALSE(future.IsReady());\n-  promise.Set(InvalidArgument(\"\"));\n-  EXPECT_EQ(future.Await().code(), tensorflow::error::INVALID_ARGUMENT);\n+  promise.Set(InvalidArgument(\"Some error\"));\n+  EXPECT_THAT(future.Await(), StatusIs(tensorflow::error::INVALID_ARGUMENT,\n+                                       HasSubstr(\"Some error\")));\n }\n \n TEST(FutureTest, JoinAllOkFutures) {\n@@ -90,9 +95,10 @@ TEST(FutureTest, JoinAllFailingFutures) {\n \n   ASSERT_FALSE(future.IsReady());\n   for (Promise<Status>& promise : promises) {\n-    promise.Set(InvalidArgument(\"\"));\n+    promise.Set(InvalidArgument(\"Some error\"));\n   }\n-  EXPECT_EQ(future.Await().code(), tensorflow::error::INVALID_ARGUMENT);\n+  EXPECT_THAT(future.Await(), StatusIs(tensorflow::error::INVALID_ARGUMENT,\n+                                       HasSubstr(\"Some error\")));\n }\n \n class JoinAllOkFuturesExceptForOneTest : public testing::TestWithParam<int> {};\n@@ -114,12 +120,13 @@ TEST_P(JoinAllOkFuturesExceptForOneTest, JoinAllOkFuturesExceptForOne) {\n   ASSERT_FALSE(future.IsReady());\n   for (int i = 0; i < kNumFutures; ++i) {\n     if (i == failing_future_idx) {\n-      promises[i].Set(InvalidArgument(\"\"));\n+      promises[i].Set(InvalidArgument(\"Some error\"));\n     } else {\n       promises[i].Set(OkStatus());\n     }\n   }\n-  EXPECT_EQ(future.Await().code(), tensorflow::error::INVALID_ARGUMENT);\n+  EXPECT_THAT(future.Await(), StatusIs(tensorflow::error::INVALID_ARGUMENT,\n+                                       HasSubstr(\"Some error\")));\n }\n \n INSTANTIATE_TEST_SUITE_P(FutureTest, JoinAllOkFuturesExceptForOneTest,"
        }
    ]
},
{
    "Id": 653,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/25a7a3324ad566c5f3a10be506fb6d3dfddd482c",
    "date": "2022-10-20T08:34:04-07:00",
    "message": "Move `is_deleted()` to C++ so that we can check if an Array is deleted without materializing `_arrays`.\n\nAlso raise a better error message when doing operations of a deleted Array rather than the current thing which says: `NoneType has no len()`. Now it says: `Array has been deleted`.\n\nPiperOrigin-RevId: 482497114",
    "label": "YES",
    "changes": [
        {
            "name": "py_array.cc",
            "path": "tensorflow/compiler/xla/python/py_array.cc",
            "patches": [
                {
                    "old_start": 264,
                    "old_length": 6,
                    "new_start": 264,
                    "new_length": 17,
                    "hunk": "@@ -264,6 +264,17 @@ Status PyArray::BlockUntilReady() const {\n   return status;\n }\n \n+bool PyArray::IsDeleted() const {\n+  if (pjrt_buffers().empty()) {\n+    return true;\n+  }\n+\n+  for (const auto& pjrt_buffer : pjrt_buffers()) {\n+    if (pjrt_buffer->IsDeleted()) return true;\n+  }\n+  return false;\n+}\n+\n py::handle PyArray::Storage::AsHandle() {\n   return reinterpret_cast<PyObject*>(reinterpret_cast<char*>(this) -\n                                      offsetof(PyArrayObject, array_storage));\n"
                },
                {
                    "old_start": 373,
                    "old_length": 6,
                    "new_start": 384,
                    "new_length": 8,
                    "hunk": "@@ -373,6 +384,8 @@ Status PyArray::RegisterTypes(py::module& m) {\n         return self;\n       },\n       py::is_method(type));\n+  type.attr(\"is_deleted\") =\n+      py::cpp_function(&PyArray::IsDeleted, py::is_method(type));\n   type.attr(\"traceback\") = jax::property_readonly(&PyArray::traceback);\n   type.attr(\"__module__\") = m.attr(\"__name__\");\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+bool PyArray::IsDeleted() const {\n+  if (pjrt_buffers().empty()) {\n+    return true;\n+  }\n+\n+  for (const auto& pjrt_buffer : pjrt_buffers()) {\n+    if (pjrt_buffer->IsDeleted()) return true;\n+  }\n+  return false;\n+}\n+\n+  type.attr(\"is_deleted\") =\n+      py::cpp_function(&PyArray::IsDeleted, py::is_method(type));\n",
            "whole_hunk": "@@ -264,6 +264,17 @@ Status PyArray::BlockUntilReady() const {\n   return status;\n }\n \n+bool PyArray::IsDeleted() const {\n+  if (pjrt_buffers().empty()) {\n+    return true;\n+  }\n+\n+  for (const auto& pjrt_buffer : pjrt_buffers()) {\n+    if (pjrt_buffer->IsDeleted()) return true;\n+  }\n+  return false;\n+}\n+\n py::handle PyArray::Storage::AsHandle() {\n   return reinterpret_cast<PyObject*>(reinterpret_cast<char*>(this) -\n                                      offsetof(PyArrayObject, array_storage));\n@@ -373,6 +384,8 @@ Status PyArray::RegisterTypes(py::module& m) {\n         return self;\n       },\n       py::is_method(type));\n+  type.attr(\"is_deleted\") =\n+      py::cpp_function(&PyArray::IsDeleted, py::is_method(type));\n   type.attr(\"traceback\") = jax::property_readonly(&PyArray::traceback);\n   type.attr(\"__module__\") = m.attr(\"__name__\");\n \n"
        },
        {
            "name": "py_array.h",
            "path": "tensorflow/compiler/xla/python/py_array.h",
            "patches": [
                {
                    "old_start": 170,
                    "old_length": 6,
                    "new_start": 170,
                    "new_length": 8,
                    "hunk": "@@ -170,6 +170,8 @@ class PyArray : public pybind11::object {\n \n   Status BlockUntilReady() const;\n \n+  bool IsDeleted() const;\n+\n  private:\n   void CheckAndRearrange();\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  bool IsDeleted() const;\n+\n",
            "whole_hunk": "@@ -170,6 +170,8 @@ class PyArray : public pybind11::object {\n \n   Status BlockUntilReady() const;\n \n+  bool IsDeleted() const;\n+\n  private:\n   void CheckAndRearrange();\n \n"
        },
        {
            "name": "xla_client.py",
            "path": "tensorflow/compiler/xla/python/xla_client.py",
            "patches": [
                {
                    "old_start": 43,
                    "old_length": 7,
                    "new_start": 43,
                    "new_length": 7,
                    "hunk": "@@ -43,7 +43,7 @@ profiler = _xla.profiler\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes.\n-_version = 98\n+_version = 99\n \n # Version number for MLIR:Python components.\n mlir_api_version = 35\n"
                }
            ],
            "whole_deleted": "-_version = 98\n",
            "whole_added": "+_version = 99\n",
            "whole_hunk": "@@ -43,7 +43,7 @@ profiler = _xla.profiler\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes.\n-_version = 98\n+_version = 99\n \n # Version number for MLIR:Python components.\n mlir_api_version = 35\n"
        },
        {
            "name": "__init__.pyi",
            "path": "tensorflow/compiler/xla/python/xla_extension/__init__.pyi",
            "patches": [
                {
                    "old_start": 480,
                    "old_length": 6,
                    "new_start": 480,
                    "new_length": 7,
                    "hunk": "@@ -480,6 +480,7 @@ class ArrayImpl:\n                committed: bool,\n                _skip_checks: bool = ...): ...\n   def block_until_ready(self) -> ArrayImpl: ...\n+  def is_deleted(self) -> bool: ...\n   dtype: np.dtype\n   shape: Tuple[int, ...]\n   _arrays: Any"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def is_deleted(self) -> bool: ...\n",
            "whole_hunk": "@@ -480,6 +480,7 @@ class ArrayImpl:\n                committed: bool,\n                _skip_checks: bool = ...): ...\n   def block_until_ready(self) -> ArrayImpl: ...\n+  def is_deleted(self) -> bool: ...\n   dtype: np.dtype\n   shape: Tuple[int, ...]\n   _arrays: Any"
        }
    ]
},
{
    "Id": 535,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/bbbe095a3ae243d10752ca16b914a8fa11221d07",
    "date": "2023-02-13T19:40:36+00:00",
    "message": "Fix crash in ragged_cross where the ragged tensor input is invalid\n\nThis PR tries to address the issue raised in 59114 where\nragged_cross will crash when input is invalid.\n\nThis PR fixes 59114.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\n\nAdd test case for GitHub issue 59114.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\n\nAdd additional check to return back immediately\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "label": "YES",
    "changes": [
        {
            "name": "ragged_cross_op.cc",
            "path": "tensorflow/core/kernels/ragged_cross_op.cc",
            "patches": [
                {
                    "old_start": 596,
                    "old_length": 7,
                    "new_start": 596,
                    "new_length": 11,
                    "hunk": "@@ -596,7 +596,11 @@ class RaggedCrossOp : public OpKernel {\n     int64_t cross_count_total = 0;\n     flat_row_splits(0) = 0;\n     for (int64_t b = 0; b < batch_size; b++) {\n-      cross_count_total += CrossCountByBatchIndex(features, b);\n+      int64_t cross_count_by_batch_index = CrossCountByBatchIndex(features, b);\n+      if (cross_count_by_batch_index < 0) {\n+        return errors::InvalidArgument(\"Invalid RaggedTensor\");\n+      }\n+      cross_count_total += cross_count_by_batch_index;\n       flat_row_splits(b + 1) = cross_count_total;\n     }\n \n"
                },
                {
                    "old_start": 613,
                    "old_length": 6,
                    "new_start": 617,
                    "new_length": 8,
                    "hunk": "@@ -613,6 +617,8 @@ class RaggedCrossOp : public OpKernel {\n     int64_t cross_count = 1;\n     for (int i = 0; i < features.size(); ++i) {\n       const auto feature_count = features[i]->FeatureCount(batch_index);\n+      // If feature_count is invalid, return -1 to let caller know.\n+      if (feature_count < 0) return -1;\n       if (feature_count == 0) return 0;\n       cross_count *= feature_count;\n     }\n"
                }
            ],
            "whole_deleted": "-      cross_count_total += CrossCountByBatchIndex(features, b);\n",
            "whole_added": "+      int64_t cross_count_by_batch_index = CrossCountByBatchIndex(features, b);\n+      if (cross_count_by_batch_index < 0) {\n+        return errors::InvalidArgument(\"Invalid RaggedTensor\");\n+      }\n+      cross_count_total += cross_count_by_batch_index;\n+      // If feature_count is invalid, return -1 to let caller know.\n+      if (feature_count < 0) return -1;\n",
            "whole_hunk": "@@ -596,7 +596,11 @@ class RaggedCrossOp : public OpKernel {\n     int64_t cross_count_total = 0;\n     flat_row_splits(0) = 0;\n     for (int64_t b = 0; b < batch_size; b++) {\n-      cross_count_total += CrossCountByBatchIndex(features, b);\n+      int64_t cross_count_by_batch_index = CrossCountByBatchIndex(features, b);\n+      if (cross_count_by_batch_index < 0) {\n+        return errors::InvalidArgument(\"Invalid RaggedTensor\");\n+      }\n+      cross_count_total += cross_count_by_batch_index;\n       flat_row_splits(b + 1) = cross_count_total;\n     }\n \n@@ -613,6 +617,8 @@ class RaggedCrossOp : public OpKernel {\n     int64_t cross_count = 1;\n     for (int i = 0; i < features.size(); ++i) {\n       const auto feature_count = features[i]->FeatureCount(batch_index);\n+      // If feature_count is invalid, return -1 to let caller know.\n+      if (feature_count < 0) return -1;\n       if (feature_count == 0) return 0;\n       cross_count *= feature_count;\n     }\n"
        },
        {
            "name": "ragged_cross_op_test.py",
            "path": "tensorflow/python/ops/ragged/ragged_cross_op_test.py",
            "patches": [
                {
                    "old_start": 25,
                    "old_length": 7,
                    "new_start": 25,
                    "new_length": 9,
                    "hunk": "@@ -25,7 +25,9 @@ from tensorflow.python.framework import ops\n from tensorflow.python.framework import sparse_tensor\n from tensorflow.python.framework import tensor_spec\n from tensorflow.python.framework import test_util\n+from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gen_ragged_array_ops\n+from tensorflow.python.ops import random_ops\n from tensorflow.python.ops import sparse_ops\n from tensorflow.python.ops.ragged import ragged_array_ops\n from tensorflow.python.ops.ragged import ragged_factory_ops\n"
                },
                {
                    "old_start": 456,
                    "old_length": 6,
                    "new_start": 458,
                    "new_length": 32,
                    "hunk": "@@ -456,6 +458,32 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n           out_values_type=dtypes.string,\n           out_row_splits_type=dtypes.int64))\n \n+  def testRaggedCrossInvalidValue(self):\n+    # Test case in GitHub isseu 59114.\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        'Invalid RaggedTensor'):\n+      ragged_values_0_tensor = ops.convert_to_tensor(np.ones([3], dtype=str))\n+      ragged_values_0 = array_ops.identity(ragged_values_0_tensor)\n+      ragged_values = [ragged_values_0,]\n+      ragged_row_splits_0_tensor = random_ops.random_uniform(\n+          [4], minval=-256, maxval=257, dtype=dtypes.int64)\n+      ragged_row_splits_0 = array_ops.identity(ragged_row_splits_0_tensor)\n+      ragged_row_splits = [ragged_row_splits_0,]\n+      self.evaluate(gen_ragged_array_ops.RaggedCross(\n+          ragged_values=ragged_values,\n+          ragged_row_splits=ragged_row_splits,\n+          sparse_indices=[],\n+          sparse_values=[],\n+          sparse_shape=[],\n+          dense_inputs=[],\n+          input_order='R',\n+          hashed_output=False,\n+          num_buckets=0,\n+          hash_key=956888297470,\n+          out_values_type=7,\n+          out_row_splits_type=9))\n+\n \n if __name__ == '__main__':\n   googletest.main()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import random_ops\n+  def testRaggedCrossInvalidValue(self):\n+    # Test case in GitHub isseu 59114.\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        'Invalid RaggedTensor'):\n+      ragged_values_0_tensor = ops.convert_to_tensor(np.ones([3], dtype=str))\n+      ragged_values_0 = array_ops.identity(ragged_values_0_tensor)\n+      ragged_values = [ragged_values_0,]\n+      ragged_row_splits_0_tensor = random_ops.random_uniform(\n+          [4], minval=-256, maxval=257, dtype=dtypes.int64)\n+      ragged_row_splits_0 = array_ops.identity(ragged_row_splits_0_tensor)\n+      ragged_row_splits = [ragged_row_splits_0,]\n+      self.evaluate(gen_ragged_array_ops.RaggedCross(\n+          ragged_values=ragged_values,\n+          ragged_row_splits=ragged_row_splits,\n+          sparse_indices=[],\n+          sparse_values=[],\n+          sparse_shape=[],\n+          dense_inputs=[],\n+          input_order='R',\n+          hashed_output=False,\n+          num_buckets=0,\n+          hash_key=956888297470,\n+          out_values_type=7,\n+          out_row_splits_type=9))\n+\n",
            "whole_hunk": "@@ -25,7 +25,9 @@ from tensorflow.python.framework import ops\n from tensorflow.python.framework import sparse_tensor\n from tensorflow.python.framework import tensor_spec\n from tensorflow.python.framework import test_util\n+from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gen_ragged_array_ops\n+from tensorflow.python.ops import random_ops\n from tensorflow.python.ops import sparse_ops\n from tensorflow.python.ops.ragged import ragged_array_ops\n from tensorflow.python.ops.ragged import ragged_factory_ops\n@@ -456,6 +458,32 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n           out_values_type=dtypes.string,\n           out_row_splits_type=dtypes.int64))\n \n+  def testRaggedCrossInvalidValue(self):\n+    # Test case in GitHub isseu 59114.\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        'Invalid RaggedTensor'):\n+      ragged_values_0_tensor = ops.convert_to_tensor(np.ones([3], dtype=str))\n+      ragged_values_0 = array_ops.identity(ragged_values_0_tensor)\n+      ragged_values = [ragged_values_0,]\n+      ragged_row_splits_0_tensor = random_ops.random_uniform(\n+          [4], minval=-256, maxval=257, dtype=dtypes.int64)\n+      ragged_row_splits_0 = array_ops.identity(ragged_row_splits_0_tensor)\n+      ragged_row_splits = [ragged_row_splits_0,]\n+      self.evaluate(gen_ragged_array_ops.RaggedCross(\n+          ragged_values=ragged_values,\n+          ragged_row_splits=ragged_row_splits,\n+          sparse_indices=[],\n+          sparse_values=[],\n+          sparse_shape=[],\n+          dense_inputs=[],\n+          input_order='R',\n+          hashed_output=False,\n+          num_buckets=0,\n+          hash_key=956888297470,\n+          out_values_type=7,\n+          out_row_splits_type=9))\n+\n \n if __name__ == '__main__':\n   googletest.main()"
        }
    ]
},
{
    "Id": 599,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/105d5f3137d75f873fe3d7ad9aab1cb98356488d",
    "date": "2023-01-03T10:41:29-08:00",
    "message": "Rollback of PR #58391\nPR #58391: OP_REQUIRES for dtype check in AssignAddVariableOp\n\nImported from GitHub PR https://github.com/tensorflow/tensorflow/pull/58391\n\nFixes https://github.com/tensorflow/tensorflow/issues/58318\nCopybara import of the project:\n\n--\n0dd390280ec46773f859c66d2f835ac2fdc4d977 by bhack <bhack@users.noreply.github.com>:\n\nOP_REQUIRES for dtype check in AssignAddVariableOp\n\nMerging this change closes #58391\n\nPiperOrigin-RevId: 499259827",
    "label": "YES",
    "changes": [
        {
            "name": "resource_variable_ops.cc",
            "path": "tensorflow/core/kernels/resource_variable_ops.cc",
            "patches": [
                {
                    "old_start": 590,
                    "old_length": 19,
                    "new_start": 590,
                    "new_length": 10,
                    "hunk": "@@ -590,19 +590,10 @@ class AssignUpdateVariableOp : public OpKernel {\n                                            &variable));\n \n     const Tensor& value = context->input(1);\n-    const DataType& dtype_ = value.dtype();\n     // TODO(apassos): We could possibly avoid the copy done by\n     // PrepareToUpdateVariable() for commutative operations like Op ==\n     // ADD if value's refcount was 1.\n     mutex_lock ml(*variable->mu());\n-    OP_REQUIRES(context,\n-                (variable->tensor()->dtype() == DT_INVALID &&\n-                 !variable->is_initialized) ||\n-                    variable->tensor()->dtype() == dtype_,\n-                errors::InvalidArgument(\n-                    \"Trying to assign variable with wrong dtype. Expected \",\n-                    DataTypeString(variable->tensor()->dtype()), \" got \",\n-                    DataTypeString(dtype_)));\n     Tensor* var_tensor = variable->tensor();\n     OP_REQUIRES_OK(context, ValidateAssignUpdateVariableOpShapes(\n                                 var_tensor->shape(), value.shape()));\n"
                }
            ],
            "whole_deleted": "-    const DataType& dtype_ = value.dtype();\n-    OP_REQUIRES(context,\n-                (variable->tensor()->dtype() == DT_INVALID &&\n-                 !variable->is_initialized) ||\n-                    variable->tensor()->dtype() == dtype_,\n-                errors::InvalidArgument(\n-                    \"Trying to assign variable with wrong dtype. Expected \",\n-                    DataTypeString(variable->tensor()->dtype()), \" got \",\n-                    DataTypeString(dtype_)));\n",
            "whole_added": "",
            "whole_hunk": "@@ -590,19 +590,10 @@ class AssignUpdateVariableOp : public OpKernel {\n                                            &variable));\n \n     const Tensor& value = context->input(1);\n-    const DataType& dtype_ = value.dtype();\n     // TODO(apassos): We could possibly avoid the copy done by\n     // PrepareToUpdateVariable() for commutative operations like Op ==\n     // ADD if value's refcount was 1.\n     mutex_lock ml(*variable->mu());\n-    OP_REQUIRES(context,\n-                (variable->tensor()->dtype() == DT_INVALID &&\n-                 !variable->is_initialized) ||\n-                    variable->tensor()->dtype() == dtype_,\n-                errors::InvalidArgument(\n-                    \"Trying to assign variable with wrong dtype. Expected \",\n-                    DataTypeString(variable->tensor()->dtype()), \" got \",\n-                    DataTypeString(dtype_)));\n     Tensor* var_tensor = variable->tensor();\n     OP_REQUIRES_OK(context, ValidateAssignUpdateVariableOpShapes(\n                                 var_tensor->shape(), value.shape()));\n"
        },
        {
            "name": "resource_variable_ops_test.py",
            "path": "tensorflow/python/kernel_tests/variables/resource_variable_ops_test.py",
            "patches": [
                {
                    "old_start": 375,
                    "old_length": 6,
                    "new_start": 375,
                    "new_length": 7,
                    "hunk": "@@ -375,6 +375,7 @@ class ResourceVariableOpsTest(test_util.TensorFlowTestCase,\n         resource_variable_ops.read_variable_op(handle, dtype=dtypes.int32))\n     self.assertEqual(read, 2)\n \n+  @test_util.run_in_graph_and_eager_modes\n   def testScatterAdd(self):\n     handle = _eager_safe_var_handle_op(dtype=dtypes.int32, shape=[1, 1])\n     self.evaluate(\n"
                },
                {
                    "old_start": 1069,
                    "old_length": 22,
                    "new_start": 1070,
                    "new_length": 6,
                    "hunk": "@@ -1069,22 +1070,6 @@ class ResourceVariableOpsTest(test_util.TensorFlowTestCase,\n     self.evaluate(assign_without_read)\n     self.assertEqual(4.0, self.evaluate(v.value()))\n \n-  def testAssignAddVariableDtypeMismatchEager(self):\n-    with context.eager_mode():\n-      handle = _eager_safe_var_handle_op(\n-          dtype=dtypes.int32, shape=[1], name=\"foo\")\n-      resource_variable_ops.assign_variable_op(handle,\n-                                               constant_op.constant([1]))\n-      # The error message varies depending on whether it is being raised\n-      # by the kernel or shape inference. The shape inference code path can\n-      # be reached when running in eager op as function mode where each op\n-      # is wrapped in a tf.function.\n-      with self.assertRaisesRegex(\n-          errors.InvalidArgumentError, r\"Trying to .* variable with wrong \"\n-          r\"dtype. Expected int32 got float\"):\n-        resource_variable_ops.assign_add_variable_op(\n-            handle, constant_op.constant([1.], dtype=dtypes.float32))\n-\n   @test_util.run_in_graph_and_eager_modes\n   def testAssignSubMethod(self):\n     v = resource_variable_ops.ResourceVariable(3.0, name=\"var0\")"
                }
            ],
            "whole_deleted": "-  def testAssignAddVariableDtypeMismatchEager(self):\n-    with context.eager_mode():\n-      handle = _eager_safe_var_handle_op(\n-          dtype=dtypes.int32, shape=[1], name=\"foo\")\n-      resource_variable_ops.assign_variable_op(handle,\n-                                               constant_op.constant([1]))\n-      # The error message varies depending on whether it is being raised\n-      # by the kernel or shape inference. The shape inference code path can\n-      # be reached when running in eager op as function mode where each op\n-      # is wrapped in a tf.function.\n-      with self.assertRaisesRegex(\n-          errors.InvalidArgumentError, r\"Trying to .* variable with wrong \"\n-          r\"dtype. Expected int32 got float\"):\n-        resource_variable_ops.assign_add_variable_op(\n-            handle, constant_op.constant([1.], dtype=dtypes.float32))\n-\n",
            "whole_added": "+  @test_util.run_in_graph_and_eager_modes\n",
            "whole_hunk": "@@ -375,6 +375,7 @@ class ResourceVariableOpsTest(test_util.TensorFlowTestCase,\n         resource_variable_ops.read_variable_op(handle, dtype=dtypes.int32))\n     self.assertEqual(read, 2)\n \n+  @test_util.run_in_graph_and_eager_modes\n   def testScatterAdd(self):\n     handle = _eager_safe_var_handle_op(dtype=dtypes.int32, shape=[1, 1])\n     self.evaluate(\n@@ -1069,22 +1070,6 @@ class ResourceVariableOpsTest(test_util.TensorFlowTestCase,\n     self.evaluate(assign_without_read)\n     self.assertEqual(4.0, self.evaluate(v.value()))\n \n-  def testAssignAddVariableDtypeMismatchEager(self):\n-    with context.eager_mode():\n-      handle = _eager_safe_var_handle_op(\n-          dtype=dtypes.int32, shape=[1], name=\"foo\")\n-      resource_variable_ops.assign_variable_op(handle,\n-                                               constant_op.constant([1]))\n-      # The error message varies depending on whether it is being raised\n-      # by the kernel or shape inference. The shape inference code path can\n-      # be reached when running in eager op as function mode where each op\n-      # is wrapped in a tf.function.\n-      with self.assertRaisesRegex(\n-          errors.InvalidArgumentError, r\"Trying to .* variable with wrong \"\n-          r\"dtype. Expected int32 got float\"):\n-        resource_variable_ops.assign_add_variable_op(\n-            handle, constant_op.constant([1.], dtype=dtypes.float32))\n-\n   @test_util.run_in_graph_and_eager_modes\n   def testAssignSubMethod(self):\n     v = resource_variable_ops.ResourceVariable(3.0, name=\"var0\")"
        }
    ]
},
{
    "Id": 284,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/917c9e33a40cf07725f86700ab3960681a98180c",
    "date": "2023-09-19T23:06:27-07:00",
    "message": "Add memory kind check in `PjRtArray::Create`.\n\nPiperOrigin-RevId: 566851924",
    "label": "NO",
    "changes": [
        {
            "name": "pjrt_array.cc",
            "path": "third_party/xla/xla/python/pjrt_ifrt/pjrt_array.cc",
            "patches": [
                {
                    "old_start": 126,
                    "old_length": 6,
                    "new_start": 126,
                    "new_length": 9,
                    "hunk": "@@ -126,6 +126,9 @@ StatusOr<tsl::RCReference<PjRtArray>> PjRtArray::Create(\n                            sharding->devices().size(), pjrt_buffers.size());\n   }\n \n+  // Canonicalize memory kind in case it hasn't been done before.\n+  MemoryKind canonicalized_sharding_memory_kind = CanonicalizeMemoryKind(\n+      sharding->memory_kind(), sharding->devices().front());\n   for (int i = 0; i < sharding->devices().size(); ++i) {\n     if (pjrt_buffers[i]->device() != sharding->devices()[i]) {\n       return InvalidArgument(\n"
                },
                {
                    "old_start": 134,
                    "old_length": 8,
                    "new_start": 137,
                    "new_length": 15,
                    "hunk": "@@ -134,8 +137,15 @@ StatusOr<tsl::RCReference<PjRtArray>> PjRtArray::Create(\n           pjrt_buffers[i]->device()->DebugString(),\n           sharding->devices()[i]->DebugString());\n     }\n-    // TODO(yashkatariya): Check for memory kind after PJRT C API supports\n-    // memories on PJRT_Buffer.\n+    MemoryKind buffer_memory_kind =\n+        MakeMemoryKindFromPjRtBuffer(pjrt_buffers[i].get());\n+    if (canonicalized_sharding_memory_kind != buffer_memory_kind) {\n+      return InvalidArgument(\n+          \"PjRtBuffer's memory kind does not match sharding's memory kind. Got \"\n+          \"PjRtBuffer's memory kind: %s vs shardings's memory kind: %s\",\n+          buffer_memory_kind.DebugString(),\n+          canonicalized_sharding_memory_kind.DebugString());\n+    }\n   }\n   return tsl::MakeRef<PjRtArray>(client, dtype, std::move(shape),\n                                  std::move(sharding), std::move(pjrt_buffers));\n"
                }
            ],
            "whole_deleted": "-    // TODO(yashkatariya): Check for memory kind after PJRT C API supports\n-    // memories on PJRT_Buffer.\n",
            "whole_added": "+  // Canonicalize memory kind in case it hasn't been done before.\n+  MemoryKind canonicalized_sharding_memory_kind = CanonicalizeMemoryKind(\n+      sharding->memory_kind(), sharding->devices().front());\n+    MemoryKind buffer_memory_kind =\n+        MakeMemoryKindFromPjRtBuffer(pjrt_buffers[i].get());\n+    if (canonicalized_sharding_memory_kind != buffer_memory_kind) {\n+      return InvalidArgument(\n+          \"PjRtBuffer's memory kind does not match sharding's memory kind. Got \"\n+          \"PjRtBuffer's memory kind: %s vs shardings's memory kind: %s\",\n+          buffer_memory_kind.DebugString(),\n+          canonicalized_sharding_memory_kind.DebugString());\n+    }\n",
            "whole_hunk": "@@ -126,6 +126,9 @@ StatusOr<tsl::RCReference<PjRtArray>> PjRtArray::Create(\n                            sharding->devices().size(), pjrt_buffers.size());\n   }\n \n+  // Canonicalize memory kind in case it hasn't been done before.\n+  MemoryKind canonicalized_sharding_memory_kind = CanonicalizeMemoryKind(\n+      sharding->memory_kind(), sharding->devices().front());\n   for (int i = 0; i < sharding->devices().size(); ++i) {\n     if (pjrt_buffers[i]->device() != sharding->devices()[i]) {\n       return InvalidArgument(\n@@ -134,8 +137,15 @@ StatusOr<tsl::RCReference<PjRtArray>> PjRtArray::Create(\n           pjrt_buffers[i]->device()->DebugString(),\n           sharding->devices()[i]->DebugString());\n     }\n-    // TODO(yashkatariya): Check for memory kind after PJRT C API supports\n-    // memories on PJRT_Buffer.\n+    MemoryKind buffer_memory_kind =\n+        MakeMemoryKindFromPjRtBuffer(pjrt_buffers[i].get());\n+    if (canonicalized_sharding_memory_kind != buffer_memory_kind) {\n+      return InvalidArgument(\n+          \"PjRtBuffer's memory kind does not match sharding's memory kind. Got \"\n+          \"PjRtBuffer's memory kind: %s vs shardings's memory kind: %s\",\n+          buffer_memory_kind.DebugString(),\n+          canonicalized_sharding_memory_kind.DebugString());\n+    }\n   }\n   return tsl::MakeRef<PjRtArray>(client, dtype, std::move(shape),\n                                  std::move(sharding), std::move(pjrt_buffers));\n"
        },
        {
            "name": "xla_client.py",
            "path": "third_party/xla/xla/python/xla_client.py",
            "patches": [
                {
                    "old_start": 44,
                    "old_length": 7,
                    "new_start": 44,
                    "new_length": 7,
                    "hunk": "@@ -44,7 +44,7 @@ profiler = _xla.profiler\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.xla_extension_version.\n-_version = 194\n+_version = 195\n \n # Version number for MLIR:Python components.\n mlir_api_version = 54"
                }
            ],
            "whole_deleted": "-_version = 194\n",
            "whole_added": "+_version = 195\n",
            "whole_hunk": "@@ -44,7 +44,7 @@ profiler = _xla.profiler\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.xla_extension_version.\n-_version = 194\n+_version = 195\n \n # Version number for MLIR:Python components.\n mlir_api_version = 54"
        }
    ]
},
{
    "Id": 439,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/3dc509f31848c7778dc68fabc59ab39c2e0d1e4a",
    "date": "2023-05-02T13:49:26-07:00",
    "message": "[tflite-gpu] Memory leak in Nvidia driver 525.85.12; disable heap check until resolved.\n\nPiperOrigin-RevId: 528878157",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/lite/delegates/gpu/cl/kernels/BUILD",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 10,
                    "new_start": 20,
                    "new_length": 11,
                    "hunk": "@@ -20,10 +20,11 @@ cc_test(\n     ],\n     deps = [\n         \":cl_test\",\n+        # TODO(b/279977471) Once b/279347631 is resolved, check for heap again\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:add_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 41,
                    "old_length": 7,
                    "new_start": 42,
                    "new_length": 7,
                    "hunk": "@@ -41,7 +42,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:cast_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 75,
                    "old_length": 7,
                    "new_start": 76,
                    "new_length": 7,
                    "hunk": "@@ -75,7 +76,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:concat_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 111,
                    "old_length": 7,
                    "new_start": 112,
                    "new_length": 7,
                    "hunk": "@@ -111,7 +112,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:conv_generic_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 129,
                    "old_length": 7,
                    "new_start": 130,
                    "new_length": 7,
                    "hunk": "@@ -129,7 +130,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:conv_weights_converter_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 172,
                    "old_length": 7,
                    "new_start": 173,
                    "new_length": 7,
                    "hunk": "@@ -172,7 +173,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:convolution_transposed_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 190,
                    "old_length": 7,
                    "new_start": 191,
                    "new_length": 7,
                    "hunk": "@@ -190,7 +191,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:convolution_transposed_3x3_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 226,
                    "old_length": 7,
                    "new_start": 227,
                    "new_length": 7,
                    "hunk": "@@ -226,7 +227,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:convolution_transposed_4x4_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 262,
                    "old_length": 7,
                    "new_start": 263,
                    "new_length": 7,
                    "hunk": "@@ -262,7 +263,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:cumsum_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 280,
                    "old_length": 7,
                    "new_start": 281,
                    "new_length": 7,
                    "hunk": "@@ -280,7 +281,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:depthwise_conv_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 299,
                    "old_length": 7,
                    "new_start": 300,
                    "new_length": 7,
                    "hunk": "@@ -299,7 +300,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:depthwise_conv_3x3_stride_h2_test_util\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:depthwise_conv_3x3_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 317,
                    "old_length": 7,
                    "new_start": 318,
                    "new_length": 7,
                    "hunk": "@@ -317,7 +318,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:elementwise_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 340,
                    "old_length": 7,
                    "new_start": 341,
                    "new_length": 7,
                    "hunk": "@@ -340,7 +341,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common/task:gpu_operation\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:fully_connected\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:fully_connected_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 358,
                    "old_length": 7,
                    "new_start": 359,
                    "new_length": 7,
                    "hunk": "@@ -358,7 +359,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:gather_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 376,
                    "old_length": 7,
                    "new_start": 377,
                    "new_length": 7,
                    "hunk": "@@ -376,7 +377,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:lstm_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 413,
                    "old_length": 7,
                    "new_start": 414,
                    "new_length": 7,
                    "hunk": "@@ -413,7 +414,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:max_unpooling_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 431,
                    "old_length": 7,
                    "new_start": 432,
                    "new_length": 7,
                    "hunk": "@@ -431,7 +432,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:mean_stddev_normalization_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 449,
                    "old_length": 7,
                    "new_start": 450,
                    "new_length": 7,
                    "hunk": "@@ -449,7 +450,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:one_hot_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 467,
                    "old_length": 7,
                    "new_start": 468,
                    "new_length": 7,
                    "hunk": "@@ -467,7 +468,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:padding_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 485,
                    "old_length": 7,
                    "new_start": 486,
                    "new_length": 7,
                    "hunk": "@@ -485,7 +486,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:pooling_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 503,
                    "old_length": 7,
                    "new_start": 504,
                    "new_length": 7,
                    "hunk": "@@ -503,7 +504,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:prelu_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 521,
                    "old_length": 7,
                    "new_start": 522,
                    "new_length": 7,
                    "hunk": "@@ -521,7 +522,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:quantize_and_dequantize_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 539,
                    "old_length": 7,
                    "new_start": 540,
                    "new_length": 7,
                    "hunk": "@@ -539,7 +540,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:reduce_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 557,
                    "old_length": 7,
                    "new_start": 558,
                    "new_length": 7,
                    "hunk": "@@ -557,7 +558,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:relu_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 575,
                    "old_length": 7,
                    "new_start": 576,
                    "new_length": 7,
                    "hunk": "@@ -575,7 +576,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:resampler_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 593,
                    "old_length": 7,
                    "new_start": 594,
                    "new_length": 7,
                    "hunk": "@@ -593,7 +594,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:reshape_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 611,
                    "old_length": 7,
                    "new_start": 612,
                    "new_length": 7,
                    "hunk": "@@ -611,7 +612,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:reshape_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 629,
                    "old_length": 7,
                    "new_start": 630,
                    "new_length": 7,
                    "hunk": "@@ -629,7 +630,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:select_v2_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 647,
                    "old_length": 7,
                    "new_start": 648,
                    "new_length": 7,
                    "hunk": "@@ -647,7 +648,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:softmax_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 665,
                    "old_length": 7,
                    "new_start": 666,
                    "new_length": 7,
                    "hunk": "@@ -665,7 +666,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:softmax_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 683,
                    "old_length": 7,
                    "new_start": 684,
                    "new_length": 7,
                    "hunk": "@@ -683,7 +684,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:space_to_depth_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 701,
                    "old_length": 7,
                    "new_start": 702,
                    "new_length": 7,
                    "hunk": "@@ -701,7 +702,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:split_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 719,
                    "old_length": 7,
                    "new_start": 720,
                    "new_length": 7,
                    "hunk": "@@ -719,7 +720,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:strided_slice_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 737,
                    "old_length": 7,
                    "new_start": 738,
                    "new_length": 7,
                    "hunk": "@@ -737,7 +738,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:tile_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 755,
                    "old_length": 7,
                    "new_start": 756,
                    "new_length": 7,
                    "hunk": "@@ -755,7 +756,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:transpose_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 773,
                    "old_length": 7,
                    "new_start": 774,
                    "new_length": 7,
                    "hunk": "@@ -773,7 +774,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:resize_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n"
                },
                {
                    "old_start": 791,
                    "old_length": 7,
                    "new_start": 792,
                    "new_length": 7,
                    "hunk": "@@ -791,7 +792,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:winograd_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n "
                }
            ],
            "whole_deleted": "-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n-        \"@com_google_googletest//:gtest_main\",\n",
            "whole_added": "+        # TODO(b/279977471) Once b/279347631 is resolved, check for heap again\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n",
            "whole_hunk": "@@ -20,10 +20,11 @@ cc_test(\n     ],\n     deps = [\n         \":cl_test\",\n+        # TODO(b/279977471) Once b/279347631 is resolved, check for heap again\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:add_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n     ],\n )\n \n@@ -41,7 +42,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:cast_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -75,7 +76,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:concat_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -111,7 +112,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:conv_generic_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -129,7 +130,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:conv_weights_converter_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -172,7 +173,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:convolution_transposed_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -190,7 +191,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:convolution_transposed_3x3_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -226,7 +227,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:convolution_transposed_4x4_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -262,7 +263,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:cumsum_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -280,7 +281,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:depthwise_conv_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -299,7 +300,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:depthwise_conv_3x3_stride_h2_test_util\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:depthwise_conv_3x3_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -317,7 +318,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:elementwise_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -340,7 +341,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common/task:gpu_operation\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:fully_connected\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:fully_connected_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -358,7 +359,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:gather_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -376,7 +377,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:lstm_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -413,7 +414,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:max_unpooling_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -431,7 +432,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:mean_stddev_normalization_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -449,7 +450,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:one_hot_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -467,7 +468,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:padding_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -485,7 +486,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:pooling_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -503,7 +504,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:prelu_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -521,7 +522,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:quantize_and_dequantize_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -539,7 +540,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:reduce_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -557,7 +558,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:relu_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -575,7 +576,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:resampler_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -593,7 +594,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:reshape_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -611,7 +612,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:reshape_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -629,7 +630,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:select_v2_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -647,7 +648,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:softmax_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -665,7 +666,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:softmax_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -683,7 +684,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:space_to_depth_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -701,7 +702,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:split_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -719,7 +720,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:strided_slice_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -737,7 +738,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:tile_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -755,7 +756,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:transpose_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -773,7 +774,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:resize_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n \n@@ -791,7 +792,7 @@ cc_test(\n         \"//tensorflow/lite/delegates/gpu/common:operations\",\n         \"//tensorflow/lite/delegates/gpu/common:status\",\n         \"//tensorflow/lite/delegates/gpu/common/tasks:winograd_test_util\",\n-        \"@com_google_googletest//:gtest_main\",\n+        \"@com_google_googletest//:gtest_main_no_heapcheck\",\n     ],\n )\n "
        }
    ]
},
{
    "Id": 259,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/d3ffd32e33f129c2cce47d20adede1c3430cc865",
    "date": "2023-10-26T15:01:27-07:00",
    "message": "Fix the segfault crash when validating model flatbuffer.\n\nPiperOrigin-RevId: 576995368",
    "label": "YES",
    "changes": [
        {
            "name": "model_builder.cc",
            "path": "tensorflow/lite/core/model_builder.cc",
            "patches": [
                {
                    "old_start": 243,
                    "old_length": 15,
                    "new_start": 243,
                    "new_length": 13,
                    "hunk": "@@ -243,15 +243,13 @@ void FlatBufferModel::ValidateModelBuffers(ErrorReporter* error_reporter) {\n   auto buffers = model_->buffers();\n   if (buffers && buffers->size() > 0) {\n     auto first_buffer = buffers->Get(0);\n-    if (first_buffer && first_buffer->data()) {\n-      if (first_buffer->data()->size() != 0) {\n-        // Note the 0th entry of this array must be an empty buffer (sentinel).\n-        // This is a convention so that tensors without a buffer can provide 0\n-        // as their buffer.\n-        TF_LITE_REPORT_ERROR(\n-            error_reporter,\n-            \"The 0th entry of the model buffer must be an empty buffer.\");\n-      }\n+    if (first_buffer && first_buffer->size() != 0) {\n+      // Note the 0th entry of this array must be an empty buffer (sentinel).\n+      // This is a convention so that tensors without a buffer can provide 0\n+      // as their buffer.\n+      TF_LITE_REPORT_ERROR(\n+          error_reporter,\n+          \"The 0th entry of the model buffer must be an empty buffer.\");\n     }\n   }\n }"
                }
            ],
            "whole_deleted": "-    if (first_buffer && first_buffer->data()) {\n-      if (first_buffer->data()->size() != 0) {\n-        // Note the 0th entry of this array must be an empty buffer (sentinel).\n-        // This is a convention so that tensors without a buffer can provide 0\n-        // as their buffer.\n-        TF_LITE_REPORT_ERROR(\n-            error_reporter,\n-            \"The 0th entry of the model buffer must be an empty buffer.\");\n-      }\n",
            "whole_added": "+    if (first_buffer && first_buffer->size() != 0) {\n+      // Note the 0th entry of this array must be an empty buffer (sentinel).\n+      // This is a convention so that tensors without a buffer can provide 0\n+      // as their buffer.\n+      TF_LITE_REPORT_ERROR(\n+          error_reporter,\n+          \"The 0th entry of the model buffer must be an empty buffer.\");\n",
            "whole_hunk": "@@ -243,15 +243,13 @@ void FlatBufferModel::ValidateModelBuffers(ErrorReporter* error_reporter) {\n   auto buffers = model_->buffers();\n   if (buffers && buffers->size() > 0) {\n     auto first_buffer = buffers->Get(0);\n-    if (first_buffer && first_buffer->data()) {\n-      if (first_buffer->data()->size() != 0) {\n-        // Note the 0th entry of this array must be an empty buffer (sentinel).\n-        // This is a convention so that tensors without a buffer can provide 0\n-        // as their buffer.\n-        TF_LITE_REPORT_ERROR(\n-            error_reporter,\n-            \"The 0th entry of the model buffer must be an empty buffer.\");\n-      }\n+    if (first_buffer && first_buffer->size() != 0) {\n+      // Note the 0th entry of this array must be an empty buffer (sentinel).\n+      // This is a convention so that tensors without a buffer can provide 0\n+      // as their buffer.\n+      TF_LITE_REPORT_ERROR(\n+          error_reporter,\n+          \"The 0th entry of the model buffer must be an empty buffer.\");\n     }\n   }\n }"
        }
    ]
},
{
    "Id": 493,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/920f576f4a20c4e95acc3e140512affb2434a5d0",
    "date": "2023-03-14T05:54:48-07:00",
    "message": "[XLA:GPU] Improve fusion kernel reuse (runtime and compile time)\n\n1. If multiple arguments have the same buffer, pass it only once and potentially mark the argument as `noalias`. For example this speeds up the runtime of computations where the output buffer is same as the input buffer. Some of our models are sped up by up to 3% (reaching the same speed as before adding kernel reuse). This may make the compile time a bit slower but we didn't encounter that.\n2. Mark parameters noalias if their buffer is not modified in the kernel, even if they overlap. This may provide both runtime and compile time speedup, but we are not sure how many HLOs are affected.\n3. Don't print operand shapes in the fingerprint. It may provide some compilation speedup, and the fingerprints are still unique.\n4. NFC: Remove an unneeded `b_.SetInsertPoint(b_.GetInsertBlock()->getTerminator());`.\n5. NFC: Make checks/logs nicer.\n6. NFC: Remove unneeded \"IrEmitterUnnested::\" from header.\n\nPiperOrigin-RevId: 516500042",
    "label": "NO",
    "changes": [
        {
            "name": "ir_emitter_unnested.cc",
            "path": "tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc",
            "patches": [
                {
                    "old_start": 43,
                    "old_length": 6,
                    "new_start": 43,
                    "new_length": 7,
                    "hunk": "@@ -43,6 +43,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/APInt.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/StringRef.h\"\n #include \"llvm/IR/Argument.h\"\n #include \"llvm/IR/BasicBlock.h\"\n"
                },
                {
                    "old_start": 581,
                    "old_length": 17,
                    "new_start": 582,
                    "new_length": 32,
                    "hunk": "@@ -581,17 +582,32 @@ IrEmitterUnnested::BuildReusableKernelPrototype(\n     absl::string_view suggested_name,\n     absl::Span<const ReusableKernelArgument> arguments,\n     const LaunchDimensions& launch_dimensions) {\n+  // If some arguments have the same buffer, we will pass them only once.\n+  llvm::SmallVector<int> to_llvm_arg_no(arguments.size());\n+  llvm::SmallVector<int> to_arg_no;\n+  to_arg_no.reserve(arguments.size());\n+  for (const auto& [arg_no, argument] : llvm::enumerate(arguments)) {\n+    if (argument.first_with_same_slice.has_value()) {\n+      to_llvm_arg_no[arg_no] =\n+          to_llvm_arg_no[argument.first_with_same_slice.value()];\n+      continue;\n+    }\n+\n+    to_llvm_arg_no[arg_no] = to_arg_no.size();\n+    to_arg_no.push_back(arg_no);\n+  }\n+  const int kNumLlvmArgs = to_arg_no.size();\n+\n   // Compute the kernel name. The opcode string may contain \"-\" which cannot be\n   // in a PTX function name, so sanitize the name before uniquifying it.\n   std::string kernel_name = ir_emitter_context_->name_uniquer()->GetUniqueName(\n       llvm_ir::SanitizeFunctionName(std::string(suggested_name)));\n-  std::vector<llvm_ir::IrArray> ir_arrays;\n \n   // Create the kernel and add it to the module.\n   llvm::LLVMContext& context = module_->getContext();\n   llvm::FunctionType* kernel_type = llvm::FunctionType::get(\n       /*Result=*/llvm::Type::getVoidTy(context),\n-      std::vector<llvm::Type*>(arguments.size(), b_.getInt8PtrTy()),\n+      std::vector<llvm::Type*>(kNumLlvmArgs, b_.getInt8PtrTy()),\n       /*isVarArg=*/false);\n   llvm::Function* kernel = llvm::Function::Create(\n       kernel_type, llvm::GlobalValue::ExternalLinkage, kernel_name, module_);\n"
                },
                {
                    "old_start": 610,
                    "old_length": 33,
                    "new_start": 626,
                    "new_length": 42,
                    "hunk": "@@ -610,33 +626,42 @@ IrEmitterUnnested::BuildReusableKernelPrototype(\n   // that return instruction.\n   b_.SetInsertPoint(llvm::ReturnInst::Create(context, entry_bb));\n \n-  for (size_t arg_no = 0; arg_no < arguments.size(); ++arg_no) {\n-    const auto& kernel_argument = arguments.at(arg_no);\n-    llvm::Argument& fn_arg = *kernel->getArg(arg_no);\n+  for (size_t llvm_arg_no = 0; llvm_arg_no < kernel->arg_size();\n+       ++llvm_arg_no) {\n+    const ReusableKernelArgument& kernel_argument =\n+        arguments[to_arg_no[llvm_arg_no]];\n+    llvm::Argument& llvm_arg = *kernel->getArg(llvm_arg_no);\n \n-    fn_arg.setName(StrCat(\"arg\", arg_no));\n+    llvm_arg.setName(StrCat(\"arg\", llvm_arg_no));\n \n-    kernel->addDereferenceableParamAttr(arg_no, kernel_argument.slice.size());\n+    kernel->addDereferenceableParamAttr(llvm_arg_no,\n+                                        kernel_argument.slice.size());\n \n     kernel->addParamAttr(\n-        arg_no,\n-        llvm::Attribute::get(fn_arg.getContext(), llvm::Attribute::Alignment,\n+        llvm_arg_no,\n+        llvm::Attribute::get(llvm_arg.getContext(), llvm::Attribute::Alignment,\n                              kernel_argument.alignment));\n \n     if (!kernel_argument.aliased) {\n-      kernel->addParamAttr(\n-          arg_no,\n-          llvm::Attribute::get(fn_arg.getContext(), llvm::Attribute::NoAlias));\n+      kernel->addParamAttr(llvm_arg_no,\n+                           llvm::Attribute::get(llvm_arg.getContext(),\n+                                                llvm::Attribute::NoAlias));\n     }\n+  }\n+\n+  std::vector<llvm_ir::IrArray> ir_arrays;\n+  for (size_t arg_no = 0; arg_no < arguments.size(); ++arg_no) {\n+    const ReusableKernelArgument& kernel_argument = arguments[arg_no];\n+    llvm::Argument& llvm_arg = *kernel->getArg(to_llvm_arg_no[arg_no]);\n \n     llvm::Type* ir_type =\n         llvm_ir::ShapeToIrType(kernel_argument.shape, module_);\n     llvm_ir::IrArray ir_array(\n-        CastToTypedValue(kernel_argument.shape, &fn_arg, &b_), ir_type,\n+        CastToTypedValue(kernel_argument.shape, &llvm_arg, &b_), ir_type,\n         kernel_argument.shape);\n \n     if (!kernel_argument.written) {\n-      ir_array.MarkInvariantOverWholeProgram(&fn_arg.getContext());\n+      ir_array.MarkInvariantOverWholeProgram(&llvm_arg.getContext());\n     }\n \n     ir_arrays.push_back(ir_array);\n"
                },
                {
                    "old_start": 1812,
                    "old_length": 7,
                    "new_start": 1837,
                    "new_length": 9,
                    "hunk": "@@ -1812,7 +1837,9 @@ Status IrEmitterUnnested::EmitTritonFusion(\n                                           /*is_fusion=*/false));\n \n   const std::string fingerprint =\n-      hlo_computation->ToString(HloPrintOptions::Fingerprint());\n+      hlo_computation->ToString(HloPrintOptions::Fingerprint()\n+                                    .set_print_only_essential_constants(false)\n+                                    .set_print_operand_shape(false));\n \n   // TODO(tdanyluk): Consider removing this level of caching, because we already\n   // cache the wrapper_fn now.\n"
                },
                {
                    "old_start": 3466,
                    "old_length": 7,
                    "new_start": 3493,
                    "new_length": 29,
                    "hunk": "@@ -3466,7 +3493,29 @@ IrEmitterUnnested::GetReusableKernelArguments(mlir::lmhlo::FusionOp fusion_op) {\n     }\n   }\n \n-  for (ReusableKernelArgument& kernel_argument : kernel_arguments) {\n+  for (int i = 0; i < static_cast<int>(kernel_arguments.size()); ++i) {\n+    ReusableKernelArgument& kernel_argument = kernel_arguments[i];\n+\n+    kernel_argument.first_with_same_slice = [&]() -> std::optional<int> {\n+      for (int j = 0; j < i; ++j) {\n+        const ReusableKernelArgument& other_kernel_argument =\n+            kernel_arguments[j];\n+        if (kernel_argument.slice == other_kernel_argument.slice) {\n+          return j;\n+        }\n+      }\n+      return std::nullopt;\n+    }();\n+\n+    if (kernel_argument.first_with_same_slice.has_value()) {\n+      const ReusableKernelArgument& same =\n+          kernel_arguments[kernel_argument.first_with_same_slice.value()];\n+      kernel_argument.alignment = same.alignment;\n+      kernel_argument.aliased = same.aliased;\n+      kernel_argument.written = same.written;\n+      continue;\n+    }\n+\n     kernel_argument.alignment = [&] {\n       const BufferAllocation* alloc = kernel_argument.slice.allocation();\n       if (alloc->is_entry_computation_parameter()) {\n"
                },
                {
                    "old_start": 3478,
                    "old_length": 18,
                    "new_start": 3527,
                    "new_length": 19,
                    "hunk": "@@ -3478,18 +3527,19 @@ IrEmitterUnnested::GetReusableKernelArguments(mlir::lmhlo::FusionOp fusion_op) {\n       }\n     }();\n \n-    kernel_argument.aliased = [&] {\n-      for (const ReusableKernelArgument& other_kernel_argument :\n-           kernel_arguments) {\n-        if (&kernel_argument != &other_kernel_argument &&\n+    kernel_argument.written = buffers_written.contains(kernel_argument.slice);\n+\n+    kernel_argument.aliased = kernel_argument.written && [&] {\n+      for (size_t j = 0; j < kernel_arguments.size(); ++j) {\n+        const ReusableKernelArgument& other_kernel_argument =\n+            kernel_arguments[j];\n+        if (i != j && kernel_argument.slice != other_kernel_argument.slice &&\n             kernel_argument.slice.OverlapsWith(other_kernel_argument.slice)) {\n           return true;\n         }\n       }\n       return false;\n     }();\n-\n-    kernel_argument.written = buffers_written.contains(kernel_argument.slice);\n   }\n \n   return kernel_arguments;\n"
                },
                {
                    "old_start": 3499,
                    "old_length": 6,
                    "new_start": 3549,
                    "new_length": 11,
                    "hunk": "@@ -3499,6 +3549,11 @@ std::string IrEmitterUnnested::GetArgumentFingerprint(\n     absl::Span<const ReusableKernelArgument> kernel_arguments) {\n   return absl::StrJoin(kernel_arguments, \",\",\n                        [](std::string* s, const ReusableKernelArgument& arg) {\n+                         if (arg.first_with_same_slice.has_value()) {\n+                           absl::StrAppend(s, \"=\",\n+                                           arg.first_with_same_slice.value());\n+                           return;\n+                         }\n                          absl::StrAppend(s, arg.alignment);\n                          if (arg.aliased) {\n                            absl::StrAppend(s, \"a\");\n"
                },
                {
                    "old_start": 3518,
                    "old_length": 8,
                    "new_start": 3573,
                    "new_length": 9,
                    "hunk": "@@ -3518,8 +3573,9 @@ std::string IrEmitterUnnested::GetFingerprint(\n   //\n   // It is not a problem to recursively print subcomputations, because we don't\n   // have them at this point.\n-  auto print_options =\n-      HloPrintOptions::Fingerprint().set_print_only_essential_constants(false);\n+  auto print_options = HloPrintOptions::Fingerprint()\n+                           .set_print_only_essential_constants(false)\n+                           .set_print_operand_shape(false);\n \n   return absl::StrCat(discriminator, \"(\",\n                       GetArgumentFingerprint(kernel_arguments), \")\",\n"
                },
                {
                    "old_start": 3559,
                    "old_length": 15,
                    "new_start": 3615,
                    "new_length": 19,
                    "hunk": "@@ -3559,15 +3615,19 @@ StatusOr<ReusableKernelThunk*> IrEmitterUnnested::BuildReusableKernelThunkImpl(\n   std::vector<BufferAllocation::Slice> arg_slices;\n   arg_slices.reserve(kernel_arguments.size());\n   for (const auto& kernel_argument : kernel_arguments) {\n-    arg_slices.push_back(kernel_argument.slice);\n+    if (!kernel_argument.first_with_same_slice.has_value()) {\n+      arg_slices.push_back(kernel_argument.slice);\n+    }\n   }\n \n   std::vector<mlir::Value> values;\n   values.reserve(kernel_arguments.size());\n   for (const auto& kernel_argument : kernel_arguments) {\n-    TF_ASSIGN_OR_RETURN(mlir::Value value,\n-                        RemoveTransformingOperations(kernel_argument.value));\n-    values.push_back(value);\n+    if (!kernel_argument.first_with_same_slice.has_value()) {\n+      TF_ASSIGN_OR_RETURN(mlir::Value value,\n+                          RemoveTransformingOperations(kernel_argument.value));\n+      values.push_back(value);\n+    }\n   }\n \n   auto thunk_ptr = std::make_unique<ReusableKernelThunk>(\n"
                },
                {
                    "old_start": 3594,
                    "old_length": 7,
                    "new_start": 3654,
                    "new_length": 8,
                    "hunk": "@@ -3594,7 +3654,8 @@ IrEmitterUnnested::BuildReusableKernelThunk(\n                                           /*is_fusion=*/true));\n   std::string fingerprint =\n       GetFingerprint(fused_computation, kernel_arguments, discriminator);\n-  VLOG(4) << \"Fingerprint: \" << fingerprint;\n+  VLOG(4) << \"Fingerprint: \";\n+  XLA_VLOG_LINES(4, fingerprint);\n \n   auto cache_it = kernel_reuse_cache_.find(fingerprint);\n   if (cache_it != kernel_reuse_cache_.end()) {\n"
                },
                {
                    "old_start": 3607,
                    "old_length": 7,
                    "new_start": 3668,
                    "new_length": 7,
                    "hunk": "@@ -3607,7 +3668,7 @@ IrEmitterUnnested::BuildReusableKernelThunk(\n     // deduplicated.\n     // TODO(tdanyluk): Consider avoiding the recalculation of launch dimensions\n     // when reusing kernels.\n-    CHECK_EQ(old_thunk->launch_dimensions(), launch_dimensions);\n+    TF_RET_CHECK(old_thunk->launch_dimensions() == launch_dimensions);\n \n     // We are not reusing the ThunkInfo of the old thunk, because the current\n     // thunk info must reference the current HLO operation.\n"
                },
                {
                    "old_start": 3616,
                    "old_length": 8,
                    "new_start": 3677,
                    "new_length": 6,
                    "hunk": "@@ -3616,8 +3677,6 @@ IrEmitterUnnested::BuildReusableKernelThunk(\n                                      GetThunkInfo(fusion_op), launch_dimensions)\n             .status());\n \n-    b_.SetInsertPoint(b_.GetInsertBlock()->getTerminator());\n-\n     return {std::nullopt};\n   }\n \n"
                }
            ],
            "whole_deleted": "-  std::vector<llvm_ir::IrArray> ir_arrays;\n-      std::vector<llvm::Type*>(arguments.size(), b_.getInt8PtrTy()),\n-  for (size_t arg_no = 0; arg_no < arguments.size(); ++arg_no) {\n-    const auto& kernel_argument = arguments.at(arg_no);\n-    llvm::Argument& fn_arg = *kernel->getArg(arg_no);\n-    fn_arg.setName(StrCat(\"arg\", arg_no));\n-    kernel->addDereferenceableParamAttr(arg_no, kernel_argument.slice.size());\n-        arg_no,\n-        llvm::Attribute::get(fn_arg.getContext(), llvm::Attribute::Alignment,\n-      kernel->addParamAttr(\n-          arg_no,\n-          llvm::Attribute::get(fn_arg.getContext(), llvm::Attribute::NoAlias));\n-        CastToTypedValue(kernel_argument.shape, &fn_arg, &b_), ir_type,\n-      ir_array.MarkInvariantOverWholeProgram(&fn_arg.getContext());\n-      hlo_computation->ToString(HloPrintOptions::Fingerprint());\n-  for (ReusableKernelArgument& kernel_argument : kernel_arguments) {\n-    kernel_argument.aliased = [&] {\n-      for (const ReusableKernelArgument& other_kernel_argument :\n-           kernel_arguments) {\n-        if (&kernel_argument != &other_kernel_argument &&\n-\n-    kernel_argument.written = buffers_written.contains(kernel_argument.slice);\n-  auto print_options =\n-      HloPrintOptions::Fingerprint().set_print_only_essential_constants(false);\n-    arg_slices.push_back(kernel_argument.slice);\n-    TF_ASSIGN_OR_RETURN(mlir::Value value,\n-                        RemoveTransformingOperations(kernel_argument.value));\n-    values.push_back(value);\n-  VLOG(4) << \"Fingerprint: \" << fingerprint;\n-    CHECK_EQ(old_thunk->launch_dimensions(), launch_dimensions);\n-    b_.SetInsertPoint(b_.GetInsertBlock()->getTerminator());\n-\n",
            "whole_added": "+#include \"llvm/ADT/SmallVector.h\"\n+  // If some arguments have the same buffer, we will pass them only once.\n+  llvm::SmallVector<int> to_llvm_arg_no(arguments.size());\n+  llvm::SmallVector<int> to_arg_no;\n+  to_arg_no.reserve(arguments.size());\n+  for (const auto& [arg_no, argument] : llvm::enumerate(arguments)) {\n+    if (argument.first_with_same_slice.has_value()) {\n+      to_llvm_arg_no[arg_no] =\n+          to_llvm_arg_no[argument.first_with_same_slice.value()];\n+      continue;\n+    }\n+\n+    to_llvm_arg_no[arg_no] = to_arg_no.size();\n+    to_arg_no.push_back(arg_no);\n+  }\n+  const int kNumLlvmArgs = to_arg_no.size();\n+\n+      std::vector<llvm::Type*>(kNumLlvmArgs, b_.getInt8PtrTy()),\n+  for (size_t llvm_arg_no = 0; llvm_arg_no < kernel->arg_size();\n+       ++llvm_arg_no) {\n+    const ReusableKernelArgument& kernel_argument =\n+        arguments[to_arg_no[llvm_arg_no]];\n+    llvm::Argument& llvm_arg = *kernel->getArg(llvm_arg_no);\n+    llvm_arg.setName(StrCat(\"arg\", llvm_arg_no));\n+    kernel->addDereferenceableParamAttr(llvm_arg_no,\n+                                        kernel_argument.slice.size());\n+        llvm_arg_no,\n+        llvm::Attribute::get(llvm_arg.getContext(), llvm::Attribute::Alignment,\n+      kernel->addParamAttr(llvm_arg_no,\n+                           llvm::Attribute::get(llvm_arg.getContext(),\n+                                                llvm::Attribute::NoAlias));\n+  }\n+\n+  std::vector<llvm_ir::IrArray> ir_arrays;\n+  for (size_t arg_no = 0; arg_no < arguments.size(); ++arg_no) {\n+    const ReusableKernelArgument& kernel_argument = arguments[arg_no];\n+    llvm::Argument& llvm_arg = *kernel->getArg(to_llvm_arg_no[arg_no]);\n+        CastToTypedValue(kernel_argument.shape, &llvm_arg, &b_), ir_type,\n+      ir_array.MarkInvariantOverWholeProgram(&llvm_arg.getContext());\n+      hlo_computation->ToString(HloPrintOptions::Fingerprint()\n+                                    .set_print_only_essential_constants(false)\n+                                    .set_print_operand_shape(false));\n+  for (int i = 0; i < static_cast<int>(kernel_arguments.size()); ++i) {\n+    ReusableKernelArgument& kernel_argument = kernel_arguments[i];\n+\n+    kernel_argument.first_with_same_slice = [&]() -> std::optional<int> {\n+      for (int j = 0; j < i; ++j) {\n+        const ReusableKernelArgument& other_kernel_argument =\n+            kernel_arguments[j];\n+        if (kernel_argument.slice == other_kernel_argument.slice) {\n+          return j;\n+        }\n+      }\n+      return std::nullopt;\n+    }();\n+\n+    if (kernel_argument.first_with_same_slice.has_value()) {\n+      const ReusableKernelArgument& same =\n+          kernel_arguments[kernel_argument.first_with_same_slice.value()];\n+      kernel_argument.alignment = same.alignment;\n+      kernel_argument.aliased = same.aliased;\n+      kernel_argument.written = same.written;\n+      continue;\n+    }\n+\n+    kernel_argument.written = buffers_written.contains(kernel_argument.slice);\n+\n+    kernel_argument.aliased = kernel_argument.written && [&] {\n+      for (size_t j = 0; j < kernel_arguments.size(); ++j) {\n+        const ReusableKernelArgument& other_kernel_argument =\n+            kernel_arguments[j];\n+        if (i != j && kernel_argument.slice != other_kernel_argument.slice &&\n+                         if (arg.first_with_same_slice.has_value()) {\n+                           absl::StrAppend(s, \"=\",\n+                                           arg.first_with_same_slice.value());\n+                           return;\n+                         }\n+  auto print_options = HloPrintOptions::Fingerprint()\n+                           .set_print_only_essential_constants(false)\n+                           .set_print_operand_shape(false);\n+    if (!kernel_argument.first_with_same_slice.has_value()) {\n+      arg_slices.push_back(kernel_argument.slice);\n+    }\n+    if (!kernel_argument.first_with_same_slice.has_value()) {\n+      TF_ASSIGN_OR_RETURN(mlir::Value value,\n+                          RemoveTransformingOperations(kernel_argument.value));\n+      values.push_back(value);\n+    }\n+  VLOG(4) << \"Fingerprint: \";\n+  XLA_VLOG_LINES(4, fingerprint);\n+    TF_RET_CHECK(old_thunk->launch_dimensions() == launch_dimensions);\n",
            "whole_hunk": "@@ -43,6 +43,7 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/ADT/APInt.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/StringRef.h\"\n #include \"llvm/IR/Argument.h\"\n #include \"llvm/IR/BasicBlock.h\"\n@@ -581,17 +582,32 @@ IrEmitterUnnested::BuildReusableKernelPrototype(\n     absl::string_view suggested_name,\n     absl::Span<const ReusableKernelArgument> arguments,\n     const LaunchDimensions& launch_dimensions) {\n+  // If some arguments have the same buffer, we will pass them only once.\n+  llvm::SmallVector<int> to_llvm_arg_no(arguments.size());\n+  llvm::SmallVector<int> to_arg_no;\n+  to_arg_no.reserve(arguments.size());\n+  for (const auto& [arg_no, argument] : llvm::enumerate(arguments)) {\n+    if (argument.first_with_same_slice.has_value()) {\n+      to_llvm_arg_no[arg_no] =\n+          to_llvm_arg_no[argument.first_with_same_slice.value()];\n+      continue;\n+    }\n+\n+    to_llvm_arg_no[arg_no] = to_arg_no.size();\n+    to_arg_no.push_back(arg_no);\n+  }\n+  const int kNumLlvmArgs = to_arg_no.size();\n+\n   // Compute the kernel name. The opcode string may contain \"-\" which cannot be\n   // in a PTX function name, so sanitize the name before uniquifying it.\n   std::string kernel_name = ir_emitter_context_->name_uniquer()->GetUniqueName(\n       llvm_ir::SanitizeFunctionName(std::string(suggested_name)));\n-  std::vector<llvm_ir::IrArray> ir_arrays;\n \n   // Create the kernel and add it to the module.\n   llvm::LLVMContext& context = module_->getContext();\n   llvm::FunctionType* kernel_type = llvm::FunctionType::get(\n       /*Result=*/llvm::Type::getVoidTy(context),\n-      std::vector<llvm::Type*>(arguments.size(), b_.getInt8PtrTy()),\n+      std::vector<llvm::Type*>(kNumLlvmArgs, b_.getInt8PtrTy()),\n       /*isVarArg=*/false);\n   llvm::Function* kernel = llvm::Function::Create(\n       kernel_type, llvm::GlobalValue::ExternalLinkage, kernel_name, module_);\n@@ -610,33 +626,42 @@ IrEmitterUnnested::BuildReusableKernelPrototype(\n   // that return instruction.\n   b_.SetInsertPoint(llvm::ReturnInst::Create(context, entry_bb));\n \n-  for (size_t arg_no = 0; arg_no < arguments.size(); ++arg_no) {\n-    const auto& kernel_argument = arguments.at(arg_no);\n-    llvm::Argument& fn_arg = *kernel->getArg(arg_no);\n+  for (size_t llvm_arg_no = 0; llvm_arg_no < kernel->arg_size();\n+       ++llvm_arg_no) {\n+    const ReusableKernelArgument& kernel_argument =\n+        arguments[to_arg_no[llvm_arg_no]];\n+    llvm::Argument& llvm_arg = *kernel->getArg(llvm_arg_no);\n \n-    fn_arg.setName(StrCat(\"arg\", arg_no));\n+    llvm_arg.setName(StrCat(\"arg\", llvm_arg_no));\n \n-    kernel->addDereferenceableParamAttr(arg_no, kernel_argument.slice.size());\n+    kernel->addDereferenceableParamAttr(llvm_arg_no,\n+                                        kernel_argument.slice.size());\n \n     kernel->addParamAttr(\n-        arg_no,\n-        llvm::Attribute::get(fn_arg.getContext(), llvm::Attribute::Alignment,\n+        llvm_arg_no,\n+        llvm::Attribute::get(llvm_arg.getContext(), llvm::Attribute::Alignment,\n                              kernel_argument.alignment));\n \n     if (!kernel_argument.aliased) {\n-      kernel->addParamAttr(\n-          arg_no,\n-          llvm::Attribute::get(fn_arg.getContext(), llvm::Attribute::NoAlias));\n+      kernel->addParamAttr(llvm_arg_no,\n+                           llvm::Attribute::get(llvm_arg.getContext(),\n+                                                llvm::Attribute::NoAlias));\n     }\n+  }\n+\n+  std::vector<llvm_ir::IrArray> ir_arrays;\n+  for (size_t arg_no = 0; arg_no < arguments.size(); ++arg_no) {\n+    const ReusableKernelArgument& kernel_argument = arguments[arg_no];\n+    llvm::Argument& llvm_arg = *kernel->getArg(to_llvm_arg_no[arg_no]);\n \n     llvm::Type* ir_type =\n         llvm_ir::ShapeToIrType(kernel_argument.shape, module_);\n     llvm_ir::IrArray ir_array(\n-        CastToTypedValue(kernel_argument.shape, &fn_arg, &b_), ir_type,\n+        CastToTypedValue(kernel_argument.shape, &llvm_arg, &b_), ir_type,\n         kernel_argument.shape);\n \n     if (!kernel_argument.written) {\n-      ir_array.MarkInvariantOverWholeProgram(&fn_arg.getContext());\n+      ir_array.MarkInvariantOverWholeProgram(&llvm_arg.getContext());\n     }\n \n     ir_arrays.push_back(ir_array);\n@@ -1812,7 +1837,9 @@ Status IrEmitterUnnested::EmitTritonFusion(\n                                           /*is_fusion=*/false));\n \n   const std::string fingerprint =\n-      hlo_computation->ToString(HloPrintOptions::Fingerprint());\n+      hlo_computation->ToString(HloPrintOptions::Fingerprint()\n+                                    .set_print_only_essential_constants(false)\n+                                    .set_print_operand_shape(false));\n \n   // TODO(tdanyluk): Consider removing this level of caching, because we already\n   // cache the wrapper_fn now.\n@@ -3466,7 +3493,29 @@ IrEmitterUnnested::GetReusableKernelArguments(mlir::lmhlo::FusionOp fusion_op) {\n     }\n   }\n \n-  for (ReusableKernelArgument& kernel_argument : kernel_arguments) {\n+  for (int i = 0; i < static_cast<int>(kernel_arguments.size()); ++i) {\n+    ReusableKernelArgument& kernel_argument = kernel_arguments[i];\n+\n+    kernel_argument.first_with_same_slice = [&]() -> std::optional<int> {\n+      for (int j = 0; j < i; ++j) {\n+        const ReusableKernelArgument& other_kernel_argument =\n+            kernel_arguments[j];\n+        if (kernel_argument.slice == other_kernel_argument.slice) {\n+          return j;\n+        }\n+      }\n+      return std::nullopt;\n+    }();\n+\n+    if (kernel_argument.first_with_same_slice.has_value()) {\n+      const ReusableKernelArgument& same =\n+          kernel_arguments[kernel_argument.first_with_same_slice.value()];\n+      kernel_argument.alignment = same.alignment;\n+      kernel_argument.aliased = same.aliased;\n+      kernel_argument.written = same.written;\n+      continue;\n+    }\n+\n     kernel_argument.alignment = [&] {\n       const BufferAllocation* alloc = kernel_argument.slice.allocation();\n       if (alloc->is_entry_computation_parameter()) {\n@@ -3478,18 +3527,19 @@ IrEmitterUnnested::GetReusableKernelArguments(mlir::lmhlo::FusionOp fusion_op) {\n       }\n     }();\n \n-    kernel_argument.aliased = [&] {\n-      for (const ReusableKernelArgument& other_kernel_argument :\n-           kernel_arguments) {\n-        if (&kernel_argument != &other_kernel_argument &&\n+    kernel_argument.written = buffers_written.contains(kernel_argument.slice);\n+\n+    kernel_argument.aliased = kernel_argument.written && [&] {\n+      for (size_t j = 0; j < kernel_arguments.size(); ++j) {\n+        const ReusableKernelArgument& other_kernel_argument =\n+            kernel_arguments[j];\n+        if (i != j && kernel_argument.slice != other_kernel_argument.slice &&\n             kernel_argument.slice.OverlapsWith(other_kernel_argument.slice)) {\n           return true;\n         }\n       }\n       return false;\n     }();\n-\n-    kernel_argument.written = buffers_written.contains(kernel_argument.slice);\n   }\n \n   return kernel_arguments;\n@@ -3499,6 +3549,11 @@ std::string IrEmitterUnnested::GetArgumentFingerprint(\n     absl::Span<const ReusableKernelArgument> kernel_arguments) {\n   return absl::StrJoin(kernel_arguments, \",\",\n                        [](std::string* s, const ReusableKernelArgument& arg) {\n+                         if (arg.first_with_same_slice.has_value()) {\n+                           absl::StrAppend(s, \"=\",\n+                                           arg.first_with_same_slice.value());\n+                           return;\n+                         }\n                          absl::StrAppend(s, arg.alignment);\n                          if (arg.aliased) {\n                            absl::StrAppend(s, \"a\");\n@@ -3518,8 +3573,9 @@ std::string IrEmitterUnnested::GetFingerprint(\n   //\n   // It is not a problem to recursively print subcomputations, because we don't\n   // have them at this point.\n-  auto print_options =\n-      HloPrintOptions::Fingerprint().set_print_only_essential_constants(false);\n+  auto print_options = HloPrintOptions::Fingerprint()\n+                           .set_print_only_essential_constants(false)\n+                           .set_print_operand_shape(false);\n \n   return absl::StrCat(discriminator, \"(\",\n                       GetArgumentFingerprint(kernel_arguments), \")\",\n@@ -3559,15 +3615,19 @@ StatusOr<ReusableKernelThunk*> IrEmitterUnnested::BuildReusableKernelThunkImpl(\n   std::vector<BufferAllocation::Slice> arg_slices;\n   arg_slices.reserve(kernel_arguments.size());\n   for (const auto& kernel_argument : kernel_arguments) {\n-    arg_slices.push_back(kernel_argument.slice);\n+    if (!kernel_argument.first_with_same_slice.has_value()) {\n+      arg_slices.push_back(kernel_argument.slice);\n+    }\n   }\n \n   std::vector<mlir::Value> values;\n   values.reserve(kernel_arguments.size());\n   for (const auto& kernel_argument : kernel_arguments) {\n-    TF_ASSIGN_OR_RETURN(mlir::Value value,\n-                        RemoveTransformingOperations(kernel_argument.value));\n-    values.push_back(value);\n+    if (!kernel_argument.first_with_same_slice.has_value()) {\n+      TF_ASSIGN_OR_RETURN(mlir::Value value,\n+                          RemoveTransformingOperations(kernel_argument.value));\n+      values.push_back(value);\n+    }\n   }\n \n   auto thunk_ptr = std::make_unique<ReusableKernelThunk>(\n@@ -3594,7 +3654,8 @@ IrEmitterUnnested::BuildReusableKernelThunk(\n                                           /*is_fusion=*/true));\n   std::string fingerprint =\n       GetFingerprint(fused_computation, kernel_arguments, discriminator);\n-  VLOG(4) << \"Fingerprint: \" << fingerprint;\n+  VLOG(4) << \"Fingerprint: \";\n+  XLA_VLOG_LINES(4, fingerprint);\n \n   auto cache_it = kernel_reuse_cache_.find(fingerprint);\n   if (cache_it != kernel_reuse_cache_.end()) {\n@@ -3607,7 +3668,7 @@ IrEmitterUnnested::BuildReusableKernelThunk(\n     // deduplicated.\n     // TODO(tdanyluk): Consider avoiding the recalculation of launch dimensions\n     // when reusing kernels.\n-    CHECK_EQ(old_thunk->launch_dimensions(), launch_dimensions);\n+    TF_RET_CHECK(old_thunk->launch_dimensions() == launch_dimensions);\n \n     // We are not reusing the ThunkInfo of the old thunk, because the current\n     // thunk info must reference the current HLO operation.\n@@ -3616,8 +3677,6 @@ IrEmitterUnnested::BuildReusableKernelThunk(\n                                      GetThunkInfo(fusion_op), launch_dimensions)\n             .status());\n \n-    b_.SetInsertPoint(b_.GetInsertBlock()->getTerminator());\n-\n     return {std::nullopt};\n   }\n \n"
        },
        {
            "name": "ir_emitter_unnested.h",
            "path": "tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.h",
            "patches": [
                {
                    "old_start": 384,
                    "old_length": 6,
                    "new_start": 384,
                    "new_length": 9,
                    "hunk": "@@ -384,6 +384,9 @@ class IrEmitterUnnested : public IrEmitter {\n     bool aliased = true;\n     int64_t alignment = 1;\n     bool written = true;\n+    // Holds the index of the first argument which has the same slice as this,\n+    // if this is not the first such argument.\n+    std::optional<int> first_with_same_slice;\n   };\n \n   // The return type of BuildReusableKernelPrototype.\n"
                },
                {
                    "old_start": 614,
                    "old_length": 11,
                    "new_start": 617,
                    "new_length": 11,
                    "hunk": "@@ -614,11 +617,11 @@ class IrEmitterUnnested : public IrEmitter {\n   //   }\n   // }\n   //\n-  void EmitTile(\n-      const TilingScheme& tiling_scheme,\n-      const llvm_ir::IrArray::Index& tile_origin_index,\n-      const ThreadIdInfo& thread_id_info, ValueVector2 tile_dimensions,\n-      const IrEmitterUnnested::EmitElementFunction& emit_elem_function);\n+  void EmitTile(const TilingScheme& tiling_scheme,\n+                const llvm_ir::IrArray::Index& tile_origin_index,\n+                const ThreadIdInfo& thread_id_info,\n+                ValueVector2 tile_dimensions,\n+                const EmitElementFunction& emit_elem_function);\n \n   // Creates accumulator alloca's, populates them with initial values, generates\n   // __shared__ caches and returns the populated object.\n"
                },
                {
                    "old_start": 641,
                    "old_length": 7,
                    "new_start": 644,
                    "new_length": 7,
                    "hunk": "@@ -641,7 +644,7 @@ class IrEmitterUnnested : public IrEmitter {\n       int partial_result_idx, llvm::Type* index_ty,\n       const ReductionCodegenState& reduction_codegen_state,\n       const TilingKernelInfo& tiling_kernel_info,\n-      const IrEmitterUnnested::ReductionOutputMap& output_arrays,\n+      const ReductionOutputMap& output_arrays,\n       const HloReduceInstruction* reduction, int output_idx);\n \n   // Performs the actual write of the reduction result.\n"
                },
                {
                    "old_start": 728,
                    "old_length": 14,
                    "new_start": 731,
                    "new_length": 21,
                    "hunk": "@@ -728,14 +731,21 @@ class IrEmitterUnnested : public IrEmitter {\n       mlir::Operation* op, const LaunchDimensions& launch_dimensions);\n \n   // Generates the argument descriptors for a \"reusable kernel\".\n-  StatusOr<std::vector<IrEmitterUnnested::ReusableKernelArgument>>\n-  GetReusableKernelArguments(mlir::lmhlo::FusionOp fusion_op);\n+  StatusOr<std::vector<ReusableKernelArgument>> GetReusableKernelArguments(\n+      mlir::lmhlo::FusionOp fusion_op);\n \n   // Calculates a fingerprint of the kernel arguments, which can be used for\n   // checking reusability.\n   //\n   // For example 2 arguments that are aligned to 16 bytes, aliased and also\n   // written by the kernel will be represented as \"16aw,16aw\".\n+  //\n+  // Overlapping arguments are only marked aliased, if at least one of them is\n+  // written and their buffers are not exactly the same. If 2 arguments' buffers\n+  // are exactly the same, then they are not marked aliased, but marked as\n+  // duplicates, for example like this: \"16,=0,16w,=2\". The example means that\n+  // the 1st argument is the same as the 0th and the 3rd is the same as the 2nd.\n+  // These duplicated parameters are passed to the kernel only once.\n   static std::string GetArgumentFingerprint(\n       absl::Span<const ReusableKernelArgument> kernel_arguments);\n \n"
                }
            ],
            "whole_deleted": "-  void EmitTile(\n-      const TilingScheme& tiling_scheme,\n-      const llvm_ir::IrArray::Index& tile_origin_index,\n-      const ThreadIdInfo& thread_id_info, ValueVector2 tile_dimensions,\n-      const IrEmitterUnnested::EmitElementFunction& emit_elem_function);\n-      const IrEmitterUnnested::ReductionOutputMap& output_arrays,\n-  StatusOr<std::vector<IrEmitterUnnested::ReusableKernelArgument>>\n-  GetReusableKernelArguments(mlir::lmhlo::FusionOp fusion_op);\n",
            "whole_added": "+    // Holds the index of the first argument which has the same slice as this,\n+    // if this is not the first such argument.\n+    std::optional<int> first_with_same_slice;\n+  void EmitTile(const TilingScheme& tiling_scheme,\n+                const llvm_ir::IrArray::Index& tile_origin_index,\n+                const ThreadIdInfo& thread_id_info,\n+                ValueVector2 tile_dimensions,\n+                const EmitElementFunction& emit_elem_function);\n+      const ReductionOutputMap& output_arrays,\n+  StatusOr<std::vector<ReusableKernelArgument>> GetReusableKernelArguments(\n+      mlir::lmhlo::FusionOp fusion_op);\n+  //\n+  // Overlapping arguments are only marked aliased, if at least one of them is\n+  // written and their buffers are not exactly the same. If 2 arguments' buffers\n+  // are exactly the same, then they are not marked aliased, but marked as\n+  // duplicates, for example like this: \"16,=0,16w,=2\". The example means that\n+  // the 1st argument is the same as the 0th and the 3rd is the same as the 2nd.\n+  // These duplicated parameters are passed to the kernel only once.\n",
            "whole_hunk": "@@ -384,6 +384,9 @@ class IrEmitterUnnested : public IrEmitter {\n     bool aliased = true;\n     int64_t alignment = 1;\n     bool written = true;\n+    // Holds the index of the first argument which has the same slice as this,\n+    // if this is not the first such argument.\n+    std::optional<int> first_with_same_slice;\n   };\n \n   // The return type of BuildReusableKernelPrototype.\n@@ -614,11 +617,11 @@ class IrEmitterUnnested : public IrEmitter {\n   //   }\n   // }\n   //\n-  void EmitTile(\n-      const TilingScheme& tiling_scheme,\n-      const llvm_ir::IrArray::Index& tile_origin_index,\n-      const ThreadIdInfo& thread_id_info, ValueVector2 tile_dimensions,\n-      const IrEmitterUnnested::EmitElementFunction& emit_elem_function);\n+  void EmitTile(const TilingScheme& tiling_scheme,\n+                const llvm_ir::IrArray::Index& tile_origin_index,\n+                const ThreadIdInfo& thread_id_info,\n+                ValueVector2 tile_dimensions,\n+                const EmitElementFunction& emit_elem_function);\n \n   // Creates accumulator alloca's, populates them with initial values, generates\n   // __shared__ caches and returns the populated object.\n@@ -641,7 +644,7 @@ class IrEmitterUnnested : public IrEmitter {\n       int partial_result_idx, llvm::Type* index_ty,\n       const ReductionCodegenState& reduction_codegen_state,\n       const TilingKernelInfo& tiling_kernel_info,\n-      const IrEmitterUnnested::ReductionOutputMap& output_arrays,\n+      const ReductionOutputMap& output_arrays,\n       const HloReduceInstruction* reduction, int output_idx);\n \n   // Performs the actual write of the reduction result.\n@@ -728,14 +731,21 @@ class IrEmitterUnnested : public IrEmitter {\n       mlir::Operation* op, const LaunchDimensions& launch_dimensions);\n \n   // Generates the argument descriptors for a \"reusable kernel\".\n-  StatusOr<std::vector<IrEmitterUnnested::ReusableKernelArgument>>\n-  GetReusableKernelArguments(mlir::lmhlo::FusionOp fusion_op);\n+  StatusOr<std::vector<ReusableKernelArgument>> GetReusableKernelArguments(\n+      mlir::lmhlo::FusionOp fusion_op);\n \n   // Calculates a fingerprint of the kernel arguments, which can be used for\n   // checking reusability.\n   //\n   // For example 2 arguments that are aligned to 16 bytes, aliased and also\n   // written by the kernel will be represented as \"16aw,16aw\".\n+  //\n+  // Overlapping arguments are only marked aliased, if at least one of them is\n+  // written and their buffers are not exactly the same. If 2 arguments' buffers\n+  // are exactly the same, then they are not marked aliased, but marked as\n+  // duplicates, for example like this: \"16,=0,16w,=2\". The example means that\n+  // the 1st argument is the same as the 0th and the 3rd is the same as the 2nd.\n+  // These duplicated parameters are passed to the kernel only once.\n   static std::string GetArgumentFingerprint(\n       absl::Span<const ReusableKernelArgument> kernel_arguments);\n \n"
        },
        {
            "name": "gpu_noalias_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/tests/gpu_noalias_test.cc",
            "patches": [
                {
                    "old_start": 49,
                    "old_length": 15,
                    "new_start": 49,
                    "new_length": 12,
                    "hunk": "@@ -49,15 +49,12 @@ TEST_F(GpuNoAliasTest, Concat) {\n   auto hlo_module = CreateNewVerifiedModule();\n   hlo_module->AddEntryComputation(std::move(computation));\n \n-  // After optimizations we have \"concatenate(x, y, x)\".\n-  //\n-  // The kernel has these parameters (with different names): (x, y, x, output).\n-  //\n-  // This means that the 1st and 3rd parameters will be aliased and the others\n-  // will be \"noalias\".\n+  // - After optimizations we have \"concatenate(x, y, x)\".\n+  // - We only pass the same parameters once, so the kernel will have these\n+  // parameters: (x, y, output), and all of them will be noalias.\n   CompileAndVerifyIr(\n       std::move(hlo_module),\n-      R\"(CHECK: define{{.*}}void @{{[a-zA-Z0-9_]+}}(ptr align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}, ptr noalias align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}, ptr align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}, ptr noalias align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}))\",\n+      R\"(CHECK: define void @{{[a-zA-Z0-9_]+}}(ptr noalias align 16 dereferenceable(16) %arg0, ptr noalias align 16 dereferenceable(16) %arg1, ptr noalias align 128 dereferenceable(48) %arg2))\",\n       /*match_optimized_ir=*/false);\n }\n \n"
                }
            ],
            "whole_deleted": "-  // After optimizations we have \"concatenate(x, y, x)\".\n-  //\n-  // The kernel has these parameters (with different names): (x, y, x, output).\n-  //\n-  // This means that the 1st and 3rd parameters will be aliased and the others\n-  // will be \"noalias\".\n-      R\"(CHECK: define{{.*}}void @{{[a-zA-Z0-9_]+}}(ptr align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}, ptr noalias align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}, ptr align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}, ptr noalias align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}))\",\n",
            "whole_added": "+  // - After optimizations we have \"concatenate(x, y, x)\".\n+  // - We only pass the same parameters once, so the kernel will have these\n+  // parameters: (x, y, output), and all of them will be noalias.\n+      R\"(CHECK: define void @{{[a-zA-Z0-9_]+}}(ptr noalias align 16 dereferenceable(16) %arg0, ptr noalias align 16 dereferenceable(16) %arg1, ptr noalias align 128 dereferenceable(48) %arg2))\",\n",
            "whole_hunk": "@@ -49,15 +49,12 @@ TEST_F(GpuNoAliasTest, Concat) {\n   auto hlo_module = CreateNewVerifiedModule();\n   hlo_module->AddEntryComputation(std::move(computation));\n \n-  // After optimizations we have \"concatenate(x, y, x)\".\n-  //\n-  // The kernel has these parameters (with different names): (x, y, x, output).\n-  //\n-  // This means that the 1st and 3rd parameters will be aliased and the others\n-  // will be \"noalias\".\n+  // - After optimizations we have \"concatenate(x, y, x)\".\n+  // - We only pass the same parameters once, so the kernel will have these\n+  // parameters: (x, y, output), and all of them will be noalias.\n   CompileAndVerifyIr(\n       std::move(hlo_module),\n-      R\"(CHECK: define{{.*}}void @{{[a-zA-Z0-9_]+}}(ptr align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}, ptr noalias align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}, ptr align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}, ptr noalias align {{[0-9]*}} dereferenceable({{[0-9]*}}) %{{.*}}))\",\n+      R\"(CHECK: define void @{{[a-zA-Z0-9_]+}}(ptr noalias align 16 dereferenceable(16) %arg0, ptr noalias align 16 dereferenceable(16) %arg1, ptr noalias align 128 dereferenceable(48) %arg2))\",\n       /*match_optimized_ir=*/false);\n }\n \n"
        },
        {
            "name": "gpu_unrolling_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/tests/gpu_unrolling_test.cc",
            "patches": [
                {
                    "old_start": 178,
                    "old_length": 12,
                    "new_start": 178,
                    "new_length": 11,
                    "hunk": "@@ -178,12 +178,11 @@ TEST_F(GpuUnrollingTest, DisabledUnrollUnfusedPower) {\n   auto hlo_module =\n       ParseAndReturnVerifiedModule(kUnfusedAddModule, config).value();\n \n-  // There are 2 loads, because the 2 parameters are read separately - the\n-  // kernel is not aware that they are the same.\n+  // There is only 1 load, because we pass the `p0` parameter to the kernel only\n+  // once.\n   CompileAndVerifyIr(std::move(hlo_module),\n                      R\"(\n ; CHECK: load float\n-; CHECK: load float\n ; CHECK-NOT: load float\n ; CHECK: }\n       )\",\n"
                },
                {
                    "old_start": 205,
                    "old_length": 12,
                    "new_start": 204,
                    "new_length": 11,
                    "hunk": "@@ -205,12 +204,11 @@ TEST_F(GpuUnrollingTest, DisabledUnrollUnfusedAtan2) {\n   auto hlo_module =\n       ParseAndReturnVerifiedModule(kUnfusedAddModule, config).value();\n \n-  // There are 2 loads, because the 2 parameters are read separately - the\n-  // kernel is not aware that they are the same.\n+  // There is only 1 load, because we pass the `p0` parameter to the kernel only\n+  // once.\n   CompileAndVerifyIr(std::move(hlo_module),\n                      R\"(\n ; CHECK: load float\n-; CHECK: load float\n ; CHECK-NOT: load float\n ; CHECK: }\n       )\",\n"
                }
            ],
            "whole_deleted": "-  // There are 2 loads, because the 2 parameters are read separately - the\n-  // kernel is not aware that they are the same.\n-; CHECK: load float\n-  // There are 2 loads, because the 2 parameters are read separately - the\n-  // kernel is not aware that they are the same.\n-; CHECK: load float\n",
            "whole_added": "+  // There is only 1 load, because we pass the `p0` parameter to the kernel only\n+  // once.\n+  // There is only 1 load, because we pass the `p0` parameter to the kernel only\n+  // once.\n",
            "whole_hunk": "@@ -178,12 +178,11 @@ TEST_F(GpuUnrollingTest, DisabledUnrollUnfusedPower) {\n   auto hlo_module =\n       ParseAndReturnVerifiedModule(kUnfusedAddModule, config).value();\n \n-  // There are 2 loads, because the 2 parameters are read separately - the\n-  // kernel is not aware that they are the same.\n+  // There is only 1 load, because we pass the `p0` parameter to the kernel only\n+  // once.\n   CompileAndVerifyIr(std::move(hlo_module),\n                      R\"(\n ; CHECK: load float\n-; CHECK: load float\n ; CHECK-NOT: load float\n ; CHECK: }\n       )\",\n@@ -205,12 +204,11 @@ TEST_F(GpuUnrollingTest, DisabledUnrollUnfusedAtan2) {\n   auto hlo_module =\n       ParseAndReturnVerifiedModule(kUnfusedAddModule, config).value();\n \n-  // There are 2 loads, because the 2 parameters are read separately - the\n-  // kernel is not aware that they are the same.\n+  // There is only 1 load, because we pass the `p0` parameter to the kernel only\n+  // once.\n   CompileAndVerifyIr(std::move(hlo_module),\n                      R\"(\n ; CHECK: load float\n-; CHECK: load float\n ; CHECK-NOT: load float\n ; CHECK: }\n       )\",\n"
        },
        {
            "name": "kernel_reuse.hlo",
            "path": "tensorflow/compiler/xla/service/gpu/tests/kernel_reuse.hlo",
            "patches": [
                {
                    "old_start": 87,
                    "old_length": 9,
                    "new_start": 87,
                    "new_length": 10,
                    "hunk": "@@ -87,9 +87,10 @@ ENTRY main {\n // -----\n \n // We need 2 different kernels:\n-// The first fusion call is aliased, the others are not.\n+// The first has just 2 parameters (1 input, 1 output) and the second has 3 (2 input, 1 output).\n+// All the parameters are noalias, because we are not passing the same argument twice to the kernel.\n // CHECK-LABEL: target triple\n-// CHECK: define void @fusion_2(ptr align 128 dereferenceable(100) %arg0, ptr align 128 dereferenceable(100) %arg1, ptr noalias align 128 dereferenceable(100) %arg2) {\n+// CHECK: define void @fusion_2(ptr noalias align 128 dereferenceable(100) %arg0, ptr noalias align 128 dereferenceable(100) %arg1) {\n // CHECK: define void @fusion_1(ptr noalias align 128 dereferenceable(100) %arg0, ptr noalias align 128 dereferenceable(100) %arg1, ptr noalias align 128 dereferenceable(100) %arg2) {\n // CHECK-NOT: define void\n \n"
                },
                {
                    "old_start": 133,
                    "old_length": 7,
                    "new_start": 134,
                    "new_length": 7,
                    "hunk": "@@ -133,7 +134,7 @@ ENTRY main {\n // \"!invariant.load\" (thanks to ir_array.MarkInvariantOverWholeProgram).\n //\n // CHECK-LABEL: target triple\n-// CHECK: define void @fusion_2(ptr align 128 dereferenceable(100) %arg0, ptr align 128 dereferenceable(100) %arg1) {\n+// CHECK: define void @fusion_2(ptr noalias align 128 dereferenceable(100) %arg0) {\n // CHECK-NOT: !invariant.load\n // CHECK: define void @fusion(ptr noalias align 128 dereferenceable(100) %arg0, ptr noalias align 128 dereferenceable(100) %arg1) {\n // CHECK-NOT: define void\n"
                },
                {
                    "old_start": 170,
                    "old_length": 4,
                    "new_start": 171,
                    "new_length": 6,
                    "hunk": "@@ -170,4 +171,6 @@ ENTRY main {\n   ROOT tuple = (f32[5,5]{1,0}, f32[5,5]{1,0}, f32[5,5]{1,0}) tuple(custom-call.1, custom-call.2, custom-call.3)\n }\n \n-\n+// We don't have a test case for aliasing, because it is hard or impossible to\n+// create a situation when parameters are aliased, but not the same and at least\n+// one of them is written."
                }
            ],
            "whole_deleted": "-// The first fusion call is aliased, the others are not.\n-// CHECK: define void @fusion_2(ptr align 128 dereferenceable(100) %arg0, ptr align 128 dereferenceable(100) %arg1, ptr noalias align 128 dereferenceable(100) %arg2) {\n-// CHECK: define void @fusion_2(ptr align 128 dereferenceable(100) %arg0, ptr align 128 dereferenceable(100) %arg1) {\n-\n",
            "whole_added": "+// The first has just 2 parameters (1 input, 1 output) and the second has 3 (2 input, 1 output).\n+// All the parameters are noalias, because we are not passing the same argument twice to the kernel.\n+// CHECK: define void @fusion_2(ptr noalias align 128 dereferenceable(100) %arg0, ptr noalias align 128 dereferenceable(100) %arg1) {\n+// CHECK: define void @fusion_2(ptr noalias align 128 dereferenceable(100) %arg0) {\n+// We don't have a test case for aliasing, because it is hard or impossible to\n+// create a situation when parameters are aliased, but not the same and at least\n+// one of them is written.\n",
            "whole_hunk": "@@ -87,9 +87,10 @@ ENTRY main {\n // -----\n \n // We need 2 different kernels:\n-// The first fusion call is aliased, the others are not.\n+// The first has just 2 parameters (1 input, 1 output) and the second has 3 (2 input, 1 output).\n+// All the parameters are noalias, because we are not passing the same argument twice to the kernel.\n // CHECK-LABEL: target triple\n-// CHECK: define void @fusion_2(ptr align 128 dereferenceable(100) %arg0, ptr align 128 dereferenceable(100) %arg1, ptr noalias align 128 dereferenceable(100) %arg2) {\n+// CHECK: define void @fusion_2(ptr noalias align 128 dereferenceable(100) %arg0, ptr noalias align 128 dereferenceable(100) %arg1) {\n // CHECK: define void @fusion_1(ptr noalias align 128 dereferenceable(100) %arg0, ptr noalias align 128 dereferenceable(100) %arg1, ptr noalias align 128 dereferenceable(100) %arg2) {\n // CHECK-NOT: define void\n \n@@ -133,7 +134,7 @@ ENTRY main {\n // \"!invariant.load\" (thanks to ir_array.MarkInvariantOverWholeProgram).\n //\n // CHECK-LABEL: target triple\n-// CHECK: define void @fusion_2(ptr align 128 dereferenceable(100) %arg0, ptr align 128 dereferenceable(100) %arg1) {\n+// CHECK: define void @fusion_2(ptr noalias align 128 dereferenceable(100) %arg0) {\n // CHECK-NOT: !invariant.load\n // CHECK: define void @fusion(ptr noalias align 128 dereferenceable(100) %arg0, ptr noalias align 128 dereferenceable(100) %arg1) {\n // CHECK-NOT: define void\n@@ -170,4 +171,6 @@ ENTRY main {\n   ROOT tuple = (f32[5,5]{1,0}, f32[5,5]{1,0}, f32[5,5]{1,0}) tuple(custom-call.1, custom-call.2, custom-call.3)\n }\n \n-\n+// We don't have a test case for aliasing, because it is hard or impossible to\n+// create a situation when parameters are aliased, but not the same and at least\n+// one of them is written."
        }
    ]
},
{
    "Id": 215,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/cd93d80b50aaf728bb3af862134a9c4b6661e284",
    "date": "2024-01-02T13:56:17-08:00",
    "message": "Replaced errors::* with absl::* to pass code checks",
    "label": "NO",
    "changes": [
        {
            "name": "control_flow_ops.cc",
            "path": "tensorflow/core/kernels/control_flow_ops.cc",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 9,
                    "new_start": 26,
                    "new_length": 10,
                    "hunk": "@@ -26,9 +26,10 @@ namespace tensorflow {\n void SwitchOp::Compute(OpKernelContext* context) {\n   const Tensor& outputPorts = context->input(1);\n   OP_REQUIRES(context, TensorShapeUtils::IsScalar(outputPorts.shape()),\n-              errors::InvalidArgument(\"The second input must be a scalar, \"\n-                                      \"but it has shape \",\n-                                      outputPorts.shape().DebugString()));\n+              absl::InvalidArgumentError(\n+                  absl::StrCat(\"The second input must be a scalar, \"\n+                               \"but it has shape \",\n+                               outputPorts.shape().DebugString())));\n \n   bool pred = outputPorts.scalar<bool>()();\n   int port = (pred) ? 1 : 0;\n"
                },
                {
                    "old_start": 42,
                    "old_length": 9,
                    "new_start": 43,
                    "new_length": 10,
                    "hunk": "@@ -42,9 +43,10 @@ void SwitchOp::Compute(OpKernelContext* context) {\n void SwitchNOp::Compute(OpKernelContext* context) {\n   const Tensor& output_index_t = context->input(1);\n   OP_REQUIRES(context, TensorShapeUtils::IsScalar(output_index_t.shape()),\n-              errors::InvalidArgument(\"The second input must be a scalar, \"\n-                                      \"but it has shape \",\n-                                      output_index_t.shape().DebugString()));\n+              absl::InvalidArgumentError(\n+                  absl::StrCat(\"The second input must be a scalar, \"\n+                               \"but it has shape \",\n+                               output_index_t.shape().DebugString())));\n   int output_index = output_index_t.scalar<int>()();\n   if (output_index < 0 || output_index >= num_outputs()) {\n     output_index = num_outputs() - 1;\n"
                },
                {
                    "old_start": 231,
                    "old_length": 15,
                    "new_start": 233,
                    "new_length": 17,
                    "hunk": "@@ -231,15 +233,17 @@ class RefSelectOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& index_tensor = context->input(0);\n     OP_REQUIRES(context, TensorShapeUtils::IsScalar(index_tensor.shape()),\n-                errors::InvalidArgument(\"Index must be a scalar, \"\n-                                        \"but it has shape \",\n-                                        index_tensor.shape().DebugString()));\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"Index must be a scalar, \"\n+                                 \"but it has shape \",\n+                                 index_tensor.shape().DebugString())));\n \n     int32_t index = index_tensor.scalar<int32>()();\n \n     OP_REQUIRES(context, index >= 0 && index < num_ref_inputs_,\n-                errors::InvalidArgument(\"Index must be in the range [0, \",\n-                                        num_ref_inputs_, \") but got \", index));\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"Index must be in the range [0, \",\n+                                 num_ref_inputs_, \") but got \", index)));\n     context->forward_ref_input_to_ref_output(index + 1, 0);\n   }\n \n"
                },
                {
                    "old_start": 692,
                    "old_length": 7,
                    "new_start": 696,
                    "new_length": 7,
                    "hunk": "@@ -692,7 +696,7 @@ void LoopCondOp::Compute(OpKernelContext* context) {\n   if (cm != nullptr) {\n     bool already_cancelled = cm->IsCancelled();\n     OP_REQUIRES(context, !already_cancelled,\n-                errors::Cancelled(\"Loop execution was cancelled.\"));\n+                absl::CancelledError(\"Loop execution was cancelled.\"));\n   }\n \n   context->set_output(0, context->input(0));"
                }
            ],
            "whole_deleted": "-              errors::InvalidArgument(\"The second input must be a scalar, \"\n-                                      \"but it has shape \",\n-                                      outputPorts.shape().DebugString()));\n-              errors::InvalidArgument(\"The second input must be a scalar, \"\n-                                      \"but it has shape \",\n-                                      output_index_t.shape().DebugString()));\n-                errors::InvalidArgument(\"Index must be a scalar, \"\n-                                        \"but it has shape \",\n-                                        index_tensor.shape().DebugString()));\n-                errors::InvalidArgument(\"Index must be in the range [0, \",\n-                                        num_ref_inputs_, \") but got \", index));\n-                errors::Cancelled(\"Loop execution was cancelled.\"));\n",
            "whole_added": "+              absl::InvalidArgumentError(\n+                  absl::StrCat(\"The second input must be a scalar, \"\n+                               \"but it has shape \",\n+                               outputPorts.shape().DebugString())));\n+              absl::InvalidArgumentError(\n+                  absl::StrCat(\"The second input must be a scalar, \"\n+                               \"but it has shape \",\n+                               output_index_t.shape().DebugString())));\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"Index must be a scalar, \"\n+                                 \"but it has shape \",\n+                                 index_tensor.shape().DebugString())));\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"Index must be in the range [0, \",\n+                                 num_ref_inputs_, \") but got \", index)));\n+                absl::CancelledError(\"Loop execution was cancelled.\"));\n",
            "whole_hunk": "@@ -26,9 +26,10 @@ namespace tensorflow {\n void SwitchOp::Compute(OpKernelContext* context) {\n   const Tensor& outputPorts = context->input(1);\n   OP_REQUIRES(context, TensorShapeUtils::IsScalar(outputPorts.shape()),\n-              errors::InvalidArgument(\"The second input must be a scalar, \"\n-                                      \"but it has shape \",\n-                                      outputPorts.shape().DebugString()));\n+              absl::InvalidArgumentError(\n+                  absl::StrCat(\"The second input must be a scalar, \"\n+                               \"but it has shape \",\n+                               outputPorts.shape().DebugString())));\n \n   bool pred = outputPorts.scalar<bool>()();\n   int port = (pred) ? 1 : 0;\n@@ -42,9 +43,10 @@ void SwitchOp::Compute(OpKernelContext* context) {\n void SwitchNOp::Compute(OpKernelContext* context) {\n   const Tensor& output_index_t = context->input(1);\n   OP_REQUIRES(context, TensorShapeUtils::IsScalar(output_index_t.shape()),\n-              errors::InvalidArgument(\"The second input must be a scalar, \"\n-                                      \"but it has shape \",\n-                                      output_index_t.shape().DebugString()));\n+              absl::InvalidArgumentError(\n+                  absl::StrCat(\"The second input must be a scalar, \"\n+                               \"but it has shape \",\n+                               output_index_t.shape().DebugString())));\n   int output_index = output_index_t.scalar<int>()();\n   if (output_index < 0 || output_index >= num_outputs()) {\n     output_index = num_outputs() - 1;\n@@ -231,15 +233,17 @@ class RefSelectOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& index_tensor = context->input(0);\n     OP_REQUIRES(context, TensorShapeUtils::IsScalar(index_tensor.shape()),\n-                errors::InvalidArgument(\"Index must be a scalar, \"\n-                                        \"but it has shape \",\n-                                        index_tensor.shape().DebugString()));\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"Index must be a scalar, \"\n+                                 \"but it has shape \",\n+                                 index_tensor.shape().DebugString())));\n \n     int32_t index = index_tensor.scalar<int32>()();\n \n     OP_REQUIRES(context, index >= 0 && index < num_ref_inputs_,\n-                errors::InvalidArgument(\"Index must be in the range [0, \",\n-                                        num_ref_inputs_, \") but got \", index));\n+                absl::InvalidArgumentError(\n+                    absl::StrCat(\"Index must be in the range [0, \",\n+                                 num_ref_inputs_, \") but got \", index)));\n     context->forward_ref_input_to_ref_output(index + 1, 0);\n   }\n \n@@ -692,7 +696,7 @@ void LoopCondOp::Compute(OpKernelContext* context) {\n   if (cm != nullptr) {\n     bool already_cancelled = cm->IsCancelled();\n     OP_REQUIRES(context, !already_cancelled,\n-                errors::Cancelled(\"Loop execution was cancelled.\"));\n+                absl::CancelledError(\"Loop execution was cancelled.\"));\n   }\n \n   context->set_output(0, context->input(0));"
        }
    ]
},
{
    "Id": 158,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/6045ccea11b31174d5d9b147fddedf1b5f5af384",
    "date": "2024-02-09T18:00:51-08:00",
    "message": "Add utility functions to check if a fusion has collective ops inside recursively.\n\nPiperOrigin-RevId: 605777706",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/BUILD",
            "patches": [
                {
                    "old_start": 6846,
                    "old_length": 10,
                    "new_start": 6846,
                    "new_length": 15,
                    "hunk": "@@ -6846,10 +6846,15 @@ xla_cc_test(\n         \":collective_ops_utils\",\n         \":computation_placer\",\n         \":global_device_id\",\n+        \":hlo_parser\",\n+        \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@local_tsl//tsl/lib/core:status_test_util\",\n+        \"@local_tsl//tsl/platform:statusor\",\n         \"@local_tsl//tsl/platform:test\",\n     ],\n )\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \":hlo_parser\",\n+        \"//xla:shape_util\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@local_tsl//tsl/platform:statusor\",\n",
            "whole_hunk": "@@ -6846,10 +6846,15 @@ xla_cc_test(\n         \":collective_ops_utils\",\n         \":computation_placer\",\n         \":global_device_id\",\n+        \":hlo_parser\",\n+        \"//xla:shape_util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n         \"//xla/tests:xla_internal_test_main\",\n         \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/strings:string_view\",\n         \"@local_tsl//tsl/lib/core:status_test_util\",\n+        \"@local_tsl//tsl/platform:statusor\",\n         \"@local_tsl//tsl/platform:test\",\n     ],\n )\n"
        },
        {
            "name": "collective_ops_utils.cc",
            "path": "third_party/xla/xla/service/collective_ops_utils.cc",
            "patches": [
                {
                    "old_start": 602,
                    "old_length": 6,
                    "new_start": 602,
                    "new_length": 28,
                    "hunk": "@@ -602,6 +602,28 @@ bool IsCollective(const HloInstruction* instruction) {\n   }\n }\n \n+bool IsCollectiveWithChannelId(const HloInstruction* instruction) {\n+  switch (instruction->opcode()) {\n+    case HloOpcode::kAllReduce:\n+    case HloOpcode::kAllReduceStart:\n+    case HloOpcode::kAllGather:\n+    case HloOpcode::kAllGatherStart:\n+    case HloOpcode::kAllToAll:\n+    case HloOpcode::kCollectivePermute:\n+    case HloOpcode::kCollectivePermuteStart:\n+      return instruction->channel_id().has_value();\n+    case HloOpcode::kFusion:\n+      for (const auto* inner_inst : instruction->fused_instructions()) {\n+        if (IsCollectiveWithChannelId(inner_inst)) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    default:\n+      return false;\n+  }\n+}\n+\n bool IsSyncCollective(const HloInstruction* instr) {\n   auto backend_config = instr->backend_config<xla::gpu::GpuBackendConfig>();\n   if (!backend_config.ok()) {\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+bool IsCollectiveWithChannelId(const HloInstruction* instruction) {\n+  switch (instruction->opcode()) {\n+    case HloOpcode::kAllReduce:\n+    case HloOpcode::kAllReduceStart:\n+    case HloOpcode::kAllGather:\n+    case HloOpcode::kAllGatherStart:\n+    case HloOpcode::kAllToAll:\n+    case HloOpcode::kCollectivePermute:\n+    case HloOpcode::kCollectivePermuteStart:\n+      return instruction->channel_id().has_value();\n+    case HloOpcode::kFusion:\n+      for (const auto* inner_inst : instruction->fused_instructions()) {\n+        if (IsCollectiveWithChannelId(inner_inst)) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    default:\n+      return false;\n+  }\n+}\n+\n",
            "whole_hunk": "@@ -602,6 +602,28 @@ bool IsCollective(const HloInstruction* instruction) {\n   }\n }\n \n+bool IsCollectiveWithChannelId(const HloInstruction* instruction) {\n+  switch (instruction->opcode()) {\n+    case HloOpcode::kAllReduce:\n+    case HloOpcode::kAllReduceStart:\n+    case HloOpcode::kAllGather:\n+    case HloOpcode::kAllGatherStart:\n+    case HloOpcode::kAllToAll:\n+    case HloOpcode::kCollectivePermute:\n+    case HloOpcode::kCollectivePermuteStart:\n+      return instruction->channel_id().has_value();\n+    case HloOpcode::kFusion:\n+      for (const auto* inner_inst : instruction->fused_instructions()) {\n+        if (IsCollectiveWithChannelId(inner_inst)) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    default:\n+      return false;\n+  }\n+}\n+\n bool IsSyncCollective(const HloInstruction* instr) {\n   auto backend_config = instr->backend_config<xla::gpu::GpuBackendConfig>();\n   if (!backend_config.ok()) {\n"
        },
        {
            "name": "collective_ops_utils.h",
            "path": "third_party/xla/xla/service/collective_ops_utils.h",
            "patches": [
                {
                    "old_start": 172,
                    "old_length": 6,
                    "new_start": 172,
                    "new_length": 10,
                    "hunk": "@@ -172,6 +172,10 @@ inline constexpr absl::string_view kNopReturnTokenCustomCallTarget =\n // Returns true if instruction is a collective op or a collective fusion.\n bool IsCollective(const HloInstruction* instruction);\n \n+// Returns true if instruction is a collective op (or a collective fusion) with\n+// channel_id.\n+bool IsCollectiveWithChannelId(const HloInstruction* instruction);\n+\n // Returns true if instruction is a synchronous collective op.\n bool IsSyncCollective(const HloInstruction* instr);\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// Returns true if instruction is a collective op (or a collective fusion) with\n+// channel_id.\n+bool IsCollectiveWithChannelId(const HloInstruction* instruction);\n+\n",
            "whole_hunk": "@@ -172,6 +172,10 @@ inline constexpr absl::string_view kNopReturnTokenCustomCallTarget =\n // Returns true if instruction is a collective op or a collective fusion.\n bool IsCollective(const HloInstruction* instruction);\n \n+// Returns true if instruction is a collective op (or a collective fusion) with\n+// channel_id.\n+bool IsCollectiveWithChannelId(const HloInstruction* instruction);\n+\n // Returns true if instruction is a synchronous collective op.\n bool IsSyncCollective(const HloInstruction* instr);\n \n"
        },
        {
            "name": "collective_ops_utils_test.cc",
            "path": "third_party/xla/xla/service/collective_ops_utils_test.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/service/collective_ops_utils.h\"\n \n+#include <cstdint>\n #include <iterator>\n #include <optional>\n #include <sstream>\n"
                },
                {
                    "old_start": 22,
                    "old_length": 10,
                    "new_start": 23,
                    "new_length": 16,
                    "hunk": "@@ -22,10 +23,16 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/global_device_id.h\"\n+#include \"xla/service/hlo_parser.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/lib/core/status_test_util.h\"\n+#include \"tsl/platform/statusor.h\"\n #include \"tsl/platform/test.h\"\n \n namespace xla {\n"
                },
                {
                    "old_start": 60,
                    "old_length": 6,
                    "new_start": 67,
                    "new_length": 52,
                    "hunk": "@@ -60,6 +67,52 @@ TEST(CollectiveOpsUtilsTest, GetParticipatingIDs_ReplicaGroups) {\n   EXPECT_EQ(actual, expected);\n }\n \n+TEST(CollectiveOpsUtilsTest, CollectiveWithChannelId) {\n+  absl::string_view hlo_string = R\"(\n+  HloModule module, is_scheduled=true\n+\n+  ENTRY %cluster {\n+    %param0 = f32[512]{0} parameter(0)\n+    %copy0 = f32[512]{0} copy(param0)\n+    %reshape0 = f32[1,1,512]{2,0,1} reshape(f32[512]{0} %copy0)\n+    %all-gather = f32[1,4,512]{2,0,1} all-gather(f32[1,1,512]{2,0,1} %reshape0), channel_id=3621, replica_groups={{0,1,2,3}}, dimensions={1}, use_global_device_ids=true\n+    %copy1 = f32[1,4,512]{2,0,1} copy(all-gather)\n+    ROOT root = f32[1,4,512]{2,1,0} copy(%copy1)\n+  })\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnUnverifiedModule(hlo_string));\n+\n+  HloInstruction *all_gather =\n+      module->entry_computation()->GetInstructionWithName(\"all-gather\");\n+\n+  EXPECT_TRUE(IsCollectiveWithChannelId(all_gather));\n+}\n+\n+TEST(CollectiveOpsUtilsTest, CollectiveWithChannelId2) {\n+  ReplicaGroup group;\n+  for (int64_t i = 0; i < 8; i++) {\n+    group.add_replica_ids(i);\n+  }\n+\n+  auto builder = HloComputation::Builder(\"CollectiveWithChannelId2\");\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      HloInstruction * param_0,\n+      builder.AddParameter(HloInstruction::CreateParameter(\n+          0, ShapeUtil::MakeShape(BF16, {1, 512, 4096}), \"p0\")));\n+  HloInstruction *instr =\n+      builder.AddInstruction(HloInstruction::CreateAllGather(\n+          ShapeUtil::MakeShape(BF16, {1, 4096, 4096}), {param_0}, 1, {group},\n+          true, 231, true));\n+  auto computation = builder.Build(\n+      builder.AddInstruction(HloInstruction::CreateTuple({instr})));\n+  auto fusion =\n+      HloInstruction::CreateFusion(ShapeUtil::MakeShape(BF16, {1, 4096, 4096}),\n+                                   HloInstruction::FusionKind::kOutput,\n+                                   {param_0}, computation.get(), \"fusion\");\n+\n+  EXPECT_TRUE(IsCollectiveWithChannelId(fusion.get()));\n+}\n+\n }  // namespace\n \n // Tests for GetCollectOpGroupMode"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <cstdint>\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/service/hlo_parser.h\"\n+#include \"xla/shape_util.h\"\n+#include \"tsl/platform/statusor.h\"\n+TEST(CollectiveOpsUtilsTest, CollectiveWithChannelId) {\n+  absl::string_view hlo_string = R\"(\n+  HloModule module, is_scheduled=true\n+\n+  ENTRY %cluster {\n+    %param0 = f32[512]{0} parameter(0)\n+    %copy0 = f32[512]{0} copy(param0)\n+    %reshape0 = f32[1,1,512]{2,0,1} reshape(f32[512]{0} %copy0)\n+    %all-gather = f32[1,4,512]{2,0,1} all-gather(f32[1,1,512]{2,0,1} %reshape0), channel_id=3621, replica_groups={{0,1,2,3}}, dimensions={1}, use_global_device_ids=true\n+    %copy1 = f32[1,4,512]{2,0,1} copy(all-gather)\n+    ROOT root = f32[1,4,512]{2,1,0} copy(%copy1)\n+  })\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnUnverifiedModule(hlo_string));\n+\n+  HloInstruction *all_gather =\n+      module->entry_computation()->GetInstructionWithName(\"all-gather\");\n+\n+  EXPECT_TRUE(IsCollectiveWithChannelId(all_gather));\n+}\n+\n+TEST(CollectiveOpsUtilsTest, CollectiveWithChannelId2) {\n+  ReplicaGroup group;\n+  for (int64_t i = 0; i < 8; i++) {\n+    group.add_replica_ids(i);\n+  }\n+\n+  auto builder = HloComputation::Builder(\"CollectiveWithChannelId2\");\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      HloInstruction * param_0,\n+      builder.AddParameter(HloInstruction::CreateParameter(\n+          0, ShapeUtil::MakeShape(BF16, {1, 512, 4096}), \"p0\")));\n+  HloInstruction *instr =\n+      builder.AddInstruction(HloInstruction::CreateAllGather(\n+          ShapeUtil::MakeShape(BF16, {1, 4096, 4096}), {param_0}, 1, {group},\n+          true, 231, true));\n+  auto computation = builder.Build(\n+      builder.AddInstruction(HloInstruction::CreateTuple({instr})));\n+  auto fusion =\n+      HloInstruction::CreateFusion(ShapeUtil::MakeShape(BF16, {1, 4096, 4096}),\n+                                   HloInstruction::FusionKind::kOutput,\n+                                   {param_0}, computation.get(), \"fusion\");\n+\n+  EXPECT_TRUE(IsCollectiveWithChannelId(fusion.get()));\n+}\n+\n",
            "whole_hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/service/collective_ops_utils.h\"\n \n+#include <cstdint>\n #include <iterator>\n #include <optional>\n #include <sstream>\n@@ -22,10 +23,16 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/global_device_id.h\"\n+#include \"xla/service/hlo_parser.h\"\n+#include \"xla/shape_util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/lib/core/status_test_util.h\"\n+#include \"tsl/platform/statusor.h\"\n #include \"tsl/platform/test.h\"\n \n namespace xla {\n@@ -60,6 +67,52 @@ TEST(CollectiveOpsUtilsTest, GetParticipatingIDs_ReplicaGroups) {\n   EXPECT_EQ(actual, expected);\n }\n \n+TEST(CollectiveOpsUtilsTest, CollectiveWithChannelId) {\n+  absl::string_view hlo_string = R\"(\n+  HloModule module, is_scheduled=true\n+\n+  ENTRY %cluster {\n+    %param0 = f32[512]{0} parameter(0)\n+    %copy0 = f32[512]{0} copy(param0)\n+    %reshape0 = f32[1,1,512]{2,0,1} reshape(f32[512]{0} %copy0)\n+    %all-gather = f32[1,4,512]{2,0,1} all-gather(f32[1,1,512]{2,0,1} %reshape0), channel_id=3621, replica_groups={{0,1,2,3}}, dimensions={1}, use_global_device_ids=true\n+    %copy1 = f32[1,4,512]{2,0,1} copy(all-gather)\n+    ROOT root = f32[1,4,512]{2,1,0} copy(%copy1)\n+  })\";\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnUnverifiedModule(hlo_string));\n+\n+  HloInstruction *all_gather =\n+      module->entry_computation()->GetInstructionWithName(\"all-gather\");\n+\n+  EXPECT_TRUE(IsCollectiveWithChannelId(all_gather));\n+}\n+\n+TEST(CollectiveOpsUtilsTest, CollectiveWithChannelId2) {\n+  ReplicaGroup group;\n+  for (int64_t i = 0; i < 8; i++) {\n+    group.add_replica_ids(i);\n+  }\n+\n+  auto builder = HloComputation::Builder(\"CollectiveWithChannelId2\");\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      HloInstruction * param_0,\n+      builder.AddParameter(HloInstruction::CreateParameter(\n+          0, ShapeUtil::MakeShape(BF16, {1, 512, 4096}), \"p0\")));\n+  HloInstruction *instr =\n+      builder.AddInstruction(HloInstruction::CreateAllGather(\n+          ShapeUtil::MakeShape(BF16, {1, 4096, 4096}), {param_0}, 1, {group},\n+          true, 231, true));\n+  auto computation = builder.Build(\n+      builder.AddInstruction(HloInstruction::CreateTuple({instr})));\n+  auto fusion =\n+      HloInstruction::CreateFusion(ShapeUtil::MakeShape(BF16, {1, 4096, 4096}),\n+                                   HloInstruction::FusionKind::kOutput,\n+                                   {param_0}, computation.get(), \"fusion\");\n+\n+  EXPECT_TRUE(IsCollectiveWithChannelId(fusion.get()));\n+}\n+\n }  // namespace\n \n // Tests for GetCollectOpGroupMode"
        }
    ]
},
{
    "Id": 10,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/140d4a5814d62e2640cf376ab1c3ca9c32d9182c",
    "date": "2024-07-03T14:28:15-07:00",
    "message": "[xla:ffi] NFC: Optimize arguments decoding check\n\nUse `&` instead of `&&` to reduce the number of branches as we don't care about performance of the error handling path.\n\nname                old cpu/op   new cpu/op   delta\nBM_AnyBufferArgX1   13.8ns \u00b111%  13.7ns \u00b1 9%     ~     (p=0.909 n=80+80)\nBM_AnyBufferArgX4   15.2ns \u00b1 8%  15.1ns \u00b1 8%     ~     (p=0.096 n=79+80)\nBM_AnyBufferArgX8   20.0ns \u00b1 3%  17.6ns \u00b1 9%  -11.82%  (p=0.000 n=64+80)\nBM_BufferArgX1      14.3ns \u00b111%  14.3ns \u00b110%     ~     (p=0.604 n=80+80)\nBM_BufferArgX4      16.5ns \u00b1 8%  16.5ns \u00b1 7%     ~     (p=0.289 n=80+80)\nBM_BufferArgX8      24.5ns \u00b1 8%  22.6ns \u00b1 7%   -7.57%  (p=0.000 n=80+80)\nBM_TupleOfI32Attrs  67.9ns \u00b1 1%  67.9ns \u00b1 2%     ~     (p=0.961 n=66+68)\n\nPiperOrigin-RevId: 649197305",
    "label": "NO",
    "changes": [
        {
            "name": "api.h",
            "path": "third_party/xla/xla/ffi/api/api.h",
            "patches": [
                {
                    "old_start": 1331,
                    "old_length": 10,
                    "new_start": 1331,
                    "new_length": 13,
                    "hunk": "@@ -1331,10 +1331,13 @@ class Handler : public Ffi {\n     std::tuple<std::optional<FnArgType<Ts>>...> args = {\n         internal::Decode<Ts>::call(offsets, ctx, diagnostic)...};\n \n-    bool all_decoded = (std::get<Is>(args).has_value() && ...);\n-    if (XLA_FFI_PREDICT_FALSE(!all_decoded)) {\n-      return FailedDecodeError(call_frame, {std::get<Is>(args).has_value()...},\n-                               diagnostic);\n+    if constexpr (sizeof...(Ts) > 0) {\n+      // We intentionally use `&`, as it generates fewer branch instructions.\n+      bool all_decoded = (std::get<Is>(args).has_value() & ...);\n+      if (XLA_FFI_PREDICT_FALSE(!all_decoded)) {\n+        return FailedDecodeError(\n+            call_frame, {std::get<Is>(args).has_value()...}, diagnostic);\n+      }\n     }\n \n     auto result = fn_(std::move(*std::get<Is>(args))...);"
                }
            ],
            "whole_deleted": "-    bool all_decoded = (std::get<Is>(args).has_value() && ...);\n-    if (XLA_FFI_PREDICT_FALSE(!all_decoded)) {\n-      return FailedDecodeError(call_frame, {std::get<Is>(args).has_value()...},\n-                               diagnostic);\n",
            "whole_added": "+    if constexpr (sizeof...(Ts) > 0) {\n+      // We intentionally use `&`, as it generates fewer branch instructions.\n+      bool all_decoded = (std::get<Is>(args).has_value() & ...);\n+      if (XLA_FFI_PREDICT_FALSE(!all_decoded)) {\n+        return FailedDecodeError(\n+            call_frame, {std::get<Is>(args).has_value()...}, diagnostic);\n+      }\n",
            "whole_hunk": "@@ -1331,10 +1331,13 @@ class Handler : public Ffi {\n     std::tuple<std::optional<FnArgType<Ts>>...> args = {\n         internal::Decode<Ts>::call(offsets, ctx, diagnostic)...};\n \n-    bool all_decoded = (std::get<Is>(args).has_value() && ...);\n-    if (XLA_FFI_PREDICT_FALSE(!all_decoded)) {\n-      return FailedDecodeError(call_frame, {std::get<Is>(args).has_value()...},\n-                               diagnostic);\n+    if constexpr (sizeof...(Ts) > 0) {\n+      // We intentionally use `&`, as it generates fewer branch instructions.\n+      bool all_decoded = (std::get<Is>(args).has_value() & ...);\n+      if (XLA_FFI_PREDICT_FALSE(!all_decoded)) {\n+        return FailedDecodeError(\n+            call_frame, {std::get<Is>(args).has_value()...}, diagnostic);\n+      }\n     }\n \n     auto result = fn_(std::move(*std::get<Is>(args))...);"
        }
    ]
},
{
    "Id": 63,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8ac217e9cf2af6f519e3adc6790dfcd9a86440b6",
    "date": "2024-04-26T08:56:23-07:00",
    "message": "Fixes the Memory Term Reducer to properly handle the case of invalid intervals in the input.\n\nPiperOrigin-RevId: 628413845",
    "label": "YES",
    "changes": [
        {
            "name": "auto_sharding_memory.cc",
            "path": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_memory.cc",
            "patches": [
                {
                    "old_start": 114,
                    "old_length": 6,
                    "new_start": 114,
                    "new_length": 7,
                    "hunk": "@@ -114,6 +114,7 @@ std::pair<int64_t, int64_t> MemoryTermReducer::Reduce(\n   reduced_intervals_.reserve(num_primitives);\n   for (PrimIdx prim_idx = 0; prim_idx < num_primitives; ++prim_idx) {\n     reduced_intervals_.push_back(intervals(prim_idx));\n+    if (!IsValid(reduced_intervals_.back())) continue;\n     num_terms += length(reduced_intervals_.back());\n   }\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    if (!IsValid(reduced_intervals_.back())) continue;\n",
            "whole_hunk": "@@ -114,6 +114,7 @@ std::pair<int64_t, int64_t> MemoryTermReducer::Reduce(\n   reduced_intervals_.reserve(num_primitives);\n   for (PrimIdx prim_idx = 0; prim_idx < num_primitives; ++prim_idx) {\n     reduced_intervals_.push_back(intervals(prim_idx));\n+    if (!IsValid(reduced_intervals_.back())) continue;\n     num_terms += length(reduced_intervals_.back());\n   }\n \n"
        },
        {
            "name": "auto_sharding_memory_test.cc",
            "path": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_memory_test.cc",
            "patches": [
                {
                    "old_start": 559,
                    "old_length": 6,
                    "new_start": 559,
                    "new_length": 25,
                    "hunk": "@@ -559,6 +559,25 @@ TEST(AutoShardingMemoryTest, ExampleFromDocumentationUsingIntervals) {\n   EXPECT_EQ(reducer.GetReducedGroups(), expected_reduced_groups);\n }\n \n+TEST(AutoShardingMemoryTest, InvalidIntervals) {\n+  const std::vector<std::pair<int64_t, int64_t>> intervals =\n+      {{0, 4}, {9223372036854775807, 0}, {9223372036854775807, 0}};\n+\n+  MemoryTermReducer reducer;\n+  const auto num_terms = reducer.Reduce(/*num_lives=*/5, /*num_primitives=*/3,\n+                                        Convert(intervals));\n+\n+  const std::vector<std::vector<int64_t>> expected_reduced_live = {};\n+  const std::vector<std::pair<int64_t, int64_t>> expected_reduced_intervals =\n+      {{0, 4}, {9223372036854775807, 0}, {9223372036854775807, 0}};\n+  const std::vector<absl::btree_set<int64_t>> expected_reduced_groups = {};\n+  const std::pair<int64_t, int64_t> expected_num_terms = {5, 5};\n+  EXPECT_EQ(num_terms, expected_num_terms);\n+  EXPECT_EQ(reducer.GetReducedLive(), expected_reduced_live);\n+  EXPECT_EQ(reducer.GetReducedIntervals(), expected_reduced_intervals);\n+  EXPECT_EQ(reducer.GetReducedGroups(), expected_reduced_groups);\n+}\n+\n // clang-format on\n \n }  // namespace"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST(AutoShardingMemoryTest, InvalidIntervals) {\n+  const std::vector<std::pair<int64_t, int64_t>> intervals =\n+      {{0, 4}, {9223372036854775807, 0}, {9223372036854775807, 0}};\n+\n+  MemoryTermReducer reducer;\n+  const auto num_terms = reducer.Reduce(/*num_lives=*/5, /*num_primitives=*/3,\n+                                        Convert(intervals));\n+\n+  const std::vector<std::vector<int64_t>> expected_reduced_live = {};\n+  const std::vector<std::pair<int64_t, int64_t>> expected_reduced_intervals =\n+      {{0, 4}, {9223372036854775807, 0}, {9223372036854775807, 0}};\n+  const std::vector<absl::btree_set<int64_t>> expected_reduced_groups = {};\n+  const std::pair<int64_t, int64_t> expected_num_terms = {5, 5};\n+  EXPECT_EQ(num_terms, expected_num_terms);\n+  EXPECT_EQ(reducer.GetReducedLive(), expected_reduced_live);\n+  EXPECT_EQ(reducer.GetReducedIntervals(), expected_reduced_intervals);\n+  EXPECT_EQ(reducer.GetReducedGroups(), expected_reduced_groups);\n+}\n+\n",
            "whole_hunk": "@@ -559,6 +559,25 @@ TEST(AutoShardingMemoryTest, ExampleFromDocumentationUsingIntervals) {\n   EXPECT_EQ(reducer.GetReducedGroups(), expected_reduced_groups);\n }\n \n+TEST(AutoShardingMemoryTest, InvalidIntervals) {\n+  const std::vector<std::pair<int64_t, int64_t>> intervals =\n+      {{0, 4}, {9223372036854775807, 0}, {9223372036854775807, 0}};\n+\n+  MemoryTermReducer reducer;\n+  const auto num_terms = reducer.Reduce(/*num_lives=*/5, /*num_primitives=*/3,\n+                                        Convert(intervals));\n+\n+  const std::vector<std::vector<int64_t>> expected_reduced_live = {};\n+  const std::vector<std::pair<int64_t, int64_t>> expected_reduced_intervals =\n+      {{0, 4}, {9223372036854775807, 0}, {9223372036854775807, 0}};\n+  const std::vector<absl::btree_set<int64_t>> expected_reduced_groups = {};\n+  const std::pair<int64_t, int64_t> expected_num_terms = {5, 5};\n+  EXPECT_EQ(num_terms, expected_num_terms);\n+  EXPECT_EQ(reducer.GetReducedLive(), expected_reduced_live);\n+  EXPECT_EQ(reducer.GetReducedIntervals(), expected_reduced_intervals);\n+  EXPECT_EQ(reducer.GetReducedGroups(), expected_reduced_groups);\n+}\n+\n // clang-format on\n \n }  // namespace"
        }
    ]
},
{
    "Id": 354,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/6e153325b66330dafea4e4e8b67b5d56b1a37852",
    "date": "2023-07-14T04:46:03-07:00",
    "message": "[XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast produces a\nscalar. This avoids crashing within last_dimension when attempting to match.\n\nPiperOrigin-RevId: 548090995",
    "label": "YES",
    "changes": [
        {
            "name": "softmax_rewriter_triton.cc",
            "path": "tensorflow/compiler/xla/service/gpu/softmax_rewriter_triton.cc",
            "patches": [
                {
                    "old_start": 69,
                    "old_length": 6,
                    "new_start": 69,
                    "new_length": 10,
                    "hunk": "@@ -69,6 +69,10 @@ bool TrivialEdge(HloInstruction** producer, HloInstruction* consumer,\n bool BitcastIsTilingNoop(HloInstruction* bitcast) {\n   CHECK_EQ(bitcast->opcode(), HloOpcode::kBitcast);\n \n+  if (bitcast->shape().rank() == 0) {\n+    return true;\n+  }\n+\n   // In the Softmax rewriter for now, tiling is derived from a hero reduction\n   // operation, which should be reducing its input on the last axis. Therefore,\n   // a bitcast is always a no-op with regards to a tile if"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  if (bitcast->shape().rank() == 0) {\n+    return true;\n+  }\n+\n",
            "whole_hunk": "@@ -69,6 +69,10 @@ bool TrivialEdge(HloInstruction** producer, HloInstruction* consumer,\n bool BitcastIsTilingNoop(HloInstruction* bitcast) {\n   CHECK_EQ(bitcast->opcode(), HloOpcode::kBitcast);\n \n+  if (bitcast->shape().rank() == 0) {\n+    return true;\n+  }\n+\n   // In the Softmax rewriter for now, tiling is derived from a hero reduction\n   // operation, which should be reducing its input on the last axis. Therefore,\n   // a bitcast is always a no-op with regards to a tile if"
        }
    ]
},
{
    "Id": 552,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b86b0fc59db40956f36e2566e5c6d6bc4221da4e",
    "date": "2023-02-02T18:50:19+02:00",
    "message": "Rewrite check in SubBuffer() constructor to satisfy ASAN checks\n\nCompare elements count instead of pointers to omit default\nostream<<(signed char*) behaviour and fix undefined behaviour -\npointer should point into the allocated memory or one element past it.",
    "label": "YES",
    "changes": [
        {
            "name": "tensor.cc",
            "path": "tensorflow/core/framework/tensor.cc",
            "patches": [
                {
                    "old_start": 961,
                    "old_length": 7,
                    "new_start": 961,
                    "new_length": 8,
                    "hunk": "@@ -961,7 +961,8 @@ class SubBuffer : public TensorBuffer {\n     CHECK_LE(root_->base<T>(), this->base<T>());\n     T* root_limit = root_->base<T>() + root_->size() / sizeof(T);\n     CHECK_LE(this->base<T>(), root_limit);\n-    CHECK_LE(this->base<T>() + n, root_limit);\n+    size_t max_n = root_limit - this->base<T>();\n+    CHECK_LE(n, max_n);\n     // Hold a ref of the underlying root buffer.\n     // NOTE: 'buf' is a sub-buffer inside the 'root_' buffer.\n     root_->Ref();"
                }
            ],
            "whole_deleted": "-    CHECK_LE(this->base<T>() + n, root_limit);\n",
            "whole_added": "+    size_t max_n = root_limit - this->base<T>();\n+    CHECK_LE(n, max_n);\n",
            "whole_hunk": "@@ -961,7 +961,8 @@ class SubBuffer : public TensorBuffer {\n     CHECK_LE(root_->base<T>(), this->base<T>());\n     T* root_limit = root_->base<T>() + root_->size() / sizeof(T);\n     CHECK_LE(this->base<T>(), root_limit);\n-    CHECK_LE(this->base<T>() + n, root_limit);\n+    size_t max_n = root_limit - this->base<T>();\n+    CHECK_LE(n, max_n);\n     // Hold a ref of the underlying root buffer.\n     // NOTE: 'buf' is a sub-buffer inside the 'root_' buffer.\n     root_->Ref();"
        }
    ]
},
{
    "Id": 181,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1a1a381b5be7701843c3f1e34aa1846ae2a1d0ce",
    "date": "2024-01-25T10:27:28-08:00",
    "message": "Fix a SIGSEGV bug in `InferShapeForXlaGatherOp`\n\nSince `ComputeOutputComponent` may return nullptr, we need to check for null attributes explicitly to be safe.\n\nPiperOrigin-RevId: 601487562",
    "label": "YES",
    "changes": [
        {
            "name": "shape_inference.cc",
            "path": "tensorflow/compiler/mlir/tensorflow/transforms/shape_inference.cc",
            "patches": [
                {
                    "old_start": 1977,
                    "old_length": 7,
                    "new_start": 1977,
                    "new_length": 7,
                    "hunk": "@@ -1977,7 +1977,7 @@ bool ShapeInference::InferShapeForXlaGatherOp(XlaGatherOp op) {\n     slice_sizes_attr = attr;\n   } else if (const auto it = results_.find(ValuePort(op.getSliceSizes()));\n              it != results_.end() &&\n-             llvm::isa<DenseIntElementsAttr>(it->second)) {\n+             llvm::isa_and_nonnull<DenseIntElementsAttr>(it->second)) {\n     slice_sizes_attr = llvm::cast<DenseIntElementsAttr>(it->second);\n   } else {\n     return false;"
                }
            ],
            "whole_deleted": "-             llvm::isa<DenseIntElementsAttr>(it->second)) {\n",
            "whole_added": "+             llvm::isa_and_nonnull<DenseIntElementsAttr>(it->second)) {\n",
            "whole_hunk": "@@ -1977,7 +1977,7 @@ bool ShapeInference::InferShapeForXlaGatherOp(XlaGatherOp op) {\n     slice_sizes_attr = attr;\n   } else if (const auto it = results_.find(ValuePort(op.getSliceSizes()));\n              it != results_.end() &&\n-             llvm::isa<DenseIntElementsAttr>(it->second)) {\n+             llvm::isa_and_nonnull<DenseIntElementsAttr>(it->second)) {\n     slice_sizes_attr = llvm::cast<DenseIntElementsAttr>(it->second);\n   } else {\n     return false;"
        }
    ]
},
{
    "Id": 260,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ae709a6f0780160d9451fa1b81cc25aef2a5d2aa",
    "date": "2023-10-19T06:37:32-07:00",
    "message": "PR #6377: Fix the check failure where while loop is not rooted on a tuple\n\nImported from GitHub PR https://github.com/openxla/xla/pull/6377\n\nSome while loops might not be rooted on a tuple.\nThis removes the check to assert on such situation.\nAddressed issue: https://github.com/openxla/xla/issues/6353\nCopybara import of the project:\n\n--\nd454d73618f606360f2eff896093045896419c46 by TJ <tjx@nvidia.com>:\n\nFix the check failure where while loop is not rooted on a tuple\n\n--\nc5fccd1f594cf2bf1b54daa60f05e71eecea58e8 by TJ <tjx@nvidia.com>:\n\nremoved redundant code in tests\n\nMerging this change closes #6377\n\nPiperOrigin-RevId: 574847170",
    "label": "YES",
    "changes": [
        {
            "name": "loop_double_buffer_transformer.cc",
            "path": "third_party/xla/xla/service/gpu/loop_double_buffer_transformer.cc",
            "patches": [
                {
                    "old_start": 91,
                    "old_length": 7,
                    "new_start": 91,
                    "new_length": 6,
                    "hunk": "@@ -91,7 +91,6 @@ Status PeelInstructionsForOddTripCount(HloModule* module,\n   HloComputation* while_body = while_instr->while_body();\n   HloInstruction* input_parameter = while_body->parameter_instruction(0);\n   HloInstruction* input_tuple = while_instr->mutable_operand(0);\n-  CHECK(input_tuple->opcode() == HloOpcode::kTuple);\n \n   auto old_loop_roots = while_body->root_instruction()->mutable_operands();\n   HloComputation* parent_comp = while_instr->parent();\n"
                },
                {
                    "old_start": 175,
                    "old_length": 7,
                    "new_start": 174,
                    "new_length": 6,
                    "hunk": "@@ -175,7 +174,6 @@ StatusOr<bool> LoopDoubleBufferTransformer::Run(\n \n     HloComputation* while_body = while_instr->while_body();\n \n-    CHECK(while_body->root_instruction()->opcode() == HloOpcode::kTuple);\n     VLOG(2) << \"Processing root \" << while_body->root_instruction()->ToString();\n \n     auto old_loop_roots = while_body->root_instruction()->mutable_operands();\n"
                },
                {
                    "old_start": 243,
                    "old_length": 12,
                    "new_start": 241,
                    "new_length": 14,
                    "hunk": "@@ -243,12 +241,14 @@ StatusOr<bool> LoopDoubleBufferTransformer::Run(\n     }\n     for (HloInstruction* input_consumer : input_parameter->users()) {\n       for (HloInstruction* old_input : input_consumer->users()) {\n-        HloInstruction* new_input = old_to_new_map[old_input];\n-        if (skip_control_dep_injection.find(old_input) ==\n-                skip_control_dep_injection.end() &&\n-            !IsCollective(old_input)) {\n-          for (HloInstruction* old_root : old_loop_roots) {\n-            TF_RETURN_IF_ERROR(old_root->AddControlDependencyTo(new_input));\n+        if (old_to_new_map.find(old_input) != old_to_new_map.end()) {\n+          HloInstruction* new_input = old_to_new_map[old_input];\n+          if (skip_control_dep_injection.find(old_input) ==\n+                  skip_control_dep_injection.end() &&\n+              !IsCollective(old_input)) {\n+            for (HloInstruction* old_root : old_loop_roots) {\n+              TF_RETURN_IF_ERROR(old_root->AddControlDependencyTo(new_input));\n+            }\n           }\n         }\n       }\n"
                }
            ],
            "whole_deleted": "-  CHECK(input_tuple->opcode() == HloOpcode::kTuple);\n-    CHECK(while_body->root_instruction()->opcode() == HloOpcode::kTuple);\n-        HloInstruction* new_input = old_to_new_map[old_input];\n-        if (skip_control_dep_injection.find(old_input) ==\n-                skip_control_dep_injection.end() &&\n-            !IsCollective(old_input)) {\n-          for (HloInstruction* old_root : old_loop_roots) {\n-            TF_RETURN_IF_ERROR(old_root->AddControlDependencyTo(new_input));\n",
            "whole_added": "+        if (old_to_new_map.find(old_input) != old_to_new_map.end()) {\n+          HloInstruction* new_input = old_to_new_map[old_input];\n+          if (skip_control_dep_injection.find(old_input) ==\n+                  skip_control_dep_injection.end() &&\n+              !IsCollective(old_input)) {\n+            for (HloInstruction* old_root : old_loop_roots) {\n+              TF_RETURN_IF_ERROR(old_root->AddControlDependencyTo(new_input));\n+            }\n",
            "whole_hunk": "@@ -91,7 +91,6 @@ Status PeelInstructionsForOddTripCount(HloModule* module,\n   HloComputation* while_body = while_instr->while_body();\n   HloInstruction* input_parameter = while_body->parameter_instruction(0);\n   HloInstruction* input_tuple = while_instr->mutable_operand(0);\n-  CHECK(input_tuple->opcode() == HloOpcode::kTuple);\n \n   auto old_loop_roots = while_body->root_instruction()->mutable_operands();\n   HloComputation* parent_comp = while_instr->parent();\n@@ -175,7 +174,6 @@ StatusOr<bool> LoopDoubleBufferTransformer::Run(\n \n     HloComputation* while_body = while_instr->while_body();\n \n-    CHECK(while_body->root_instruction()->opcode() == HloOpcode::kTuple);\n     VLOG(2) << \"Processing root \" << while_body->root_instruction()->ToString();\n \n     auto old_loop_roots = while_body->root_instruction()->mutable_operands();\n@@ -243,12 +241,14 @@ StatusOr<bool> LoopDoubleBufferTransformer::Run(\n     }\n     for (HloInstruction* input_consumer : input_parameter->users()) {\n       for (HloInstruction* old_input : input_consumer->users()) {\n-        HloInstruction* new_input = old_to_new_map[old_input];\n-        if (skip_control_dep_injection.find(old_input) ==\n-                skip_control_dep_injection.end() &&\n-            !IsCollective(old_input)) {\n-          for (HloInstruction* old_root : old_loop_roots) {\n-            TF_RETURN_IF_ERROR(old_root->AddControlDependencyTo(new_input));\n+        if (old_to_new_map.find(old_input) != old_to_new_map.end()) {\n+          HloInstruction* new_input = old_to_new_map[old_input];\n+          if (skip_control_dep_injection.find(old_input) ==\n+                  skip_control_dep_injection.end() &&\n+              !IsCollective(old_input)) {\n+            for (HloInstruction* old_root : old_loop_roots) {\n+              TF_RETURN_IF_ERROR(old_root->AddControlDependencyTo(new_input));\n+            }\n           }\n         }\n       }\n"
        },
        {
            "name": "loop_double_buffer_transformer_test.cc",
            "path": "third_party/xla/xla/service/gpu/loop_double_buffer_transformer_test.cc",
            "patches": [
                {
                    "old_start": 356,
                    "old_length": 9,
                    "new_start": 356,
                    "new_length": 9,
                    "hunk": "@@ -356,9 +356,9 @@ ENTRY main {\n   EXPECT_EQ(channel_ids.size(), 2);\n }\n \n+// The following 2 tests also address the regression described here:\n+// https://github.com/openxla/xla/issues/6353\n TEST_F(GpuLoopDoubleBufferTransformerTest, NestedWhileLoopRemainsFlattened) {\n-  // TODO(https://github.com/openxla/xla/issues/6353): remove tuple root\n-  // workaround in the body computation.\n   const char* const kModuleString = R\"(\n HloModule loop_unrolling_nested_while_loop_remains_flattened\n \n"
                },
                {
                    "old_start": 386,
                    "old_length": 9,
                    "new_start": 386,
                    "new_length": 7,
                    "hunk": "@@ -386,9 +386,7 @@ condition {\n \n body {\n   input_tuple = (s32[]) parameter(0)\n-  nested_loop_result = (s32[]) while(input_tuple), condition=condition_nested, body=body_nested\n-  tuple_element = s32[] get-tuple-element(nested_loop_result), index=0\n-  ROOT output = (s32[]) tuple(tuple_element)\n+  ROOT output = (s32[]) while(input_tuple), condition=condition_nested, body=body_nested\n }\n \n ENTRY main {\n"
                },
                {
                    "old_start": 422,
                    "old_length": 6,
                    "new_start": 420,
                    "new_length": 68,
                    "hunk": "@@ -422,6 +420,68 @@ ENTRY main {\n   EXPECT_EQ(while_loops_callees.size(), 6);\n }\n \n+TEST_F(GpuLoopDoubleBufferTransformerTest,\n+       NestedWhileLoopRemainsFlattenedOddTripCount) {\n+  const char* const kModuleString = R\"(\n+HloModule loop_unrolling_nested_while_loop_remains_flattened\n+\n+condition_nested {\n+  input_tuple = (s32[]) parameter(0)\n+  cond = s32[] get-tuple-element(input_tuple), index=0\n+  trip_count = s32[] constant(10)\n+  ROOT done = pred[] compare(cond, trip_count), direction=LT\n+}\n+\n+body_nested {\n+ input_tuple = (s32[]) parameter(0)\n+ cond = s32[] get-tuple-element(input_tuple), index=0\n+ one = s32[] constant(1)\n+ cond_plus_1 = s32[] add(cond, one)\n+ ROOT output = (s32[]) tuple(cond_plus_1)\n+}\n+\n+condition {\n+  input_tuple = (s32[]) parameter(0)\n+  cond = s32[] get-tuple-element(input_tuple), index=0\n+  trip_count = s32[] constant(10)\n+  ROOT done = pred[] compare(cond, trip_count), direction=LT\n+}\n+\n+body {\n+  input_tuple = (s32[]) parameter(0)\n+  ROOT output = (s32[]) while(input_tuple), condition=condition_nested, body=body_nested\n+}\n+\n+ENTRY main {\n+ param_0 = (s32[]) parameter(0)\n+ ROOT while = (s32[]) while(param_0), condition=condition, body=body, backend_config={\"known_trip_count\":{\"n\":\"11\"}}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<xla::HloModule> module,\n+                          ParseAndReturnVerifiedModule(kModuleString));\n+  LoopDoubleBufferTransformer double_buffer;\n+  HloDCE dce;\n+  TupleSimplifier tuple_simp;\n+  ASSERT_IS_OK(double_buffer.Run(module.get()).status());\n+  ASSERT_IS_OK(tuple_simp.Run(module.get()).status());\n+  ASSERT_IS_OK(dce.Run(module.get()).status());\n+\n+  absl::flat_hash_set<const HloComputation*> while_loops_callees;\n+\n+  for (const HloComputation* computation : module->computations()) {\n+    for (const HloInstruction* instr : computation->instructions()) {\n+      if (instr->opcode() == HloOpcode::kWhile) {\n+        EXPECT_TRUE(\n+            while_loops_callees.insert(instr->while_condition()).second);\n+        EXPECT_TRUE(while_loops_callees.insert(instr->while_body()).second);\n+      }\n+    }\n+  }\n+\n+  // We expect that the nested while loop has been duplicated, along with its\n+  // associated computations.\n+  EXPECT_EQ(while_loops_callees.size(), 8);\n+}\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
                }
            ],
            "whole_deleted": "-  // TODO(https://github.com/openxla/xla/issues/6353): remove tuple root\n-  // workaround in the body computation.\n-  nested_loop_result = (s32[]) while(input_tuple), condition=condition_nested, body=body_nested\n-  tuple_element = s32[] get-tuple-element(nested_loop_result), index=0\n-  ROOT output = (s32[]) tuple(tuple_element)\n",
            "whole_added": "+// The following 2 tests also address the regression described here:\n+// https://github.com/openxla/xla/issues/6353\n+  ROOT output = (s32[]) while(input_tuple), condition=condition_nested, body=body_nested\n+TEST_F(GpuLoopDoubleBufferTransformerTest,\n+       NestedWhileLoopRemainsFlattenedOddTripCount) {\n+  const char* const kModuleString = R\"(\n+HloModule loop_unrolling_nested_while_loop_remains_flattened\n+\n+condition_nested {\n+  input_tuple = (s32[]) parameter(0)\n+  cond = s32[] get-tuple-element(input_tuple), index=0\n+  trip_count = s32[] constant(10)\n+  ROOT done = pred[] compare(cond, trip_count), direction=LT\n+}\n+\n+body_nested {\n+ input_tuple = (s32[]) parameter(0)\n+ cond = s32[] get-tuple-element(input_tuple), index=0\n+ one = s32[] constant(1)\n+ cond_plus_1 = s32[] add(cond, one)\n+ ROOT output = (s32[]) tuple(cond_plus_1)\n+}\n+\n+condition {\n+  input_tuple = (s32[]) parameter(0)\n+  cond = s32[] get-tuple-element(input_tuple), index=0\n+  trip_count = s32[] constant(10)\n+  ROOT done = pred[] compare(cond, trip_count), direction=LT\n+}\n+\n+body {\n+  input_tuple = (s32[]) parameter(0)\n+  ROOT output = (s32[]) while(input_tuple), condition=condition_nested, body=body_nested\n+}\n+\n+ENTRY main {\n+ param_0 = (s32[]) parameter(0)\n+ ROOT while = (s32[]) while(param_0), condition=condition, body=body, backend_config={\"known_trip_count\":{\"n\":\"11\"}}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<xla::HloModule> module,\n+                          ParseAndReturnVerifiedModule(kModuleString));\n+  LoopDoubleBufferTransformer double_buffer;\n+  HloDCE dce;\n+  TupleSimplifier tuple_simp;\n+  ASSERT_IS_OK(double_buffer.Run(module.get()).status());\n+  ASSERT_IS_OK(tuple_simp.Run(module.get()).status());\n+  ASSERT_IS_OK(dce.Run(module.get()).status());\n+\n+  absl::flat_hash_set<const HloComputation*> while_loops_callees;\n+\n+  for (const HloComputation* computation : module->computations()) {\n+    for (const HloInstruction* instr : computation->instructions()) {\n+      if (instr->opcode() == HloOpcode::kWhile) {\n+        EXPECT_TRUE(\n+            while_loops_callees.insert(instr->while_condition()).second);\n+        EXPECT_TRUE(while_loops_callees.insert(instr->while_body()).second);\n+      }\n+    }\n+  }\n+\n+  // We expect that the nested while loop has been duplicated, along with its\n+  // associated computations.\n+  EXPECT_EQ(while_loops_callees.size(), 8);\n+}\n",
            "whole_hunk": "@@ -356,9 +356,9 @@ ENTRY main {\n   EXPECT_EQ(channel_ids.size(), 2);\n }\n \n+// The following 2 tests also address the regression described here:\n+// https://github.com/openxla/xla/issues/6353\n TEST_F(GpuLoopDoubleBufferTransformerTest, NestedWhileLoopRemainsFlattened) {\n-  // TODO(https://github.com/openxla/xla/issues/6353): remove tuple root\n-  // workaround in the body computation.\n   const char* const kModuleString = R\"(\n HloModule loop_unrolling_nested_while_loop_remains_flattened\n \n@@ -386,9 +386,7 @@ condition {\n \n body {\n   input_tuple = (s32[]) parameter(0)\n-  nested_loop_result = (s32[]) while(input_tuple), condition=condition_nested, body=body_nested\n-  tuple_element = s32[] get-tuple-element(nested_loop_result), index=0\n-  ROOT output = (s32[]) tuple(tuple_element)\n+  ROOT output = (s32[]) while(input_tuple), condition=condition_nested, body=body_nested\n }\n \n ENTRY main {\n@@ -422,6 +420,68 @@ ENTRY main {\n   EXPECT_EQ(while_loops_callees.size(), 6);\n }\n \n+TEST_F(GpuLoopDoubleBufferTransformerTest,\n+       NestedWhileLoopRemainsFlattenedOddTripCount) {\n+  const char* const kModuleString = R\"(\n+HloModule loop_unrolling_nested_while_loop_remains_flattened\n+\n+condition_nested {\n+  input_tuple = (s32[]) parameter(0)\n+  cond = s32[] get-tuple-element(input_tuple), index=0\n+  trip_count = s32[] constant(10)\n+  ROOT done = pred[] compare(cond, trip_count), direction=LT\n+}\n+\n+body_nested {\n+ input_tuple = (s32[]) parameter(0)\n+ cond = s32[] get-tuple-element(input_tuple), index=0\n+ one = s32[] constant(1)\n+ cond_plus_1 = s32[] add(cond, one)\n+ ROOT output = (s32[]) tuple(cond_plus_1)\n+}\n+\n+condition {\n+  input_tuple = (s32[]) parameter(0)\n+  cond = s32[] get-tuple-element(input_tuple), index=0\n+  trip_count = s32[] constant(10)\n+  ROOT done = pred[] compare(cond, trip_count), direction=LT\n+}\n+\n+body {\n+  input_tuple = (s32[]) parameter(0)\n+  ROOT output = (s32[]) while(input_tuple), condition=condition_nested, body=body_nested\n+}\n+\n+ENTRY main {\n+ param_0 = (s32[]) parameter(0)\n+ ROOT while = (s32[]) while(param_0), condition=condition, body=body, backend_config={\"known_trip_count\":{\"n\":\"11\"}}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<xla::HloModule> module,\n+                          ParseAndReturnVerifiedModule(kModuleString));\n+  LoopDoubleBufferTransformer double_buffer;\n+  HloDCE dce;\n+  TupleSimplifier tuple_simp;\n+  ASSERT_IS_OK(double_buffer.Run(module.get()).status());\n+  ASSERT_IS_OK(tuple_simp.Run(module.get()).status());\n+  ASSERT_IS_OK(dce.Run(module.get()).status());\n+\n+  absl::flat_hash_set<const HloComputation*> while_loops_callees;\n+\n+  for (const HloComputation* computation : module->computations()) {\n+    for (const HloInstruction* instr : computation->instructions()) {\n+      if (instr->opcode() == HloOpcode::kWhile) {\n+        EXPECT_TRUE(\n+            while_loops_callees.insert(instr->while_condition()).second);\n+        EXPECT_TRUE(while_loops_callees.insert(instr->while_body()).second);\n+      }\n+    }\n+  }\n+\n+  // We expect that the nested while loop has been duplicated, along with its\n+  // associated computations.\n+  EXPECT_EQ(while_loops_callees.size(), 8);\n+}\n }  // namespace\n }  // namespace gpu\n }  // namespace xla"
        }
    ]
},
{
    "Id": 109,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8cdcf82b1f07d5f76614eb5d30a2c4f2304e35f9",
    "date": "2024-03-14T04:38:29-07:00",
    "message": "[XLA:GPU] In Multi-output fusion, use DFS to check reachability\n\nCurrently, MultiOutputFusion uses HloReachabilityMap, which builds reachability matrix on creation and element lookup on request. With multi-output fusion rebuilding the reachability after every step, that may take time.\n\nThis change makes MultiOutputFusion to use a simple depth-first search reachability check, bounded by the post-order index. Namely,\n* To build the \"map\", the instructions are sorted in the post order, and indices are stored.\n* On the request, the DFS from the destination node is started, and only goes through nodes which have post-order index >= source node.\n\nWith this approach, the MultiOutputFusion pass of one example HLO module goes from 34 minutes to 4 minutes. There are more advanced data structures that help even more (0.5 minutes), this will be done in the subsequent changes.\n\nPiperOrigin-RevId: 615730140",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/hlo/ir/BUILD",
            "patches": [
                {
                    "old_start": 134,
                    "old_length": 6,
                    "new_start": 134,
                    "new_length": 18,
                    "hunk": "@@ -134,6 +134,18 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"hlo_dfs_reachability\",\n+    srcs = [\"hlo_dfs_reachability.cc\"],\n+    hdrs = [\"hlo_dfs_reachability.h\"],\n+    deps = [\n+        \":hlo\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+    ],\n+)\n+\n cc_library(\n     name = \"ptrvec\",\n     hdrs = [\"ptrvec.h\"],\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+cc_library(\n+    name = \"hlo_dfs_reachability\",\n+    srcs = [\"hlo_dfs_reachability.cc\"],\n+    hdrs = [\"hlo_dfs_reachability.h\"],\n+    deps = [\n+        \":hlo\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+    ],\n+)\n+\n",
            "whole_hunk": "@@ -134,6 +134,18 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"hlo_dfs_reachability\",\n+    srcs = [\"hlo_dfs_reachability.cc\"],\n+    hdrs = [\"hlo_dfs_reachability.h\"],\n+    deps = [\n+        \":hlo\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+    ],\n+)\n+\n cc_library(\n     name = \"ptrvec\",\n     hdrs = [\"ptrvec.h\"],\n"
        },
        {
            "name": "hlo_dfs_reachability.cc",
            "path": "third_party/xla/xla/hlo/ir/hlo_dfs_reachability.cc",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 100,
                    "hunk": "@@ -0,0 +1,100 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/ir/hlo_dfs_reachability.h\"\n+\n+#include <cstddef>\n+#include <memory>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+\n+namespace xla {\n+\n+bool HloDfsReachability::IsPresent(const HloInstruction* instruction) const {\n+  return instruction_to_idx_.contains(instruction);\n+}\n+\n+bool HloDfsReachability::IsReachable(const HloInstruction* from,\n+                                     const HloInstruction* to) const {\n+  if (from == to) {\n+    return true;\n+  }\n+  if (to->operand_count() == 0 && from->control_predecessors().empty()) {\n+    return false;\n+  }\n+\n+  const size_t target_node_idx = instruction_to_idx_.at(from);\n+  const size_t dfs_root_idx = instruction_to_idx_.at(to);\n+  // Note that the DFS goes from the \"uses\" root towards the \"defs\", i.e. from\n+  // `to` node to `from` node, so the node indices are decreasing.\n+  if (target_node_idx > dfs_root_idx) {\n+    return false;\n+  }\n+  absl::flat_hash_set<const HloInstruction*> visited{to};\n+  std::vector<const HloInstruction*> stack{to};\n+\n+  auto check_and_enqueue = [&](const HloInstruction* instruction) {\n+    if (instruction == from) {\n+      return true;\n+    }\n+    if (visited.contains(instruction)) {\n+      return false;\n+    }\n+    if (instruction_to_idx_.at(instruction) < target_node_idx) {\n+      return false;\n+    }\n+    visited.insert(instruction);\n+    stack.push_back(instruction);\n+    return false;\n+  };\n+\n+  while (!stack.empty()) {\n+    const HloInstruction* instr = stack.back();\n+    stack.pop_back();\n+\n+    if (absl::c_any_of(instr->operands(), check_and_enqueue) ||\n+        absl::c_any_of(instr->control_predecessors(), check_and_enqueue)) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool HloDfsReachability::IsConnected(const HloInstruction* a,\n+                                     const HloInstruction* b) const {\n+  return IsReachable(a, b) || IsReachable(b, a);\n+}\n+\n+std::unique_ptr<HloDfsReachability> HloDfsReachability::Build(\n+    const HloComputation* computation) {\n+  auto res = std::make_unique<HloDfsReachability>();\n+\n+  HloComputation::ChannelDependencies channel_dependencies =\n+      computation->ComputeChannelDependencies();\n+  std::vector<HloInstruction*> instructions =\n+      computation->MakeInstructionPostOrder(channel_dependencies);\n+\n+  for (size_t i = 0; i < instructions.size(); ++i) {\n+    res->instruction_to_idx_[instructions[i]] = i;\n+  }\n+\n+  return res;\n+}\n+\n+}  // namespace xla\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/ir/hlo_dfs_reachability.h\"\n+\n+#include <cstddef>\n+#include <memory>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+\n+namespace xla {\n+\n+bool HloDfsReachability::IsPresent(const HloInstruction* instruction) const {\n+  return instruction_to_idx_.contains(instruction);\n+}\n+\n+bool HloDfsReachability::IsReachable(const HloInstruction* from,\n+                                     const HloInstruction* to) const {\n+  if (from == to) {\n+    return true;\n+  }\n+  if (to->operand_count() == 0 && from->control_predecessors().empty()) {\n+    return false;\n+  }\n+\n+  const size_t target_node_idx = instruction_to_idx_.at(from);\n+  const size_t dfs_root_idx = instruction_to_idx_.at(to);\n+  // Note that the DFS goes from the \"uses\" root towards the \"defs\", i.e. from\n+  // `to` node to `from` node, so the node indices are decreasing.\n+  if (target_node_idx > dfs_root_idx) {\n+    return false;\n+  }\n+  absl::flat_hash_set<const HloInstruction*> visited{to};\n+  std::vector<const HloInstruction*> stack{to};\n+\n+  auto check_and_enqueue = [&](const HloInstruction* instruction) {\n+    if (instruction == from) {\n+      return true;\n+    }\n+    if (visited.contains(instruction)) {\n+      return false;\n+    }\n+    if (instruction_to_idx_.at(instruction) < target_node_idx) {\n+      return false;\n+    }\n+    visited.insert(instruction);\n+    stack.push_back(instruction);\n+    return false;\n+  };\n+\n+  while (!stack.empty()) {\n+    const HloInstruction* instr = stack.back();\n+    stack.pop_back();\n+\n+    if (absl::c_any_of(instr->operands(), check_and_enqueue) ||\n+        absl::c_any_of(instr->control_predecessors(), check_and_enqueue)) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool HloDfsReachability::IsConnected(const HloInstruction* a,\n+                                     const HloInstruction* b) const {\n+  return IsReachable(a, b) || IsReachable(b, a);\n+}\n+\n+std::unique_ptr<HloDfsReachability> HloDfsReachability::Build(\n+    const HloComputation* computation) {\n+  auto res = std::make_unique<HloDfsReachability>();\n+\n+  HloComputation::ChannelDependencies channel_dependencies =\n+      computation->ComputeChannelDependencies();\n+  std::vector<HloInstruction*> instructions =\n+      computation->MakeInstructionPostOrder(channel_dependencies);\n+\n+  for (size_t i = 0; i < instructions.size(); ++i) {\n+    res->instruction_to_idx_[instructions[i]] = i;\n+  }\n+\n+  return res;\n+}\n+\n+}  // namespace xla\n",
            "whole_hunk": "@@ -0,0 +1,100 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/ir/hlo_dfs_reachability.h\"\n+\n+#include <cstddef>\n+#include <memory>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+\n+namespace xla {\n+\n+bool HloDfsReachability::IsPresent(const HloInstruction* instruction) const {\n+  return instruction_to_idx_.contains(instruction);\n+}\n+\n+bool HloDfsReachability::IsReachable(const HloInstruction* from,\n+                                     const HloInstruction* to) const {\n+  if (from == to) {\n+    return true;\n+  }\n+  if (to->operand_count() == 0 && from->control_predecessors().empty()) {\n+    return false;\n+  }\n+\n+  const size_t target_node_idx = instruction_to_idx_.at(from);\n+  const size_t dfs_root_idx = instruction_to_idx_.at(to);\n+  // Note that the DFS goes from the \"uses\" root towards the \"defs\", i.e. from\n+  // `to` node to `from` node, so the node indices are decreasing.\n+  if (target_node_idx > dfs_root_idx) {\n+    return false;\n+  }\n+  absl::flat_hash_set<const HloInstruction*> visited{to};\n+  std::vector<const HloInstruction*> stack{to};\n+\n+  auto check_and_enqueue = [&](const HloInstruction* instruction) {\n+    if (instruction == from) {\n+      return true;\n+    }\n+    if (visited.contains(instruction)) {\n+      return false;\n+    }\n+    if (instruction_to_idx_.at(instruction) < target_node_idx) {\n+      return false;\n+    }\n+    visited.insert(instruction);\n+    stack.push_back(instruction);\n+    return false;\n+  };\n+\n+  while (!stack.empty()) {\n+    const HloInstruction* instr = stack.back();\n+    stack.pop_back();\n+\n+    if (absl::c_any_of(instr->operands(), check_and_enqueue) ||\n+        absl::c_any_of(instr->control_predecessors(), check_and_enqueue)) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool HloDfsReachability::IsConnected(const HloInstruction* a,\n+                                     const HloInstruction* b) const {\n+  return IsReachable(a, b) || IsReachable(b, a);\n+}\n+\n+std::unique_ptr<HloDfsReachability> HloDfsReachability::Build(\n+    const HloComputation* computation) {\n+  auto res = std::make_unique<HloDfsReachability>();\n+\n+  HloComputation::ChannelDependencies channel_dependencies =\n+      computation->ComputeChannelDependencies();\n+  std::vector<HloInstruction*> instructions =\n+      computation->MakeInstructionPostOrder(channel_dependencies);\n+\n+  for (size_t i = 0; i < instructions.size(); ++i) {\n+    res->instruction_to_idx_[instructions[i]] = i;\n+  }\n+\n+  return res;\n+}\n+\n+}  // namespace xla\n"
        },
        {
            "name": "hlo_dfs_reachability.h",
            "path": "third_party/xla/xla/hlo/ir/hlo_dfs_reachability.h",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 60,
                    "hunk": "@@ -0,0 +1,60 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_HLO_IR_HLO_DFS_REACHABILITY_H_\n+#define XLA_HLO_IR_HLO_DFS_REACHABILITY_H_\n+\n+#include <cstddef>\n+#include <memory>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+\n+namespace xla {\n+\n+// A simple DFS-based reachability analysis for HLO instructions.\n+//\n+// When the class is created, the instructions are ordered in a defs-before-uses\n+// topological order.\n+// The reachability query runs a DFS from the destination node (going up through\n+// operands / control predecessors), and stops when the instruction's index in\n+// the defs-before-uses list is before the source node. As the reachability is\n+// tested for nodes that are close to each other, this optimization works well,\n+// and the time is dominated by the post-order sort.\n+class HloDfsReachability {\n+ public:\n+  // Returns true iff the instruction was present in the computation passed to\n+  // Build(). The calling code may want to still use the class after the\n+  // computation is modified, if it's known that the def-before-use order is\n+  // still preserved.\n+  bool IsPresent(const HloInstruction* instruction) const;\n+  // Returns true iff there is a path (with edges being users and control\n+  // successors) from 'from' to 'to'. (i.e. path from definitions to uses; from\n+  // producers to consumers)\n+  bool IsReachable(const HloInstruction* from, const HloInstruction* to) const;\n+  // Returns true iff either `a` is reachable from `b` or `b` is reachable from\n+  // `a`.\n+  bool IsConnected(const HloInstruction* a, const HloInstruction* b) const;\n+  static std::unique_ptr<HloDfsReachability> Build(\n+      const HloComputation* computation);\n+\n+ private:\n+  absl::flat_hash_map<const HloInstruction*, size_t> instruction_to_idx_;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_HLO_IR_HLO_DFS_REACHABILITY_H_\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_HLO_IR_HLO_DFS_REACHABILITY_H_\n+#define XLA_HLO_IR_HLO_DFS_REACHABILITY_H_\n+\n+#include <cstddef>\n+#include <memory>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+\n+namespace xla {\n+\n+// A simple DFS-based reachability analysis for HLO instructions.\n+//\n+// When the class is created, the instructions are ordered in a defs-before-uses\n+// topological order.\n+// The reachability query runs a DFS from the destination node (going up through\n+// operands / control predecessors), and stops when the instruction's index in\n+// the defs-before-uses list is before the source node. As the reachability is\n+// tested for nodes that are close to each other, this optimization works well,\n+// and the time is dominated by the post-order sort.\n+class HloDfsReachability {\n+ public:\n+  // Returns true iff the instruction was present in the computation passed to\n+  // Build(). The calling code may want to still use the class after the\n+  // computation is modified, if it's known that the def-before-use order is\n+  // still preserved.\n+  bool IsPresent(const HloInstruction* instruction) const;\n+  // Returns true iff there is a path (with edges being users and control\n+  // successors) from 'from' to 'to'. (i.e. path from definitions to uses; from\n+  // producers to consumers)\n+  bool IsReachable(const HloInstruction* from, const HloInstruction* to) const;\n+  // Returns true iff either `a` is reachable from `b` or `b` is reachable from\n+  // `a`.\n+  bool IsConnected(const HloInstruction* a, const HloInstruction* b) const;\n+  static std::unique_ptr<HloDfsReachability> Build(\n+      const HloComputation* computation);\n+\n+ private:\n+  absl::flat_hash_map<const HloInstruction*, size_t> instruction_to_idx_;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_HLO_IR_HLO_DFS_REACHABILITY_H_\n",
            "whole_hunk": "@@ -0,0 +1,60 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_HLO_IR_HLO_DFS_REACHABILITY_H_\n+#define XLA_HLO_IR_HLO_DFS_REACHABILITY_H_\n+\n+#include <cstddef>\n+#include <memory>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+\n+namespace xla {\n+\n+// A simple DFS-based reachability analysis for HLO instructions.\n+//\n+// When the class is created, the instructions are ordered in a defs-before-uses\n+// topological order.\n+// The reachability query runs a DFS from the destination node (going up through\n+// operands / control predecessors), and stops when the instruction's index in\n+// the defs-before-uses list is before the source node. As the reachability is\n+// tested for nodes that are close to each other, this optimization works well,\n+// and the time is dominated by the post-order sort.\n+class HloDfsReachability {\n+ public:\n+  // Returns true iff the instruction was present in the computation passed to\n+  // Build(). The calling code may want to still use the class after the\n+  // computation is modified, if it's known that the def-before-use order is\n+  // still preserved.\n+  bool IsPresent(const HloInstruction* instruction) const;\n+  // Returns true iff there is a path (with edges being users and control\n+  // successors) from 'from' to 'to'. (i.e. path from definitions to uses; from\n+  // producers to consumers)\n+  bool IsReachable(const HloInstruction* from, const HloInstruction* to) const;\n+  // Returns true iff either `a` is reachable from `b` or `b` is reachable from\n+  // `a`.\n+  bool IsConnected(const HloInstruction* a, const HloInstruction* b) const;\n+  static std::unique_ptr<HloDfsReachability> Build(\n+      const HloComputation* computation);\n+\n+ private:\n+  absl::flat_hash_map<const HloInstruction*, size_t> instruction_to_idx_;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_HLO_IR_HLO_DFS_REACHABILITY_H_\n"
        },
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 2729,
                    "old_length": 6,
                    "new_start": 2729,
                    "new_length": 7,
                    "hunk": "@@ -2729,6 +2729,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:statusor\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/ir:hlo_dfs_reachability\",\n         \"//xla/hlo/ir:hlo_reachability\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:hlo_graph_dumper\",\n"
                },
                {
                    "old_start": 2739,
                    "old_length": 7,
                    "new_start": 2740,
                    "new_length": 6,
                    "hunk": "@@ -2739,7 +2740,6 @@ cc_library(\n         \"//xla/service/gpu/model:gpu_performance_model_base\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n"
                },
                {
                    "old_start": 3411,
                    "old_length": 6,
                    "new_start": 3411,
                    "new_length": 8,
                    "hunk": "@@ -3411,6 +3411,8 @@ cc_library(\n     deps = [\n         \":backend_configs_cc\",\n         \":cublas_cudnn\",\n+        \":hlo_fusion_analysis\",\n+        \":hlo_traversal\",\n         \":ir_emission_utils\",\n         \":variant_visitor\",\n         \"//xla:shape_util\",\n"
                },
                {
                    "old_start": 3419,
                    "old_length": 8,
                    "new_start": 3421,
                    "new_length": 6,
                    "hunk": "@@ -3419,8 +3421,6 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:hlo_pass\",\n-        \"//xla/service/gpu:hlo_fusion_analysis\",\n-        \"//xla/service/gpu:hlo_traversal\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n"
                }
            ],
            "whole_deleted": "-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"//xla/service/gpu:hlo_fusion_analysis\",\n-        \"//xla/service/gpu:hlo_traversal\",\n",
            "whole_added": "+        \"//xla/hlo/ir:hlo_dfs_reachability\",\n+        \":hlo_fusion_analysis\",\n+        \":hlo_traversal\",\n",
            "whole_hunk": "@@ -2729,6 +2729,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:statusor\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/ir:hlo_dfs_reachability\",\n         \"//xla/hlo/ir:hlo_reachability\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:hlo_graph_dumper\",\n@@ -2739,7 +2740,6 @@ cc_library(\n         \"//xla/service/gpu/model:gpu_performance_model_base\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n@@ -3411,6 +3411,8 @@ cc_library(\n     deps = [\n         \":backend_configs_cc\",\n         \":cublas_cudnn\",\n+        \":hlo_fusion_analysis\",\n+        \":hlo_traversal\",\n         \":ir_emission_utils\",\n         \":variant_visitor\",\n         \"//xla:shape_util\",\n@@ -3419,8 +3421,6 @@ cc_library(\n         \"//xla:util\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:hlo_pass\",\n-        \"//xla/service/gpu:hlo_fusion_analysis\",\n-        \"//xla/service/gpu:hlo_traversal\",\n         \"//xla/stream_executor:device_description\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n"
        },
        {
            "name": "multi_output_fusion.cc",
            "path": "third_party/xla/xla/service/gpu/multi_output_fusion.cc",
            "patches": [
                {
                    "old_start": 31,
                    "old_length": 10,
                    "new_start": 31,
                    "new_length": 10,
                    "hunk": "@@ -31,10 +31,10 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_dfs_reachability.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/ir/hlo_reachability.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model.h\"\n"
                },
                {
                    "old_start": 162,
                    "old_length": 7,
                    "new_start": 162,
                    "new_length": 7,
                    "hunk": "@@ -162,7 +162,7 @@ HloInstruction* SelectPreferredFusionCandidate(\n // reachable from the producer, this would create a cycle.\n FusionDecision OperandReachableFromProducer(\n     const HloInstruction& producer, const HloInstruction& consumer,\n-    const HloReachabilityMap& reachability) {\n+    const HloDfsReachability& reachability) {\n   for (const auto* operand : consumer.operands()) {\n     // If a get-tuple-element instruction is not in the reachability\n     // map, it has been created by fusion in this pass. Simply move\n"
                },
                {
                    "old_start": 184,
                    "old_length": 7,
                    "new_start": 184,
                    "new_length": 7,
                    "hunk": "@@ -184,7 +184,7 @@ FusionDecision OperandReachableFromProducer(\n \n FusionDecision ProducerCandidateIsFusible(\n     const HloInstruction& producer, const HloInstruction& consumer,\n-    const HloReachabilityMap& reachability, FusionInfoCache* fusion_info_cache,\n+    const HloDfsReachability& reachability, FusionInfoCache* fusion_info_cache,\n     GpuHloCostAnalysis* cost_analysis) {\n   if (!IsFusibleAsMultiOutputFusionRoot(consumer)) {\n     return \"consumer not eligible as multi-output fusion root.\";\n"
                },
                {
                    "old_start": 216,
                    "old_length": 7,
                    "new_start": 216,
                    "new_length": 7,
                    "hunk": "@@ -216,7 +216,7 @@ FusionDecision ProducerCandidateIsFusible(\n }\n \n std::vector<HloInstruction*> GetProducerConsumerMultiOutputFusionCandidates(\n-    const HloInstruction* producer, const HloReachabilityMap& reachability,\n+    const HloInstruction* producer, const HloDfsReachability& reachability,\n     FusionInfoCache* fusion_info_cache, GpuHloCostAnalysis* cost_analysis) {\n   std::vector<HloInstruction*> fusion_candidates;\n   const HloComputation* computation = producer->parent();\n"
                },
                {
                    "old_start": 275,
                    "old_length": 7,
                    "new_start": 275,
                    "new_length": 7,
                    "hunk": "@@ -275,7 +275,7 @@ bool IsSiblingFusionCandidate(const HloInstruction* instr) {\n FusionDecision CanFuseSiblings(const HloInstruction& sibling_consumer_1,\n                                const HloInstruction& sibling_consumer_2,\n                                const HloInstruction& common_producer,\n-                               const HloReachabilityMap& reachability,\n+                               const HloDfsReachability& reachability,\n                                FusionInfoCache* fusion_info_cache,\n                                GpuHloCostAnalysis* cost_analysis) {\n   if (reachability.IsConnected(&sibling_consumer_1, &sibling_consumer_2)) {\n"
                },
                {
                    "old_start": 305,
                    "old_length": 7,
                    "new_start": 305,
                    "new_length": 7,
                    "hunk": "@@ -305,7 +305,7 @@ FusionDecision CanFuseSiblings(const HloInstruction& sibling_consumer_1,\n }  // namespace\n \n void GpuMultiOutputFusion::RecomputeReachability() {\n-  reachability_ = HloReachabilityMap::Build(computation_);\n+  reachability_ = HloDfsReachability::Build(computation_);\n }\n \n bool GpuMultiOutputFusion::FuseSiblings(HloInstruction* parent,\n"
                }
            ],
            "whole_deleted": "-#include \"xla/hlo/ir/hlo_reachability.h\"\n-    const HloReachabilityMap& reachability) {\n-    const HloReachabilityMap& reachability, FusionInfoCache* fusion_info_cache,\n-    const HloInstruction* producer, const HloReachabilityMap& reachability,\n-                               const HloReachabilityMap& reachability,\n-  reachability_ = HloReachabilityMap::Build(computation_);\n",
            "whole_added": "+#include \"xla/hlo/ir/hlo_dfs_reachability.h\"\n+    const HloDfsReachability& reachability) {\n+    const HloDfsReachability& reachability, FusionInfoCache* fusion_info_cache,\n+    const HloInstruction* producer, const HloDfsReachability& reachability,\n+                               const HloDfsReachability& reachability,\n+  reachability_ = HloDfsReachability::Build(computation_);\n",
            "whole_hunk": "@@ -31,10 +31,10 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_dfs_reachability.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n-#include \"xla/hlo/ir/hlo_reachability.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/gpu/model/gpu_performance_model.h\"\n@@ -162,7 +162,7 @@ HloInstruction* SelectPreferredFusionCandidate(\n // reachable from the producer, this would create a cycle.\n FusionDecision OperandReachableFromProducer(\n     const HloInstruction& producer, const HloInstruction& consumer,\n-    const HloReachabilityMap& reachability) {\n+    const HloDfsReachability& reachability) {\n   for (const auto* operand : consumer.operands()) {\n     // If a get-tuple-element instruction is not in the reachability\n     // map, it has been created by fusion in this pass. Simply move\n@@ -184,7 +184,7 @@ FusionDecision OperandReachableFromProducer(\n \n FusionDecision ProducerCandidateIsFusible(\n     const HloInstruction& producer, const HloInstruction& consumer,\n-    const HloReachabilityMap& reachability, FusionInfoCache* fusion_info_cache,\n+    const HloDfsReachability& reachability, FusionInfoCache* fusion_info_cache,\n     GpuHloCostAnalysis* cost_analysis) {\n   if (!IsFusibleAsMultiOutputFusionRoot(consumer)) {\n     return \"consumer not eligible as multi-output fusion root.\";\n@@ -216,7 +216,7 @@ FusionDecision ProducerCandidateIsFusible(\n }\n \n std::vector<HloInstruction*> GetProducerConsumerMultiOutputFusionCandidates(\n-    const HloInstruction* producer, const HloReachabilityMap& reachability,\n+    const HloInstruction* producer, const HloDfsReachability& reachability,\n     FusionInfoCache* fusion_info_cache, GpuHloCostAnalysis* cost_analysis) {\n   std::vector<HloInstruction*> fusion_candidates;\n   const HloComputation* computation = producer->parent();\n@@ -275,7 +275,7 @@ bool IsSiblingFusionCandidate(const HloInstruction* instr) {\n FusionDecision CanFuseSiblings(const HloInstruction& sibling_consumer_1,\n                                const HloInstruction& sibling_consumer_2,\n                                const HloInstruction& common_producer,\n-                               const HloReachabilityMap& reachability,\n+                               const HloDfsReachability& reachability,\n                                FusionInfoCache* fusion_info_cache,\n                                GpuHloCostAnalysis* cost_analysis) {\n   if (reachability.IsConnected(&sibling_consumer_1, &sibling_consumer_2)) {\n@@ -305,7 +305,7 @@ FusionDecision CanFuseSiblings(const HloInstruction& sibling_consumer_1,\n }  // namespace\n \n void GpuMultiOutputFusion::RecomputeReachability() {\n-  reachability_ = HloReachabilityMap::Build(computation_);\n+  reachability_ = HloDfsReachability::Build(computation_);\n }\n \n bool GpuMultiOutputFusion::FuseSiblings(HloInstruction* parent,\n"
        },
        {
            "name": "multi_output_fusion.h",
            "path": "third_party/xla/xla/service/gpu/multi_output_fusion.h",
            "patches": [
                {
                    "old_start": 22,
                    "old_length": 9,
                    "new_start": 22,
                    "new_length": 9,
                    "hunk": "@@ -22,9 +22,9 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_dfs_reachability.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_reachability.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n"
                },
                {
                    "old_start": 122,
                    "old_length": 7,
                    "new_start": 122,
                    "new_length": 7,
                    "hunk": "@@ -122,7 +122,7 @@ class GpuMultiOutputFusion : public HloModulePass {\n   HloComputation* computation_;\n \n   // The reachability map of current computation.\n-  std::unique_ptr<HloReachabilityMap> reachability_;\n+  std::unique_ptr<HloDfsReachability> reachability_;\n \n   se::DeviceDescription device_info_;\n   HloCostAnalysis::ShapeSizeFunction shape_size_function_;"
                }
            ],
            "whole_deleted": "-#include \"xla/hlo/ir/hlo_reachability.h\"\n-  std::unique_ptr<HloReachabilityMap> reachability_;\n",
            "whole_added": "+#include \"xla/hlo/ir/hlo_dfs_reachability.h\"\n+  std::unique_ptr<HloDfsReachability> reachability_;\n",
            "whole_hunk": "@@ -22,9 +22,9 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_dfs_reachability.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/hlo/ir/hlo_reachability.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n #include \"xla/service/gpu/model/gpu_hlo_cost_analysis.h\"\n #include \"xla/service/hlo_cost_analysis.h\"\n@@ -122,7 +122,7 @@ class GpuMultiOutputFusion : public HloModulePass {\n   HloComputation* computation_;\n \n   // The reachability map of current computation.\n-  std::unique_ptr<HloReachabilityMap> reachability_;\n+  std::unique_ptr<HloDfsReachability> reachability_;\n \n   se::DeviceDescription device_info_;\n   HloCostAnalysis::ShapeSizeFunction shape_size_function_;"
        }
    ]
},
{
    "Id": 327,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/675d73c284396a100555300fe2eab5d02f4bd847",
    "date": "2023-08-09T08:26:55-07:00",
    "message": "[XLA:GPU] Relaxing shape check on collective ops during setting operand layouts. This was triggering for semantically correct ReduceScatter HLO.\n\nPiperOrigin-RevId: 555169308",
    "label": "NO",
    "changes": [
        {
            "name": "layout_assignment.cc",
            "path": "tensorflow/compiler/xla/service/layout_assignment.cc",
            "patches": [
                {
                    "old_start": 154,
                    "old_length": 8,
                    "new_start": 154,
                    "new_length": 8,
                    "hunk": "@@ -154,8 +154,8 @@ OperandLayoutConstraint::OperandLayoutConstraint(\n       instruction_(instruction),\n       operand_no_(operand_no) {\n   CHECK(shape_layout.LayoutIsSet());\n-  CHECK(ShapeUtil::CompatibleIgnoringElementType(\n-      shape_layout.shape(), instruction->operand(operand_no)->shape()))\n+  CHECK(ShapeUtil::CompatibleKind(shape_layout.shape(),\n+                                  instruction->operand(operand_no)->shape()))\n       << shape_layout.shape() << \" is not compatible with \"\n       << instruction->operand(operand_no)->shape() << \" (for operand \"\n       << operand_no << \" of instruction \" << instruction->ToString() << \")\";\n"
                }
            ],
            "whole_deleted": "-  CHECK(ShapeUtil::CompatibleIgnoringElementType(\n-      shape_layout.shape(), instruction->operand(operand_no)->shape()))\n",
            "whole_added": "+  CHECK(ShapeUtil::CompatibleKind(shape_layout.shape(),\n+                                  instruction->operand(operand_no)->shape()))\n",
            "whole_hunk": "@@ -154,8 +154,8 @@ OperandLayoutConstraint::OperandLayoutConstraint(\n       instruction_(instruction),\n       operand_no_(operand_no) {\n   CHECK(shape_layout.LayoutIsSet());\n-  CHECK(ShapeUtil::CompatibleIgnoringElementType(\n-      shape_layout.shape(), instruction->operand(operand_no)->shape()))\n+  CHECK(ShapeUtil::CompatibleKind(shape_layout.shape(),\n+                                  instruction->operand(operand_no)->shape()))\n       << shape_layout.shape() << \" is not compatible with \"\n       << instruction->operand(operand_no)->shape() << \" (for operand \"\n       << operand_no << \" of instruction \" << instruction->ToString() << \")\";\n"
        },
        {
            "name": "collective_ops_test.cc",
            "path": "tensorflow/compiler/xla/tests/collective_ops_test.cc",
            "patches": [
                {
                    "old_start": 1134,
                    "old_length": 6,
                    "new_start": 1134,
                    "new_length": 40,
                    "hunk": "@@ -1134,6 +1134,40 @@ XLA_TEST_F(CollectiveOpsTest, ReduceScatter) {\n   LiteralTestUtil::ExpectR1Equal<uint32_t>({19, 21, 23, 25}, results[1]);\n }\n \n+XLA_TEST_F(CollectiveOpsTest, ReduceScatterConstrainLayout) {\n+  const char* const kModuleStr = R\"(\n+  HloModule reduce-scatter\n+    %sum (a: u32[], b: u32[]) -> u32[] {\n+    %a = u32[] parameter(0)\n+    %b = u32[] parameter(1)\n+    ROOT %add = u32[] add(u32[] a, u32[] b)\n+  }\n+  ENTRY main {\n+    %param = u32[16] parameter(0)\n+    ROOT %rs = u32[8] reduce-scatter(u32[16] %param), replica_groups={},\n+                       constrain_layout=true, to_apply=%sum, dimensions={0}\n+  }\n+  )\";\n+\n+  const int64_t kNumReplicas = 2;\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kModuleStr, config));\n+\n+  std::vector<uint32_t> input_vec = {\n+      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}};\n+  auto input_literal = LiteralUtil::CreateR1<uint32_t>(input_vec);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), {&input_literal}, kNumReplicas,\n+                        /*use_threads=*/true, /*run_hlo_passes=*/true));\n+  LiteralTestUtil::ExpectR1Equal<uint32_t>({2, 4, 6, 8, 10, 12, 14, 16},\n+                                           results[0]);\n+  LiteralTestUtil::ExpectR1Equal<uint32_t>({18, 20, 22, 24, 26, 28, 30, 32},\n+                                           results[1]);\n+}\n+\n XLA_TEST_F(CollectiveOpsTest, ReduceScatter_Dim1) {\n   const char* const kModuleStr = R\"(\n   HloModule test"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+XLA_TEST_F(CollectiveOpsTest, ReduceScatterConstrainLayout) {\n+  const char* const kModuleStr = R\"(\n+  HloModule reduce-scatter\n+    %sum (a: u32[], b: u32[]) -> u32[] {\n+    %a = u32[] parameter(0)\n+    %b = u32[] parameter(1)\n+    ROOT %add = u32[] add(u32[] a, u32[] b)\n+  }\n+  ENTRY main {\n+    %param = u32[16] parameter(0)\n+    ROOT %rs = u32[8] reduce-scatter(u32[16] %param), replica_groups={},\n+                       constrain_layout=true, to_apply=%sum, dimensions={0}\n+  }\n+  )\";\n+\n+  const int64_t kNumReplicas = 2;\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kModuleStr, config));\n+\n+  std::vector<uint32_t> input_vec = {\n+      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}};\n+  auto input_literal = LiteralUtil::CreateR1<uint32_t>(input_vec);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), {&input_literal}, kNumReplicas,\n+                        /*use_threads=*/true, /*run_hlo_passes=*/true));\n+  LiteralTestUtil::ExpectR1Equal<uint32_t>({2, 4, 6, 8, 10, 12, 14, 16},\n+                                           results[0]);\n+  LiteralTestUtil::ExpectR1Equal<uint32_t>({18, 20, 22, 24, 26, 28, 30, 32},\n+                                           results[1]);\n+}\n+\n",
            "whole_hunk": "@@ -1134,6 +1134,40 @@ XLA_TEST_F(CollectiveOpsTest, ReduceScatter) {\n   LiteralTestUtil::ExpectR1Equal<uint32_t>({19, 21, 23, 25}, results[1]);\n }\n \n+XLA_TEST_F(CollectiveOpsTest, ReduceScatterConstrainLayout) {\n+  const char* const kModuleStr = R\"(\n+  HloModule reduce-scatter\n+    %sum (a: u32[], b: u32[]) -> u32[] {\n+    %a = u32[] parameter(0)\n+    %b = u32[] parameter(1)\n+    ROOT %add = u32[] add(u32[] a, u32[] b)\n+  }\n+  ENTRY main {\n+    %param = u32[16] parameter(0)\n+    ROOT %rs = u32[8] reduce-scatter(u32[16] %param), replica_groups={},\n+                       constrain_layout=true, to_apply=%sum, dimensions={0}\n+  }\n+  )\";\n+\n+  const int64_t kNumReplicas = 2;\n+  HloModuleConfig config =\n+      GetModuleConfigForTest(/*replica_count=*/kNumReplicas);\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(kModuleStr, config));\n+\n+  std::vector<uint32_t> input_vec = {\n+      {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}};\n+  auto input_literal = LiteralUtil::CreateR1<uint32_t>(input_vec);\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::vector<Literal> results,\n+      ExecuteReplicated(std::move(module), {&input_literal}, kNumReplicas,\n+                        /*use_threads=*/true, /*run_hlo_passes=*/true));\n+  LiteralTestUtil::ExpectR1Equal<uint32_t>({2, 4, 6, 8, 10, 12, 14, 16},\n+                                           results[0]);\n+  LiteralTestUtil::ExpectR1Equal<uint32_t>({18, 20, 22, 24, 26, 28, 30, 32},\n+                                           results[1]);\n+}\n+\n XLA_TEST_F(CollectiveOpsTest, ReduceScatter_Dim1) {\n   const char* const kModuleStr = R\"(\n   HloModule test"
        }
    ]
},
{
    "Id": 31,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/397096ed623d97bd9a44dbd4c9ac91f7a1455449",
    "date": "2024-06-04T03:40:55-07:00",
    "message": "PR #13109: Add flag for termination on nccl error\n\nImported from GitHub PR https://github.com/openxla/xla/pull/13109\n\nThis introduces a flag for termination on NCCL async error. With the flag on, XLA will terminate the process on NCCL error. With the flag off, the existing behavior should remain unchanged.\n\nThe patch is motivated by several problems:\n\n- Without this patch, the heartbeat monitor only checks communicators that are currently not use by the running executable (because it obtains the communicators with TryAcquire). Since NCCL errors cause a hang in the running communicator, most failing communicators are locked, so their async errors just go undetected. As a result, XLA often hangs until Grpc timeout even in cases when ncclCommGetAsyncError would report an error.\n\n- Ideally we would recover by aborting the faulty communicators, but that seems to be unreliable (aborts can cause hangs if NCCL currently hangs on a different communicator than the one being aborted). NCCL team is aware of this and working on a fix (https://github.com/NVIDIA/nccl/issues/1013). At the moment, there does not seem to be a reliable fast recovery mechanism short of process termination.\n\nWe propose to expose a flag for terminating the process on failure so that there is some way to detect and recover from a NCCL failure. Once the comm-abort works reliably, we will use that and propagate the error to the API user.\n\nThe patch is based on a PoC from pshamis@nvidia.com and vincentz@nvidia.com.\nCopybara import of the project:\n\n--\n858aeacb2d689e4b03f4e3bcc0595223119143d5 by Jaroslav Sevcik <jsevcik@nvidia.com>:\n\nAdd flag for termination on nccl error\n\nMerging this change closes #13109\n\nPiperOrigin-RevId: 640085317",
    "label": "NO",
    "changes": [
        {
            "name": "debug_options_flags.cc",
            "path": "third_party/xla/xla/debug_options_flags.cc",
            "patches": [
                {
                    "old_start": 256,
                    "old_length": 6,
                    "new_start": 256,
                    "new_length": 8,
                    "hunk": "@@ -256,6 +256,8 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n \n   opts.set_xla_gpu_enable_host_memory_offloading(false);\n \n+  opts.set_xla_gpu_nccl_terminate_on_error(false);\n+\n   return opts;\n }\n \n"
                },
                {
                    "old_start": 1698,
                    "old_length": 6,
                    "new_start": 1700,
                    "new_length": 11,
                    "hunk": "@@ -1698,6 +1700,11 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       bool_setter_for(&DebugOptions::set_xla_gpu_enable_host_memory_offloading),\n       debug_options->xla_gpu_enable_host_memory_offloading(),\n       \"Whether to trigger host memory offloading on a device.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_nccl_terminate_on_error\",\n+      bool_setter_for(&DebugOptions::set_xla_gpu_nccl_terminate_on_error),\n+      debug_options->xla_gpu_nccl_terminate_on_error(),\n+      \"If set, then NCCL errors will terminate the process.\"));\n }  // NOLINT(readability/fn_size)\n \n // Allocates flag_values and flag_objects; this function must not be called more\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  opts.set_xla_gpu_nccl_terminate_on_error(false);\n+\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_nccl_terminate_on_error\",\n+      bool_setter_for(&DebugOptions::set_xla_gpu_nccl_terminate_on_error),\n+      debug_options->xla_gpu_nccl_terminate_on_error(),\n+      \"If set, then NCCL errors will terminate the process.\"));\n",
            "whole_hunk": "@@ -256,6 +256,8 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {\n \n   opts.set_xla_gpu_enable_host_memory_offloading(false);\n \n+  opts.set_xla_gpu_nccl_terminate_on_error(false);\n+\n   return opts;\n }\n \n@@ -1698,6 +1700,11 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n       bool_setter_for(&DebugOptions::set_xla_gpu_enable_host_memory_offloading),\n       debug_options->xla_gpu_enable_host_memory_offloading(),\n       \"Whether to trigger host memory offloading on a device.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_nccl_terminate_on_error\",\n+      bool_setter_for(&DebugOptions::set_xla_gpu_nccl_terminate_on_error),\n+      debug_options->xla_gpu_nccl_terminate_on_error(),\n+      \"If set, then NCCL errors will terminate the process.\"));\n }  // NOLINT(readability/fn_size)\n \n // Allocates flag_values and flag_objects; this function must not be called more\n"
        },
        {
            "name": "nccl_clique.cc",
            "path": "third_party/xla/xla/service/gpu/runtime/nccl_clique.cc",
            "patches": [
                {
                    "old_start": 98,
                    "old_length": 6,
                    "new_start": 98,
                    "new_length": 10,
                    "hunk": "@@ -98,6 +98,10 @@ static absl::Duration TerminateTimeout() {\n                                   : absl::InfiniteDuration();\n }\n \n+static bool TerminateOnNcclError() {\n+  return xla::GetDebugOptionsFromFlags().xla_gpu_nccl_terminate_on_error();\n+}\n+\n //===----------------------------------------------------------------------===//\n // NcclClique\n //===----------------------------------------------------------------------===//\n"
                },
                {
                    "old_start": 180,
                    "old_length": 6,
                    "new_start": 184,
                    "new_length": 13,
                    "hunk": "@@ -180,6 +184,13 @@ static absl::Status CheckComm(NcclApi::NcclCommHandle comm) {\n // Runs async check on all communicators in a clique.\n static void CheckClique(const NcclCliqueKey& clique_key,\n                         NcclClique& lockable_clique) {\n+  if (TerminateOnNcclError()) {\n+    absl::Status status = lockable_clique.CheckAsyncErrors();\n+    if (!status.ok()) {\n+      LOG(FATAL) << \"Terminating process due to async NCCL error: \" << status;\n+    }\n+    return;\n+  }\n   if (NcclClique::Lock clique = lockable_clique.TryAcquire()) {\n     VLOG(5) << \"Checking NCCL clique \" << clique_key.ToString()\n             << \" for async errors; num_communicators=\"\n"
                },
                {
                    "old_start": 510,
                    "old_length": 4,
                    "new_start": 521,
                    "new_length": 22,
                    "hunk": "@@ -510,4 +521,22 @@ absl::StatusOr<std::shared_ptr<NcclClique::Lock>> AcquireNcclClique(\n                               num_local_participants, rank, config);\n }\n \n+absl::Status NcclClique::CheckAsyncErrors() {\n+  return async_error_checker_.Check();\n+}\n+\n+absl::Status NcclCliqueCommunicators::AsyncErrorChecker::Check() {\n+  absl::Status status = absl::OkStatus();\n+  communicators_.ForEachComm(\n+      [&status](int32_t rank, NcclApi::NcclCommHandle comm) {\n+        // Do not overwrite previous errors.\n+        if (!status.ok()) return;\n+        status = NcclApi::Default()->CommGetAsyncError(comm);\n+        if (!status.ok()) {\n+          LOG(ERROR) << \"NCCL async error (rank \" << rank << \"): \" << status;\n+        }\n+      });\n+  return status;\n+}\n+\n }  // namespace xla::gpu\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+static bool TerminateOnNcclError() {\n+  return xla::GetDebugOptionsFromFlags().xla_gpu_nccl_terminate_on_error();\n+}\n+\n+  if (TerminateOnNcclError()) {\n+    absl::Status status = lockable_clique.CheckAsyncErrors();\n+    if (!status.ok()) {\n+      LOG(FATAL) << \"Terminating process due to async NCCL error: \" << status;\n+    }\n+    return;\n+  }\n+absl::Status NcclClique::CheckAsyncErrors() {\n+  return async_error_checker_.Check();\n+}\n+\n+absl::Status NcclCliqueCommunicators::AsyncErrorChecker::Check() {\n+  absl::Status status = absl::OkStatus();\n+  communicators_.ForEachComm(\n+      [&status](int32_t rank, NcclApi::NcclCommHandle comm) {\n+        // Do not overwrite previous errors.\n+        if (!status.ok()) return;\n+        status = NcclApi::Default()->CommGetAsyncError(comm);\n+        if (!status.ok()) {\n+          LOG(ERROR) << \"NCCL async error (rank \" << rank << \"): \" << status;\n+        }\n+      });\n+  return status;\n+}\n+\n",
            "whole_hunk": "@@ -98,6 +98,10 @@ static absl::Duration TerminateTimeout() {\n                                   : absl::InfiniteDuration();\n }\n \n+static bool TerminateOnNcclError() {\n+  return xla::GetDebugOptionsFromFlags().xla_gpu_nccl_terminate_on_error();\n+}\n+\n //===----------------------------------------------------------------------===//\n // NcclClique\n //===----------------------------------------------------------------------===//\n@@ -180,6 +184,13 @@ static absl::Status CheckComm(NcclApi::NcclCommHandle comm) {\n // Runs async check on all communicators in a clique.\n static void CheckClique(const NcclCliqueKey& clique_key,\n                         NcclClique& lockable_clique) {\n+  if (TerminateOnNcclError()) {\n+    absl::Status status = lockable_clique.CheckAsyncErrors();\n+    if (!status.ok()) {\n+      LOG(FATAL) << \"Terminating process due to async NCCL error: \" << status;\n+    }\n+    return;\n+  }\n   if (NcclClique::Lock clique = lockable_clique.TryAcquire()) {\n     VLOG(5) << \"Checking NCCL clique \" << clique_key.ToString()\n             << \" for async errors; num_communicators=\"\n@@ -510,4 +521,22 @@ absl::StatusOr<std::shared_ptr<NcclClique::Lock>> AcquireNcclClique(\n                               num_local_participants, rank, config);\n }\n \n+absl::Status NcclClique::CheckAsyncErrors() {\n+  return async_error_checker_.Check();\n+}\n+\n+absl::Status NcclCliqueCommunicators::AsyncErrorChecker::Check() {\n+  absl::Status status = absl::OkStatus();\n+  communicators_.ForEachComm(\n+      [&status](int32_t rank, NcclApi::NcclCommHandle comm) {\n+        // Do not overwrite previous errors.\n+        if (!status.ok()) return;\n+        status = NcclApi::Default()->CommGetAsyncError(comm);\n+        if (!status.ok()) {\n+          LOG(ERROR) << \"NCCL async error (rank \" << rank << \"): \" << status;\n+        }\n+      });\n+  return status;\n+}\n+\n }  // namespace xla::gpu\n"
        },
        {
            "name": "nccl_clique.h",
            "path": "third_party/xla/xla/service/gpu/runtime/nccl_clique.h",
            "patches": [
                {
                    "old_start": 77,
                    "old_length": 6,
                    "new_start": 77,
                    "new_length": 17,
                    "hunk": "@@ -77,6 +77,17 @@ absl::StatusOr<const NcclCliqueIdCallback*> GetNcclCliqueIdCallback(\n // operations that does not lead to deadlocks.\n class NcclCliqueCommunicators {\n  public:\n+  class AsyncErrorChecker {\n+   public:\n+    absl::Status Check();\n+\n+   private:\n+    friend class NcclCliqueCommunicators;\n+    AsyncErrorChecker(NcclCliqueCommunicators& comms) : communicators_(comms) {}\n+\n+    NcclCliqueCommunicators& communicators_;\n+  };\n+\n   NcclCliqueCommunicators(\n       NcclCliqueKey clique_key, std::optional<NcclCliqueId> clique_id,\n       absl::btree_map<int32_t, NcclApi::OwnedNcclComm> communicators);\n"
                },
                {
                    "old_start": 98,
                    "old_length": 6,
                    "new_start": 109,
                    "new_length": 8,
                    "hunk": "@@ -98,6 +109,8 @@ class NcclCliqueCommunicators {\n \n   std::string DebugString() const;\n \n+  AsyncErrorChecker GetChecker() { return AsyncErrorChecker(*this); }\n+\n  private:\n   NcclCliqueKey clique_key_;\n   std::optional<NcclCliqueId> clique_id_;\n"
                },
                {
                    "old_start": 112,
                    "old_length": 18,
                    "new_start": 125,
                    "new_length": 33,
                    "hunk": "@@ -112,18 +125,33 @@ struct NcclCliqueName {\n   }\n };\n \n-struct NcclClique : public Lockable<NcclCliqueCommunicators, NcclCliqueName> {\n+class NcclClique : public Lockable<NcclCliqueCommunicators, NcclCliqueName> {\n+ public:\n   // We keep acquired cliques in a sorted container to guarantee that all\n   // participants iterate over cliques in the same order.\n   using AcquiredCliquesMap =\n       absl::btree_map<NcclCliqueKey, std::shared_ptr<NcclClique::Lock>,\n                       std::greater<NcclCliqueKey>>;\n \n+  // Construct the lockable clique.\n+  // Note that async errors can be checked without acquiring the lock.\n+  // To get the lock-free reference to the communicators for the async\n+  // error checks, the constructor intentionally leaks the reference\n+  // to the communicators from an acquired lock.\n   NcclClique(NcclCliqueKey clique_key, std::optional<NcclCliqueId> clique_id,\n              absl::btree_map<int32_t, NcclApi::OwnedNcclComm> communicators)\n-      : Lockable(std::move(clique_key), clique_id, std::move(communicators)) {}\n+      : Lockable(std::move(clique_key), clique_id, std::move(communicators)),\n+        async_error_checker_(Acquire()->GetChecker()) {}\n \n   std::string DebugString() const;\n+\n+  // Checks for async errors for all the communicators in the clique without\n+  // taking the lock. If at least one of the communicators has an async error,\n+  // it returns one of the errors.\n+  absl::Status CheckAsyncErrors();\n+\n+ private:\n+  NcclCliqueCommunicators::AsyncErrorChecker async_error_checker_;\n };\n \n // Acquires an shared access to a NCCL clique (NcclClique::Lock collectively\n"
                }
            ],
            "whole_deleted": "-struct NcclClique : public Lockable<NcclCliqueCommunicators, NcclCliqueName> {\n-      : Lockable(std::move(clique_key), clique_id, std::move(communicators)) {}\n",
            "whole_added": "+  class AsyncErrorChecker {\n+   public:\n+    absl::Status Check();\n+\n+   private:\n+    friend class NcclCliqueCommunicators;\n+    AsyncErrorChecker(NcclCliqueCommunicators& comms) : communicators_(comms) {}\n+\n+    NcclCliqueCommunicators& communicators_;\n+  };\n+\n+  AsyncErrorChecker GetChecker() { return AsyncErrorChecker(*this); }\n+\n+class NcclClique : public Lockable<NcclCliqueCommunicators, NcclCliqueName> {\n+ public:\n+  // Construct the lockable clique.\n+  // Note that async errors can be checked without acquiring the lock.\n+  // To get the lock-free reference to the communicators for the async\n+  // error checks, the constructor intentionally leaks the reference\n+  // to the communicators from an acquired lock.\n+      : Lockable(std::move(clique_key), clique_id, std::move(communicators)),\n+        async_error_checker_(Acquire()->GetChecker()) {}\n+\n+  // Checks for async errors for all the communicators in the clique without\n+  // taking the lock. If at least one of the communicators has an async error,\n+  // it returns one of the errors.\n+  absl::Status CheckAsyncErrors();\n+\n+ private:\n+  NcclCliqueCommunicators::AsyncErrorChecker async_error_checker_;\n",
            "whole_hunk": "@@ -77,6 +77,17 @@ absl::StatusOr<const NcclCliqueIdCallback*> GetNcclCliqueIdCallback(\n // operations that does not lead to deadlocks.\n class NcclCliqueCommunicators {\n  public:\n+  class AsyncErrorChecker {\n+   public:\n+    absl::Status Check();\n+\n+   private:\n+    friend class NcclCliqueCommunicators;\n+    AsyncErrorChecker(NcclCliqueCommunicators& comms) : communicators_(comms) {}\n+\n+    NcclCliqueCommunicators& communicators_;\n+  };\n+\n   NcclCliqueCommunicators(\n       NcclCliqueKey clique_key, std::optional<NcclCliqueId> clique_id,\n       absl::btree_map<int32_t, NcclApi::OwnedNcclComm> communicators);\n@@ -98,6 +109,8 @@ class NcclCliqueCommunicators {\n \n   std::string DebugString() const;\n \n+  AsyncErrorChecker GetChecker() { return AsyncErrorChecker(*this); }\n+\n  private:\n   NcclCliqueKey clique_key_;\n   std::optional<NcclCliqueId> clique_id_;\n@@ -112,18 +125,33 @@ struct NcclCliqueName {\n   }\n };\n \n-struct NcclClique : public Lockable<NcclCliqueCommunicators, NcclCliqueName> {\n+class NcclClique : public Lockable<NcclCliqueCommunicators, NcclCliqueName> {\n+ public:\n   // We keep acquired cliques in a sorted container to guarantee that all\n   // participants iterate over cliques in the same order.\n   using AcquiredCliquesMap =\n       absl::btree_map<NcclCliqueKey, std::shared_ptr<NcclClique::Lock>,\n                       std::greater<NcclCliqueKey>>;\n \n+  // Construct the lockable clique.\n+  // Note that async errors can be checked without acquiring the lock.\n+  // To get the lock-free reference to the communicators for the async\n+  // error checks, the constructor intentionally leaks the reference\n+  // to the communicators from an acquired lock.\n   NcclClique(NcclCliqueKey clique_key, std::optional<NcclCliqueId> clique_id,\n              absl::btree_map<int32_t, NcclApi::OwnedNcclComm> communicators)\n-      : Lockable(std::move(clique_key), clique_id, std::move(communicators)) {}\n+      : Lockable(std::move(clique_key), clique_id, std::move(communicators)),\n+        async_error_checker_(Acquire()->GetChecker()) {}\n \n   std::string DebugString() const;\n+\n+  // Checks for async errors for all the communicators in the clique without\n+  // taking the lock. If at least one of the communicators has an async error,\n+  // it returns one of the errors.\n+  absl::Status CheckAsyncErrors();\n+\n+ private:\n+  NcclCliqueCommunicators::AsyncErrorChecker async_error_checker_;\n };\n \n // Acquires an shared access to a NCCL clique (NcclClique::Lock collectively\n"
        },
        {
            "name": "xla.proto",
            "path": "third_party/xla/xla/xla.proto",
            "patches": [
                {
                    "old_start": 786,
                    "old_length": 7,
                    "new_start": 786,
                    "new_length": 10,
                    "hunk": "@@ -786,7 +786,10 @@ message DebugOptions {\n   // a deterministic implementation.\n   bool xla_gpu_exclude_nondeterministic_ops = 297;\n \n-  // Next id: 300\n+  // If true, Nccl errors will terminate the process.\n+  bool xla_gpu_nccl_terminate_on_error = 301;\n+\n+  // Next id: 302\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
                }
            ],
            "whole_deleted": "-  // Next id: 300\n",
            "whole_added": "+  // If true, Nccl errors will terminate the process.\n+  bool xla_gpu_nccl_terminate_on_error = 301;\n+\n+  // Next id: 302\n",
            "whole_hunk": "@@ -786,7 +786,10 @@ message DebugOptions {\n   // a deterministic implementation.\n   bool xla_gpu_exclude_nondeterministic_ops = 297;\n \n-  // Next id: 300\n+  // If true, Nccl errors will terminate the process.\n+  bool xla_gpu_nccl_terminate_on_error = 301;\n+\n+  // Next id: 302\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ]
},
{
    "Id": 690,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/558192ef2a0945b8149e646fd462b441a9ff3574",
    "date": "2022-09-16T15:36:35-07:00",
    "message": "Improve LiteralTestUtil support for dynamic literals. Previously, the utility would check every element of each dynamic literal against one another, even if those elements were beyond the specified dynamic bound. Now the LiteralTestUtil only checks up until the maximum index specified by the Literal's dynamic dimension.\n\nA previous fix was added for LiteralTestUtil::Equal. This is the same change but for LiteralTestUtil::Near.\n\nPiperOrigin-RevId: 474915396",
    "label": "NO",
    "changes": [
        {
            "name": "literal_comparison.cc",
            "path": "tensorflow/compiler/xla/literal_comparison.cc",
            "patches": [
                {
                    "old_start": 517,
                    "old_length": 9,
                    "new_start": 517,
                    "new_length": 11,
                    "hunk": "@@ -517,9 +517,11 @@ class NearComparator {\n \n   // Compares the two literals elementwise.\n   void CompareLiterals() {\n-    // Fast path optimization for the case were layouts match.\n+    // Fast path optimization for the case were layouts match and the shapes are\n+    // static.\n     if (LayoutUtil::Equal(actual_.shape().layout(),\n-                          expected_.shape().layout())) {\n+                          expected_.shape().layout()) &&\n+        expected_.shape().is_static() && actual_.shape().is_static()) {\n       absl::Span<const NativeT> expected_data = expected_.data<NativeT>();\n       absl::Span<const NativeT> actual_data = actual_.data<NativeT>();\n       const int64_t len = expected_data.size();\n"
                },
                {
                    "old_start": 532,
                    "old_length": 9,
                    "new_start": 534,
                    "new_length": 9,
                    "hunk": "@@ -532,9 +534,9 @@ class NearComparator {\n     CompareLiteralsSlow(0, &multi_index);\n   }\n \n-  // Slow path for CompareLiterals when 'actual' and 'expected' literals have\n-  // different layouts. In this case, multidimensional indices are constructed\n-  // and indexed for each element.\n+  // Slow path for CompareLiterals when 'actual' and 'expected' literals are\n+  // dynamic or have different layouts. In this case, multidimensional indices\n+  // are constructed and indexed for each element.\n   void CompareLiteralsSlow(int64_t dimension,\n                            std::vector<int64_t>* multi_index) {\n     if (dimension == multi_index->size()) {\n"
                },
                {
                    "old_start": 543,
                    "old_length": 7,
                    "new_start": 545,
                    "new_length": 13,
                    "hunk": "@@ -543,7 +545,13 @@ class NearComparator {\n                     IndexUtil::MultidimensionalIndexToLinearIndex(\n                         actual_.shape(), *multi_index));\n     } else {\n-      for (int64_t i = 0; i < expected_.shape().dimensions(dimension); ++i) {\n+      int64_t upper_bound = expected_.shape().dimensions(dimension);\n+      if (expected_.shape().is_dynamic_dimension(dimension)) {\n+        // If the dimension is dynamic, we only want to check up until the\n+        // actual dynamic size specified by the literal.\n+        upper_bound = expected_.GetDynamicSize(dimension);\n+      }\n+      for (int64_t i = 0; i < upper_bound; ++i) {\n         (*multi_index)[dimension] = i;\n         CompareLiteralsSlow(dimension + 1, multi_index);\n       }\n"
                }
            ],
            "whole_deleted": "-    // Fast path optimization for the case were layouts match.\n-                          expected_.shape().layout())) {\n-  // Slow path for CompareLiterals when 'actual' and 'expected' literals have\n-  // different layouts. In this case, multidimensional indices are constructed\n-  // and indexed for each element.\n-      for (int64_t i = 0; i < expected_.shape().dimensions(dimension); ++i) {\n",
            "whole_added": "+    // Fast path optimization for the case were layouts match and the shapes are\n+    // static.\n+                          expected_.shape().layout()) &&\n+        expected_.shape().is_static() && actual_.shape().is_static()) {\n+  // Slow path for CompareLiterals when 'actual' and 'expected' literals are\n+  // dynamic or have different layouts. In this case, multidimensional indices\n+  // are constructed and indexed for each element.\n+      int64_t upper_bound = expected_.shape().dimensions(dimension);\n+      if (expected_.shape().is_dynamic_dimension(dimension)) {\n+        // If the dimension is dynamic, we only want to check up until the\n+        // actual dynamic size specified by the literal.\n+        upper_bound = expected_.GetDynamicSize(dimension);\n+      }\n+      for (int64_t i = 0; i < upper_bound; ++i) {\n",
            "whole_hunk": "@@ -517,9 +517,11 @@ class NearComparator {\n \n   // Compares the two literals elementwise.\n   void CompareLiterals() {\n-    // Fast path optimization for the case were layouts match.\n+    // Fast path optimization for the case were layouts match and the shapes are\n+    // static.\n     if (LayoutUtil::Equal(actual_.shape().layout(),\n-                          expected_.shape().layout())) {\n+                          expected_.shape().layout()) &&\n+        expected_.shape().is_static() && actual_.shape().is_static()) {\n       absl::Span<const NativeT> expected_data = expected_.data<NativeT>();\n       absl::Span<const NativeT> actual_data = actual_.data<NativeT>();\n       const int64_t len = expected_data.size();\n@@ -532,9 +534,9 @@ class NearComparator {\n     CompareLiteralsSlow(0, &multi_index);\n   }\n \n-  // Slow path for CompareLiterals when 'actual' and 'expected' literals have\n-  // different layouts. In this case, multidimensional indices are constructed\n-  // and indexed for each element.\n+  // Slow path for CompareLiterals when 'actual' and 'expected' literals are\n+  // dynamic or have different layouts. In this case, multidimensional indices\n+  // are constructed and indexed for each element.\n   void CompareLiteralsSlow(int64_t dimension,\n                            std::vector<int64_t>* multi_index) {\n     if (dimension == multi_index->size()) {\n@@ -543,7 +545,13 @@ class NearComparator {\n                     IndexUtil::MultidimensionalIndexToLinearIndex(\n                         actual_.shape(), *multi_index));\n     } else {\n-      for (int64_t i = 0; i < expected_.shape().dimensions(dimension); ++i) {\n+      int64_t upper_bound = expected_.shape().dimensions(dimension);\n+      if (expected_.shape().is_dynamic_dimension(dimension)) {\n+        // If the dimension is dynamic, we only want to check up until the\n+        // actual dynamic size specified by the literal.\n+        upper_bound = expected_.GetDynamicSize(dimension);\n+      }\n+      for (int64_t i = 0; i < upper_bound; ++i) {\n         (*multi_index)[dimension] = i;\n         CompareLiteralsSlow(dimension + 1, multi_index);\n       }\n"
        },
        {
            "name": "literal_test_util_test.cc",
            "path": "tensorflow/compiler/xla/tests/literal_test_util_test.cc",
            "patches": [
                {
                    "old_start": 324,
                    "old_length": 5,
                    "new_start": 324,
                    "new_length": 38,
                    "hunk": "@@ -324,5 +324,38 @@ TEST(LiteralTestUtilTest, DynamicEqualityR2Dim1) {\n   EXPECT_TRUE(LiteralTestUtil::Equal(literal1, literal2));\n }\n \n+TEST(LiteralTestUtilTest, DynamicNearEqualityR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 99, 99, 99, 99, 99});\n+  literal2.SetDynamicSize(0, 5);\n+  ErrorSpec error(0.001);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicNearEqualityR2Dim) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(0, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {99, 99, 99}});\n+  literal2.SetDynamicSize(0, 2);\n+  ErrorSpec error(0.001);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicNearEqualityR2Dim1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 99}, {4, 5, 99}, {7, 8, 99}});\n+  literal2.SetDynamicSize(1, 2);\n+  ErrorSpec error(0.001);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n+}\n+\n }  // namespace\n }  // namespace xla"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST(LiteralTestUtilTest, DynamicNearEqualityR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 99, 99, 99, 99, 99});\n+  literal2.SetDynamicSize(0, 5);\n+  ErrorSpec error(0.001);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicNearEqualityR2Dim) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(0, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {99, 99, 99}});\n+  literal2.SetDynamicSize(0, 2);\n+  ErrorSpec error(0.001);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicNearEqualityR2Dim1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 99}, {4, 5, 99}, {7, 8, 99}});\n+  literal2.SetDynamicSize(1, 2);\n+  ErrorSpec error(0.001);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n+}\n+\n",
            "whole_hunk": "@@ -324,5 +324,38 @@ TEST(LiteralTestUtilTest, DynamicEqualityR2Dim1) {\n   EXPECT_TRUE(LiteralTestUtil::Equal(literal1, literal2));\n }\n \n+TEST(LiteralTestUtilTest, DynamicNearEqualityR1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal1.PopulateR1<float>({1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+  literal1.SetDynamicSize(0, 5);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {10}));\n+  literal2.PopulateR1<float>({1, 2, 3, 4, 5, 99, 99, 99, 99, 99});\n+  literal2.SetDynamicSize(0, 5);\n+  ErrorSpec error(0.001);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicNearEqualityR2Dim) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(0, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {99, 99, 99}});\n+  literal2.SetDynamicSize(0, 2);\n+  ErrorSpec error(0.001);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n+}\n+\n+TEST(LiteralTestUtilTest, DynamicNearEqualityR2Dim1) {\n+  auto literal1 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal1.PopulateR2<float>({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}});\n+  literal1.SetDynamicSize(1, 2);\n+  auto literal2 = Literal(ShapeUtil::MakeShape(F32, {3, 3}));\n+  literal2.PopulateR2<float>({{1, 2, 99}, {4, 5, 99}, {7, 8, 99}});\n+  literal2.SetDynamicSize(1, 2);\n+  ErrorSpec error(0.001);\n+  EXPECT_TRUE(LiteralTestUtil::Near(literal1, literal2, error));\n+}\n+\n }  // namespace\n }  // namespace xla"
        }
    ]
},
{
    "Id": 642,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1e40c1b0f8b6154db39dcd38767286ab307a0686",
    "date": "2022-11-03T10:55:57-07:00",
    "message": "[XLA] Make num_devices optional in sharding validation\n- This will allow validating sharding even if num_devices is not known\n- Also fix validation to use \"devices\" instead of \"core\" in error messages\n- When num_devices is known, check that all devices are listed in the sharding.\n\nPiperOrigin-RevId: 485916872",
    "label": "NO",
    "changes": [
        {
            "name": "hlo_sharding.cc",
            "path": "tensorflow/compiler/xla/hlo/ir/hlo_sharding.cc",
            "patches": [
                {
                    "old_start": 36,
                    "old_length": 6,
                    "new_start": 36,
                    "new_length": 7,
                    "hunk": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/status_macros.h\"\n #include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/compiler/xla/xla_data.pb.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace xla {\n \n"
                },
                {
                    "old_start": 545,
                    "old_length": 7,
                    "new_start": 546,
                    "new_length": 7,
                    "hunk": "@@ -545,7 +546,7 @@ int64_t HloSharding::GetUniqueDevice() const {\n }\n \n Status HloSharding::ValidateTuple(const Shape& shape,\n-                                  int64_t num_devices) const {\n+                                  std::optional<int64_t> num_devices) const {\n   if (!shape.IsTuple()) {\n     return tsl::errors::InvalidArgument(\n         StrCat(\"Sharding is tuple-shaped but validation shape is not.\"));\n"
                },
                {
                    "old_start": 573,
                    "old_length": 7,
                    "new_start": 574,
                    "new_length": 8,
                    "hunk": "@@ -573,7 +574,8 @@ Status HloSharding::ValidateTuple(const Shape& shape,\n   return OkStatus();\n }\n \n-Status HloSharding::Validate(const Shape& shape, int64_t num_devices) const {\n+Status HloSharding::Validate(const Shape& shape,\n+                             std::optional<int64_t> num_devices) const {\n   if (shape.IsToken()) {\n     return OkStatus();\n   }\n"
                },
                {
                    "old_start": 588,
                    "old_length": 7,
                    "new_start": 590,
                    "new_length": 7,
                    "hunk": "@@ -588,7 +590,7 @@ Status HloSharding::Validate(const Shape& shape, int64_t num_devices) const {\n }\n \n Status HloSharding::ValidateNonTuple(const Shape& shape,\n-                                     int64_t num_devices) const {\n+                                     std::optional<int64_t> num_devices) const {\n   if (shape.IsTuple()) {\n     return tsl::errors::InvalidArgument(\n         StrCat(\"Validation shape is a tuple but sharding is not.\"));\n"
                },
                {
                    "old_start": 597,
                    "old_length": 42,
                    "new_start": 599,
                    "new_length": 44,
                    "hunk": "@@ -597,42 +599,44 @@ Status HloSharding::ValidateNonTuple(const Shape& shape,\n     return OkStatus();\n   }\n \n-  // All tile assignments must be less than the number of available cores and\n+  // All tile assignments must be less than the number of available devices and\n   // unique.\n-  Status status = OkStatus();\n-  absl::flat_hash_set<int64_t> seen_cores;\n-  tile_assignment_.Each([&](absl::Span<const int64_t> indices, int32_t core) {\n-    // Don't overwrite a bad status, so we report the first error.\n-    if (status.ok()) {\n-      if (core >= num_devices) {\n-        status = tsl::errors::InvalidArgument(\n-            StrCat(\"core \", core, \" > \", num_devices, \" in tile assignment\"));\n-      } else if (seen_cores.contains(core)) {\n-        status = tsl::errors::InvalidArgument(\n-            StrCat(\"core \", core, \" is not unique in tile assignment\"));\n-      }\n-      seen_cores.insert(core);\n-    }\n-  });\n-  if (!status.ok()) {\n-    return status;\n-  }\n+  absl::flat_hash_set<int64_t> seen_devices;\n+  Status status = tile_assignment_.EachStatus(\n+      [&num_devices, &seen_devices](absl::Span<const int64_t> /*indices*/,\n+                                    int32_t device) {\n+        if (num_devices.has_value() && device >= *num_devices) {\n+          return tsl::errors::InvalidArgument(\n+              StrCat(\"device \", device, \" > num_devices (\", *num_devices,\n+                     \") in tile assignment\"));\n+        } else if (seen_devices.contains(device)) {\n+          return tsl::errors::InvalidArgument(\n+              StrCat(\"device \", device, \" is not unique in tile assignment\"));\n+        }\n+        seen_devices.insert(device);\n+        return OkStatus();\n+      });\n+  TF_RETURN_IF_ERROR(status);\n \n   if (IsTileMaximal() || IsManual()) {\n     return OkStatus();\n   }\n \n-  // The tile assignment tensor must have the same rank as the input, or input\n-  // rank + 1 for replicate_on_last_tile_dim_.\n-  if (shape.rank() + (replicate_on_last_tile_dim_ ? 1 : 0) +\n-          subgroup_types_.size() !=\n-      tile_assignment_.num_dimensions()) {\n+  // The tile assignment tensor must have the same rank as the tiled data rank.\n+  if (shape.rank() != TiledDataRank()) {\n     return tsl::errors::InvalidArgument(\n-        \"Number of tile assignment dimensions is different to the input rank. \"\n+        \"Number of tile assignment dimensions (excluding subgroups) is \"\n+        \"different than the input rank. \"\n         \"sharding=\",\n         ToString(), \", input_shape=\", ShapeUtil::HumanString(shape));\n   }\n \n+  // All devices should be seen in the tile assignment.\n+  if (num_devices.has_value() && seen_devices.size() != *num_devices) {\n+    return tsl::errors::InvalidArgument(\"tile_assignment should have \",\n+                                        *num_devices, \" devices\");\n+  }\n+\n   // The correct constructor has to be used to create tile maximal shardings.\n   if (tile_assignment_.num_elements() == 1) {\n     return tsl::errors::InvalidArgument(\n"
                }
            ],
            "whole_deleted": "-                                  int64_t num_devices) const {\n-Status HloSharding::Validate(const Shape& shape, int64_t num_devices) const {\n-                                     int64_t num_devices) const {\n-  // All tile assignments must be less than the number of available cores and\n-  Status status = OkStatus();\n-  absl::flat_hash_set<int64_t> seen_cores;\n-  tile_assignment_.Each([&](absl::Span<const int64_t> indices, int32_t core) {\n-    // Don't overwrite a bad status, so we report the first error.\n-    if (status.ok()) {\n-      if (core >= num_devices) {\n-        status = tsl::errors::InvalidArgument(\n-            StrCat(\"core \", core, \" > \", num_devices, \" in tile assignment\"));\n-      } else if (seen_cores.contains(core)) {\n-        status = tsl::errors::InvalidArgument(\n-            StrCat(\"core \", core, \" is not unique in tile assignment\"));\n-      }\n-      seen_cores.insert(core);\n-    }\n-  });\n-  if (!status.ok()) {\n-    return status;\n-  }\n-  // The tile assignment tensor must have the same rank as the input, or input\n-  // rank + 1 for replicate_on_last_tile_dim_.\n-  if (shape.rank() + (replicate_on_last_tile_dim_ ? 1 : 0) +\n-          subgroup_types_.size() !=\n-      tile_assignment_.num_dimensions()) {\n-        \"Number of tile assignment dimensions is different to the input rank. \"\n",
            "whole_added": "+#include \"tensorflow/tsl/platform/errors.h\"\n+                                  std::optional<int64_t> num_devices) const {\n+Status HloSharding::Validate(const Shape& shape,\n+                             std::optional<int64_t> num_devices) const {\n+                                     std::optional<int64_t> num_devices) const {\n+  // All tile assignments must be less than the number of available devices and\n+  absl::flat_hash_set<int64_t> seen_devices;\n+  Status status = tile_assignment_.EachStatus(\n+      [&num_devices, &seen_devices](absl::Span<const int64_t> /*indices*/,\n+                                    int32_t device) {\n+        if (num_devices.has_value() && device >= *num_devices) {\n+          return tsl::errors::InvalidArgument(\n+              StrCat(\"device \", device, \" > num_devices (\", *num_devices,\n+                     \") in tile assignment\"));\n+        } else if (seen_devices.contains(device)) {\n+          return tsl::errors::InvalidArgument(\n+              StrCat(\"device \", device, \" is not unique in tile assignment\"));\n+        }\n+        seen_devices.insert(device);\n+        return OkStatus();\n+      });\n+  TF_RETURN_IF_ERROR(status);\n+  // The tile assignment tensor must have the same rank as the tiled data rank.\n+  if (shape.rank() != TiledDataRank()) {\n+        \"Number of tile assignment dimensions (excluding subgroups) is \"\n+        \"different than the input rank. \"\n+  // All devices should be seen in the tile assignment.\n+  if (num_devices.has_value() && seen_devices.size() != *num_devices) {\n+    return tsl::errors::InvalidArgument(\"tile_assignment should have \",\n+                                        *num_devices, \" devices\");\n+  }\n+\n",
            "whole_hunk": "@@ -36,6 +36,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/status_macros.h\"\n #include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/compiler/xla/xla_data.pb.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace xla {\n \n@@ -545,7 +546,7 @@ int64_t HloSharding::GetUniqueDevice() const {\n }\n \n Status HloSharding::ValidateTuple(const Shape& shape,\n-                                  int64_t num_devices) const {\n+                                  std::optional<int64_t> num_devices) const {\n   if (!shape.IsTuple()) {\n     return tsl::errors::InvalidArgument(\n         StrCat(\"Sharding is tuple-shaped but validation shape is not.\"));\n@@ -573,7 +574,8 @@ Status HloSharding::ValidateTuple(const Shape& shape,\n   return OkStatus();\n }\n \n-Status HloSharding::Validate(const Shape& shape, int64_t num_devices) const {\n+Status HloSharding::Validate(const Shape& shape,\n+                             std::optional<int64_t> num_devices) const {\n   if (shape.IsToken()) {\n     return OkStatus();\n   }\n@@ -588,7 +590,7 @@ Status HloSharding::Validate(const Shape& shape, int64_t num_devices) const {\n }\n \n Status HloSharding::ValidateNonTuple(const Shape& shape,\n-                                     int64_t num_devices) const {\n+                                     std::optional<int64_t> num_devices) const {\n   if (shape.IsTuple()) {\n     return tsl::errors::InvalidArgument(\n         StrCat(\"Validation shape is a tuple but sharding is not.\"));\n@@ -597,42 +599,44 @@ Status HloSharding::ValidateNonTuple(const Shape& shape,\n     return OkStatus();\n   }\n \n-  // All tile assignments must be less than the number of available cores and\n+  // All tile assignments must be less than the number of available devices and\n   // unique.\n-  Status status = OkStatus();\n-  absl::flat_hash_set<int64_t> seen_cores;\n-  tile_assignment_.Each([&](absl::Span<const int64_t> indices, int32_t core) {\n-    // Don't overwrite a bad status, so we report the first error.\n-    if (status.ok()) {\n-      if (core >= num_devices) {\n-        status = tsl::errors::InvalidArgument(\n-            StrCat(\"core \", core, \" > \", num_devices, \" in tile assignment\"));\n-      } else if (seen_cores.contains(core)) {\n-        status = tsl::errors::InvalidArgument(\n-            StrCat(\"core \", core, \" is not unique in tile assignment\"));\n-      }\n-      seen_cores.insert(core);\n-    }\n-  });\n-  if (!status.ok()) {\n-    return status;\n-  }\n+  absl::flat_hash_set<int64_t> seen_devices;\n+  Status status = tile_assignment_.EachStatus(\n+      [&num_devices, &seen_devices](absl::Span<const int64_t> /*indices*/,\n+                                    int32_t device) {\n+        if (num_devices.has_value() && device >= *num_devices) {\n+          return tsl::errors::InvalidArgument(\n+              StrCat(\"device \", device, \" > num_devices (\", *num_devices,\n+                     \") in tile assignment\"));\n+        } else if (seen_devices.contains(device)) {\n+          return tsl::errors::InvalidArgument(\n+              StrCat(\"device \", device, \" is not unique in tile assignment\"));\n+        }\n+        seen_devices.insert(device);\n+        return OkStatus();\n+      });\n+  TF_RETURN_IF_ERROR(status);\n \n   if (IsTileMaximal() || IsManual()) {\n     return OkStatus();\n   }\n \n-  // The tile assignment tensor must have the same rank as the input, or input\n-  // rank + 1 for replicate_on_last_tile_dim_.\n-  if (shape.rank() + (replicate_on_last_tile_dim_ ? 1 : 0) +\n-          subgroup_types_.size() !=\n-      tile_assignment_.num_dimensions()) {\n+  // The tile assignment tensor must have the same rank as the tiled data rank.\n+  if (shape.rank() != TiledDataRank()) {\n     return tsl::errors::InvalidArgument(\n-        \"Number of tile assignment dimensions is different to the input rank. \"\n+        \"Number of tile assignment dimensions (excluding subgroups) is \"\n+        \"different than the input rank. \"\n         \"sharding=\",\n         ToString(), \", input_shape=\", ShapeUtil::HumanString(shape));\n   }\n \n+  // All devices should be seen in the tile assignment.\n+  if (num_devices.has_value() && seen_devices.size() != *num_devices) {\n+    return tsl::errors::InvalidArgument(\"tile_assignment should have \",\n+                                        *num_devices, \" devices\");\n+  }\n+\n   // The correct constructor has to be used to create tile maximal shardings.\n   if (tile_assignment_.num_elements() == 1) {\n     return tsl::errors::InvalidArgument(\n"
        },
        {
            "name": "hlo_sharding.h",
            "path": "tensorflow/compiler/xla/hlo/ir/hlo_sharding.h",
            "patches": [
                {
                    "old_start": 121,
                    "old_length": 7,
                    "new_start": 121,
                    "new_length": 8,
                    "hunk": "@@ -121,7 +121,8 @@ class HloSharding {\n   std::string ToString(bool include_metadata = false) const;\n \n   // Validate that this sharding can be applied to a tensor with shape `shape`.\n-  Status Validate(const Shape& shape, int64_t num_devices) const;\n+  Status Validate(const Shape& shape,\n+                  std::optional<int64_t> num_devices = {}) const;\n \n   // Returns true if the sharding has tuple type.\n   bool IsTuple() const { return tuple_; }\n"
                },
                {
                    "old_start": 419,
                    "old_length": 10,
                    "new_start": 420,
                    "new_length": 12,
                    "hunk": "@@ -419,10 +420,12 @@ class HloSharding {\n   Status CheckLeafCount(const Shape& shape) const;\n \n   // Internal helper to validate a tuple sharding.\n-  Status ValidateTuple(const Shape& shape, int64_t num_devices) const;\n+  Status ValidateTuple(const Shape& shape,\n+                       std::optional<int64_t> num_devices) const;\n \n   // Internal helper to validate a non-tuple (leaf) sharding.\n-  Status ValidateNonTuple(const Shape& shape, int64_t num_devices) const;\n+  Status ValidateNonTuple(const Shape& shape,\n+                          std::optional<int64_t> num_devices) const;\n \n   bool replicated_;\n   bool maximal_;\n"
                }
            ],
            "whole_deleted": "-  Status Validate(const Shape& shape, int64_t num_devices) const;\n-  Status ValidateTuple(const Shape& shape, int64_t num_devices) const;\n-  Status ValidateNonTuple(const Shape& shape, int64_t num_devices) const;\n",
            "whole_added": "+  Status Validate(const Shape& shape,\n+                  std::optional<int64_t> num_devices = {}) const;\n+  Status ValidateTuple(const Shape& shape,\n+                       std::optional<int64_t> num_devices) const;\n+  Status ValidateNonTuple(const Shape& shape,\n+                          std::optional<int64_t> num_devices) const;\n",
            "whole_hunk": "@@ -121,7 +121,8 @@ class HloSharding {\n   std::string ToString(bool include_metadata = false) const;\n \n   // Validate that this sharding can be applied to a tensor with shape `shape`.\n-  Status Validate(const Shape& shape, int64_t num_devices) const;\n+  Status Validate(const Shape& shape,\n+                  std::optional<int64_t> num_devices = {}) const;\n \n   // Returns true if the sharding has tuple type.\n   bool IsTuple() const { return tuple_; }\n@@ -419,10 +420,12 @@ class HloSharding {\n   Status CheckLeafCount(const Shape& shape) const;\n \n   // Internal helper to validate a tuple sharding.\n-  Status ValidateTuple(const Shape& shape, int64_t num_devices) const;\n+  Status ValidateTuple(const Shape& shape,\n+                       std::optional<int64_t> num_devices) const;\n \n   // Internal helper to validate a non-tuple (leaf) sharding.\n-  Status ValidateNonTuple(const Shape& shape, int64_t num_devices) const;\n+  Status ValidateNonTuple(const Shape& shape,\n+                          std::optional<int64_t> num_devices) const;\n \n   bool replicated_;\n   bool maximal_;\n"
        },
        {
            "name": "hlo_sharding_test.cc",
            "path": "tensorflow/compiler/xla/service/hlo_sharding_test.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 19,
                    "new_start": 15,
                    "new_length": 18,
                    "hunk": "@@ -15,19 +15,18 @@ limitations under the License.\n \n #include <algorithm>\n #include <set>\n+#include <sstream>\n #include <string>\n #include <tuple>\n #include <utility>\n #include <vector>\n \n-#include \"tensorflow/compiler/xla/literal.h\"\n #include \"tensorflow/compiler/xla/protobuf_util.h\"\n #include \"tensorflow/compiler/xla/service/hlo_parser.h\"\n #include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/test.h\"\n #include \"tensorflow/compiler/xla/test_helpers.h\"\n #include \"tensorflow/compiler/xla/tests/hlo_test_base.h\"\n-#include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/compiler/xla/xla_data.pb.h\"\n \n namespace xla {\n"
                },
                {
                    "old_start": 126,
                    "old_length": 12,
                    "new_start": 125,
                    "new_length": 19,
                    "hunk": "@@ -126,12 +125,19 @@ TEST_F(HloShardingTest, Tile) {\n                                        /*num_devices=*/2));\n   }\n \n+  {\n+    // Test should fail because not all devices present in tile assignment.\n+    HloSharding sharding = HloSharding::Tile(MakeArray({2, 2}, {0, 1, 2, 3}));\n+    EXPECT_IS_NOT_OK(sharding.Validate(ShapeUtil::MakeShape(U32, {4, 6}),\n+                                       /*num_devices=*/5));\n+  }\n+\n   {\n     // Test should pass.\n     Shape shape = ShapeUtil::MakeShape(U32, {4, 5});\n     HloSharding sharding = HloSharding::Tile(MakeArray({2, 2}, {0, 3, 2, 1}));\n     EXPECT_IS_OK(sharding.Validate(ShapeUtil::MakeShape(F32, {3, 5}),\n-                                   /*num_devices=*/5));\n+                                   /*num_devices=*/4));\n \n     EXPECT_EQ(0, sharding.DeviceForTileIndex({0, 0}));\n     EXPECT_EQ(3, sharding.DeviceForTileIndex({0, 1}));\n"
                },
                {
                    "old_start": 180,
                    "old_length": 7,
                    "new_start": 186,
                    "new_length": 7,
                    "hunk": "@@ -180,7 +186,7 @@ TEST_F(HloShardingTest, NestedTuple) {\n   EXPECT_EQ(shape_tree.element({1, 0}), HloSharding::AssignDevice(0));\n   EXPECT_EQ(shape_tree.element({2}), tiled_sharding);\n \n-  EXPECT_IS_OK(tuple_sharding.Validate(nested_tuple_shape, /*num_devices=*/5));\n+  EXPECT_IS_OK(tuple_sharding.Validate(nested_tuple_shape, /*num_devices=*/2));\n   // Test should fail because tuple element count does not match.\n   EXPECT_IS_NOT_OK(tuple_sharding.Validate(ShapeUtil::MakeTupleShape({}),\n                                            /*num_devices=*/5));"
                }
            ],
            "whole_deleted": "-#include \"tensorflow/compiler/xla/literal.h\"\n-#include \"tensorflow/compiler/xla/util.h\"\n-                                   /*num_devices=*/5));\n-  EXPECT_IS_OK(tuple_sharding.Validate(nested_tuple_shape, /*num_devices=*/5));\n",
            "whole_added": "+#include <sstream>\n+  {\n+    // Test should fail because not all devices present in tile assignment.\n+    HloSharding sharding = HloSharding::Tile(MakeArray({2, 2}, {0, 1, 2, 3}));\n+    EXPECT_IS_NOT_OK(sharding.Validate(ShapeUtil::MakeShape(U32, {4, 6}),\n+                                       /*num_devices=*/5));\n+  }\n+\n+                                   /*num_devices=*/4));\n+  EXPECT_IS_OK(tuple_sharding.Validate(nested_tuple_shape, /*num_devices=*/2));\n",
            "whole_hunk": "@@ -15,19 +15,18 @@ limitations under the License.\n \n #include <algorithm>\n #include <set>\n+#include <sstream>\n #include <string>\n #include <tuple>\n #include <utility>\n #include <vector>\n \n-#include \"tensorflow/compiler/xla/literal.h\"\n #include \"tensorflow/compiler/xla/protobuf_util.h\"\n #include \"tensorflow/compiler/xla/service/hlo_parser.h\"\n #include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/test.h\"\n #include \"tensorflow/compiler/xla/test_helpers.h\"\n #include \"tensorflow/compiler/xla/tests/hlo_test_base.h\"\n-#include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/compiler/xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -126,12 +125,19 @@ TEST_F(HloShardingTest, Tile) {\n                                        /*num_devices=*/2));\n   }\n \n+  {\n+    // Test should fail because not all devices present in tile assignment.\n+    HloSharding sharding = HloSharding::Tile(MakeArray({2, 2}, {0, 1, 2, 3}));\n+    EXPECT_IS_NOT_OK(sharding.Validate(ShapeUtil::MakeShape(U32, {4, 6}),\n+                                       /*num_devices=*/5));\n+  }\n+\n   {\n     // Test should pass.\n     Shape shape = ShapeUtil::MakeShape(U32, {4, 5});\n     HloSharding sharding = HloSharding::Tile(MakeArray({2, 2}, {0, 3, 2, 1}));\n     EXPECT_IS_OK(sharding.Validate(ShapeUtil::MakeShape(F32, {3, 5}),\n-                                   /*num_devices=*/5));\n+                                   /*num_devices=*/4));\n \n     EXPECT_EQ(0, sharding.DeviceForTileIndex({0, 0}));\n     EXPECT_EQ(3, sharding.DeviceForTileIndex({0, 1}));\n@@ -180,7 +186,7 @@ TEST_F(HloShardingTest, NestedTuple) {\n   EXPECT_EQ(shape_tree.element({1, 0}), HloSharding::AssignDevice(0));\n   EXPECT_EQ(shape_tree.element({2}), tiled_sharding);\n \n-  EXPECT_IS_OK(tuple_sharding.Validate(nested_tuple_shape, /*num_devices=*/5));\n+  EXPECT_IS_OK(tuple_sharding.Validate(nested_tuple_shape, /*num_devices=*/2));\n   // Test should fail because tuple element count does not match.\n   EXPECT_IS_NOT_OK(tuple_sharding.Validate(ShapeUtil::MakeTupleShape({}),\n                                            /*num_devices=*/5));"
        }
    ]
},
{
    "Id": 699,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fbc02b521b121bb8f080d4ade5d543ff7594caa7",
    "date": "2022-09-01T11:36:39-07:00",
    "message": "Arena planner optimizations\n\nCache num_tensors, return status without checking as it is checked by caller and reserve vector.\n\nPiperOrigin-RevId: 471585854",
    "label": "NO",
    "changes": [
        {
            "name": "arena_planner.cc",
            "path": "tensorflow/lite/arena_planner.cc",
            "patches": [
                {
                    "old_start": 298,
                    "old_length": 7,
                    "new_start": 298,
                    "new_length": 9,
                    "hunk": "@@ -298,7 +298,9 @@ std::vector<int32_t> ArenaPlanner::CreateTensorAllocationVector(int first_node,\n   };\n \n   std::vector<int32_t> tensor_order;\n-  for (int i = 0; i < static_cast<int>(graph_info_->num_tensors()); ++i) {\n+  int num_tensors = static_cast<int>(graph_info_->num_tensors());\n+  tensor_order.reserve(num_tensors);\n+  for (int i = 0; i < num_tensors; ++i) {\n     if (alloc_node_[i] >= first_node && alloc_node_[i] <= last_node) {\n       tensor_order.push_back(i);\n     }\n"
                },
                {
                    "old_start": 351,
                    "old_length": 13,
                    "new_start": 353,
                    "new_length": 13,
                    "hunk": "@@ -351,13 +353,13 @@ TfLiteStatus ArenaPlanner::ResolveTensorAllocation(int tensor_index) {\n     // Skip resolution if the size of the tensor is zero, leaving it as a\n     // nullptr.\n     if (allocs_[tensor_index].size != 0) {\n-      TF_LITE_ENSURE_STATUS(arena_.ResolveAlloc(context_, allocs_[tensor_index],\n-                                                &tensor.data.raw));\n+      return arena_.ResolveAlloc(context_, allocs_[tensor_index],\n+                                 &tensor.data.raw);\n     }\n   }\n   if (tensor.allocation_type == kTfLiteArenaRwPersistent) {\n-    TF_LITE_ENSURE_STATUS(persistent_arena_.ResolveAlloc(\n-        context_, allocs_[tensor_index], &tensor.data.raw));\n+    return persistent_arena_.ResolveAlloc(context_, allocs_[tensor_index],\n+                                          &tensor.data.raw);\n   }\n   return kTfLiteOk;\n }"
                }
            ],
            "whole_deleted": "-  for (int i = 0; i < static_cast<int>(graph_info_->num_tensors()); ++i) {\n-      TF_LITE_ENSURE_STATUS(arena_.ResolveAlloc(context_, allocs_[tensor_index],\n-                                                &tensor.data.raw));\n-    TF_LITE_ENSURE_STATUS(persistent_arena_.ResolveAlloc(\n-        context_, allocs_[tensor_index], &tensor.data.raw));\n",
            "whole_added": "+  int num_tensors = static_cast<int>(graph_info_->num_tensors());\n+  tensor_order.reserve(num_tensors);\n+  for (int i = 0; i < num_tensors; ++i) {\n+      return arena_.ResolveAlloc(context_, allocs_[tensor_index],\n+                                 &tensor.data.raw);\n+    return persistent_arena_.ResolveAlloc(context_, allocs_[tensor_index],\n+                                          &tensor.data.raw);\n",
            "whole_hunk": "@@ -298,7 +298,9 @@ std::vector<int32_t> ArenaPlanner::CreateTensorAllocationVector(int first_node,\n   };\n \n   std::vector<int32_t> tensor_order;\n-  for (int i = 0; i < static_cast<int>(graph_info_->num_tensors()); ++i) {\n+  int num_tensors = static_cast<int>(graph_info_->num_tensors());\n+  tensor_order.reserve(num_tensors);\n+  for (int i = 0; i < num_tensors; ++i) {\n     if (alloc_node_[i] >= first_node && alloc_node_[i] <= last_node) {\n       tensor_order.push_back(i);\n     }\n@@ -351,13 +353,13 @@ TfLiteStatus ArenaPlanner::ResolveTensorAllocation(int tensor_index) {\n     // Skip resolution if the size of the tensor is zero, leaving it as a\n     // nullptr.\n     if (allocs_[tensor_index].size != 0) {\n-      TF_LITE_ENSURE_STATUS(arena_.ResolveAlloc(context_, allocs_[tensor_index],\n-                                                &tensor.data.raw));\n+      return arena_.ResolveAlloc(context_, allocs_[tensor_index],\n+                                 &tensor.data.raw);\n     }\n   }\n   if (tensor.allocation_type == kTfLiteArenaRwPersistent) {\n-    TF_LITE_ENSURE_STATUS(persistent_arena_.ResolveAlloc(\n-        context_, allocs_[tensor_index], &tensor.data.raw));\n+    return persistent_arena_.ResolveAlloc(context_, allocs_[tensor_index],\n+                                          &tensor.data.raw);\n   }\n   return kTfLiteOk;\n }"
        }
    ]
},
{
    "Id": 501,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/82481f6e967b0523165067687384395570712236",
    "date": "2023-03-08T14:53:13-08:00",
    "message": "Add minimum supported version checking to XlaCallModuleOp.\n\nAlso stop supporting version 1 (no StableHLO).\n\nPiperOrigin-RevId: 515147273",
    "label": "NO",
    "changes": [
        {
            "name": "xla_call_module_op.cc",
            "path": "tensorflow/compiler/tf2xla/kernels/xla_call_module_op.cc",
            "patches": [
                {
                    "old_start": 52,
                    "old_length": 10,
                    "new_start": 52,
                    "new_length": 12,
                    "hunk": "@@ -52,10 +52,12 @@ limitations under the License.\n namespace tensorflow {\n namespace {\n \n-// Version 1 uses MHLO, starting with version 2 use StableHLO.\n+// Version 1 used MHLO, not supported anymore.\n+// Version 2 supports StableHLO. From 10/2022. Minimum from 03/2023.\n const int VERSION_START_STABLE_HLO = 2;\n-// Version 3 supports platform checking and multiple platforms\n+// Version 3 supports platform checking and multiple platforms. From 02/2023.\n const int VERSION_START_PLATFORMS = 3;\n+const int VERSION_MINIMUM_SUPPORTED = VERSION_START_STABLE_HLO;\n \n // Computes a dimension value from the dim_arg specification.\n // The specification is of the form \"<arg_idx>.<arg_axis_idx>\".\n"
                },
                {
                    "old_start": 92,
                    "old_length": 22,
                    "new_start": 94,
                    "new_length": 12,
                    "hunk": "@@ -92,22 +94,12 @@ StatusOr<mlir::Value> ComputeDimensionValue(int version, string dim_arg_spec,\n   mlir::Value val;\n   mlir::Type get_dim_type =\n       mlir::RankedTensorType::get({}, op_builder.getI32Type());\n-  if (version >= VERSION_START_STABLE_HLO) {\n-    val = op_builder.create<mlir::stablehlo::GetDimensionSizeOp>(\n-        arguments[arg_idx].getLoc(), get_dim_type, arguments[arg_idx],\n-        op_builder.getI64IntegerAttr(arg_axis_idx));\n-    if (dim_arg_type != get_dim_type) {\n-      val = op_builder.create<mlir::stablehlo::ConvertOp>(\n-          arguments[arg_idx].getLoc(), dim_arg_type, val);\n-    }\n-  } else {\n-    val = op_builder.create<mlir::mhlo::GetDimensionSizeOp>(\n-        arguments[arg_idx].getLoc(), get_dim_type, arguments[arg_idx],\n-        op_builder.getI64IntegerAttr(arg_axis_idx));\n-    if (dim_arg_type != get_dim_type) {\n-      val = op_builder.create<mlir::mhlo::ConvertOp>(\n-          arguments[arg_idx].getLoc(), dim_arg_type, val);\n-    }\n+  val = op_builder.create<mlir::stablehlo::GetDimensionSizeOp>(\n+      arguments[arg_idx].getLoc(), get_dim_type, arguments[arg_idx],\n+      op_builder.getI64IntegerAttr(arg_axis_idx));\n+  if (dim_arg_type != get_dim_type) {\n+    val = op_builder.create<mlir::stablehlo::ConvertOp>(\n+        arguments[arg_idx].getLoc(), dim_arg_type, val);\n   }\n   return val;\n }\n"
                },
                {
                    "old_start": 436,
                    "old_length": 6,
                    "new_start": 428,
                    "new_length": 11,
                    "hunk": "@@ -436,6 +428,11 @@ class XlaCallModuleOp : public XlaOpKernel {\n  public:\n   explicit XlaCallModuleOp(OpKernelConstruction *ctx) : XlaOpKernel(ctx) {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"version\", &version_));\n+    OP_REQUIRES(\n+        ctx, version_ >= VERSION_MINIMUM_SUPPORTED,\n+        errors::InvalidArgument(\"XlaCallModuleOp with version \", version_,\n+                                \" is not supported anymore. Must be >= \",\n+                                VERSION_MINIMUM_SUPPORTED));\n     string module_str;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"module\", &module_str));\n     std::vector<PartialTensorShape> expected_output_shapes;\n"
                }
            ],
            "whole_deleted": "-// Version 1 uses MHLO, starting with version 2 use StableHLO.\n-// Version 3 supports platform checking and multiple platforms\n-  if (version >= VERSION_START_STABLE_HLO) {\n-    val = op_builder.create<mlir::stablehlo::GetDimensionSizeOp>(\n-        arguments[arg_idx].getLoc(), get_dim_type, arguments[arg_idx],\n-        op_builder.getI64IntegerAttr(arg_axis_idx));\n-    if (dim_arg_type != get_dim_type) {\n-      val = op_builder.create<mlir::stablehlo::ConvertOp>(\n-          arguments[arg_idx].getLoc(), dim_arg_type, val);\n-    }\n-  } else {\n-    val = op_builder.create<mlir::mhlo::GetDimensionSizeOp>(\n-        arguments[arg_idx].getLoc(), get_dim_type, arguments[arg_idx],\n-        op_builder.getI64IntegerAttr(arg_axis_idx));\n-    if (dim_arg_type != get_dim_type) {\n-      val = op_builder.create<mlir::mhlo::ConvertOp>(\n-          arguments[arg_idx].getLoc(), dim_arg_type, val);\n-    }\n",
            "whole_added": "+// Version 1 used MHLO, not supported anymore.\n+// Version 2 supports StableHLO. From 10/2022. Minimum from 03/2023.\n+// Version 3 supports platform checking and multiple platforms. From 02/2023.\n+const int VERSION_MINIMUM_SUPPORTED = VERSION_START_STABLE_HLO;\n+  val = op_builder.create<mlir::stablehlo::GetDimensionSizeOp>(\n+      arguments[arg_idx].getLoc(), get_dim_type, arguments[arg_idx],\n+      op_builder.getI64IntegerAttr(arg_axis_idx));\n+  if (dim_arg_type != get_dim_type) {\n+    val = op_builder.create<mlir::stablehlo::ConvertOp>(\n+        arguments[arg_idx].getLoc(), dim_arg_type, val);\n+    OP_REQUIRES(\n+        ctx, version_ >= VERSION_MINIMUM_SUPPORTED,\n+        errors::InvalidArgument(\"XlaCallModuleOp with version \", version_,\n+                                \" is not supported anymore. Must be >= \",\n+                                VERSION_MINIMUM_SUPPORTED));\n",
            "whole_hunk": "@@ -52,10 +52,12 @@ limitations under the License.\n namespace tensorflow {\n namespace {\n \n-// Version 1 uses MHLO, starting with version 2 use StableHLO.\n+// Version 1 used MHLO, not supported anymore.\n+// Version 2 supports StableHLO. From 10/2022. Minimum from 03/2023.\n const int VERSION_START_STABLE_HLO = 2;\n-// Version 3 supports platform checking and multiple platforms\n+// Version 3 supports platform checking and multiple platforms. From 02/2023.\n const int VERSION_START_PLATFORMS = 3;\n+const int VERSION_MINIMUM_SUPPORTED = VERSION_START_STABLE_HLO;\n \n // Computes a dimension value from the dim_arg specification.\n // The specification is of the form \"<arg_idx>.<arg_axis_idx>\".\n@@ -92,22 +94,12 @@ StatusOr<mlir::Value> ComputeDimensionValue(int version, string dim_arg_spec,\n   mlir::Value val;\n   mlir::Type get_dim_type =\n       mlir::RankedTensorType::get({}, op_builder.getI32Type());\n-  if (version >= VERSION_START_STABLE_HLO) {\n-    val = op_builder.create<mlir::stablehlo::GetDimensionSizeOp>(\n-        arguments[arg_idx].getLoc(), get_dim_type, arguments[arg_idx],\n-        op_builder.getI64IntegerAttr(arg_axis_idx));\n-    if (dim_arg_type != get_dim_type) {\n-      val = op_builder.create<mlir::stablehlo::ConvertOp>(\n-          arguments[arg_idx].getLoc(), dim_arg_type, val);\n-    }\n-  } else {\n-    val = op_builder.create<mlir::mhlo::GetDimensionSizeOp>(\n-        arguments[arg_idx].getLoc(), get_dim_type, arguments[arg_idx],\n-        op_builder.getI64IntegerAttr(arg_axis_idx));\n-    if (dim_arg_type != get_dim_type) {\n-      val = op_builder.create<mlir::mhlo::ConvertOp>(\n-          arguments[arg_idx].getLoc(), dim_arg_type, val);\n-    }\n+  val = op_builder.create<mlir::stablehlo::GetDimensionSizeOp>(\n+      arguments[arg_idx].getLoc(), get_dim_type, arguments[arg_idx],\n+      op_builder.getI64IntegerAttr(arg_axis_idx));\n+  if (dim_arg_type != get_dim_type) {\n+    val = op_builder.create<mlir::stablehlo::ConvertOp>(\n+        arguments[arg_idx].getLoc(), dim_arg_type, val);\n   }\n   return val;\n }\n@@ -436,6 +428,11 @@ class XlaCallModuleOp : public XlaOpKernel {\n  public:\n   explicit XlaCallModuleOp(OpKernelConstruction *ctx) : XlaOpKernel(ctx) {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"version\", &version_));\n+    OP_REQUIRES(\n+        ctx, version_ >= VERSION_MINIMUM_SUPPORTED,\n+        errors::InvalidArgument(\"XlaCallModuleOp with version \", version_,\n+                                \" is not supported anymore. Must be >= \",\n+                                VERSION_MINIMUM_SUPPORTED));\n     string module_str;\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"module\", &module_str));\n     std::vector<PartialTensorShape> expected_output_shapes;\n"
        },
        {
            "name": "xla_ops.cc",
            "path": "tensorflow/compiler/tf2xla/ops/xla_ops.cc",
            "patches": [
                {
                    "old_start": 1355,
                    "old_length": 7,
                    "new_start": 1355,
                    "new_length": 7,
                    "hunk": "@@ -1355,7 +1355,7 @@ args: A list of `Tensor` with possibly different types to be passed as arguments\n   platform argument (see `platforms`) nor the dimension arguments (see\n   `dim_args_spec`).\n version: Tracks changes the semantics of the op, to support backwards\n-  compatibility. Version 1 carries an MHLO text or bytecode `module`. From\n+  compatibility. Minimum supported version is 2. From\n   version 2, the op carries a StableHLO text or bytecode `module`. From\n   version 3, the op also supports the `platforms` attribute.\n module: A serialized computation, a text or bytecode representation of"
                }
            ],
            "whole_deleted": "-  compatibility. Version 1 carries an MHLO text or bytecode `module`. From\n",
            "whole_added": "+  compatibility. Minimum supported version is 2. From\n",
            "whole_hunk": "@@ -1355,7 +1355,7 @@ args: A list of `Tensor` with possibly different types to be passed as arguments\n   platform argument (see `platforms`) nor the dimension arguments (see\n   `dim_args_spec`).\n version: Tracks changes the semantics of the op, to support backwards\n-  compatibility. Version 1 carries an MHLO text or bytecode `module`. From\n+  compatibility. Minimum supported version is 2. From\n   version 2, the op carries a StableHLO text or bytecode `module`. From\n   version 3, the op also supports the `platforms` attribute.\n module: A serialized computation, a text or bytecode representation of"
        }
    ]
},
{
    "Id": 55,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/aee99960dc090f7f9f18fa37e889689eede0aab8",
    "date": "2024-05-08T19:35:34-07:00",
    "message": "Fix shape mismatch when handling reshape in SPMD partitioner.\n\n1. Move shape check for reshape from `HloInstruction::CreateShape` into the constructor, so that illegal reshape is caught at creation time from clone.\n2. Enhance `hlo_sharding_util::ReshapeSharding`. It can handle the cases where\n* `source_shape_size % source_tile_size == 0`\n* `source_shape_size % target_shape_size == 0`\n* `(source_tile_size % target_shape_size != 0) || (target_shape_size % source_tile_size != 0)`\n3. Enhance `SpmdPartitioningVisitor::HandleReshape`. Unify the two cases in one lambda function. It attempts the original output sharding and then the desired output sharding.\n\nPiperOrigin-RevId: 631996882",
    "label": "YES",
    "changes": [
        {
            "name": "hlo_instruction.cc",
            "path": "third_party/xla/xla/hlo/ir/hlo_instruction.cc",
            "patches": [
                {
                    "old_start": 2097,
                    "old_length": 12,
                    "new_start": 2097,
                    "new_length": 6,
                    "hunk": "@@ -2097,12 +2097,6 @@ HloInstruction::CreateBroadcastSequence(\n \n /* static */ std::unique_ptr<HloInstruction> HloInstruction::CreateReshape(\n     const Shape& shape, HloInstruction* operand, int64_t inferred_dimension) {\n-  CHECK(operand->shape().is_unbounded_dynamic() ||\n-        ShapeUtil::StaticExtentProduct(shape) ==\n-            ShapeUtil::StaticExtentProduct(operand->shape()))\n-      << \"shape: \" << ShapeUtil::HumanString(shape)\n-      << \" operand: \" << ShapeUtil::HumanString(operand->shape());\n-\n   return std::make_unique<HloReshapeInstruction>(shape, operand,\n                                                  inferred_dimension);\n }\n"
                }
            ],
            "whole_deleted": "-  CHECK(operand->shape().is_unbounded_dynamic() ||\n-        ShapeUtil::StaticExtentProduct(shape) ==\n-            ShapeUtil::StaticExtentProduct(operand->shape()))\n-      << \"shape: \" << ShapeUtil::HumanString(shape)\n-      << \" operand: \" << ShapeUtil::HumanString(operand->shape());\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -2097,12 +2097,6 @@ HloInstruction::CreateBroadcastSequence(\n \n /* static */ std::unique_ptr<HloInstruction> HloInstruction::CreateReshape(\n     const Shape& shape, HloInstruction* operand, int64_t inferred_dimension) {\n-  CHECK(operand->shape().is_unbounded_dynamic() ||\n-        ShapeUtil::StaticExtentProduct(shape) ==\n-            ShapeUtil::StaticExtentProduct(operand->shape()))\n-      << \"shape: \" << ShapeUtil::HumanString(shape)\n-      << \" operand: \" << ShapeUtil::HumanString(operand->shape());\n-\n   return std::make_unique<HloReshapeInstruction>(shape, operand,\n                                                  inferred_dimension);\n }\n"
        },
        {
            "name": "hlo_instructions.cc",
            "path": "third_party/xla/xla/hlo/ir/hlo_instructions.cc",
            "patches": [
                {
                    "old_start": 1555,
                    "old_length": 6,
                    "new_start": 1555,
                    "new_length": 11,
                    "hunk": "@@ -1555,6 +1555,11 @@ HloReshapeInstruction::HloReshapeInstruction(const Shape& shape,\n                                              int64_t inferred_dimension)\n     : HloInstruction(HloOpcode::kReshape, shape),\n       inferred_dimension_(inferred_dimension) {\n+  CHECK(operand->shape().is_unbounded_dynamic() ||\n+        ShapeUtil::StaticExtentProduct(shape) ==\n+            ShapeUtil::StaticExtentProduct(operand->shape()))\n+      << \"shape: \" << ShapeUtil::HumanString(shape)\n+      << \" operand: \" << ShapeUtil::HumanString(operand->shape());\n   AppendOperand(operand);\n }\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  CHECK(operand->shape().is_unbounded_dynamic() ||\n+        ShapeUtil::StaticExtentProduct(shape) ==\n+            ShapeUtil::StaticExtentProduct(operand->shape()))\n+      << \"shape: \" << ShapeUtil::HumanString(shape)\n+      << \" operand: \" << ShapeUtil::HumanString(operand->shape());\n",
            "whole_hunk": "@@ -1555,6 +1555,11 @@ HloReshapeInstruction::HloReshapeInstruction(const Shape& shape,\n                                              int64_t inferred_dimension)\n     : HloInstruction(HloOpcode::kReshape, shape),\n       inferred_dimension_(inferred_dimension) {\n+  CHECK(operand->shape().is_unbounded_dynamic() ||\n+        ShapeUtil::StaticExtentProduct(shape) ==\n+            ShapeUtil::StaticExtentProduct(operand->shape()))\n+      << \"shape: \" << ShapeUtil::HumanString(shape)\n+      << \" operand: \" << ShapeUtil::HumanString(operand->shape());\n   AppendOperand(operand);\n }\n \n"
        },
        {
            "name": "hlo_sharding_util.cc",
            "path": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "patches": [
                {
                    "old_start": 813,
                    "old_length": 6,
                    "new_start": 813,
                    "new_length": 7,
                    "hunk": "@@ -813,6 +813,7 @@ std::optional<HloSharding> ReshapeSharding(const Shape& source_shape,\n         source_dims_stack.push_back(s_size / t_size);\n         sharding_tile_dims_stack.push_back(s_partitions / t_size);\n       } else {\n+        append_sharding_dim(std::gcd(t_size, s_partitions));\n         break;\n       }\n     } else {\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        append_sharding_dim(std::gcd(t_size, s_partitions));\n",
            "whole_hunk": "@@ -813,6 +813,7 @@ std::optional<HloSharding> ReshapeSharding(const Shape& source_shape,\n         source_dims_stack.push_back(s_size / t_size);\n         sharding_tile_dims_stack.push_back(s_partitions / t_size);\n       } else {\n+        append_sharding_dim(std::gcd(t_size, s_partitions));\n         break;\n       }\n     } else {\n"
        },
        {
            "name": "hlo_sharding_util_test.cc",
            "path": "third_party/xla/xla/hlo/utils/hlo_sharding_util_test.cc",
            "patches": [
                {
                    "old_start": 179,
                    "old_length": 6,
                    "new_start": 179,
                    "new_length": 18,
                    "hunk": "@@ -179,6 +179,18 @@ TEST(HloShardingUtilTest, ReshapeShardingTiledSplit2) {\n   EXPECT_EQ(result.value(), output_sharding);\n }\n \n+TEST(HloShardingUtilTest, ReshapeShardingTiledSplit3) {\n+  Shape input_shape = ShapeUtil::MakeShape(F32, {36});\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {6, 6});\n+  HloSharding input_sharding = HloSharding::IotaTile({4});\n+  HloSharding output_sharding =\n+      HloSharding::PartialTile(TileAssignment({2, 1, 2}));\n+  std::optional<HloSharding> result =\n+      ReshapeSharding(input_shape, output_shape, input_sharding);\n+  EXPECT_TRUE(result.has_value());\n+  EXPECT_EQ(result.value(), output_sharding);\n+}\n+\n TEST(HloShardingUtilTest, ReshapeShardingTiledSplitThenMerge) {\n   Shape input_shape = ShapeUtil::MakeShape(F32, {16, 4, 7});\n   Shape output_shape = ShapeUtil::MakeShape(F32, {4, 16, 7});\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST(HloShardingUtilTest, ReshapeShardingTiledSplit3) {\n+  Shape input_shape = ShapeUtil::MakeShape(F32, {36});\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {6, 6});\n+  HloSharding input_sharding = HloSharding::IotaTile({4});\n+  HloSharding output_sharding =\n+      HloSharding::PartialTile(TileAssignment({2, 1, 2}));\n+  std::optional<HloSharding> result =\n+      ReshapeSharding(input_shape, output_shape, input_sharding);\n+  EXPECT_TRUE(result.has_value());\n+  EXPECT_EQ(result.value(), output_sharding);\n+}\n+\n",
            "whole_hunk": "@@ -179,6 +179,18 @@ TEST(HloShardingUtilTest, ReshapeShardingTiledSplit2) {\n   EXPECT_EQ(result.value(), output_sharding);\n }\n \n+TEST(HloShardingUtilTest, ReshapeShardingTiledSplit3) {\n+  Shape input_shape = ShapeUtil::MakeShape(F32, {36});\n+  Shape output_shape = ShapeUtil::MakeShape(F32, {6, 6});\n+  HloSharding input_sharding = HloSharding::IotaTile({4});\n+  HloSharding output_sharding =\n+      HloSharding::PartialTile(TileAssignment({2, 1, 2}));\n+  std::optional<HloSharding> result =\n+      ReshapeSharding(input_shape, output_shape, input_sharding);\n+  EXPECT_TRUE(result.has_value());\n+  EXPECT_EQ(result.value(), output_sharding);\n+}\n+\n TEST(HloShardingUtilTest, ReshapeShardingTiledSplitThenMerge) {\n   Shape input_shape = ShapeUtil::MakeShape(F32, {16, 4, 7});\n   Shape output_shape = ShapeUtil::MakeShape(F32, {4, 16, 7});\n"
        },
        {
            "name": "spmd_partitioner.cc",
            "path": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "patches": [
                {
                    "old_start": 2963,
                    "old_length": 36,
                    "new_start": 2963,
                    "new_length": 43,
                    "hunk": "@@ -2963,36 +2963,43 @@ Status SpmdPartitioningVisitor::HandleReshape(HloInstruction* hlo) {\n   }\n \n   auto operand = GetPartitionedHlo(hlo->operand(0));\n-  // The output shape is the source and the operand shape is the target to get\n-  // the aligned sharding for the operand.\n-  std::optional<HloSharding> desired_operand_sharding =\n-      hlo_sharding_util::ReshapeSharding(hlo->shape(), hlo->operand(0)->shape(),\n-                                         hlo->sharding());\n-  // Use the desired operand sharding only if the number of tiles returned\n-  // matches the number of tiles in the output.\n-  if (desired_operand_sharding.has_value() &&\n-      hlo->sharding().NumTiles() == desired_operand_sharding->NumTiles()) {\n-    auto operand_hlo = operand.Reshard(*desired_operand_sharding).hlo();\n-    SetPartitionedHlo(hlo, [&] {\n+  auto desired_operand = [&](const HloSharding& output_sharding)\n+      -> std::optional<HloInstruction*> {\n+    // The output shape is the source and the operand shape is the target to get\n+    // desired_operand_sharding.\n+    std::optional<HloSharding> desired_operand_sharding =\n+        hlo_sharding_util::ReshapeSharding(\n+            hlo->shape(), hlo->operand(0)->shape(), output_sharding);\n+    if (desired_operand_sharding.has_value() &&\n+        output_sharding.NumTiles() == desired_operand_sharding->NumTiles()) {\n       return b_.AddInstruction(hlo->CloneWithNewOperands(\n-          MakePartitionedShape(hlo->shape(), hlo->sharding()), {operand_hlo}));\n-    });\n+          MakePartitionedShape(hlo->shape(), output_sharding),\n+          {operand.Reshard(*desired_operand_sharding).hlo()}));\n+    }\n+    return std::nullopt;\n+  };\n+\n+  // Try the original output sharding at first.\n+  if (auto operand_hlo = desired_operand(hlo->sharding())) {\n+    SetPartitionedHlo(hlo, [&] { return *operand_hlo; });\n     return OkStatus();\n   }\n+\n+  // Then try the desired_output_sharding.\n   std::optional<HloSharding> desired_output_sharding =\n       hlo_sharding_util::ReshapeSharding(hlo->operand(0)->shape(), hlo->shape(),\n                                          operand.sharding());\n   if (desired_output_sharding.has_value()) {\n-    auto reshape = b_.AddInstruction(hlo->CloneWithNewOperands(\n-        MakePartitionedShape(hlo->shape(), *desired_output_sharding),\n-        {operand.hlo()}));\n-    reshape->set_sharding(*desired_output_sharding);\n-    SetPartitionedHlo(hlo, [&] {\n-      return PartitionedHlo(reshape, hlo->shape(), MakePartitioningState())\n-          .Reshard(sharding)\n-          .hlo();\n-    });\n-    return OkStatus();\n+    if (auto operand_hlo = desired_operand(*desired_output_sharding)) {\n+      (*operand_hlo)->set_sharding(*desired_output_sharding);\n+      SetPartitionedHlo(hlo, [&] {\n+        return PartitionedHlo(*operand_hlo, hlo->shape(),\n+                              MakePartitioningState())\n+            .Reshard(hlo->sharding())\n+            .hlo();\n+      });\n+      return OkStatus();\n+    }\n   }\n \n   auto shard_reshape =\n"
                }
            ],
            "whole_deleted": "-  // The output shape is the source and the operand shape is the target to get\n-  // the aligned sharding for the operand.\n-  std::optional<HloSharding> desired_operand_sharding =\n-      hlo_sharding_util::ReshapeSharding(hlo->shape(), hlo->operand(0)->shape(),\n-                                         hlo->sharding());\n-  // Use the desired operand sharding only if the number of tiles returned\n-  // matches the number of tiles in the output.\n-  if (desired_operand_sharding.has_value() &&\n-      hlo->sharding().NumTiles() == desired_operand_sharding->NumTiles()) {\n-    auto operand_hlo = operand.Reshard(*desired_operand_sharding).hlo();\n-    SetPartitionedHlo(hlo, [&] {\n-          MakePartitionedShape(hlo->shape(), hlo->sharding()), {operand_hlo}));\n-    });\n-    auto reshape = b_.AddInstruction(hlo->CloneWithNewOperands(\n-        MakePartitionedShape(hlo->shape(), *desired_output_sharding),\n-        {operand.hlo()}));\n-    reshape->set_sharding(*desired_output_sharding);\n-    SetPartitionedHlo(hlo, [&] {\n-      return PartitionedHlo(reshape, hlo->shape(), MakePartitioningState())\n-          .Reshard(sharding)\n-          .hlo();\n-    });\n-    return OkStatus();\n",
            "whole_added": "+  auto desired_operand = [&](const HloSharding& output_sharding)\n+      -> std::optional<HloInstruction*> {\n+    // The output shape is the source and the operand shape is the target to get\n+    // desired_operand_sharding.\n+    std::optional<HloSharding> desired_operand_sharding =\n+        hlo_sharding_util::ReshapeSharding(\n+            hlo->shape(), hlo->operand(0)->shape(), output_sharding);\n+    if (desired_operand_sharding.has_value() &&\n+        output_sharding.NumTiles() == desired_operand_sharding->NumTiles()) {\n+          MakePartitionedShape(hlo->shape(), output_sharding),\n+          {operand.Reshard(*desired_operand_sharding).hlo()}));\n+    }\n+    return std::nullopt;\n+  };\n+\n+  // Try the original output sharding at first.\n+  if (auto operand_hlo = desired_operand(hlo->sharding())) {\n+    SetPartitionedHlo(hlo, [&] { return *operand_hlo; });\n+\n+  // Then try the desired_output_sharding.\n+    if (auto operand_hlo = desired_operand(*desired_output_sharding)) {\n+      (*operand_hlo)->set_sharding(*desired_output_sharding);\n+      SetPartitionedHlo(hlo, [&] {\n+        return PartitionedHlo(*operand_hlo, hlo->shape(),\n+                              MakePartitioningState())\n+            .Reshard(hlo->sharding())\n+            .hlo();\n+      });\n+      return OkStatus();\n+    }\n",
            "whole_hunk": "@@ -2963,36 +2963,43 @@ Status SpmdPartitioningVisitor::HandleReshape(HloInstruction* hlo) {\n   }\n \n   auto operand = GetPartitionedHlo(hlo->operand(0));\n-  // The output shape is the source and the operand shape is the target to get\n-  // the aligned sharding for the operand.\n-  std::optional<HloSharding> desired_operand_sharding =\n-      hlo_sharding_util::ReshapeSharding(hlo->shape(), hlo->operand(0)->shape(),\n-                                         hlo->sharding());\n-  // Use the desired operand sharding only if the number of tiles returned\n-  // matches the number of tiles in the output.\n-  if (desired_operand_sharding.has_value() &&\n-      hlo->sharding().NumTiles() == desired_operand_sharding->NumTiles()) {\n-    auto operand_hlo = operand.Reshard(*desired_operand_sharding).hlo();\n-    SetPartitionedHlo(hlo, [&] {\n+  auto desired_operand = [&](const HloSharding& output_sharding)\n+      -> std::optional<HloInstruction*> {\n+    // The output shape is the source and the operand shape is the target to get\n+    // desired_operand_sharding.\n+    std::optional<HloSharding> desired_operand_sharding =\n+        hlo_sharding_util::ReshapeSharding(\n+            hlo->shape(), hlo->operand(0)->shape(), output_sharding);\n+    if (desired_operand_sharding.has_value() &&\n+        output_sharding.NumTiles() == desired_operand_sharding->NumTiles()) {\n       return b_.AddInstruction(hlo->CloneWithNewOperands(\n-          MakePartitionedShape(hlo->shape(), hlo->sharding()), {operand_hlo}));\n-    });\n+          MakePartitionedShape(hlo->shape(), output_sharding),\n+          {operand.Reshard(*desired_operand_sharding).hlo()}));\n+    }\n+    return std::nullopt;\n+  };\n+\n+  // Try the original output sharding at first.\n+  if (auto operand_hlo = desired_operand(hlo->sharding())) {\n+    SetPartitionedHlo(hlo, [&] { return *operand_hlo; });\n     return OkStatus();\n   }\n+\n+  // Then try the desired_output_sharding.\n   std::optional<HloSharding> desired_output_sharding =\n       hlo_sharding_util::ReshapeSharding(hlo->operand(0)->shape(), hlo->shape(),\n                                          operand.sharding());\n   if (desired_output_sharding.has_value()) {\n-    auto reshape = b_.AddInstruction(hlo->CloneWithNewOperands(\n-        MakePartitionedShape(hlo->shape(), *desired_output_sharding),\n-        {operand.hlo()}));\n-    reshape->set_sharding(*desired_output_sharding);\n-    SetPartitionedHlo(hlo, [&] {\n-      return PartitionedHlo(reshape, hlo->shape(), MakePartitioningState())\n-          .Reshard(sharding)\n-          .hlo();\n-    });\n-    return OkStatus();\n+    if (auto operand_hlo = desired_operand(*desired_output_sharding)) {\n+      (*operand_hlo)->set_sharding(*desired_output_sharding);\n+      SetPartitionedHlo(hlo, [&] {\n+        return PartitionedHlo(*operand_hlo, hlo->shape(),\n+                              MakePartitioningState())\n+            .Reshard(hlo->sharding())\n+            .hlo();\n+      });\n+      return OkStatus();\n+    }\n   }\n \n   auto shard_reshape =\n"
        },
        {
            "name": "spmd_partitioner_test.cc",
            "path": "third_party/xla/xla/service/spmd/spmd_partitioner_test.cc",
            "patches": [
                {
                    "old_start": 3656,
                    "old_length": 6,
                    "new_start": 3656,
                    "new_length": 45,
                    "hunk": "@@ -3656,6 +3656,45 @@ ENTRY entry {\n                               op::AllToAll(op::Reshape(local_reshape))))));\n }\n \n+// The test case is derived from b/338145758.\n+TEST_P(SpmdPartitioningTest, ReshapeWithReshard3) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY %reshape {\n+  p0 = bf16[80,64,2,2,2,2,2] parameter(0), sharding={devices=[16,8,1,1,1,1,1]<=[128]}\n+  ROOT reshape = bf16[5120,4,8] reshape(p0), sharding={devices=[128,1,1]<=[128]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, PartitionComputation(hlo_string, /*num_devices=*/128));\n+  VLOG(1) << module->ToString();\n+  const auto root = module->entry_computation()->root_instruction();\n+  auto reshape = AllOf(op::Reshape(op::AllReduce(op::DynamicUpdateSlice(\n+                           _, op::Parameter(0), _, _, _, _, _, _, _))),\n+                       op::Shape(\"bf16[320,4,8]\"));\n+  EXPECT_THAT(root, AllOf(op::DynamicSlice(reshape, _, _, _),\n+                          op::Shape(\"bf16[40,4,8]\")));\n+}\n+\n+TEST_P(SpmdPartitioningTest, ReshapeWithReshard4) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY %reshape {\n+  p0 = bf16[80,64,8,2,2,2,2] parameter(0), sharding={devices=[16,1,8,1,1,1,1]<=[128]}\n+  ROOT reshape = bf16[5120,16,8] reshape(p0), sharding={devices=[128,1,1]<=[128]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, PartitionComputation(hlo_string, /*num_devices=*/128));\n+  VLOG(1) << module->ToString();\n+  const auto root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root,\n+              AllOf(op::Reshape(op::Reshape(op::Transpose(op::AllToAll()))),\n+                    op::Shape(\"bf16[40,16,8]\")));\n+}\n+\n TEST_P(SpmdPartitioningTest, PartialReplicateShardableReshape) {\n   absl::string_view hlo_string = R\"(\n HloModule module"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// The test case is derived from b/338145758.\n+TEST_P(SpmdPartitioningTest, ReshapeWithReshard3) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY %reshape {\n+  p0 = bf16[80,64,2,2,2,2,2] parameter(0), sharding={devices=[16,8,1,1,1,1,1]<=[128]}\n+  ROOT reshape = bf16[5120,4,8] reshape(p0), sharding={devices=[128,1,1]<=[128]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, PartitionComputation(hlo_string, /*num_devices=*/128));\n+  VLOG(1) << module->ToString();\n+  const auto root = module->entry_computation()->root_instruction();\n+  auto reshape = AllOf(op::Reshape(op::AllReduce(op::DynamicUpdateSlice(\n+                           _, op::Parameter(0), _, _, _, _, _, _, _))),\n+                       op::Shape(\"bf16[320,4,8]\"));\n+  EXPECT_THAT(root, AllOf(op::DynamicSlice(reshape, _, _, _),\n+                          op::Shape(\"bf16[40,4,8]\")));\n+}\n+\n+TEST_P(SpmdPartitioningTest, ReshapeWithReshard4) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY %reshape {\n+  p0 = bf16[80,64,8,2,2,2,2] parameter(0), sharding={devices=[16,1,8,1,1,1,1]<=[128]}\n+  ROOT reshape = bf16[5120,16,8] reshape(p0), sharding={devices=[128,1,1]<=[128]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, PartitionComputation(hlo_string, /*num_devices=*/128));\n+  VLOG(1) << module->ToString();\n+  const auto root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root,\n+              AllOf(op::Reshape(op::Reshape(op::Transpose(op::AllToAll()))),\n+                    op::Shape(\"bf16[40,16,8]\")));\n+}\n+\n",
            "whole_hunk": "@@ -3656,6 +3656,45 @@ ENTRY entry {\n                               op::AllToAll(op::Reshape(local_reshape))))));\n }\n \n+// The test case is derived from b/338145758.\n+TEST_P(SpmdPartitioningTest, ReshapeWithReshard3) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY %reshape {\n+  p0 = bf16[80,64,2,2,2,2,2] parameter(0), sharding={devices=[16,8,1,1,1,1,1]<=[128]}\n+  ROOT reshape = bf16[5120,4,8] reshape(p0), sharding={devices=[128,1,1]<=[128]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, PartitionComputation(hlo_string, /*num_devices=*/128));\n+  VLOG(1) << module->ToString();\n+  const auto root = module->entry_computation()->root_instruction();\n+  auto reshape = AllOf(op::Reshape(op::AllReduce(op::DynamicUpdateSlice(\n+                           _, op::Parameter(0), _, _, _, _, _, _, _))),\n+                       op::Shape(\"bf16[320,4,8]\"));\n+  EXPECT_THAT(root, AllOf(op::DynamicSlice(reshape, _, _, _),\n+                          op::Shape(\"bf16[40,4,8]\")));\n+}\n+\n+TEST_P(SpmdPartitioningTest, ReshapeWithReshard4) {\n+  absl::string_view hlo_string = R\"(\n+HloModule module\n+\n+ENTRY %reshape {\n+  p0 = bf16[80,64,8,2,2,2,2] parameter(0), sharding={devices=[16,1,8,1,1,1,1]<=[128]}\n+  ROOT reshape = bf16[5120,16,8] reshape(p0), sharding={devices=[128,1,1]<=[128]}\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto module, PartitionComputation(hlo_string, /*num_devices=*/128));\n+  VLOG(1) << module->ToString();\n+  const auto root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root,\n+              AllOf(op::Reshape(op::Reshape(op::Transpose(op::AllToAll()))),\n+                    op::Shape(\"bf16[40,16,8]\")));\n+}\n+\n TEST_P(SpmdPartitioningTest, PartialReplicateShardableReshape) {\n   absl::string_view hlo_string = R\"(\n HloModule module"
        }
    ]
},
{
    "Id": 618,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/61ad42acb4fa48e2fc6140a4fad41aa8ad85ea52",
    "date": "2022-12-06T10:46:53+00:00",
    "message": "Merge int16 EXP zero-point check into the else if",
    "label": "NO",
    "changes": [
        {
            "name": "exp.cc",
            "path": "tensorflow/lite/kernels/exp.cc",
            "patches": [
                {
                    "old_start": 66,
                    "old_length": 17,
                    "new_start": 66,
                    "new_length": 15,
                    "hunk": "@@ -66,17 +66,15 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TfLiteIntArray* output_dims = TfLiteIntArrayCopy(input->dims);\n   output->type = input->type;\n \n-  if (input->type == kTfLiteInt16) {\n-    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n-    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n-  }\n-\n   if (input->type == kTfLiteInt8) {\n     LUTPopulate<int8_t>(input->params.scale, input->params.zero_point,\n                         output->params.scale, output->params.zero_point,\n                         [](float value) { return std::exp(value); },\n                         data->lut_int8);\n   } else if (input->type == kTfLiteInt16) {\n+    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n+    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n+\n     LUTPopulate<int16_t>(input->params.scale, input->params.zero_point,\n                          output->params.scale, output->params.zero_point,\n                          [](float value) { return std::exp(value); },"
                }
            ],
            "whole_deleted": "-  if (input->type == kTfLiteInt16) {\n-    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n-    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n-  }\n-\n",
            "whole_added": "+    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n+    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n+\n",
            "whole_hunk": "@@ -66,17 +66,15 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TfLiteIntArray* output_dims = TfLiteIntArrayCopy(input->dims);\n   output->type = input->type;\n \n-  if (input->type == kTfLiteInt16) {\n-    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n-    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n-  }\n-\n   if (input->type == kTfLiteInt8) {\n     LUTPopulate<int8_t>(input->params.scale, input->params.zero_point,\n                         output->params.scale, output->params.zero_point,\n                         [](float value) { return std::exp(value); },\n                         data->lut_int8);\n   } else if (input->type == kTfLiteInt16) {\n+    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n+    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n+\n     LUTPopulate<int16_t>(input->params.scale, input->params.zero_point,\n                          output->params.scale, output->params.zero_point,\n                          [](float value) { return std::exp(value); },"
        }
    ]
},
{
    "Id": 319,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/43fd10302bcc8447e7a7205bae848a3a88624775",
    "date": "2023-08-14T08:27:11-07:00",
    "message": "Return error on invalid input in `tfl.atan2_custom`\n\nPiperOrigin-RevId: 556797683",
    "label": "YES",
    "changes": [
        {
            "name": "atan2_custom.cc",
            "path": "tensorflow/lite/kernels/atan2_custom.cc",
            "patches": [
                {
                    "old_start": 79,
                    "old_length": 9,
                    "new_start": 79,
                    "new_length": 11,
                    "hunk": "@@ -79,9 +79,11 @@ TfLiteStatus Atan2Eval(TfLiteContext* context, TfLiteNode* node) {\n     case kTfLiteFloat64:\n       TF_LITE_ENSURE_OK(context, Atan2<double>(input_y, input_x, output));\n       break;\n-    default:\n+    default: {\n       TF_LITE_KERNEL_LOG(context, \"Unsupported datatype for atan2 output: %s\",\n                          TfLiteTypeGetName(output->type));\n+      return TfLiteStatus::kTfLiteError;\n+    }\n   }\n \n   return TfLiteStatus::kTfLiteOk;"
                }
            ],
            "whole_deleted": "-    default:\n",
            "whole_added": "+    default: {\n+      return TfLiteStatus::kTfLiteError;\n+    }\n",
            "whole_hunk": "@@ -79,9 +79,11 @@ TfLiteStatus Atan2Eval(TfLiteContext* context, TfLiteNode* node) {\n     case kTfLiteFloat64:\n       TF_LITE_ENSURE_OK(context, Atan2<double>(input_y, input_x, output));\n       break;\n-    default:\n+    default: {\n       TF_LITE_KERNEL_LOG(context, \"Unsupported datatype for atan2 output: %s\",\n                          TfLiteTypeGetName(output->type));\n+      return TfLiteStatus::kTfLiteError;\n+    }\n   }\n \n   return TfLiteStatus::kTfLiteOk;"
        }
    ]
},
{
    "Id": 209,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/788486f6c3c5ae651cf0cb409c5d85286d2f61ce",
    "date": "2024-01-08T09:15:22-08:00",
    "message": "Fix NCCL and CUDA definition check.\n\nPiperOrigin-RevId: 596612409",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "patches": [
                {
                    "old_start": 11,
                    "old_length": 7,
                    "new_start": 11,
                    "new_length": 7,
                    "hunk": "@@ -11,7 +11,7 @@ load(\n     \"tf_additional_cudnn_plugin_copts\",\n     \"tf_additional_gpu_compilation_copts\",\n )\n-load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"set_external_visibility\", \"tsl_copts\")\n+load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"if_nccl\", \"set_external_visibility\", \"tsl_copts\")\n load(\n     \"@local_tsl//tsl/platform:build_config_root.bzl\",\n     \"if_static\",\n"
                },
                {
                    "old_start": 130,
                    "old_length": 7,
                    "new_start": 130,
                    "new_length": 7,
                    "hunk": "@@ -130,7 +130,7 @@ cc_library(\n     name = \"cuda_driver\",\n     srcs = if_cuda_is_configured([\"cuda_driver.cc\"]),\n     hdrs = if_cuda_is_configured([\"cuda_driver.h\"]),\n-    defines = if_cuda([\"XLA_ENABLE_XCCL\"]),\n+    defines = if_cuda([\"XLA_ENABLE_CUDA\"]) + if_nccl([\"XLA_ENABLE_XCCL\"]),\n     visibility = [\"//visibility:public\"],\n     deps = if_cuda_is_configured([\n         \":cuda_diagnostics\",\n"
                },
                {
                    "old_start": 164,
                    "old_length": 7,
                    "new_start": 164,
                    "new_length": 7,
                    "hunk": "@@ -164,7 +164,7 @@ cc_library(\n         \"@local_tsl//tsl/platform:stacktrace\",\n         \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n-    ]) + if_cuda([\"@local_config_nccl//:nccl\"]),\n+    ]) + if_nccl([\"@local_config_nccl//:nccl\"]),\n )\n \n cc_library(\n"
                }
            ],
            "whole_deleted": "-load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"set_external_visibility\", \"tsl_copts\")\n-    defines = if_cuda([\"XLA_ENABLE_XCCL\"]),\n-    ]) + if_cuda([\"@local_config_nccl//:nccl\"]),\n",
            "whole_added": "+load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"if_nccl\", \"set_external_visibility\", \"tsl_copts\")\n+    defines = if_cuda([\"XLA_ENABLE_CUDA\"]) + if_nccl([\"XLA_ENABLE_XCCL\"]),\n+    ]) + if_nccl([\"@local_config_nccl//:nccl\"]),\n",
            "whole_hunk": "@@ -11,7 +11,7 @@ load(\n     \"tf_additional_cudnn_plugin_copts\",\n     \"tf_additional_gpu_compilation_copts\",\n )\n-load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"set_external_visibility\", \"tsl_copts\")\n+load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"if_nccl\", \"set_external_visibility\", \"tsl_copts\")\n load(\n     \"@local_tsl//tsl/platform:build_config_root.bzl\",\n     \"if_static\",\n@@ -130,7 +130,7 @@ cc_library(\n     name = \"cuda_driver\",\n     srcs = if_cuda_is_configured([\"cuda_driver.cc\"]),\n     hdrs = if_cuda_is_configured([\"cuda_driver.h\"]),\n-    defines = if_cuda([\"XLA_ENABLE_XCCL\"]),\n+    defines = if_cuda([\"XLA_ENABLE_CUDA\"]) + if_nccl([\"XLA_ENABLE_XCCL\"]),\n     visibility = [\"//visibility:public\"],\n     deps = if_cuda_is_configured([\n         \":cuda_diagnostics\",\n@@ -164,7 +164,7 @@ cc_library(\n         \"@local_tsl//tsl/platform:stacktrace\",\n         \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n-    ]) + if_cuda([\"@local_config_nccl//:nccl\"]),\n+    ]) + if_nccl([\"@local_config_nccl//:nccl\"]),\n )\n \n cc_library(\n"
        },
        {
            "name": "cuda_driver.cc",
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_driver.cc",
            "patches": [
                {
                    "old_start": 57,
                    "old_length": 9,
                    "new_start": 57,
                    "new_length": 9,
                    "hunk": "@@ -57,9 +57,9 @@ limitations under the License.\n #include \"tsl/platform/statusor.h\"\n #include \"tsl/platform/threadpool.h\"\n \n-#ifdef XLA_ENABLE_XCCL\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n #include \"third_party/nccl/nccl.h\"\n-#endif  // XLA_ENABLE_XCCL\n+#endif  // XLA_ENABLE_XCCL && XLA_ENABLE_CUDA\n \n static constexpr bool FLAGS_gpuexec_cuda_driver_inject_init_error = false;\n static constexpr bool FLAGS_gpuexec_cuda_sync_around_driver_calls = false;\n"
                },
                {
                    "old_start": 1636,
                    "old_length": 7,
                    "new_start": 1636,
                    "new_length": 7,
                    "hunk": "@@ -1636,7 +1636,7 @@ struct BitPatternToValue {\n   ScopedActivateContext activated{context};\n   void* ptr = nullptr;\n \n-#ifdef XLA_ENABLE_XCCL\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n   ncclResult_t res = ncclMemAlloc(&ptr, bytes);\n   if (res != ncclSuccess) {\n     return absl::InternalError(absl::StrFormat(\n"
                },
                {
                    "old_start": 1659,
                    "old_length": 7,
                    "new_start": 1659,
                    "new_length": 7,
                    "hunk": "@@ -1659,7 +1659,7 @@ struct BitPatternToValue {\n     GpuContext* context, void* location) {\n   ScopedActivateContext activation(context);\n \n-#ifdef XLA_ENABLE_XCCL\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n   ncclResult_t res = ncclMemFree(location);\n   if (res != ncclSuccess) {\n     return absl::InternalError(absl::StrFormat("
                }
            ],
            "whole_deleted": "-#ifdef XLA_ENABLE_XCCL\n-#endif  // XLA_ENABLE_XCCL\n-#ifdef XLA_ENABLE_XCCL\n-#ifdef XLA_ENABLE_XCCL\n",
            "whole_added": "+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n+#endif  // XLA_ENABLE_XCCL && XLA_ENABLE_CUDA\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n",
            "whole_hunk": "@@ -57,9 +57,9 @@ limitations under the License.\n #include \"tsl/platform/statusor.h\"\n #include \"tsl/platform/threadpool.h\"\n \n-#ifdef XLA_ENABLE_XCCL\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n #include \"third_party/nccl/nccl.h\"\n-#endif  // XLA_ENABLE_XCCL\n+#endif  // XLA_ENABLE_XCCL && XLA_ENABLE_CUDA\n \n static constexpr bool FLAGS_gpuexec_cuda_driver_inject_init_error = false;\n static constexpr bool FLAGS_gpuexec_cuda_sync_around_driver_calls = false;\n@@ -1636,7 +1636,7 @@ struct BitPatternToValue {\n   ScopedActivateContext activated{context};\n   void* ptr = nullptr;\n \n-#ifdef XLA_ENABLE_XCCL\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n   ncclResult_t res = ncclMemAlloc(&ptr, bytes);\n   if (res != ncclSuccess) {\n     return absl::InternalError(absl::StrFormat(\n@@ -1659,7 +1659,7 @@ struct BitPatternToValue {\n     GpuContext* context, void* location) {\n   ScopedActivateContext activation(context);\n \n-#ifdef XLA_ENABLE_XCCL\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n   ncclResult_t res = ncclMemFree(location);\n   if (res != ncclSuccess) {\n     return absl::InternalError(absl::StrFormat("
        }
    ]
},
{
    "Id": 76,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7186b5b24a616a4ad44c703677a7a74c10ae8b64",
    "date": "2024-04-15T09:58:31+05:30",
    "message": "diag_rank validation in MatrixDiagV3\n\nSimplified diag_rank validation in MatrixDiagV3.",
    "label": "YES",
    "changes": [
        {
            "name": "matrix_diag_op.cc",
            "path": "tensorflow/core/kernels/linalg/matrix_diag_op.cc",
            "patches": [
                {
                    "old_start": 236,
                    "old_length": 21,
                    "new_start": 236,
                    "new_length": 13,
                    "hunk": "@@ -236,21 +236,13 @@ class MatrixDiagOp : public OpKernel {\n         errors::InvalidArgument(\n             \"lower_diag_index must not be larger than upper_diag_index: \",\n             lower_diag_index, \" > \", upper_diag_index));\n-    if (diag_rank == 1) {\n-        OP_REQUIRES(context,\n-            lower_diag_index == upper_diag_index ||\n-                diagonal_shape.dim_size(diag_rank - 1) == num_diags,\n-            absl::InvalidArgumentError(\n-                \"The number of diagonals provided in the input does not \"\n-                \"match the lower_diag_index and upper_diag_index range.\"));\n-    } else if (diag_rank > 1) {\n-        OP_REQUIRES(context,\n-            lower_diag_index == upper_diag_index ||\n-                diagonal_shape.dim_size(diag_rank - 2) == num_diags,\n-            absl::InvalidArgumentError(\n-                \"The number of diagonals provided in the input does not \"\n-                \"match the lower_diag_index and upper_diag_index range.\"));\n-    }\n+    OP_REQUIRES(context,\n+        lower_diag_index == upper_diag_index ||\n+        diagonal_shape.dim_size(std::max(diag_rank - 2, 0)) == num_diags,\n+        errors::InvalidArgument(\n+            \"The number of diagonals provided in the input does not \"\n+            \"match the lower_diag_index and upper_diag_index range.\"));\n+\n     const Eigen::Index max_diag_len = diagonal_shape.dim_size(diag_rank - 1);\n     const Eigen::Index min_num_rows =\n         max_diag_len - std::min(upper_diag_index, 0);"
                }
            ],
            "whole_deleted": "-    if (diag_rank == 1) {\n-        OP_REQUIRES(context,\n-            lower_diag_index == upper_diag_index ||\n-                diagonal_shape.dim_size(diag_rank - 1) == num_diags,\n-            absl::InvalidArgumentError(\n-                \"The number of diagonals provided in the input does not \"\n-                \"match the lower_diag_index and upper_diag_index range.\"));\n-    } else if (diag_rank > 1) {\n-        OP_REQUIRES(context,\n-            lower_diag_index == upper_diag_index ||\n-                diagonal_shape.dim_size(diag_rank - 2) == num_diags,\n-            absl::InvalidArgumentError(\n-                \"The number of diagonals provided in the input does not \"\n-                \"match the lower_diag_index and upper_diag_index range.\"));\n-    }\n",
            "whole_added": "+    OP_REQUIRES(context,\n+        lower_diag_index == upper_diag_index ||\n+        diagonal_shape.dim_size(std::max(diag_rank - 2, 0)) == num_diags,\n+        errors::InvalidArgument(\n+            \"The number of diagonals provided in the input does not \"\n+            \"match the lower_diag_index and upper_diag_index range.\"));\n+\n",
            "whole_hunk": "@@ -236,21 +236,13 @@ class MatrixDiagOp : public OpKernel {\n         errors::InvalidArgument(\n             \"lower_diag_index must not be larger than upper_diag_index: \",\n             lower_diag_index, \" > \", upper_diag_index));\n-    if (diag_rank == 1) {\n-        OP_REQUIRES(context,\n-            lower_diag_index == upper_diag_index ||\n-                diagonal_shape.dim_size(diag_rank - 1) == num_diags,\n-            absl::InvalidArgumentError(\n-                \"The number of diagonals provided in the input does not \"\n-                \"match the lower_diag_index and upper_diag_index range.\"));\n-    } else if (diag_rank > 1) {\n-        OP_REQUIRES(context,\n-            lower_diag_index == upper_diag_index ||\n-                diagonal_shape.dim_size(diag_rank - 2) == num_diags,\n-            absl::InvalidArgumentError(\n-                \"The number of diagonals provided in the input does not \"\n-                \"match the lower_diag_index and upper_diag_index range.\"));\n-    }\n+    OP_REQUIRES(context,\n+        lower_diag_index == upper_diag_index ||\n+        diagonal_shape.dim_size(std::max(diag_rank - 2, 0)) == num_diags,\n+        errors::InvalidArgument(\n+            \"The number of diagonals provided in the input does not \"\n+            \"match the lower_diag_index and upper_diag_index range.\"));\n+\n     const Eigen::Index max_diag_len = diagonal_shape.dim_size(diag_rank - 1);\n     const Eigen::Index min_num_rows =\n         max_diag_len - std::min(upper_diag_index, 0);"
        }
    ]
},
{
    "Id": 2,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c3b618f959f8afcec8c42a6ab9639c047e61c518",
    "date": "2024-07-18T11:14:30-07:00",
    "message": "Move optional checking for creating GpuTimers into callsites to remove convenience method.\n\nPiperOrigin-RevId: 653681508",
    "label": "NO",
    "changes": [
        {
            "name": "cuda_blas.cc",
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_blas.cc",
            "patches": [
                {
                    "old_start": 718,
                    "old_length": 12,
                    "new_start": 718,
                    "new_length": 12,
                    "hunk": "@@ -718,12 +718,12 @@ absl::Status CUDABlas::DoBlasGemmWithAlgorithm(\n       cublasMath_t math_type,\n       GetMathTypeForGemmEx(stream, algorithm, type_a, type_b, numeric_options));\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          output_profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   // Since we are converting 'algorithm' to cublasGemmAlgo_t by static_cast,\n   // we do the following compile-time check on the default value:\n"
                },
                {
                    "old_start": 753,
                    "old_length": 12,
                    "new_start": 753,
                    "new_length": 12,
                    "hunk": "@@ -753,12 +753,12 @@ absl::Status CUDABlas::DoBlasGemmStridedBatchedWithAlgorithm(\n   TF_ASSIGN_OR_RETURN(\n       cublasMath_t math_type,\n       GetMathTypeForGemmEx(stream, algorithm, type_a, type_b, numeric_options));\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          output_profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n   cudaDataType_t cuda_in_type = AsCudaDataType(type_a);\n \n #if CUDA_VERSION >= 11000\n"
                }
            ],
            "whole_deleted": "-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          output_profile_result != nullptr));\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          output_profile_result != nullptr));\n",
            "whole_added": "+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n",
            "whole_hunk": "@@ -718,12 +718,12 @@ absl::Status CUDABlas::DoBlasGemmWithAlgorithm(\n       cublasMath_t math_type,\n       GetMathTypeForGemmEx(stream, algorithm, type_a, type_b, numeric_options));\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          output_profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   // Since we are converting 'algorithm' to cublasGemmAlgo_t by static_cast,\n   // we do the following compile-time check on the default value:\n@@ -753,12 +753,12 @@ absl::Status CUDABlas::DoBlasGemmStridedBatchedWithAlgorithm(\n   TF_ASSIGN_OR_RETURN(\n       cublasMath_t math_type,\n       GetMathTypeForGemmEx(stream, algorithm, type_a, type_b, numeric_options));\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          output_profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n   cudaDataType_t cuda_in_type = AsCudaDataType(type_a);\n \n #if CUDA_VERSION >= 11000\n"
        },
        {
            "name": "cuda_blas_lt.cc",
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_blas_lt.cc",
            "patches": [
                {
                    "old_start": 406,
                    "old_length": 11,
                    "new_start": 406,
                    "new_length": 12,
                    "hunk": "@@ -406,11 +406,12 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n     std::optional<DeviceMemoryBase> workspace,\n     std::optional<ScratchAllocator*> scratch_allocator,\n     blas::ProfileResult* profile_result = nullptr) const {\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<gpu::GpuTimer> timer,\n-      gpu::GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n+  std::optional<gpu::GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        gpu::GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   void* workspace_addr;\n   uint64_t workspace_size = 0;\n"
                }
            ],
            "whole_deleted": "-  TF_ASSIGN_OR_RETURN(\n-      std::optional<gpu::GpuTimer> timer,\n-      gpu::GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n",
            "whole_added": "+  std::optional<gpu::GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        gpu::GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n",
            "whole_hunk": "@@ -406,11 +406,12 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n     std::optional<DeviceMemoryBase> workspace,\n     std::optional<ScratchAllocator*> scratch_allocator,\n     blas::ProfileResult* profile_result = nullptr) const {\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<gpu::GpuTimer> timer,\n-      gpu::GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n+  std::optional<gpu::GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        gpu::GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   void* workspace_addr;\n   uint64_t workspace_size = 0;\n"
        },
        {
            "name": "cuda_dnn.cc",
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "patches": [
                {
                    "old_start": 2306,
                    "old_length": 13,
                    "new_start": 2306,
                    "new_length": 12,
                    "hunk": "@@ -2306,13 +2306,12 @@ absl::Status CudnnSupport::DoRnnForwardImpl(\n       stream, cudnn, rnn_desc, model_dims, input_desc, workspace_allocator,\n       reserve_space_allocator, is_training, &workspace, &reserve_space));\n \n-  const bool is_profiling = output_profile_result != nullptr;\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   if (input_desc.is_var_seq_lengths()) {\n     // In CUDNN v8, the cudnnRNNForward*** and cudnnRNNForward***Ex have been\n"
                },
                {
                    "old_start": 2409,
                    "old_length": 7,
                    "new_start": 2408,
                    "new_length": 7,
                    "hunk": "@@ -2409,7 +2408,7 @@ absl::Status CudnnSupport::DoRnnForwardImpl(\n #endif  // CUDNN_VERSION >= 90000\n   }\n \n-  if (is_profiling) {\n+  if (timer.has_value()) {\n     TF_RETURN_IF_ERROR(PopulateProfileFromTimer(\n         timer, *rnn_desc.algorithm_config().algorithm(),\n         output_profile_result));\n"
                },
                {
                    "old_start": 2460,
                    "old_length": 13,
                    "new_start": 2459,
                    "new_length": 12,
                    "hunk": "@@ -2460,13 +2459,12 @@ absl::Status CudnnSupport::DoRnnBackwardImpl(\n                                         input_desc, workspace_allocator,\n                                         nullptr, true, &workspace, nullptr));\n \n-  const bool is_profiling = output_profile_result != nullptr;\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  std::optional<GpuTimer> timer;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   if (input_desc.is_var_seq_lengths()) {\n     // In CUDNN v8, the cudnnRNNBackward*** and cudnnRNNBackward***Ex have\n"
                },
                {
                    "old_start": 2604,
                    "old_length": 7,
                    "new_start": 2602,
                    "new_length": 7,
                    "hunk": "@@ -2604,7 +2602,7 @@ absl::Status CudnnSupport::DoRnnBackwardImpl(\n #endif  // CUDNN_VERSION >= 90000\n   }\n \n-  if (is_profiling) {\n+  if (timer.has_value()) {\n     TF_RETURN_IF_ERROR(PopulateProfileFromTimer(\n         timer, *rnn_desc.algorithm_config().algorithm(),\n         output_profile_result));\n"
                },
                {
                    "old_start": 5589,
                    "old_length": 12,
                    "new_start": 5587,
                    "new_length": 13,
                    "hunk": "@@ -5589,12 +5587,13 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n                      ? static_cast<void*>(&dbeta)\n                      : static_cast<void*>(&fbeta);\n \n-    const bool is_profiling = profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            is_profiling));\n+    std::optional<GpuTimer> timer = std::nullopt;\n+\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n \n     const auto get_fwd_bugs = [&]() -> absl::Status {\n #if CUDNN_VERSION < 8000\n"
                },
                {
                    "old_start": 5671,
                    "old_length": 7,
                    "new_start": 5670,
                    "new_length": 7,
                    "hunk": "@@ -5671,7 +5670,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n                                      static_cast<int>(kind_));\n     }\n \n-    if (is_profiling) {\n+    if (timer.has_value()) {\n       TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer, algo, profile_result,\n                                                   scratch_memory.size()));\n     }\n"
                },
                {
                    "old_start": 6035,
                    "old_length": 18,
                    "new_start": 6034,
                    "new_length": 18,
                    "hunk": "@@ -6035,18 +6034,18 @@ class CudnnExecutionPlanRunner<void(Args...)>\n             << \"\\nWorkspace size in bytes: \" << workspace_size\n             << \"\\nVariantPack: \" << variantPack.describe();\n \n-    const bool is_profiling = profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            is_profiling));\n+    std::optional<GpuTimer> timer = std::nullopt;\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n \n     cudnnStatus_t status = cudnnBackendExecute(\n         cudnn.handle(), plan_.get_raw_desc(), variantPack.get_raw_desc());\n     RETURN_IF_CUDNN_ERROR(status);\n \n-    if (is_profiling) {\n+    if (timer.has_value()) {\n       TF_ASSIGN_OR_RETURN(auto desc, ToAlgorithmDesc());\n       TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer, desc, profile_result,\n                                                   scratch_memory.size()));\n"
                },
                {
                    "old_start": 6616,
                    "old_length": 12,
                    "new_start": 6615,
                    "new_length": 13,
                    "hunk": "@@ -6616,12 +6615,13 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n     }\n \n     auto algo = MakeAlgorithmDesc();\n+    std::optional<GpuTimer> timer = std::nullopt;\n \n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            profile_result != nullptr));\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n     auto side_input_data_ptr = (side_input_scale_ == 0)\n                                    ? output_data.opaque()\n                                    : side_input_data.opaque();\n"
                },
                {
                    "old_start": 6675,
                    "old_length": 7,
                    "new_start": 6675,
                    "new_length": 7,
                    "hunk": "@@ -6675,7 +6675,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n     }\n     RETURN_IF_CUDNN_ERROR(status);\n \n-    if (profile_result) {\n+    if (timer.has_value()) {\n       TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer, algo, profile_result,\n                                                   scratch_memory.size()));\n       VLOG(4) << \"conv with algorithm \" << ToConvForwardAlgo(algo)\n"
                }
            ],
            "whole_deleted": "-  const bool is_profiling = output_profile_result != nullptr;\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n-  if (is_profiling) {\n-  const bool is_profiling = output_profile_result != nullptr;\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n-  if (is_profiling) {\n-    const bool is_profiling = profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            is_profiling));\n-    if (is_profiling) {\n-    const bool is_profiling = profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            is_profiling));\n-    if (is_profiling) {\n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            profile_result != nullptr));\n-    if (profile_result) {\n",
            "whole_added": "+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n+  if (timer.has_value()) {\n+  std::optional<GpuTimer> timer;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n+  if (timer.has_value()) {\n+    std::optional<GpuTimer> timer = std::nullopt;\n+\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n+    if (timer.has_value()) {\n+    std::optional<GpuTimer> timer = std::nullopt;\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n+    if (timer.has_value()) {\n+    std::optional<GpuTimer> timer = std::nullopt;\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n+    if (timer.has_value()) {\n",
            "whole_hunk": "@@ -2306,13 +2306,12 @@ absl::Status CudnnSupport::DoRnnForwardImpl(\n       stream, cudnn, rnn_desc, model_dims, input_desc, workspace_allocator,\n       reserve_space_allocator, is_training, &workspace, &reserve_space));\n \n-  const bool is_profiling = output_profile_result != nullptr;\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   if (input_desc.is_var_seq_lengths()) {\n     // In CUDNN v8, the cudnnRNNForward*** and cudnnRNNForward***Ex have been\n@@ -2409,7 +2408,7 @@ absl::Status CudnnSupport::DoRnnForwardImpl(\n #endif  // CUDNN_VERSION >= 90000\n   }\n \n-  if (is_profiling) {\n+  if (timer.has_value()) {\n     TF_RETURN_IF_ERROR(PopulateProfileFromTimer(\n         timer, *rnn_desc.algorithm_config().algorithm(),\n         output_profile_result));\n@@ -2460,13 +2459,12 @@ absl::Status CudnnSupport::DoRnnBackwardImpl(\n                                         input_desc, workspace_allocator,\n                                         nullptr, true, &workspace, nullptr));\n \n-  const bool is_profiling = output_profile_result != nullptr;\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  std::optional<GpuTimer> timer;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   if (input_desc.is_var_seq_lengths()) {\n     // In CUDNN v8, the cudnnRNNBackward*** and cudnnRNNBackward***Ex have\n@@ -2604,7 +2602,7 @@ absl::Status CudnnSupport::DoRnnBackwardImpl(\n #endif  // CUDNN_VERSION >= 90000\n   }\n \n-  if (is_profiling) {\n+  if (timer.has_value()) {\n     TF_RETURN_IF_ERROR(PopulateProfileFromTimer(\n         timer, *rnn_desc.algorithm_config().algorithm(),\n         output_profile_result));\n@@ -5589,12 +5587,13 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n                      ? static_cast<void*>(&dbeta)\n                      : static_cast<void*>(&fbeta);\n \n-    const bool is_profiling = profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            is_profiling));\n+    std::optional<GpuTimer> timer = std::nullopt;\n+\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n \n     const auto get_fwd_bugs = [&]() -> absl::Status {\n #if CUDNN_VERSION < 8000\n@@ -5671,7 +5670,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n                                      static_cast<int>(kind_));\n     }\n \n-    if (is_profiling) {\n+    if (timer.has_value()) {\n       TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer, algo, profile_result,\n                                                   scratch_memory.size()));\n     }\n@@ -6035,18 +6034,18 @@ class CudnnExecutionPlanRunner<void(Args...)>\n             << \"\\nWorkspace size in bytes: \" << workspace_size\n             << \"\\nVariantPack: \" << variantPack.describe();\n \n-    const bool is_profiling = profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            is_profiling));\n+    std::optional<GpuTimer> timer = std::nullopt;\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n \n     cudnnStatus_t status = cudnnBackendExecute(\n         cudnn.handle(), plan_.get_raw_desc(), variantPack.get_raw_desc());\n     RETURN_IF_CUDNN_ERROR(status);\n \n-    if (is_profiling) {\n+    if (timer.has_value()) {\n       TF_ASSIGN_OR_RETURN(auto desc, ToAlgorithmDesc());\n       TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer, desc, profile_result,\n                                                   scratch_memory.size()));\n@@ -6616,12 +6615,13 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n     }\n \n     auto algo = MakeAlgorithmDesc();\n+    std::optional<GpuTimer> timer = std::nullopt;\n \n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            profile_result != nullptr));\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n     auto side_input_data_ptr = (side_input_scale_ == 0)\n                                    ? output_data.opaque()\n                                    : side_input_data.opaque();\n@@ -6675,7 +6675,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n     }\n     RETURN_IF_CUDNN_ERROR(status);\n \n-    if (profile_result) {\n+    if (timer.has_value()) {\n       TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer, algo, profile_result,\n                                                   scratch_memory.size()));\n       VLOG(4) << \"conv with algorithm \" << ToConvForwardAlgo(algo)\n"
        },
        {
            "name": "gpu_timer.cc",
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer.cc",
            "patches": [
                {
                    "old_start": 119,
                    "old_length": 15,
                    "new_start": 119,
                    "new_length": 6,
                    "hunk": "@@ -119,15 +119,6 @@ bool ShouldLaunchDelayKernel() {\n                                   stop_event,     stream, std::move(semaphore)};\n }\n \n-/*static*/ absl::StatusOr<std::optional<GpuTimer>> GpuTimer::CreateIfNeeded(\n-    Stream* stream, bool use_delay_kernel, bool is_needed) {\n-  if (is_needed) {\n-    TF_ASSIGN_OR_RETURN(GpuTimer t, GpuTimer::Create(stream, use_delay_kernel));\n-    return {std::make_optional(std::move(t))};\n-  }\n-  return std::nullopt;\n-}\n-\n /*static*/ void GpuTimer::ReturnRandomDurationsForTesting() {\n   return_random_durations = true;\n }\n"
                }
            ],
            "whole_deleted": "-/*static*/ absl::StatusOr<std::optional<GpuTimer>> GpuTimer::CreateIfNeeded(\n-    Stream* stream, bool use_delay_kernel, bool is_needed) {\n-  if (is_needed) {\n-    TF_ASSIGN_OR_RETURN(GpuTimer t, GpuTimer::Create(stream, use_delay_kernel));\n-    return {std::make_optional(std::move(t))};\n-  }\n-  return std::nullopt;\n-}\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -119,15 +119,6 @@ bool ShouldLaunchDelayKernel() {\n                                   stop_event,     stream, std::move(semaphore)};\n }\n \n-/*static*/ absl::StatusOr<std::optional<GpuTimer>> GpuTimer::CreateIfNeeded(\n-    Stream* stream, bool use_delay_kernel, bool is_needed) {\n-  if (is_needed) {\n-    TF_ASSIGN_OR_RETURN(GpuTimer t, GpuTimer::Create(stream, use_delay_kernel));\n-    return {std::make_optional(std::move(t))};\n-  }\n-  return std::nullopt;\n-}\n-\n /*static*/ void GpuTimer::ReturnRandomDurationsForTesting() {\n   return_random_durations = true;\n }\n"
        },
        {
            "name": "gpu_timer.h",
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer.h",
            "patches": [
                {
                    "old_start": 50,
                    "old_length": 12,
                    "new_start": 50,
                    "new_length": 6,
                    "hunk": "@@ -50,12 +50,6 @@ class GpuTimer {\n   [[deprecated(\"Pass Stream* not GpuStream*\")]] static absl::StatusOr<GpuTimer>\n   Create(GpuStream* stream);\n \n-  // An ugly but a very convenient helper: creates a timer only when we need\n-  // one, but always returns an object. If `is_needed` is false, returns an\n-  // empty optional, acts like `Create` otherwise.\n-  static absl::StatusOr<std::optional<GpuTimer>> CreateIfNeeded(\n-      Stream* stream, bool use_delay_kernel, bool is_needed);\n-\n   explicit GpuTimer(GpuExecutor* parent, GpuEventHandle start_event,\n                     GpuEventHandle stop_event, GpuStream* stream,\n                     GpuSemaphore semaphore = {})\n"
                }
            ],
            "whole_deleted": "-  // An ugly but a very convenient helper: creates a timer only when we need\n-  // one, but always returns an object. If `is_needed` is false, returns an\n-  // empty optional, acts like `Create` otherwise.\n-  static absl::StatusOr<std::optional<GpuTimer>> CreateIfNeeded(\n-      Stream* stream, bool use_delay_kernel, bool is_needed);\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -50,12 +50,6 @@ class GpuTimer {\n   [[deprecated(\"Pass Stream* not GpuStream*\")]] static absl::StatusOr<GpuTimer>\n   Create(GpuStream* stream);\n \n-  // An ugly but a very convenient helper: creates a timer only when we need\n-  // one, but always returns an object. If `is_needed` is false, returns an\n-  // empty optional, acts like `Create` otherwise.\n-  static absl::StatusOr<std::optional<GpuTimer>> CreateIfNeeded(\n-      Stream* stream, bool use_delay_kernel, bool is_needed);\n-\n   explicit GpuTimer(GpuExecutor* parent, GpuEventHandle start_event,\n                     GpuEventHandle stop_event, GpuStream* stream,\n                     GpuSemaphore semaphore = {})\n"
        },
        {
            "name": "hip_blas_lt.cc",
            "path": "third_party/xla/xla/stream_executor/rocm/hip_blas_lt.cc",
            "patches": [
                {
                    "old_start": 395,
                    "old_length": 12,
                    "new_start": 395,
                    "new_length": 13,
                    "hunk": "@@ -395,12 +395,13 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n       blas_lt_ref_.parent_->RecordApiTrace(StreamExecutor::GemmCallTrace{\n           StreamExecutor::GemmCallTrace::GemmType::kBlasLt, 0, a.size(),\n           b.size()});\n+  std::optional<gpu::GpuTimer> timer = std::nullopt;\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<gpu::GpuTimer> timer,\n-      gpu::GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result));\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        gpu::GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   void* workspace_addr = nullptr;\n   uint64_t workspace_size = 0;\n"
                }
            ],
            "whole_deleted": "-  TF_ASSIGN_OR_RETURN(\n-      std::optional<gpu::GpuTimer> timer,\n-      gpu::GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result));\n",
            "whole_added": "+  std::optional<gpu::GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        gpu::GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n",
            "whole_hunk": "@@ -395,12 +395,13 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n       blas_lt_ref_.parent_->RecordApiTrace(StreamExecutor::GemmCallTrace{\n           StreamExecutor::GemmCallTrace::GemmType::kBlasLt, 0, a.size(),\n           b.size()});\n+  std::optional<gpu::GpuTimer> timer = std::nullopt;\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<gpu::GpuTimer> timer,\n-      gpu::GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result));\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        gpu::GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   void* workspace_addr = nullptr;\n   uint64_t workspace_size = 0;\n"
        },
        {
            "name": "rocm_blas.cc",
            "path": "third_party/xla/xla/stream_executor/rocm/rocm_blas.cc",
            "patches": [
                {
                    "old_start": 544,
                    "old_length": 11,
                    "new_start": 544,
                    "new_length": 11,
                    "hunk": "@@ -544,11 +544,11 @@ absl::Status ROCMBlas::DoBlasGemmWithAlgorithm(\n         \"datatypes for the inputs a (%d) and b (%d) are unsupported\",\n         static_cast<int>(type_a), static_cast<int>(type_b)));\n   }\n-  TF_ASSIGN_OR_RETURN(\n-      auto timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer, GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   // fall back to the default implementation\n   if (algorithm == blas::kDefaultAlgorithm && type_a == type_c) {\n"
                },
                {
                    "old_start": 605,
                    "old_length": 11,
                    "new_start": 605,
                    "new_length": 11,
                    "hunk": "@@ -605,11 +605,11 @@ absl::Status ROCMBlas::DoBlasGemmStridedBatchedWithAlgorithm(\n         \"datatypes for the inputs a (%d) and b (%d) are unsupported\",\n         static_cast<int>(type_a), static_cast<int>(type_b)));\n   }\n-  TF_ASSIGN_OR_RETURN(\n-      auto timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer, GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   // fall back to the default implementation\n   if (algorithm == blas::kDefaultAlgorithm && type_a == type_c) {\n"
                }
            ],
            "whole_deleted": "-  TF_ASSIGN_OR_RETURN(\n-      auto timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n-  TF_ASSIGN_OR_RETURN(\n-      auto timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n",
            "whole_added": "+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer, GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer, GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n",
            "whole_hunk": "@@ -544,11 +544,11 @@ absl::Status ROCMBlas::DoBlasGemmWithAlgorithm(\n         \"datatypes for the inputs a (%d) and b (%d) are unsupported\",\n         static_cast<int>(type_a), static_cast<int>(type_b)));\n   }\n-  TF_ASSIGN_OR_RETURN(\n-      auto timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer, GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   // fall back to the default implementation\n   if (algorithm == blas::kDefaultAlgorithm && type_a == type_c) {\n@@ -605,11 +605,11 @@ absl::Status ROCMBlas::DoBlasGemmStridedBatchedWithAlgorithm(\n         \"datatypes for the inputs a (%d) and b (%d) are unsupported\",\n         static_cast<int>(type_a), static_cast<int>(type_b)));\n   }\n-  TF_ASSIGN_OR_RETURN(\n-      auto timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer, GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   // fall back to the default implementation\n   if (algorithm == blas::kDefaultAlgorithm && type_a == type_c) {\n"
        },
        {
            "name": "rocm_dnn.cc",
            "path": "third_party/xla/xla/stream_executor/rocm/rocm_dnn.cc",
            "patches": [
                {
                    "old_start": 2494,
                    "old_length": 13,
                    "new_start": 2494,
                    "new_length": 13,
                    "hunk": "@@ -2494,13 +2494,13 @@ absl::Status MIOpenSupport::DoRnnForwardImpl(\n   }\n \n   const bool is_profiling = output_profile_result != nullptr;\n+  std::optional<GpuTimer> timer = std::nullopt;\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  if (is_profiling) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   // make the forward call\n   if (!is_training) {\n"
                },
                {
                    "old_start": 2626,
                    "old_length": 13,
                    "new_start": 2626,
                    "new_length": 13,
                    "hunk": "@@ -2626,13 +2626,13 @@ absl::Status MIOpenSupport::DoRnnBackwardImpl(\n         stream->MemZero(input_c_backprop_data, size_data * type_size));\n \n   const bool is_profiling = output_profile_result != nullptr;\n+  std::optional<GpuTimer> timer = std::nullopt;\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  if (is_profiling) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   // make the backward data call\n   auto status = wrap::miopenRNNBackwardData(\n"
                },
                {
                    "old_start": 3326,
                    "old_length": 12,
                    "new_start": 3326,
                    "new_length": 12,
                    "hunk": "@@ -3326,12 +3326,12 @@ class RocmConvRunner : public dnn::ConvRunner {\n     float beta = 0.0;\n \n     const bool is_profiling = output_profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(std::optional<GpuTimer> timer,\n-                        GpuTimer::CreateIfNeeded(\n-                            stream,\n-                            output_profile_result &&\n-                                output_profile_result->warmup_run_executed(),\n-                            is_profiling));\n+    std::optional<GpuTimer> timer = std::nullopt;\n+    if (is_profiling) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer, GpuTimer::Create(\n+                     stream, output_profile_result->warmup_run_executed()));\n+    }\n \n     miopenStatus_t status = miopenStatusSuccess;\n     switch (kind_) {"
                }
            ],
            "whole_deleted": "-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n-    TF_ASSIGN_OR_RETURN(std::optional<GpuTimer> timer,\n-                        GpuTimer::CreateIfNeeded(\n-                            stream,\n-                            output_profile_result &&\n-                                output_profile_result->warmup_run_executed(),\n-                            is_profiling));\n",
            "whole_added": "+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (is_profiling) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (is_profiling) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n+    std::optional<GpuTimer> timer = std::nullopt;\n+    if (is_profiling) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer, GpuTimer::Create(\n+                     stream, output_profile_result->warmup_run_executed()));\n+    }\n",
            "whole_hunk": "@@ -2494,13 +2494,13 @@ absl::Status MIOpenSupport::DoRnnForwardImpl(\n   }\n \n   const bool is_profiling = output_profile_result != nullptr;\n+  std::optional<GpuTimer> timer = std::nullopt;\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  if (is_profiling) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   // make the forward call\n   if (!is_training) {\n@@ -2626,13 +2626,13 @@ absl::Status MIOpenSupport::DoRnnBackwardImpl(\n         stream->MemZero(input_c_backprop_data, size_data * type_size));\n \n   const bool is_profiling = output_profile_result != nullptr;\n+  std::optional<GpuTimer> timer = std::nullopt;\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  if (is_profiling) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   // make the backward data call\n   auto status = wrap::miopenRNNBackwardData(\n@@ -3326,12 +3326,12 @@ class RocmConvRunner : public dnn::ConvRunner {\n     float beta = 0.0;\n \n     const bool is_profiling = output_profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(std::optional<GpuTimer> timer,\n-                        GpuTimer::CreateIfNeeded(\n-                            stream,\n-                            output_profile_result &&\n-                                output_profile_result->warmup_run_executed(),\n-                            is_profiling));\n+    std::optional<GpuTimer> timer = std::nullopt;\n+    if (is_profiling) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer, GpuTimer::Create(\n+                     stream, output_profile_result->warmup_run_executed()));\n+    }\n \n     miopenStatus_t status = miopenStatusSuccess;\n     switch (kind_) {"
        }
    ]
},
{
    "Id": 289,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e41bd3f8ddf9a85356f10977b432afb4b4f6d6a6",
    "date": "2023-09-15T10:32:48-07:00",
    "message": "Explicitly check if the input module to auto-sharding uses manual sharding, and crash if so as we currently do not support handling such modules.\n\nPiperOrigin-RevId: 565712685",
    "label": "NO",
    "changes": [
        {
            "name": "auto_sharding.cc",
            "path": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding.cc",
            "patches": [
                {
                    "old_start": 4305,
                    "old_length": 6,
                    "new_start": 4305,
                    "new_length": 18,
                    "hunk": "@@ -4305,6 +4305,18 @@ bool IsSmallTensor(const HloInstruction* ins,\n          option.small_tensor_byte_size;\n }\n \n+bool IsModuleManuallySharded(const HloModule* module) {\n+  for (const auto* computation : module->computations()) {\n+    for (const auto* instruction : computation->instructions()) {\n+      if (HloCollectiveInstruction::ClassOf(instruction) ||\n+          spmd::IsManualShardingBoundaryCustomCall(instruction)) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n StatusOr<bool> AutoSharding::Run(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n"
                },
                {
                    "old_start": 4313,
                    "old_length": 6,
                    "new_start": 4325,
                    "new_length": 13,
                    "hunk": "@@ -4313,6 +4325,13 @@ StatusOr<bool> AutoSharding::Run(\n   }\n   VLOG(1) << \"Start auto sharding pass\";\n \n+  if (IsModuleManuallySharded(module)) {\n+    LOG(ERROR)\n+        << \"Auto-sharding on partially manually sharded modules is not yet \"\n+           \"supported. Please fall back on the sharding propagation pass.\";\n+    return false;\n+  }\n+\n   XLA_VLOG_LINES(6,\n                  absl::StrCat(\"Before auto sharding:\\n\", module->ToString()));\n   DumpHloModuleIfEnabled(*module, \"before_auto_spmd_sharding\");\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+bool IsModuleManuallySharded(const HloModule* module) {\n+  for (const auto* computation : module->computations()) {\n+    for (const auto* instruction : computation->instructions()) {\n+      if (HloCollectiveInstruction::ClassOf(instruction) ||\n+          spmd::IsManualShardingBoundaryCustomCall(instruction)) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+  if (IsModuleManuallySharded(module)) {\n+    LOG(ERROR)\n+        << \"Auto-sharding on partially manually sharded modules is not yet \"\n+           \"supported. Please fall back on the sharding propagation pass.\";\n+    return false;\n+  }\n+\n",
            "whole_hunk": "@@ -4305,6 +4305,18 @@ bool IsSmallTensor(const HloInstruction* ins,\n          option.small_tensor_byte_size;\n }\n \n+bool IsModuleManuallySharded(const HloModule* module) {\n+  for (const auto* computation : module->computations()) {\n+    for (const auto* instruction : computation->instructions()) {\n+      if (HloCollectiveInstruction::ClassOf(instruction) ||\n+          spmd::IsManualShardingBoundaryCustomCall(instruction)) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n StatusOr<bool> AutoSharding::Run(\n     HloModule* module,\n     const absl::flat_hash_set<absl::string_view>& execution_threads) {\n@@ -4313,6 +4325,13 @@ StatusOr<bool> AutoSharding::Run(\n   }\n   VLOG(1) << \"Start auto sharding pass\";\n \n+  if (IsModuleManuallySharded(module)) {\n+    LOG(ERROR)\n+        << \"Auto-sharding on partially manually sharded modules is not yet \"\n+           \"supported. Please fall back on the sharding propagation pass.\";\n+    return false;\n+  }\n+\n   XLA_VLOG_LINES(6,\n                  absl::StrCat(\"Before auto sharding:\\n\", module->ToString()));\n   DumpHloModuleIfEnabled(*module, \"before_auto_spmd_sharding\");\n"
        },
        {
            "name": "auto_sharding_util.h",
            "path": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_util.h",
            "patches": [
                {
                    "old_start": 52,
                    "old_length": 6,
                    "new_start": 52,
                    "new_length": 11,
                    "hunk": "@@ -52,6 +52,11 @@ inline constexpr absl::string_view kIdentityMarker = \"identity\";\n inline constexpr absl::string_view kPipelineMarkerStartType = \"start\";\n inline constexpr absl::string_view kPipelineMarkerEndType = \"end\";\n \n+inline bool IsManualShardingBoundaryCustomCall(const HloInstruction* ins) {\n+  return ins->IsCustomCall(\"SPMDFullToShardShape\") ||\n+         ins->IsCustomCall(\"SPMDShardToFullShape\");\n+}\n+\n inline std::pair<int, int> ParseMeshDims(const std::string& strategy_name) {\n   if (absl::StrContains(strategy_name, \"{0,1}\")) {\n     return {0, 1};"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+inline bool IsManualShardingBoundaryCustomCall(const HloInstruction* ins) {\n+  return ins->IsCustomCall(\"SPMDFullToShardShape\") ||\n+         ins->IsCustomCall(\"SPMDShardToFullShape\");\n+}\n+\n",
            "whole_hunk": "@@ -52,6 +52,11 @@ inline constexpr absl::string_view kIdentityMarker = \"identity\";\n inline constexpr absl::string_view kPipelineMarkerStartType = \"start\";\n inline constexpr absl::string_view kPipelineMarkerEndType = \"end\";\n \n+inline bool IsManualShardingBoundaryCustomCall(const HloInstruction* ins) {\n+  return ins->IsCustomCall(\"SPMDFullToShardShape\") ||\n+         ins->IsCustomCall(\"SPMDShardToFullShape\");\n+}\n+\n inline std::pair<int, int> ParseMeshDims(const std::string& strategy_name) {\n   if (absl::StrContains(strategy_name, \"{0,1}\")) {\n     return {0, 1};"
        }
    ]
},
{
    "Id": 394,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8dcc7fc1fb1aa0274bed7c401a9f2bc344f16609",
    "date": "2023-06-21T10:43:39-07:00",
    "message": "Add validation to check the order of xspace paths and preload xplane.\n\nPiperOrigin-RevId: 542298563",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/core/profiler/convert/BUILD",
            "patches": [
                {
                    "old_start": 850,
                    "old_length": 6,
                    "new_start": 850,
                    "new_length": 7,
                    "hunk": "@@ -850,6 +850,7 @@ tf_cc_test(\n     srcs = [\"repository_test.cc\"],\n     deps = [\n         \":repository\",\n+        \"//tensorflow/core/platform:errors\",\n         \"//tensorflow/tsl/profiler/protobuf:xplane_proto_cc\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/core/platform:errors\",\n",
            "whole_hunk": "@@ -850,6 +850,7 @@ tf_cc_test(\n     srcs = [\"repository_test.cc\"],\n     deps = [\n         \":repository\",\n+        \"//tensorflow/core/platform:errors\",\n         \"//tensorflow/tsl/profiler/protobuf:xplane_proto_cc\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n"
        },
        {
            "name": "repository.cc",
            "path": "tensorflow/core/profiler/convert/repository.cc",
            "patches": [
                {
                    "old_start": 22,
                    "old_length": 6,
                    "new_start": 22,
                    "new_length": 7,
                    "hunk": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/strings/match.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/platform/env.h\"\n #include \"tensorflow/core/platform/errors.h\"\n"
                },
                {
                    "old_start": 47,
                    "old_length": 13,
                    "new_start": 48,
                    "new_length": 28,
                    "hunk": "@@ -47,13 +48,28 @@ StatusOr<SessionSnapshot> SessionSnapshot::Create(\n     return errors::InvalidArgument(\"Can not find XSpace path.\");\n   }\n \n-  if (xspaces.has_value() && xspaces->size() != xspace_paths.size()) {\n-    return errors::InvalidArgument(\n-        \"The size of the XSpace paths: \", xspace_paths.size(), \" is not equal \",\n-        \"to the size of the XSpace proto: \", xspaces->size());\n+  if (xspaces.has_value()) {\n+    if (xspaces->size() != xspace_paths.size()) {\n+      return errors::InvalidArgument(\n+          \"The size of the XSpace paths: \", xspace_paths.size(),\n+          \" is not equal \",\n+          \"to the size of the XSpace proto: \", xspaces->size());\n+    }\n+    for (size_t i = 0; i < xspace_paths.size(); ++i) {\n+      auto host_name = GetHostnameByPath(xspace_paths.at(i));\n+      if (xspaces->at(i)->hostnames_size() > 0 && !host_name.empty()) {\n+        if (!absl::StrContains(host_name, xspaces->at(i)->hostnames(0))) {\n+          return errors::InvalidArgument(\n+              \"The hostname of xspace path and preloaded xpace don't match at \"\n+              \"index: \",\n+              i, \". \\nThe host name of xpace path is \", host_name,\n+              \" but the host name of preloaded xpace is \",\n+              xspaces->at(i)->hostnames(0), \".\");\n+        }\n+      }\n+    }\n   }\n \n-  // TODO(profiler): How to validate xspace_paths for pre-loaded XSpaces.\n   return SessionSnapshot(std::move(xspace_paths), std::move(xspaces));\n }\n \n"
                }
            ],
            "whole_deleted": "-  if (xspaces.has_value() && xspaces->size() != xspace_paths.size()) {\n-    return errors::InvalidArgument(\n-        \"The size of the XSpace paths: \", xspace_paths.size(), \" is not equal \",\n-        \"to the size of the XSpace proto: \", xspaces->size());\n-  // TODO(profiler): How to validate xspace_paths for pre-loaded XSpaces.\n",
            "whole_added": "+#include \"absl/strings/match.h\"\n+  if (xspaces.has_value()) {\n+    if (xspaces->size() != xspace_paths.size()) {\n+      return errors::InvalidArgument(\n+          \"The size of the XSpace paths: \", xspace_paths.size(),\n+          \" is not equal \",\n+          \"to the size of the XSpace proto: \", xspaces->size());\n+    }\n+    for (size_t i = 0; i < xspace_paths.size(); ++i) {\n+      auto host_name = GetHostnameByPath(xspace_paths.at(i));\n+      if (xspaces->at(i)->hostnames_size() > 0 && !host_name.empty()) {\n+        if (!absl::StrContains(host_name, xspaces->at(i)->hostnames(0))) {\n+          return errors::InvalidArgument(\n+              \"The hostname of xspace path and preloaded xpace don't match at \"\n+              \"index: \",\n+              i, \". \\nThe host name of xpace path is \", host_name,\n+              \" but the host name of preloaded xpace is \",\n+              xspaces->at(i)->hostnames(0), \".\");\n+        }\n+      }\n+    }\n",
            "whole_hunk": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/strings/match.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"tensorflow/core/platform/env.h\"\n #include \"tensorflow/core/platform/errors.h\"\n@@ -47,13 +48,28 @@ StatusOr<SessionSnapshot> SessionSnapshot::Create(\n     return errors::InvalidArgument(\"Can not find XSpace path.\");\n   }\n \n-  if (xspaces.has_value() && xspaces->size() != xspace_paths.size()) {\n-    return errors::InvalidArgument(\n-        \"The size of the XSpace paths: \", xspace_paths.size(), \" is not equal \",\n-        \"to the size of the XSpace proto: \", xspaces->size());\n+  if (xspaces.has_value()) {\n+    if (xspaces->size() != xspace_paths.size()) {\n+      return errors::InvalidArgument(\n+          \"The size of the XSpace paths: \", xspace_paths.size(),\n+          \" is not equal \",\n+          \"to the size of the XSpace proto: \", xspaces->size());\n+    }\n+    for (size_t i = 0; i < xspace_paths.size(); ++i) {\n+      auto host_name = GetHostnameByPath(xspace_paths.at(i));\n+      if (xspaces->at(i)->hostnames_size() > 0 && !host_name.empty()) {\n+        if (!absl::StrContains(host_name, xspaces->at(i)->hostnames(0))) {\n+          return errors::InvalidArgument(\n+              \"The hostname of xspace path and preloaded xpace don't match at \"\n+              \"index: \",\n+              i, \". \\nThe host name of xpace path is \", host_name,\n+              \" but the host name of preloaded xpace is \",\n+              xspaces->at(i)->hostnames(0), \".\");\n+        }\n+      }\n+    }\n   }\n \n-  // TODO(profiler): How to validate xspace_paths for pre-loaded XSpaces.\n   return SessionSnapshot(std::move(xspace_paths), std::move(xspaces));\n }\n \n"
        },
        {
            "name": "repository_test.cc",
            "path": "tensorflow/core/profiler/convert/repository_test.cc",
            "patches": [
                {
                    "old_start": 22,
                    "old_length": 6,
                    "new_start": 22,
                    "new_length": 7,
                    "hunk": "@@ -22,6 +22,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/tsl/profiler/protobuf/xplane.pb.h\"\n \n namespace tensorflow {\n"
                },
                {
                    "old_start": 98,
                    "old_length": 6,
                    "new_start": 99,
                    "new_length": 30,
                    "hunk": "@@ -98,6 +99,30 @@ TEST(Repository, GetSSTableFileWithXSpace) {\n   EXPECT_THAT(file_path_init_by_xspace, Eq(std::nullopt));\n }\n \n+TEST(Repository, MismatchedXSpaceAndPath) {\n+  std::vector<std::unique_ptr<XSpace>> xspaces;\n+  // prepare host 1.\n+  auto space1 = std::make_unique<XSpace>();\n+  *(space1->add_hostnames()) = \"hostname1\";\n+  // with index 0 which shouldn't impact the space finding by name.\n+  xspaces.push_back(std::move(space1));\n+\n+  // prepare host 0.\n+  auto space0 = std::make_unique<XSpace>();\n+  *(space0->add_hostnames()) = \"hostname0\";\n+  // with index 1 which shouldn't impact the space finding by name.\n+  xspaces.push_back(std::move(space0));\n+\n+  auto session_snapshot_or =\n+      SessionSnapshot::Create({\"log/plugins/profile/hostname0.xplane.pb\",\n+                               \"log/plugins/profile/hostname1.xplane.pb\"},\n+                              std::move(xspaces));\n+  auto error =\n+      R\"(The hostname of xspace path and preloaded xpace don't match at index: 0. \n+The host name of xpace path is hostname0 but the host name of preloaded xpace is hostname1.)\";\n+  EXPECT_THAT(session_snapshot_or.status(), Eq(errors::InvalidArgument(error)));\n+}\n+\n }  // namespace\n }  // namespace profiler\n }  // namespace tensorflow\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"tensorflow/core/platform/errors.h\"\n+TEST(Repository, MismatchedXSpaceAndPath) {\n+  std::vector<std::unique_ptr<XSpace>> xspaces;\n+  // prepare host 1.\n+  auto space1 = std::make_unique<XSpace>();\n+  *(space1->add_hostnames()) = \"hostname1\";\n+  // with index 0 which shouldn't impact the space finding by name.\n+  xspaces.push_back(std::move(space1));\n+\n+  // prepare host 0.\n+  auto space0 = std::make_unique<XSpace>();\n+  *(space0->add_hostnames()) = \"hostname0\";\n+  // with index 1 which shouldn't impact the space finding by name.\n+  xspaces.push_back(std::move(space0));\n+\n+  auto session_snapshot_or =\n+      SessionSnapshot::Create({\"log/plugins/profile/hostname0.xplane.pb\",\n+                               \"log/plugins/profile/hostname1.xplane.pb\"},\n+                              std::move(xspaces));\n+  auto error =\n+      R\"(The hostname of xspace path and preloaded xpace don't match at index: 0. \n+The host name of xpace path is hostname0 but the host name of preloaded xpace is hostname1.)\";\n+  EXPECT_THAT(session_snapshot_or.status(), Eq(errors::InvalidArgument(error)));\n+}\n+\n",
            "whole_hunk": "@@ -22,6 +22,7 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/tsl/profiler/protobuf/xplane.pb.h\"\n \n namespace tensorflow {\n@@ -98,6 +99,30 @@ TEST(Repository, GetSSTableFileWithXSpace) {\n   EXPECT_THAT(file_path_init_by_xspace, Eq(std::nullopt));\n }\n \n+TEST(Repository, MismatchedXSpaceAndPath) {\n+  std::vector<std::unique_ptr<XSpace>> xspaces;\n+  // prepare host 1.\n+  auto space1 = std::make_unique<XSpace>();\n+  *(space1->add_hostnames()) = \"hostname1\";\n+  // with index 0 which shouldn't impact the space finding by name.\n+  xspaces.push_back(std::move(space1));\n+\n+  // prepare host 0.\n+  auto space0 = std::make_unique<XSpace>();\n+  *(space0->add_hostnames()) = \"hostname0\";\n+  // with index 1 which shouldn't impact the space finding by name.\n+  xspaces.push_back(std::move(space0));\n+\n+  auto session_snapshot_or =\n+      SessionSnapshot::Create({\"log/plugins/profile/hostname0.xplane.pb\",\n+                               \"log/plugins/profile/hostname1.xplane.pb\"},\n+                              std::move(xspaces));\n+  auto error =\n+      R\"(The hostname of xspace path and preloaded xpace don't match at index: 0. \n+The host name of xpace path is hostname0 but the host name of preloaded xpace is hostname1.)\";\n+  EXPECT_THAT(session_snapshot_or.status(), Eq(errors::InvalidArgument(error)));\n+}\n+\n }  // namespace\n }  // namespace profiler\n }  // namespace tensorflow\n"
        },
        {
            "name": "xplane_to_op_stats_test.cc",
            "path": "tensorflow/core/profiler/convert/xplane_to_op_stats_test.cc",
            "patches": [
                {
                    "old_start": 281,
                    "old_length": 8,
                    "new_start": 281,
                    "new_length": 8,
                    "hunk": "@@ -281,8 +281,8 @@ TEST(ConvertXPlaneToOpStats, TestConvertMultiXSpacesToCombinedOpStats) {\n   BuildXSpaceForTest(*xspace2, kHost2);\n \n   std::vector<std::string> xspace_paths;\n-  xspace_paths.push_back(\"xspace_path1\");\n-  xspace_paths.push_back(\"xspace_path2\");\n+  xspace_paths.push_back(\"host1.pb\");\n+  xspace_paths.push_back(\"host2.pb\");\n \n   std::vector<std::unique_ptr<XSpace>> xspaces;\n   xspaces.push_back(std::move(xspace1));"
                }
            ],
            "whole_deleted": "-  xspace_paths.push_back(\"xspace_path1\");\n-  xspace_paths.push_back(\"xspace_path2\");\n",
            "whole_added": "+  xspace_paths.push_back(\"host1.pb\");\n+  xspace_paths.push_back(\"host2.pb\");\n",
            "whole_hunk": "@@ -281,8 +281,8 @@ TEST(ConvertXPlaneToOpStats, TestConvertMultiXSpacesToCombinedOpStats) {\n   BuildXSpaceForTest(*xspace2, kHost2);\n \n   std::vector<std::string> xspace_paths;\n-  xspace_paths.push_back(\"xspace_path1\");\n-  xspace_paths.push_back(\"xspace_path2\");\n+  xspace_paths.push_back(\"host1.pb\");\n+  xspace_paths.push_back(\"host2.pb\");\n \n   std::vector<std::unique_ptr<XSpace>> xspaces;\n   xspaces.push_back(std::move(xspace1));"
        }
    ]
},
{
    "Id": 698,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c17d13a42c578fc1038af42fb2170a9f9a312347",
    "date": "2022-09-06T18:54:03-07:00",
    "message": "[NFC] Add variant of IsCustomCall that checks one of several targets.\n\nPiperOrigin-RevId: 472608375",
    "label": "NO",
    "changes": [
        {
            "name": "hlo_instruction.cc",
            "path": "tensorflow/compiler/xla/service/hlo_instruction.cc",
            "patches": [
                {
                    "old_start": 3427,
                    "old_length": 6,
                    "new_start": 3427,
                    "new_length": 12,
                    "hunk": "@@ -3427,6 +3427,12 @@ bool HloInstruction::IsCustomCall(absl::string_view target) const {\n   return opcode() == HloOpcode::kCustomCall && custom_call_target() == target;\n }\n \n+bool HloInstruction::IsCustomCall(\n+    absl::Span<const absl::string_view> targets) const {\n+  return opcode() == HloOpcode::kCustomCall &&\n+         absl::c_linear_search(targets, custom_call_target());\n+}\n+\n bool HloInstruction::IsInputFusion() const {\n   return opcode() == HloOpcode::kFusion && fusion_kind() == FusionKind::kInput;\n }\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+bool HloInstruction::IsCustomCall(\n+    absl::Span<const absl::string_view> targets) const {\n+  return opcode() == HloOpcode::kCustomCall &&\n+         absl::c_linear_search(targets, custom_call_target());\n+}\n+\n",
            "whole_hunk": "@@ -3427,6 +3427,12 @@ bool HloInstruction::IsCustomCall(absl::string_view target) const {\n   return opcode() == HloOpcode::kCustomCall && custom_call_target() == target;\n }\n \n+bool HloInstruction::IsCustomCall(\n+    absl::Span<const absl::string_view> targets) const {\n+  return opcode() == HloOpcode::kCustomCall &&\n+         absl::c_linear_search(targets, custom_call_target());\n+}\n+\n bool HloInstruction::IsInputFusion() const {\n   return opcode() == HloOpcode::kFusion && fusion_kind() == FusionKind::kInput;\n }\n"
        },
        {
            "name": "hlo_instruction.h",
            "path": "tensorflow/compiler/xla/service/hlo_instruction.h",
            "patches": [
                {
                    "old_start": 1549,
                    "old_length": 6,
                    "new_start": 1549,
                    "new_length": 7,
                    "hunk": "@@ -1549,6 +1549,7 @@ class HloInstruction {\n   bool IsFusible() const;\n \n   bool IsCustomCall(absl::string_view target) const;\n+  bool IsCustomCall(absl::Span<const absl::string_view> targets) const;\n \n   // Returns the sharding applied to this operator.\n   // REQUIRES: has_sharding() is true.\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  bool IsCustomCall(absl::Span<const absl::string_view> targets) const;\n",
            "whole_hunk": "@@ -1549,6 +1549,7 @@ class HloInstruction {\n   bool IsFusible() const;\n \n   bool IsCustomCall(absl::string_view target) const;\n+  bool IsCustomCall(absl::Span<const absl::string_view> targets) const;\n \n   // Returns the sharding applied to this operator.\n   // REQUIRES: has_sharding() is true.\n"
        },
        {
            "name": "sharding_propagation.cc",
            "path": "tensorflow/compiler/xla/service/sharding_propagation.cc",
            "patches": [
                {
                    "old_start": 149,
                    "old_length": 10,
                    "new_start": 149,
                    "new_length": 7,
                    "hunk": "@@ -149,10 +149,7 @@ bool IsConvolutionKernelSmall(const HloInstruction* instruction) {\n }\n \n bool IsPassthroughCustomOps(const HloInstruction* hlo) {\n-  if (hlo->IsCustomCall(\"Sharding\")) {\n-    return true;\n-  }\n-  if (hlo->IsCustomCall(\"X64Combine\")) {\n+  if (hlo->IsCustomCall({\"Sharding\", \"X64Combine\"})) {\n     return true;\n   }\n   if (hlo->operand_count() != 1 || !hlo->shape().IsArray() ||\n"
                },
                {
                    "old_start": 160,
                    "old_length": 11,
                    "new_start": 157,
                    "new_length": 9,
                    "hunk": "@@ -160,11 +157,9 @@ bool IsPassthroughCustomOps(const HloInstruction* hlo) {\n       hlo->operand(0)->shape().rank() != hlo->shape().rank()) {\n     return false;\n   }\n-  return hlo->IsCustomCall(\"ResizeNearest\") ||\n-         hlo->IsCustomCall(\"ResizeBilinear\") ||\n-         hlo->IsCustomCall(\"ResizeNearestGrad\") ||\n-         hlo->IsCustomCall(\"ResizeBilinearGrad\") ||\n-         hlo->IsCustomCall(\"Cholesky\");\n+  return hlo->IsCustomCall({\"ResizeNearest\", \"ResizeBilinear\",\n+                            \"ResizeNearestGrad\", \"ResizeBilinearGrad\",\n+                            \"Cholesky\"});\n }\n \n // Return the operand which is the most suitable for determining the sharding"
                }
            ],
            "whole_deleted": "-  if (hlo->IsCustomCall(\"Sharding\")) {\n-    return true;\n-  }\n-  if (hlo->IsCustomCall(\"X64Combine\")) {\n-  return hlo->IsCustomCall(\"ResizeNearest\") ||\n-         hlo->IsCustomCall(\"ResizeBilinear\") ||\n-         hlo->IsCustomCall(\"ResizeNearestGrad\") ||\n-         hlo->IsCustomCall(\"ResizeBilinearGrad\") ||\n-         hlo->IsCustomCall(\"Cholesky\");\n",
            "whole_added": "+  if (hlo->IsCustomCall({\"Sharding\", \"X64Combine\"})) {\n+  return hlo->IsCustomCall({\"ResizeNearest\", \"ResizeBilinear\",\n+                            \"ResizeNearestGrad\", \"ResizeBilinearGrad\",\n+                            \"Cholesky\"});\n",
            "whole_hunk": "@@ -149,10 +149,7 @@ bool IsConvolutionKernelSmall(const HloInstruction* instruction) {\n }\n \n bool IsPassthroughCustomOps(const HloInstruction* hlo) {\n-  if (hlo->IsCustomCall(\"Sharding\")) {\n-    return true;\n-  }\n-  if (hlo->IsCustomCall(\"X64Combine\")) {\n+  if (hlo->IsCustomCall({\"Sharding\", \"X64Combine\"})) {\n     return true;\n   }\n   if (hlo->operand_count() != 1 || !hlo->shape().IsArray() ||\n@@ -160,11 +157,9 @@ bool IsPassthroughCustomOps(const HloInstruction* hlo) {\n       hlo->operand(0)->shape().rank() != hlo->shape().rank()) {\n     return false;\n   }\n-  return hlo->IsCustomCall(\"ResizeNearest\") ||\n-         hlo->IsCustomCall(\"ResizeBilinear\") ||\n-         hlo->IsCustomCall(\"ResizeNearestGrad\") ||\n-         hlo->IsCustomCall(\"ResizeBilinearGrad\") ||\n-         hlo->IsCustomCall(\"Cholesky\");\n+  return hlo->IsCustomCall({\"ResizeNearest\", \"ResizeBilinear\",\n+                            \"ResizeNearestGrad\", \"ResizeBilinearGrad\",\n+                            \"Cholesky\"});\n }\n \n // Return the operand which is the most suitable for determining the sharding"
        }
    ]
},
{
    "Id": 331,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ad2702d2ea290544ea4b72b9aded9b56e2ee8853",
    "date": "2023-08-08T01:54:45-07:00",
    "message": "PR #4528: [NVIDIA XLA:GPU] Enable reduction epilogue fusion for some ops\n\nImported from GitHub PR https://github.com/openxla/xla/pull/4528\n\nEnable reduction epilogue fusion for some ops, convert, bitcast, reshape that's actually a bitcast.\nBefore enabling\nother_ops -> reduce -> convert\nwould become fusion_kernel->convert\nafter enabling\nIt will become a single fusion kernel.\nWe will progressively enable this for more elementwise ops with more benchmarking.\nCopybara import of the project:\n\n--\n11730ca050fdc4bd60f1b8b0e760090415682cd6 by TJ <tjx@nvidia.com>:\n\nEnable reduction epilogue fusion for some ops\n\n--\nea6ba4d7df79066d231b1d18bea20ecc8f579fae by TJ <tjx@nvidia.com>:\n\nremoved kcopy from reduction consumer check\n\n--\nd583df0196e74a67c8eda0d4ebca25b187beb5f6 by TJ <tjx@nvidia.com>:\n\nadded codegen support for reduction epilogue fusion\n\n--\n7aae75c86013dfef4332b2db7bbbf4c411187f0e by TJ <tjx@nvidia.com>:\n\nexplicitly check for reduce op in hlo fusion analysis when emitting kind\n\n--\n421549b62fde13e7f6d1e4301c3e9da5351da3e6 by TJ <tjx@nvidia.com>:\n\nfixed parallel reduction test failure\naddressed pr comments\n\nMerging this change closes #4528\n\nPiperOrigin-RevId: 554742300",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 1824,
                    "old_length": 6,
                    "new_start": 1824,
                    "new_length": 7,
                    "hunk": "@@ -1824,6 +1824,7 @@ xla_cc_test(\n         \":gpu_fusible\",\n         \":instruction_fusion\",\n         \"//tensorflow/compiler/xla:util\",\n+        \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n         \"//tensorflow/compiler/xla/hlo/utils:hlo_matchers\",\n         \"//tensorflow/compiler/xla/tests:hlo_test_base\",\n         \"//tensorflow/compiler/xla/tests:test_utils\",\n"
                },
                {
                    "old_start": 3203,
                    "old_length": 8,
                    "new_start": 3204,
                    "new_length": 10,
                    "hunk": "@@ -3203,8 +3204,10 @@ cc_library(\n         \":ir_emission_utils\",\n         \":reduction_utils\",\n         \"//tensorflow/compiler/xla:shape_util\",\n+        \"//tensorflow/compiler/xla:util\",\n         \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n         \"//tensorflow/compiler/xla/service:instruction_fusion\",\n+        \"@com_google_absl//absl/algorithm:container\",\n     ],\n )\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n+        \"//tensorflow/compiler/xla:util\",\n+        \"@com_google_absl//absl/algorithm:container\",\n",
            "whole_hunk": "@@ -1824,6 +1824,7 @@ xla_cc_test(\n         \":gpu_fusible\",\n         \":instruction_fusion\",\n         \"//tensorflow/compiler/xla:util\",\n+        \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n         \"//tensorflow/compiler/xla/hlo/utils:hlo_matchers\",\n         \"//tensorflow/compiler/xla/tests:hlo_test_base\",\n         \"//tensorflow/compiler/xla/tests:test_utils\",\n@@ -3203,8 +3204,10 @@ cc_library(\n         \":ir_emission_utils\",\n         \":reduction_utils\",\n         \"//tensorflow/compiler/xla:shape_util\",\n+        \"//tensorflow/compiler/xla:util\",\n         \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n         \"//tensorflow/compiler/xla/service:instruction_fusion\",\n+        \"@com_google_absl//absl/algorithm:container\",\n     ],\n )\n \n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/service/gpu/fusions/BUILD",
            "patches": [
                {
                    "old_start": 134,
                    "old_length": 8,
                    "new_start": 134,
                    "new_length": 16,
                    "hunk": "@@ -134,8 +134,16 @@ cc_library(\n         \":fusion_emitter\",\n         \":thunk_util\",\n         \":tiling_util\",\n+        \"//tensorflow/compiler/xla:shape_util\",\n+        \"//tensorflow/compiler/xla:status\",\n+        \"//tensorflow/compiler/xla:status_macros\",\n+        \"//tensorflow/compiler/xla:statusor\",\n         \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n+        \"//tensorflow/compiler/xla/mlir_hlo\",\n+        \"//tensorflow/compiler/xla/mlir_hlo:lhlo\",\n+        \"//tensorflow/compiler/xla/service:elemental_ir_emitter\",\n         \"//tensorflow/compiler/xla/service/gpu:gpu_executable\",\n+        \"//tensorflow/compiler/xla/service/gpu:gpu_fusible\",\n         \"//tensorflow/compiler/xla/service/gpu:hlo_fusion_analysis\",\n         \"//tensorflow/compiler/xla/service/gpu:ir_emission_utils\",\n         \"//tensorflow/compiler/xla/service/gpu:ir_emitter\",\n"
                },
                {
                    "old_start": 143,
                    "old_length": 12,
                    "new_start": 151,
                    "new_length": 21,
                    "hunk": "@@ -143,12 +151,21 @@ cc_library(\n         \"//tensorflow/compiler/xla/service/gpu:kernel_reuse_cache\",\n         \"//tensorflow/compiler/xla/service/gpu:parallel_loop_emitter\",\n         \"//tensorflow/compiler/xla/service/gpu:target_util\",\n+        \"//tensorflow/compiler/xla/service/gpu:thunk\",\n         \"//tensorflow/compiler/xla/service/llvm_ir:fused_ir_emitter\",\n         \"//tensorflow/compiler/xla/service/llvm_ir:ir_array\",\n         \"//tensorflow/compiler/xla/service/llvm_ir:kernel_support_library\",\n         \"//tensorflow/compiler/xla/service/llvm_ir:llvm_util\",\n+        \"//tensorflow/compiler/xla/service/llvm_ir:loop_emitter\",\n         \"//tensorflow/compiler/xla/translate/mhlo_to_hlo:location_exporter\",\n+        \"//tensorflow/tsl/platform:logging\",\n+        \"//tensorflow/tsl/platform:status\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//llvm:Support\",\n         \"@llvm-project//llvm:ir_headers\",\n+        \"@llvm-project//mlir:Support\",\n     ],\n )\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/compiler/xla:shape_util\",\n+        \"//tensorflow/compiler/xla:status\",\n+        \"//tensorflow/compiler/xla:status_macros\",\n+        \"//tensorflow/compiler/xla:statusor\",\n+        \"//tensorflow/compiler/xla/mlir_hlo\",\n+        \"//tensorflow/compiler/xla/mlir_hlo:lhlo\",\n+        \"//tensorflow/compiler/xla/service:elemental_ir_emitter\",\n+        \"//tensorflow/compiler/xla/service/gpu:gpu_fusible\",\n+        \"//tensorflow/compiler/xla/service/gpu:thunk\",\n+        \"//tensorflow/compiler/xla/service/llvm_ir:loop_emitter\",\n+        \"//tensorflow/tsl/platform:logging\",\n+        \"//tensorflow/tsl/platform:status\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:Support\",\n",
            "whole_hunk": "@@ -134,8 +134,16 @@ cc_library(\n         \":fusion_emitter\",\n         \":thunk_util\",\n         \":tiling_util\",\n+        \"//tensorflow/compiler/xla:shape_util\",\n+        \"//tensorflow/compiler/xla:status\",\n+        \"//tensorflow/compiler/xla:status_macros\",\n+        \"//tensorflow/compiler/xla:statusor\",\n         \"//tensorflow/compiler/xla/hlo/ir:hlo\",\n+        \"//tensorflow/compiler/xla/mlir_hlo\",\n+        \"//tensorflow/compiler/xla/mlir_hlo:lhlo\",\n+        \"//tensorflow/compiler/xla/service:elemental_ir_emitter\",\n         \"//tensorflow/compiler/xla/service/gpu:gpu_executable\",\n+        \"//tensorflow/compiler/xla/service/gpu:gpu_fusible\",\n         \"//tensorflow/compiler/xla/service/gpu:hlo_fusion_analysis\",\n         \"//tensorflow/compiler/xla/service/gpu:ir_emission_utils\",\n         \"//tensorflow/compiler/xla/service/gpu:ir_emitter\",\n@@ -143,12 +151,21 @@ cc_library(\n         \"//tensorflow/compiler/xla/service/gpu:kernel_reuse_cache\",\n         \"//tensorflow/compiler/xla/service/gpu:parallel_loop_emitter\",\n         \"//tensorflow/compiler/xla/service/gpu:target_util\",\n+        \"//tensorflow/compiler/xla/service/gpu:thunk\",\n         \"//tensorflow/compiler/xla/service/llvm_ir:fused_ir_emitter\",\n         \"//tensorflow/compiler/xla/service/llvm_ir:ir_array\",\n         \"//tensorflow/compiler/xla/service/llvm_ir:kernel_support_library\",\n         \"//tensorflow/compiler/xla/service/llvm_ir:llvm_util\",\n+        \"//tensorflow/compiler/xla/service/llvm_ir:loop_emitter\",\n         \"//tensorflow/compiler/xla/translate/mhlo_to_hlo:location_exporter\",\n+        \"//tensorflow/tsl/platform:logging\",\n+        \"//tensorflow/tsl/platform:status\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//llvm:Support\",\n         \"@llvm-project//llvm:ir_headers\",\n+        \"@llvm-project//mlir:Support\",\n     ],\n )\n \n"
        },
        {
            "name": "reduction.cc",
            "path": "tensorflow/compiler/xla/service/gpu/fusions/reduction.cc",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 17,
                    "new_start": 14,
                    "new_length": 39,
                    "hunk": "@@ -14,17 +14,39 @@ limitations under the License.\n ==============================================================================*/\n #include \"tensorflow/compiler/xla/service/gpu/fusions/reduction.h\"\n \n+#include <cstdint>\n+#include <functional>\n #include <memory>\n+#include <string>\n #include <utility>\n #include <vector>\n \n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/Twine.h\"\n+#include \"llvm/IR/Constants.h\"\n+#include \"llvm/IR/DerivedTypes.h\"\n+#include \"llvm/IR/GlobalVariable.h\"\n #include \"llvm/IR/IRBuilder.h\"\n+#include \"llvm/IR/Instructions.h\"\n+#include \"llvm/IR/Type.h\"\n+#include \"llvm/IR/Value.h\"\n+#include \"llvm/Support/AtomicOrdering.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_casting_utils.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instructions.h\"\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_opcode.h\"\n+#include \"tensorflow/compiler/xla/mlir_hlo/lhlo/IR/lhlo_ops.h\"\n+#include \"tensorflow/compiler/xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+#include \"tensorflow/compiler/xla/service/elemental_ir_emitter.h\"\n #include \"tensorflow/compiler/xla/service/gpu/fusions/fusion_emitter.h\"\n #include \"tensorflow/compiler/xla/service/gpu/fusions/thunk_util.h\"\n #include \"tensorflow/compiler/xla/service/gpu/fusions/tiling_util.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/gpu_fusible.h\"\n #include \"tensorflow/compiler/xla/service/gpu/ir_emission_utils.h\"\n #include \"tensorflow/compiler/xla/service/gpu/ir_emitter_context.h\"\n #include \"tensorflow/compiler/xla/service/gpu/ir_emitter_nested.h\"\n"
                },
                {
                    "old_start": 33,
                    "old_length": 11,
                    "new_start": 55,
                    "new_length": 20,
                    "hunk": "@@ -33,11 +55,20 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/service/gpu/kernel_thunk.h\"\n #include \"tensorflow/compiler/xla/service/gpu/parallel_loop_emitter.h\"\n #include \"tensorflow/compiler/xla/service/gpu/target_util.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/thunk.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/fused_ir_emitter.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/ir_array.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/llvm_util.h\"\n+#include \"tensorflow/compiler/xla/service/llvm_ir/loop_emitter.h\"\n+#include \"tensorflow/compiler/xla/shape.h\"\n+#include \"tensorflow/compiler/xla/shape_util.h\"\n+#include \"tensorflow/compiler/xla/status.h\"\n+#include \"tensorflow/compiler/xla/status_macros.h\"\n+#include \"tensorflow/compiler/xla/statusor.h\"\n #include \"tensorflow/compiler/xla/translate/mhlo_to_hlo/location_exporter.h\"\n+#include \"tensorflow/tsl/platform/logging.h\"\n+#include \"tensorflow/tsl/platform/status.h\"\n \n namespace xla {\n namespace gpu {\n"
                },
                {
                    "old_start": 433,
                    "old_length": 12,
                    "new_start": 464,
                    "new_length": 12,
                    "hunk": "@@ -433,12 +464,12 @@ void EmitFullWarpShuffleDownLoopForReduce(\n   }\n }\n \n-llvm::Value* GetOutputAddressForReduction(\n+llvm_ir::IrArray::Index GetOutputIndexForReduction(\n     llvm::IRBuilder<>* builder, int partial_result_idx, llvm::Type* index_ty,\n     const ReductionCodegenState& reduction_codegen_state,\n     const TilingKernelInfo& tiling_kernel_info,\n-    const ReductionOutputMap& output_arrays,\n-    const HloReduceInstruction* reduction, int output_idx) {\n+    const HloReduceInstruction* reduction, const HloInstruction* root,\n+    int output_idx) {\n   auto constant = [&](uint64_t c) -> llvm::Constant* {\n     return llvm::ConstantInt::get(index_ty, c);\n   };\n"
                },
                {
                    "old_start": 459,
                    "old_length": 8,
                    "new_start": 490,
                    "new_length": 6,
                    "hunk": "@@ -459,8 +490,6 @@ llvm::Value* GetOutputAddressForReduction(\n         .AddOffsetToDim(start_offset_x, TilingScheme::DimX, builder);\n   }();\n \n-  const llvm_ir::IrArray& output_array =\n-      output_arrays.at(reduction)[output_idx];\n   const Shape& operand_shape = reduction->inputs()[output_idx]->shape();\n   Shape reduction_kept_element_shape =\n       ShapeUtil::DeleteDimensions(reduction->dimensions(), operand_shape);\n"
                },
                {
                    "old_start": 495,
                    "old_length": 12,
                    "new_start": 524,
                    "new_length": 19,
                    "hunk": "@@ -495,12 +524,19 @@ llvm::Value* GetOutputAddressForReduction(\n   llvm_ir::IrArray::Index element_index(\n       /*linear=*/untransposed_output_linear_address,\n       reduction_kept_element_shape, builder);\n-  llvm_ir::IrArray::Index output_index(element_index.multidim(),\n-                                       output_array.GetShape(),\n+  const Shape& output_shape = !reduction->shape().IsTuple()\n+                                  ? reduction->shape()\n+                                  : reduction->shape().tuple_shapes(output_idx);\n+  llvm_ir::IrArray::Index output_index(element_index.multidim(), output_shape,\n                                        element_index.GetType());\n-\n-  return output_array.EmitArrayElementAddress(output_index, builder,\n-                                              \"output_element_address\");\n+  // We need to check for root == reduction separately, because for variadic\n+  // reduce the root shape would be a tuple, while 'output_shape' is the\n+  // subshape.\n+  return (root == reduction ||\n+          ShapeUtil::EqualIgnoringElementType(output_shape, root->shape()))\n+             ? output_index\n+             : output_index.SourceIndexOfBitcast(output_shape, root->shape(),\n+                                                 builder);\n }\n \n llvm::Value* CastSharedToGlobal(llvm::IRBuilder<>* builder, llvm::Value* input,\n"
                },
                {
                    "old_start": 519,
                    "old_length": 19,
                    "new_start": 555,
                    "new_length": 32,
                    "hunk": "@@ -519,19 +555,32 @@ void WriteReductionOutput(llvm::IRBuilder<>* builder,\n                           const TilingKernelInfo& tiling_kernel_info,\n                           const ReductionOutputMap& output_arrays,\n                           const HloReduceInstruction* reduction,\n-                          int partial_result_idx,\n-                          const absl::Span<TypedPointer const> values) {\n+                          const HloInstruction* root, int partial_result_idx,\n+                          const absl::Span<TypedPointer const> values,\n+                          ElementalIrEmitter& elemental_emitter) {\n   const HloComputation* reducer = reduction->to_apply();\n   for (const auto& [oidx, typed_ptr] : llvm::enumerate(values)) {\n     auto [output_ptr, type] = typed_ptr;\n-    llvm::Value* output_address = GetOutputAddressForReduction(\n+    llvm_ir::IrArray::Index output_index = GetOutputIndexForReduction(\n         builder, partial_result_idx, index_ty, reduction_codegen_state,\n-        tiling_kernel_info, output_arrays, reduction, oidx);\n+        tiling_kernel_info, reduction, root, oidx);\n+\n+    llvm::Value* output_address =\n+        output_arrays.at(root)[oidx].EmitArrayElementAddress(\n+            output_index, builder, \"output_element_address\");\n     if (reduction_codegen_state.IsRaceFree()) {\n-      builder->CreateStore(builder->CreateLoad(type, output_ptr, \"output\"),\n-                           output_address);\n+      FusedIrEmitter fused_emitter(elemental_emitter);\n+      llvm::Value* loaded = builder->CreateLoad(type, output_ptr, \"output\");\n+      fused_emitter.BindGenerator(\n+          *reduction,\n+          [&](const llvm_ir::IrArray::Index& index) { return loaded; });\n+      llvm_ir::ElementGenerator gen = *fused_emitter.GetGenerator(*root);\n+      llvm::Value* generated = *gen(output_index);\n+      builder->CreateStore(generated, output_address);\n     } else {\n       CHECK_EQ(values.size(), 1);\n+      CHECK_EQ(reduction, root)\n+          << \"output fusion is not allowed for racing reductions\";\n       TF_CHECK_OK(EmitAtomicOperationForNestedComputation(\n           builder, ir_emitter_context, *reducer, output_address, output_ptr,\n           type));\n"
                },
                {
                    "old_start": 546,
                    "old_length": 7,
                    "new_start": 595,
                    "new_length": 8,
                    "hunk": "@@ -546,7 +595,8 @@ void EmitReductionOutputForRowReduction(\n     const TilingKernelInfo& tiling_kernel_info,\n     const ReductionCodegenState& reduction_codegen_state, llvm::Type* index_ty,\n     const ReductionOutputMap& output_arrays,\n-    const HloReduceInstruction* reduction, int partial_result_idx) {\n+    const HloReduceInstruction* reduction, const HloInstruction* root,\n+    int partial_result_idx, ElementalIrEmitter& elemental_emitter) {\n   const HloComputation* reducer = reduction->to_apply();\n   const auto& thread_id_info = tiling_kernel_info.thread_id_info;\n   auto constant = [&](uint64_t c) -> llvm::Constant* {\n"
                },
                {
                    "old_start": 585,
                    "old_length": 8,
                    "new_start": 635,
                    "new_length": 8,
                    "hunk": "@@ -585,8 +635,8 @@ void EmitReductionOutputForRowReduction(\n     ksl.If(\"reduction_write_output\", write_condition, [&] {\n       WriteReductionOutput(builder, ir_emitter_context, index_ty,\n                            reduction_codegen_state, tiling_kernel_info,\n-                           output_arrays, reduction, partial_result_idx,\n-                           values);\n+                           output_arrays, reduction, root, partial_result_idx,\n+                           values, elemental_emitter);\n     });\n   };\n \n"
                },
                {
                    "old_start": 664,
                    "old_length": 7,
                    "new_start": 714,
                    "new_length": 8,
                    "hunk": "@@ -664,7 +714,8 @@ void EmitReductionOutputForColumnReduction(\n     const TilingKernelInfo& tiling_kernel_info,\n     const ReductionCodegenState& reduction_codegen_state, llvm::Type* index_ty,\n     const ReductionOutputMap& output_arrays,\n-    const HloReduceInstruction* reduction, int partial_result_idx) {\n+    const HloReduceInstruction* reduction, const HloInstruction* root,\n+    int partial_result_idx, ElementalIrEmitter& elemental_emitter) {\n   KernelSupportLibrary ksl(builder);\n   const HloComputation* reducer = reduction->to_apply();\n   const auto& thread_id_info = tiling_kernel_info.thread_id_info;\n"
                },
                {
                    "old_start": 740,
                    "old_length": 10,
                    "new_start": 791,
                    "new_length": 10,
                    "hunk": "@@ -740,10 +791,10 @@ void EmitReductionOutputForColumnReduction(\n \n   ksl.If(\"reduction_write_output\",\n          builder->CreateAnd(has_output, is_zero(thread_id_info.lane_id)), [&] {\n-           WriteReductionOutput(builder, ir_emitter_context, index_ty,\n-                                reduction_codegen_state, tiling_kernel_info,\n-                                output_arrays, reduction, partial_result_idx,\n-                                shmem_transposed_addrs);\n+           WriteReductionOutput(\n+               builder, ir_emitter_context, index_ty, reduction_codegen_state,\n+               tiling_kernel_info, output_arrays, reduction, root,\n+               partial_result_idx, shmem_transposed_addrs, elemental_emitter);\n          });\n }\n \n"
                },
                {
                    "old_start": 811,
                    "old_length": 19,
                    "new_start": 862,
                    "new_length": 25,
                    "hunk": "@@ -811,19 +862,25 @@ Status EmitIRForReduction(llvm::IRBuilder<>* builder,\n                           FusedIrEmitter& fused_emitter,\n                           const ReductionOutputMap& result_ir_arrays,\n                           const ReductionCodegenInfo& reduction_info,\n-                          const Shape& input_shape) {\n-  std::vector<const HloReduceInstruction*> reductions;\n+                          const Shape& input_shape,\n+                          ElementalIrEmitter& elemental_emitter) {\n+  std::vector<const HloInstruction*> roots;\n+  std::vector<const HloReduceInstruction*> heroes;\n   ExtraOutputGensMap extra_output_gens;\n \n   for (const HloInstruction* hlo : instr_index_group) {\n-    if (IsReductionFromOrToContiguousDimensions(*hlo)) {\n-      reductions.push_back(Cast<HloReduceInstruction>(hlo));\n+    const HloInstruction* reduction_hero =\n+        FindRealReductionHero(const_cast<HloInstruction*>(hlo));\n+    if (reduction_hero != nullptr) {\n+      auto hero = Cast<HloReduceInstruction>(reduction_hero);\n+      roots.push_back(hlo);\n+      heroes.push_back(hero);\n     } else {\n       extra_output_gens[hlo] = *fused_emitter.GetGenerator(*hlo);\n     }\n   }\n \n-  CHECK(!reductions.empty()) << \" expect at least one reduce instructions.\";\n+  CHECK(!heroes.empty()) << \" expect at least one reduce instructions.\";\n   const TilingScheme& tiling_scheme = reduction_info.GetTilingScheme();\n   CHECK_EQ(tiling_scheme.GetNumThreadsPerBlockPhysical() % WarpSize(), 0);\n   llvm::Type* index_ty =\n"
                },
                {
                    "old_start": 832,
                    "old_length": 7,
                    "new_start": 889,
                    "new_length": 7,
                    "hunk": "@@ -832,7 +889,7 @@ Status EmitIRForReduction(llvm::IRBuilder<>* builder,\n                                 tiling_scheme.GetNumberOfBlocksPhysical(),\n                             builder);\n   ReductionCodegenState codegen_state = GenerateReductionCodegenState(\n-      builder, fusion, reduction_info, reductions, fused_emitter);\n+      builder, fusion, reduction_info, heroes, fused_emitter);\n \n   EmitTileElementFunction emit_reduction_element =\n       [&](const TilingThreadIdInfo& thread_id_info,\n"
                },
                {
                    "old_start": 859,
                    "old_length": 7,
                    "new_start": 916,
                    "new_length": 7,
                    "hunk": "@@ -859,7 +916,7 @@ Status EmitIRForReduction(llvm::IRBuilder<>* builder,\n \n         // Emit code to generate the input and perform the reduction computation\n         // for each reduction instruction.\n-        for (const HloReduceInstruction* reduce : reductions) {\n+        for (const HloReduceInstruction* reduce : heroes) {\n           GenerateElementForReducer(builder, ir_emitter_context, reduce,\n                                     partial_result_index, codegen_state,\n                                     index_without_linear, input_index,\n"
                },
                {
                    "old_start": 885,
                    "old_length": 18,
                    "new_start": 942,
                    "new_length": 20,
                    "hunk": "@@ -885,18 +942,20 @@ Status EmitIRForReduction(llvm::IRBuilder<>* builder,\n                        }));\n \n   KernelSupportLibrary ksl(builder);\n-  for (const HloReduceInstruction* reduce : reductions) {\n+  for (auto [reduce, root] : llvm::zip(heroes, roots)) {\n     for (int partial_result_idx = 0;\n          partial_result_idx < reduction_info.GetNumPartialResults();\n          ++partial_result_idx) {\n       if (codegen_state.IsRowReduction()) {\n         EmitReductionOutputForRowReduction(\n             builder, ir_emitter_context, tiling_kernel_info, codegen_state,\n-            index_ty, result_ir_arrays, reduce, partial_result_idx);\n+            index_ty, result_ir_arrays, reduce, root, partial_result_idx,\n+            elemental_emitter);\n       } else {\n         EmitReductionOutputForColumnReduction(\n             builder, ir_emitter_context, tiling_kernel_info, codegen_state,\n-            index_ty, result_ir_arrays, reduce, partial_result_idx);\n+            index_ty, result_ir_arrays, reduce, root, partial_result_idx,\n+            elemental_emitter);\n       }\n     }\n   }\n"
                },
                {
                    "old_start": 922,
                    "old_length": 7,
                    "new_start": 981,
                    "new_length": 7,
                    "hunk": "@@ -922,7 +981,7 @@ StatusOr<FusionEmissionResult> ReductionFusion::Emit(\n   if (!reduction_codegen_info->IsRaceFree()) {\n     absl::Span<HloInstruction* const> fusion_roots = analysis_.fusion_roots();\n     for (int i = 0; i < fusion_roots.size(); ++i) {\n-      if (IsReductionFromOrToContiguousDimensions(*fusion_roots[i])) {\n+      if (HasRealReductionHero(fusion_roots[i])) {\n         TF_ASSIGN_OR_RETURN(result.thunks.emplace_back(),\n                             BuildFusedInitializerThunk(\n                                 ir_emitter_context_, fusion_op(), analysis_,\n"
                },
                {
                    "old_start": 979,
                    "old_length": 7,
                    "new_start": 1038,
                    "new_length": 7,
                    "hunk": "@@ -979,7 +1038,7 @@ StatusOr<FusionEmissionResult> ReductionFusion::Emit(\n             return EmitIRForReduction(builder, ir_emitter_context_, fusion_op(),\n                                       instr_index_groups[i], fused_emitter,\n                                       result_ir_arrays, *reduction_codegen_info,\n-                                      reduce_operand_shape);\n+                                      reduce_operand_shape, elemental_emitter_);\n           }));\n     }\n \n"
                }
            ],
            "whole_deleted": "-llvm::Value* GetOutputAddressForReduction(\n-    const ReductionOutputMap& output_arrays,\n-    const HloReduceInstruction* reduction, int output_idx) {\n-  const llvm_ir::IrArray& output_array =\n-      output_arrays.at(reduction)[output_idx];\n-  llvm_ir::IrArray::Index output_index(element_index.multidim(),\n-                                       output_array.GetShape(),\n-\n-  return output_array.EmitArrayElementAddress(output_index, builder,\n-                                              \"output_element_address\");\n-                          int partial_result_idx,\n-                          const absl::Span<TypedPointer const> values) {\n-    llvm::Value* output_address = GetOutputAddressForReduction(\n-        tiling_kernel_info, output_arrays, reduction, oidx);\n-      builder->CreateStore(builder->CreateLoad(type, output_ptr, \"output\"),\n-                           output_address);\n-    const HloReduceInstruction* reduction, int partial_result_idx) {\n-                           output_arrays, reduction, partial_result_idx,\n-                           values);\n-    const HloReduceInstruction* reduction, int partial_result_idx) {\n-           WriteReductionOutput(builder, ir_emitter_context, index_ty,\n-                                reduction_codegen_state, tiling_kernel_info,\n-                                output_arrays, reduction, partial_result_idx,\n-                                shmem_transposed_addrs);\n-                          const Shape& input_shape) {\n-  std::vector<const HloReduceInstruction*> reductions;\n-    if (IsReductionFromOrToContiguousDimensions(*hlo)) {\n-      reductions.push_back(Cast<HloReduceInstruction>(hlo));\n-  CHECK(!reductions.empty()) << \" expect at least one reduce instructions.\";\n-      builder, fusion, reduction_info, reductions, fused_emitter);\n-        for (const HloReduceInstruction* reduce : reductions) {\n-  for (const HloReduceInstruction* reduce : reductions) {\n-            index_ty, result_ir_arrays, reduce, partial_result_idx);\n-            index_ty, result_ir_arrays, reduce, partial_result_idx);\n-      if (IsReductionFromOrToContiguousDimensions(*fusion_roots[i])) {\n-                                      reduce_operand_shape);\n",
            "whole_added": "+#include <cstdint>\n+#include <functional>\n+#include <string>\n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/Twine.h\"\n+#include \"llvm/IR/Constants.h\"\n+#include \"llvm/IR/DerivedTypes.h\"\n+#include \"llvm/IR/GlobalVariable.h\"\n+#include \"llvm/IR/Instructions.h\"\n+#include \"llvm/IR/Type.h\"\n+#include \"llvm/IR/Value.h\"\n+#include \"llvm/Support/AtomicOrdering.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_opcode.h\"\n+#include \"tensorflow/compiler/xla/mlir_hlo/lhlo/IR/lhlo_ops.h\"\n+#include \"tensorflow/compiler/xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+#include \"tensorflow/compiler/xla/service/elemental_ir_emitter.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/gpu_fusible.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/thunk.h\"\n+#include \"tensorflow/compiler/xla/service/llvm_ir/loop_emitter.h\"\n+#include \"tensorflow/compiler/xla/shape.h\"\n+#include \"tensorflow/compiler/xla/shape_util.h\"\n+#include \"tensorflow/compiler/xla/status.h\"\n+#include \"tensorflow/compiler/xla/status_macros.h\"\n+#include \"tensorflow/compiler/xla/statusor.h\"\n+#include \"tensorflow/tsl/platform/logging.h\"\n+#include \"tensorflow/tsl/platform/status.h\"\n+llvm_ir::IrArray::Index GetOutputIndexForReduction(\n+    const HloReduceInstruction* reduction, const HloInstruction* root,\n+    int output_idx) {\n+  const Shape& output_shape = !reduction->shape().IsTuple()\n+                                  ? reduction->shape()\n+                                  : reduction->shape().tuple_shapes(output_idx);\n+  llvm_ir::IrArray::Index output_index(element_index.multidim(), output_shape,\n+  // We need to check for root == reduction separately, because for variadic\n+  // reduce the root shape would be a tuple, while 'output_shape' is the\n+  // subshape.\n+  return (root == reduction ||\n+          ShapeUtil::EqualIgnoringElementType(output_shape, root->shape()))\n+             ? output_index\n+             : output_index.SourceIndexOfBitcast(output_shape, root->shape(),\n+                                                 builder);\n+                          const HloInstruction* root, int partial_result_idx,\n+                          const absl::Span<TypedPointer const> values,\n+                          ElementalIrEmitter& elemental_emitter) {\n+    llvm_ir::IrArray::Index output_index = GetOutputIndexForReduction(\n+        tiling_kernel_info, reduction, root, oidx);\n+\n+    llvm::Value* output_address =\n+        output_arrays.at(root)[oidx].EmitArrayElementAddress(\n+            output_index, builder, \"output_element_address\");\n+      FusedIrEmitter fused_emitter(elemental_emitter);\n+      llvm::Value* loaded = builder->CreateLoad(type, output_ptr, \"output\");\n+      fused_emitter.BindGenerator(\n+          *reduction,\n+          [&](const llvm_ir::IrArray::Index& index) { return loaded; });\n+      llvm_ir::ElementGenerator gen = *fused_emitter.GetGenerator(*root);\n+      llvm::Value* generated = *gen(output_index);\n+      builder->CreateStore(generated, output_address);\n+      CHECK_EQ(reduction, root)\n+          << \"output fusion is not allowed for racing reductions\";\n+    const HloReduceInstruction* reduction, const HloInstruction* root,\n+    int partial_result_idx, ElementalIrEmitter& elemental_emitter) {\n+                           output_arrays, reduction, root, partial_result_idx,\n+                           values, elemental_emitter);\n+    const HloReduceInstruction* reduction, const HloInstruction* root,\n+    int partial_result_idx, ElementalIrEmitter& elemental_emitter) {\n+           WriteReductionOutput(\n+               builder, ir_emitter_context, index_ty, reduction_codegen_state,\n+               tiling_kernel_info, output_arrays, reduction, root,\n+               partial_result_idx, shmem_transposed_addrs, elemental_emitter);\n+                          const Shape& input_shape,\n+                          ElementalIrEmitter& elemental_emitter) {\n+  std::vector<const HloInstruction*> roots;\n+  std::vector<const HloReduceInstruction*> heroes;\n+    const HloInstruction* reduction_hero =\n+        FindRealReductionHero(const_cast<HloInstruction*>(hlo));\n+    if (reduction_hero != nullptr) {\n+      auto hero = Cast<HloReduceInstruction>(reduction_hero);\n+      roots.push_back(hlo);\n+      heroes.push_back(hero);\n+  CHECK(!heroes.empty()) << \" expect at least one reduce instructions.\";\n+      builder, fusion, reduction_info, heroes, fused_emitter);\n+        for (const HloReduceInstruction* reduce : heroes) {\n+  for (auto [reduce, root] : llvm::zip(heroes, roots)) {\n+            index_ty, result_ir_arrays, reduce, root, partial_result_idx,\n+            elemental_emitter);\n+            index_ty, result_ir_arrays, reduce, root, partial_result_idx,\n+            elemental_emitter);\n+      if (HasRealReductionHero(fusion_roots[i])) {\n+                                      reduce_operand_shape, elemental_emitter_);\n",
            "whole_hunk": "@@ -14,17 +14,39 @@ limitations under the License.\n ==============================================================================*/\n #include \"tensorflow/compiler/xla/service/gpu/fusions/reduction.h\"\n \n+#include <cstdint>\n+#include <functional>\n #include <memory>\n+#include <string>\n #include <utility>\n #include <vector>\n \n+#include \"absl/container/inlined_vector.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/Twine.h\"\n+#include \"llvm/IR/Constants.h\"\n+#include \"llvm/IR/DerivedTypes.h\"\n+#include \"llvm/IR/GlobalVariable.h\"\n #include \"llvm/IR/IRBuilder.h\"\n+#include \"llvm/IR/Instructions.h\"\n+#include \"llvm/IR/Type.h\"\n+#include \"llvm/IR/Value.h\"\n+#include \"llvm/Support/AtomicOrdering.h\"\n+#include \"llvm/Support/Casting.h\"\n+#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_casting_utils.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instructions.h\"\n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_opcode.h\"\n+#include \"tensorflow/compiler/xla/mlir_hlo/lhlo/IR/lhlo_ops.h\"\n+#include \"tensorflow/compiler/xla/mlir_hlo/mhlo/IR/hlo_ops.h\"\n+#include \"tensorflow/compiler/xla/service/elemental_ir_emitter.h\"\n #include \"tensorflow/compiler/xla/service/gpu/fusions/fusion_emitter.h\"\n #include \"tensorflow/compiler/xla/service/gpu/fusions/thunk_util.h\"\n #include \"tensorflow/compiler/xla/service/gpu/fusions/tiling_util.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/gpu_fusible.h\"\n #include \"tensorflow/compiler/xla/service/gpu/ir_emission_utils.h\"\n #include \"tensorflow/compiler/xla/service/gpu/ir_emitter_context.h\"\n #include \"tensorflow/compiler/xla/service/gpu/ir_emitter_nested.h\"\n@@ -33,11 +55,20 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/service/gpu/kernel_thunk.h\"\n #include \"tensorflow/compiler/xla/service/gpu/parallel_loop_emitter.h\"\n #include \"tensorflow/compiler/xla/service/gpu/target_util.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/thunk.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/fused_ir_emitter.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/ir_array.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/llvm_util.h\"\n+#include \"tensorflow/compiler/xla/service/llvm_ir/loop_emitter.h\"\n+#include \"tensorflow/compiler/xla/shape.h\"\n+#include \"tensorflow/compiler/xla/shape_util.h\"\n+#include \"tensorflow/compiler/xla/status.h\"\n+#include \"tensorflow/compiler/xla/status_macros.h\"\n+#include \"tensorflow/compiler/xla/statusor.h\"\n #include \"tensorflow/compiler/xla/translate/mhlo_to_hlo/location_exporter.h\"\n+#include \"tensorflow/tsl/platform/logging.h\"\n+#include \"tensorflow/tsl/platform/status.h\"\n \n namespace xla {\n namespace gpu {\n@@ -433,12 +464,12 @@ void EmitFullWarpShuffleDownLoopForReduce(\n   }\n }\n \n-llvm::Value* GetOutputAddressForReduction(\n+llvm_ir::IrArray::Index GetOutputIndexForReduction(\n     llvm::IRBuilder<>* builder, int partial_result_idx, llvm::Type* index_ty,\n     const ReductionCodegenState& reduction_codegen_state,\n     const TilingKernelInfo& tiling_kernel_info,\n-    const ReductionOutputMap& output_arrays,\n-    const HloReduceInstruction* reduction, int output_idx) {\n+    const HloReduceInstruction* reduction, const HloInstruction* root,\n+    int output_idx) {\n   auto constant = [&](uint64_t c) -> llvm::Constant* {\n     return llvm::ConstantInt::get(index_ty, c);\n   };\n@@ -459,8 +490,6 @@ llvm::Value* GetOutputAddressForReduction(\n         .AddOffsetToDim(start_offset_x, TilingScheme::DimX, builder);\n   }();\n \n-  const llvm_ir::IrArray& output_array =\n-      output_arrays.at(reduction)[output_idx];\n   const Shape& operand_shape = reduction->inputs()[output_idx]->shape();\n   Shape reduction_kept_element_shape =\n       ShapeUtil::DeleteDimensions(reduction->dimensions(), operand_shape);\n@@ -495,12 +524,19 @@ llvm::Value* GetOutputAddressForReduction(\n   llvm_ir::IrArray::Index element_index(\n       /*linear=*/untransposed_output_linear_address,\n       reduction_kept_element_shape, builder);\n-  llvm_ir::IrArray::Index output_index(element_index.multidim(),\n-                                       output_array.GetShape(),\n+  const Shape& output_shape = !reduction->shape().IsTuple()\n+                                  ? reduction->shape()\n+                                  : reduction->shape().tuple_shapes(output_idx);\n+  llvm_ir::IrArray::Index output_index(element_index.multidim(), output_shape,\n                                        element_index.GetType());\n-\n-  return output_array.EmitArrayElementAddress(output_index, builder,\n-                                              \"output_element_address\");\n+  // We need to check for root == reduction separately, because for variadic\n+  // reduce the root shape would be a tuple, while 'output_shape' is the\n+  // subshape.\n+  return (root == reduction ||\n+          ShapeUtil::EqualIgnoringElementType(output_shape, root->shape()))\n+             ? output_index\n+             : output_index.SourceIndexOfBitcast(output_shape, root->shape(),\n+                                                 builder);\n }\n \n llvm::Value* CastSharedToGlobal(llvm::IRBuilder<>* builder, llvm::Value* input,\n@@ -519,19 +555,32 @@ void WriteReductionOutput(llvm::IRBuilder<>* builder,\n                           const TilingKernelInfo& tiling_kernel_info,\n                           const ReductionOutputMap& output_arrays,\n                           const HloReduceInstruction* reduction,\n-                          int partial_result_idx,\n-                          const absl::Span<TypedPointer const> values) {\n+                          const HloInstruction* root, int partial_result_idx,\n+                          const absl::Span<TypedPointer const> values,\n+                          ElementalIrEmitter& elemental_emitter) {\n   const HloComputation* reducer = reduction->to_apply();\n   for (const auto& [oidx, typed_ptr] : llvm::enumerate(values)) {\n     auto [output_ptr, type] = typed_ptr;\n-    llvm::Value* output_address = GetOutputAddressForReduction(\n+    llvm_ir::IrArray::Index output_index = GetOutputIndexForReduction(\n         builder, partial_result_idx, index_ty, reduction_codegen_state,\n-        tiling_kernel_info, output_arrays, reduction, oidx);\n+        tiling_kernel_info, reduction, root, oidx);\n+\n+    llvm::Value* output_address =\n+        output_arrays.at(root)[oidx].EmitArrayElementAddress(\n+            output_index, builder, \"output_element_address\");\n     if (reduction_codegen_state.IsRaceFree()) {\n-      builder->CreateStore(builder->CreateLoad(type, output_ptr, \"output\"),\n-                           output_address);\n+      FusedIrEmitter fused_emitter(elemental_emitter);\n+      llvm::Value* loaded = builder->CreateLoad(type, output_ptr, \"output\");\n+      fused_emitter.BindGenerator(\n+          *reduction,\n+          [&](const llvm_ir::IrArray::Index& index) { return loaded; });\n+      llvm_ir::ElementGenerator gen = *fused_emitter.GetGenerator(*root);\n+      llvm::Value* generated = *gen(output_index);\n+      builder->CreateStore(generated, output_address);\n     } else {\n       CHECK_EQ(values.size(), 1);\n+      CHECK_EQ(reduction, root)\n+          << \"output fusion is not allowed for racing reductions\";\n       TF_CHECK_OK(EmitAtomicOperationForNestedComputation(\n           builder, ir_emitter_context, *reducer, output_address, output_ptr,\n           type));\n@@ -546,7 +595,8 @@ void EmitReductionOutputForRowReduction(\n     const TilingKernelInfo& tiling_kernel_info,\n     const ReductionCodegenState& reduction_codegen_state, llvm::Type* index_ty,\n     const ReductionOutputMap& output_arrays,\n-    const HloReduceInstruction* reduction, int partial_result_idx) {\n+    const HloReduceInstruction* reduction, const HloInstruction* root,\n+    int partial_result_idx, ElementalIrEmitter& elemental_emitter) {\n   const HloComputation* reducer = reduction->to_apply();\n   const auto& thread_id_info = tiling_kernel_info.thread_id_info;\n   auto constant = [&](uint64_t c) -> llvm::Constant* {\n@@ -585,8 +635,8 @@ void EmitReductionOutputForRowReduction(\n     ksl.If(\"reduction_write_output\", write_condition, [&] {\n       WriteReductionOutput(builder, ir_emitter_context, index_ty,\n                            reduction_codegen_state, tiling_kernel_info,\n-                           output_arrays, reduction, partial_result_idx,\n-                           values);\n+                           output_arrays, reduction, root, partial_result_idx,\n+                           values, elemental_emitter);\n     });\n   };\n \n@@ -664,7 +714,8 @@ void EmitReductionOutputForColumnReduction(\n     const TilingKernelInfo& tiling_kernel_info,\n     const ReductionCodegenState& reduction_codegen_state, llvm::Type* index_ty,\n     const ReductionOutputMap& output_arrays,\n-    const HloReduceInstruction* reduction, int partial_result_idx) {\n+    const HloReduceInstruction* reduction, const HloInstruction* root,\n+    int partial_result_idx, ElementalIrEmitter& elemental_emitter) {\n   KernelSupportLibrary ksl(builder);\n   const HloComputation* reducer = reduction->to_apply();\n   const auto& thread_id_info = tiling_kernel_info.thread_id_info;\n@@ -740,10 +791,10 @@ void EmitReductionOutputForColumnReduction(\n \n   ksl.If(\"reduction_write_output\",\n          builder->CreateAnd(has_output, is_zero(thread_id_info.lane_id)), [&] {\n-           WriteReductionOutput(builder, ir_emitter_context, index_ty,\n-                                reduction_codegen_state, tiling_kernel_info,\n-                                output_arrays, reduction, partial_result_idx,\n-                                shmem_transposed_addrs);\n+           WriteReductionOutput(\n+               builder, ir_emitter_context, index_ty, reduction_codegen_state,\n+               tiling_kernel_info, output_arrays, reduction, root,\n+               partial_result_idx, shmem_transposed_addrs, elemental_emitter);\n          });\n }\n \n@@ -811,19 +862,25 @@ Status EmitIRForReduction(llvm::IRBuilder<>* builder,\n                           FusedIrEmitter& fused_emitter,\n                           const ReductionOutputMap& result_ir_arrays,\n                           const ReductionCodegenInfo& reduction_info,\n-                          const Shape& input_shape) {\n-  std::vector<const HloReduceInstruction*> reductions;\n+                          const Shape& input_shape,\n+                          ElementalIrEmitter& elemental_emitter) {\n+  std::vector<const HloInstruction*> roots;\n+  std::vector<const HloReduceInstruction*> heroes;\n   ExtraOutputGensMap extra_output_gens;\n \n   for (const HloInstruction* hlo : instr_index_group) {\n-    if (IsReductionFromOrToContiguousDimensions(*hlo)) {\n-      reductions.push_back(Cast<HloReduceInstruction>(hlo));\n+    const HloInstruction* reduction_hero =\n+        FindRealReductionHero(const_cast<HloInstruction*>(hlo));\n+    if (reduction_hero != nullptr) {\n+      auto hero = Cast<HloReduceInstruction>(reduction_hero);\n+      roots.push_back(hlo);\n+      heroes.push_back(hero);\n     } else {\n       extra_output_gens[hlo] = *fused_emitter.GetGenerator(*hlo);\n     }\n   }\n \n-  CHECK(!reductions.empty()) << \" expect at least one reduce instructions.\";\n+  CHECK(!heroes.empty()) << \" expect at least one reduce instructions.\";\n   const TilingScheme& tiling_scheme = reduction_info.GetTilingScheme();\n   CHECK_EQ(tiling_scheme.GetNumThreadsPerBlockPhysical() % WarpSize(), 0);\n   llvm::Type* index_ty =\n@@ -832,7 +889,7 @@ Status EmitIRForReduction(llvm::IRBuilder<>* builder,\n                                 tiling_scheme.GetNumberOfBlocksPhysical(),\n                             builder);\n   ReductionCodegenState codegen_state = GenerateReductionCodegenState(\n-      builder, fusion, reduction_info, reductions, fused_emitter);\n+      builder, fusion, reduction_info, heroes, fused_emitter);\n \n   EmitTileElementFunction emit_reduction_element =\n       [&](const TilingThreadIdInfo& thread_id_info,\n@@ -859,7 +916,7 @@ Status EmitIRForReduction(llvm::IRBuilder<>* builder,\n \n         // Emit code to generate the input and perform the reduction computation\n         // for each reduction instruction.\n-        for (const HloReduceInstruction* reduce : reductions) {\n+        for (const HloReduceInstruction* reduce : heroes) {\n           GenerateElementForReducer(builder, ir_emitter_context, reduce,\n                                     partial_result_index, codegen_state,\n                                     index_without_linear, input_index,\n@@ -885,18 +942,20 @@ Status EmitIRForReduction(llvm::IRBuilder<>* builder,\n                        }));\n \n   KernelSupportLibrary ksl(builder);\n-  for (const HloReduceInstruction* reduce : reductions) {\n+  for (auto [reduce, root] : llvm::zip(heroes, roots)) {\n     for (int partial_result_idx = 0;\n          partial_result_idx < reduction_info.GetNumPartialResults();\n          ++partial_result_idx) {\n       if (codegen_state.IsRowReduction()) {\n         EmitReductionOutputForRowReduction(\n             builder, ir_emitter_context, tiling_kernel_info, codegen_state,\n-            index_ty, result_ir_arrays, reduce, partial_result_idx);\n+            index_ty, result_ir_arrays, reduce, root, partial_result_idx,\n+            elemental_emitter);\n       } else {\n         EmitReductionOutputForColumnReduction(\n             builder, ir_emitter_context, tiling_kernel_info, codegen_state,\n-            index_ty, result_ir_arrays, reduce, partial_result_idx);\n+            index_ty, result_ir_arrays, reduce, root, partial_result_idx,\n+            elemental_emitter);\n       }\n     }\n   }\n@@ -922,7 +981,7 @@ StatusOr<FusionEmissionResult> ReductionFusion::Emit(\n   if (!reduction_codegen_info->IsRaceFree()) {\n     absl::Span<HloInstruction* const> fusion_roots = analysis_.fusion_roots();\n     for (int i = 0; i < fusion_roots.size(); ++i) {\n-      if (IsReductionFromOrToContiguousDimensions(*fusion_roots[i])) {\n+      if (HasRealReductionHero(fusion_roots[i])) {\n         TF_ASSIGN_OR_RETURN(result.thunks.emplace_back(),\n                             BuildFusedInitializerThunk(\n                                 ir_emitter_context_, fusion_op(), analysis_,\n@@ -979,7 +1038,7 @@ StatusOr<FusionEmissionResult> ReductionFusion::Emit(\n             return EmitIRForReduction(builder, ir_emitter_context_, fusion_op(),\n                                       instr_index_groups[i], fused_emitter,\n                                       result_ir_arrays, *reduction_codegen_info,\n-                                      reduce_operand_shape);\n+                                      reduce_operand_shape, elemental_emitter_);\n           }));\n     }\n \n"
        },
        {
            "name": "gpu_fusible.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible.cc",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 6,
                    "new_start": 21,
                    "new_length": 7,
                    "hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <stack>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_computation.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_opcode.h\"\n"
                },
                {
                    "old_start": 29,
                    "old_length": 6,
                    "new_start": 30,
                    "new_length": 7,
                    "hunk": "@@ -29,6 +30,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/service/instruction_fusion.h\"\n #include \"tensorflow/compiler/xla/shape.h\"\n #include \"tensorflow/compiler/xla/shape_util.h\"\n+#include \"tensorflow/compiler/xla/util.h\"\n \n namespace xla {\n namespace gpu {\n"
                },
                {
                    "old_start": 94,
                    "old_length": 12,
                    "new_start": 96,
                    "new_length": 11,
                    "hunk": "@@ -94,12 +96,11 @@ bool IsPhysicallyTransposing(const HloInstruction& instr) {\n \n bool IsReduceInputFusion(const HloInstruction& instr) {\n   return instr.opcode() == HloOpcode::kFusion &&\n-         HasAnyUnnestedReductionRoot(instr.called_computations()[0]);\n+         HasAnyUnnestedReductionRoot(*instr.called_computations()[0]);\n }\n \n bool IsInputFusibleReduction(const HloInstruction& instr) {\n-  return IsReduceInputFusion(instr) ||\n-         IsReductionFromOrToContiguousDimensions(instr);\n+  return IsReduceInputFusion(instr) || HasRealReductionHero(&instr);\n }\n \n bool IsNestableVariadicReduction(const HloInstruction& instr) {\n"
                },
                {
                    "old_start": 116,
                    "old_length": 7,
                    "new_start": 117,
                    "new_length": 7,
                    "hunk": "@@ -116,7 +117,7 @@ bool IsTransposeInputFusion(const HloInstruction& instr) {\n     return false;\n   }\n   return instr.opcode() == HloOpcode::kFusion &&\n-         HasAnyTiledTransposeRoot(instr.called_computations()[0]);\n+         HasAnyTiledTransposeRoot(*instr.called_computations()[0]);\n }\n \n bool IsInputFusibleTranspose(const HloInstruction& instr) {\n"
                },
                {
                    "old_start": 130,
                    "old_length": 7,
                    "new_start": 131,
                    "new_length": 7,
                    "hunk": "@@ -130,7 +131,7 @@ const HloInstruction* GetRealHeroForMultiOutputFusion(\n   }\n   auto fused_expression_root = instr.fused_expression_root();\n   if (!instr.IsMultiOutputFusion()) {\n-    if (IsReductionFromOrToContiguousDimensions(*fused_expression_root) ||\n+    if (HasRealReductionHero(fused_expression_root) ||\n         FindAnyTiledTranspose(*fused_expression_root)) {\n       return &FindNonTrivialHero(*fused_expression_root);\n     }\n"
                },
                {
                    "old_start": 140,
                    "old_length": 9,
                    "new_start": 141,
                    "new_length": 8,
                    "hunk": "@@ -140,9 +141,8 @@ const HloInstruction* GetRealHeroForMultiOutputFusion(\n   // operand of the fusion root or a tiled transpose, because they have the most\n   // constraints. Note that we cannot have both kinds at the same time, so once\n   // we find any, we can immediately return it.\n-  for (const auto* inst : fused_expression_root->operands()) {\n-    if (IsReductionFromOrToContiguousDimensions(*inst) ||\n-        FindAnyTiledTranspose(*inst)) {\n+  for (auto* inst : fused_expression_root->mutable_operands()) {\n+    if (HasRealReductionHero(inst) || FindAnyTiledTranspose(*inst)) {\n       return &FindNonTrivialHero(*inst);\n     }\n   }\n"
                },
                {
                    "old_start": 153,
                    "old_length": 7,
                    "new_start": 153,
                    "new_length": 7,
                    "hunk": "@@ -153,7 +153,7 @@ const HloInstruction* GetRealHeroForMultiOutputFusion(\n // `first_reduce`.\n static bool IsFusedReductionOutputConsistent(\n     const HloInstruction* inst, const HloInstruction* first_reduce) {\n-  if (IsReductionFromOrToContiguousDimensions(*inst)) {\n+  if (HasRealReductionHero(inst)) {\n     // Shapes, layouts and dimensions must be the same for all reduces\n     // inside of this fusion.\n     return ShapeUtil::EqualIgnoringElementType(first_reduce->shape(),\n"
                },
                {
                    "old_start": 329,
                    "old_length": 12,
                    "new_start": 329,
                    "new_length": 46,
                    "hunk": "@@ -329,12 +329,46 @@ bool IsLoopFusibleAsProducer(const HloInstruction& instr) {\n          (IsUniversallyLoopFusible(instr) ||\n           (instr.opcode() == HloOpcode::kIota ||\n            instr.opcode() == HloOpcode::kConstant ||\n-           // Non-variadic elemental reductions can be fused as producers.\n-           (instr.opcode() == HloOpcode::kReduce &&\n-            !IsReductionFromOrToContiguousDimensions(instr) &&\n-            !instr.shape().IsTuple())));\n+           // Non-variadic reductions can be fused as producers.\n+           (instr.opcode() == HloOpcode::kReduce && !instr.shape().IsTuple())));\n }\n \n+static bool AllSatisfy(const HloInstruction& instr,\n+                       const HloPredicate& predicate) {\n+  if (instr.opcode() != HloOpcode::kFusion) {\n+    return predicate(&instr);\n+  }\n+\n+  return absl::c_all_of(\n+      instr.fused_instructions(), [&](const HloInstruction* i) {\n+        return i->opcode() == HloOpcode::kParameter || predicate(i);\n+      });\n+}\n+\n+namespace {\n+// Whether 'instr' is an intermediate node for reduction fusion.\n+bool IsReduceIntermediate(const HloInstruction* instr) {\n+  if (instr->operand_count() > 1 || instr->user_count() > 1) {\n+    return false;\n+  }\n+\n+  // Only support elementwise ops that don't introduce additional compute.\n+  // More benchmarking and better cost model are needed to enable this for\n+  // more compute ops.\n+  switch (instr->opcode()) {\n+    case HloOpcode::kBitcast:\n+    case HloOpcode::kBitcastConvert:\n+    case HloOpcode::kConvert:\n+      return true;\n+    case HloOpcode::kReshape:\n+      return ShapeUtil::ReshapeIsBitcast(instr->operand(0)->shape(),\n+                                         instr->shape());\n+    default:\n+      return false;\n+  }\n+}\n+}  // namespace\n+\n FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n                                          const HloInstruction& consumer) {\n   if (!IsLoopFusibleAsProducer(producer) &&\n"
                },
                {
                    "old_start": 343,
                    "old_length": 6,
                    "new_start": 377,
                    "new_length": 16,
                    "hunk": "@@ -343,6 +377,16 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n     return \"the producer is not loop-fusible\";\n   }\n \n+  if (IsReductionFromOrToContiguousDimensions(producer)) {\n+    if (!AllSatisfy(consumer, &IsReduceIntermediate)) {\n+      return \"Reductions from/to continuous dims epilogue not fusible\";\n+    }\n+\n+    if (producer.user_count() > 1) {\n+      return \"reduction output fusion only works for single user\";\n+    }\n+  }\n+\n   if (!IsInputFusible(consumer) && !IsLoopFusibleAsConsumer(consumer)) {\n     return \"the consumer is not input-fusible and not loop-fusible\";\n   }\n"
                },
                {
                    "old_start": 760,
                    "old_length": 24,
                    "new_start": 804,
                    "new_length": 67,
                    "hunk": "@@ -760,24 +804,67 @@ static void GetFusionRootsRec(HloInstruction* root,\n   }\n }\n \n-std::vector<HloInstruction*> GetFusionRoots(HloComputation* computation) {\n+std::vector<HloInstruction*> GetFusionRoots(const HloComputation& computation) {\n   std::vector<HloInstruction*> out;\n-  GetFusionRootsRec(computation->root_instruction(), out);\n+  GetFusionRootsRec(computation.root_instruction(), out);\n   return out;\n }\n \n-bool HasAnyTiledTransposeRoot(HloComputation* computation) {\n+bool HasAnyTiledTransposeRoot(const HloComputation& computation) {\n   return absl::c_any_of(GetFusionRoots(computation),\n                         [&](const HloInstruction* instr) {\n                           return FindAnyTiledTranspose(*instr);\n                         });\n }\n \n-bool HasAnyUnnestedReductionRoot(HloComputation* computation) {\n+bool HasAnyUnnestedReductionRoot(const HloComputation& computation) {\n   return absl::c_any_of(\n-      GetFusionRoots(computation), [&](const HloInstruction* instr) {\n-        return IsReductionFromOrToContiguousDimensions(*instr);\n-      });\n+      GetFusionRoots(computation),\n+      [&](const HloInstruction* instr) { return HasRealReductionHero(instr); });\n+}\n+\n+static const HloInstruction* FindNonTrivialReductionHero(\n+    const HloInstruction& instr) {\n+  const HloInstruction* idx = &instr;\n+  while (IsReduceIntermediate(idx) && idx->operand_count() == 1) {\n+    idx = idx->operand(0);\n+  }\n+  if (IsReductionFromOrToContiguousDimensions(*idx)) {\n+    return idx;\n+  }\n+  return nullptr;\n+}\n+\n+const HloInstruction* FindFirstRealReductionHero(const HloComputation& cmp) {\n+  std::vector<HloInstruction*> roots = GetFusionRoots(cmp);\n+  CHECK(!roots.empty());\n+  for (HloInstruction* r : roots) {\n+    const HloInstruction* hero = FindRealReductionHero(r);\n+    if (hero != nullptr) {\n+      return hero;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+const HloInstruction* FindRealReductionHero(const HloInstruction* hlo) {\n+  if (const HloInstruction* rh = FindNonTrivialReductionHero(*hlo)) {\n+    if (rh == hlo ||\n+        (rh->user_count() == 1 &&\n+         ReductionIsRaceFree(hlo->GetModule()->config(),\n+                             GetReductionKindAndContiguousComponents(*rh)))) {\n+      return rh;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+bool HasFirstRealReductionHero(const HloComputation& cmp) {\n+  return FindFirstRealReductionHero(cmp) != nullptr;\n+}\n+\n+bool HasRealReductionHero(const HloInstruction* hlo) {\n+  return FindRealReductionHero(hlo) != nullptr;\n }\n \n }  // namespace gpu\n"
                }
            ],
            "whole_deleted": "-         HasAnyUnnestedReductionRoot(instr.called_computations()[0]);\n-  return IsReduceInputFusion(instr) ||\n-         IsReductionFromOrToContiguousDimensions(instr);\n-         HasAnyTiledTransposeRoot(instr.called_computations()[0]);\n-    if (IsReductionFromOrToContiguousDimensions(*fused_expression_root) ||\n-  for (const auto* inst : fused_expression_root->operands()) {\n-    if (IsReductionFromOrToContiguousDimensions(*inst) ||\n-        FindAnyTiledTranspose(*inst)) {\n-  if (IsReductionFromOrToContiguousDimensions(*inst)) {\n-           // Non-variadic elemental reductions can be fused as producers.\n-           (instr.opcode() == HloOpcode::kReduce &&\n-            !IsReductionFromOrToContiguousDimensions(instr) &&\n-            !instr.shape().IsTuple())));\n-std::vector<HloInstruction*> GetFusionRoots(HloComputation* computation) {\n-  GetFusionRootsRec(computation->root_instruction(), out);\n-bool HasAnyTiledTransposeRoot(HloComputation* computation) {\n-bool HasAnyUnnestedReductionRoot(HloComputation* computation) {\n-      GetFusionRoots(computation), [&](const HloInstruction* instr) {\n-        return IsReductionFromOrToContiguousDimensions(*instr);\n-      });\n",
            "whole_added": "+#include \"absl/algorithm/container.h\"\n+#include \"tensorflow/compiler/xla/util.h\"\n+         HasAnyUnnestedReductionRoot(*instr.called_computations()[0]);\n+  return IsReduceInputFusion(instr) || HasRealReductionHero(&instr);\n+         HasAnyTiledTransposeRoot(*instr.called_computations()[0]);\n+    if (HasRealReductionHero(fused_expression_root) ||\n+  for (auto* inst : fused_expression_root->mutable_operands()) {\n+    if (HasRealReductionHero(inst) || FindAnyTiledTranspose(*inst)) {\n+  if (HasRealReductionHero(inst)) {\n+           // Non-variadic reductions can be fused as producers.\n+           (instr.opcode() == HloOpcode::kReduce && !instr.shape().IsTuple())));\n+static bool AllSatisfy(const HloInstruction& instr,\n+                       const HloPredicate& predicate) {\n+  if (instr.opcode() != HloOpcode::kFusion) {\n+    return predicate(&instr);\n+  }\n+\n+  return absl::c_all_of(\n+      instr.fused_instructions(), [&](const HloInstruction* i) {\n+        return i->opcode() == HloOpcode::kParameter || predicate(i);\n+      });\n+}\n+\n+namespace {\n+// Whether 'instr' is an intermediate node for reduction fusion.\n+bool IsReduceIntermediate(const HloInstruction* instr) {\n+  if (instr->operand_count() > 1 || instr->user_count() > 1) {\n+    return false;\n+  }\n+\n+  // Only support elementwise ops that don't introduce additional compute.\n+  // More benchmarking and better cost model are needed to enable this for\n+  // more compute ops.\n+  switch (instr->opcode()) {\n+    case HloOpcode::kBitcast:\n+    case HloOpcode::kBitcastConvert:\n+    case HloOpcode::kConvert:\n+      return true;\n+    case HloOpcode::kReshape:\n+      return ShapeUtil::ReshapeIsBitcast(instr->operand(0)->shape(),\n+                                         instr->shape());\n+    default:\n+      return false;\n+  }\n+}\n+}  // namespace\n+\n+  if (IsReductionFromOrToContiguousDimensions(producer)) {\n+    if (!AllSatisfy(consumer, &IsReduceIntermediate)) {\n+      return \"Reductions from/to continuous dims epilogue not fusible\";\n+    }\n+\n+    if (producer.user_count() > 1) {\n+      return \"reduction output fusion only works for single user\";\n+    }\n+  }\n+\n+std::vector<HloInstruction*> GetFusionRoots(const HloComputation& computation) {\n+  GetFusionRootsRec(computation.root_instruction(), out);\n+bool HasAnyTiledTransposeRoot(const HloComputation& computation) {\n+bool HasAnyUnnestedReductionRoot(const HloComputation& computation) {\n+      GetFusionRoots(computation),\n+      [&](const HloInstruction* instr) { return HasRealReductionHero(instr); });\n+}\n+\n+static const HloInstruction* FindNonTrivialReductionHero(\n+    const HloInstruction& instr) {\n+  const HloInstruction* idx = &instr;\n+  while (IsReduceIntermediate(idx) && idx->operand_count() == 1) {\n+    idx = idx->operand(0);\n+  }\n+  if (IsReductionFromOrToContiguousDimensions(*idx)) {\n+    return idx;\n+  }\n+  return nullptr;\n+}\n+\n+const HloInstruction* FindFirstRealReductionHero(const HloComputation& cmp) {\n+  std::vector<HloInstruction*> roots = GetFusionRoots(cmp);\n+  CHECK(!roots.empty());\n+  for (HloInstruction* r : roots) {\n+    const HloInstruction* hero = FindRealReductionHero(r);\n+    if (hero != nullptr) {\n+      return hero;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+const HloInstruction* FindRealReductionHero(const HloInstruction* hlo) {\n+  if (const HloInstruction* rh = FindNonTrivialReductionHero(*hlo)) {\n+    if (rh == hlo ||\n+        (rh->user_count() == 1 &&\n+         ReductionIsRaceFree(hlo->GetModule()->config(),\n+                             GetReductionKindAndContiguousComponents(*rh)))) {\n+      return rh;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+bool HasFirstRealReductionHero(const HloComputation& cmp) {\n+  return FindFirstRealReductionHero(cmp) != nullptr;\n+}\n+\n+bool HasRealReductionHero(const HloInstruction* hlo) {\n+  return FindRealReductionHero(hlo) != nullptr;\n",
            "whole_hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <stack>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_computation.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_opcode.h\"\n@@ -29,6 +30,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/service/instruction_fusion.h\"\n #include \"tensorflow/compiler/xla/shape.h\"\n #include \"tensorflow/compiler/xla/shape_util.h\"\n+#include \"tensorflow/compiler/xla/util.h\"\n \n namespace xla {\n namespace gpu {\n@@ -94,12 +96,11 @@ bool IsPhysicallyTransposing(const HloInstruction& instr) {\n \n bool IsReduceInputFusion(const HloInstruction& instr) {\n   return instr.opcode() == HloOpcode::kFusion &&\n-         HasAnyUnnestedReductionRoot(instr.called_computations()[0]);\n+         HasAnyUnnestedReductionRoot(*instr.called_computations()[0]);\n }\n \n bool IsInputFusibleReduction(const HloInstruction& instr) {\n-  return IsReduceInputFusion(instr) ||\n-         IsReductionFromOrToContiguousDimensions(instr);\n+  return IsReduceInputFusion(instr) || HasRealReductionHero(&instr);\n }\n \n bool IsNestableVariadicReduction(const HloInstruction& instr) {\n@@ -116,7 +117,7 @@ bool IsTransposeInputFusion(const HloInstruction& instr) {\n     return false;\n   }\n   return instr.opcode() == HloOpcode::kFusion &&\n-         HasAnyTiledTransposeRoot(instr.called_computations()[0]);\n+         HasAnyTiledTransposeRoot(*instr.called_computations()[0]);\n }\n \n bool IsInputFusibleTranspose(const HloInstruction& instr) {\n@@ -130,7 +131,7 @@ const HloInstruction* GetRealHeroForMultiOutputFusion(\n   }\n   auto fused_expression_root = instr.fused_expression_root();\n   if (!instr.IsMultiOutputFusion()) {\n-    if (IsReductionFromOrToContiguousDimensions(*fused_expression_root) ||\n+    if (HasRealReductionHero(fused_expression_root) ||\n         FindAnyTiledTranspose(*fused_expression_root)) {\n       return &FindNonTrivialHero(*fused_expression_root);\n     }\n@@ -140,9 +141,8 @@ const HloInstruction* GetRealHeroForMultiOutputFusion(\n   // operand of the fusion root or a tiled transpose, because they have the most\n   // constraints. Note that we cannot have both kinds at the same time, so once\n   // we find any, we can immediately return it.\n-  for (const auto* inst : fused_expression_root->operands()) {\n-    if (IsReductionFromOrToContiguousDimensions(*inst) ||\n-        FindAnyTiledTranspose(*inst)) {\n+  for (auto* inst : fused_expression_root->mutable_operands()) {\n+    if (HasRealReductionHero(inst) || FindAnyTiledTranspose(*inst)) {\n       return &FindNonTrivialHero(*inst);\n     }\n   }\n@@ -153,7 +153,7 @@ const HloInstruction* GetRealHeroForMultiOutputFusion(\n // `first_reduce`.\n static bool IsFusedReductionOutputConsistent(\n     const HloInstruction* inst, const HloInstruction* first_reduce) {\n-  if (IsReductionFromOrToContiguousDimensions(*inst)) {\n+  if (HasRealReductionHero(inst)) {\n     // Shapes, layouts and dimensions must be the same for all reduces\n     // inside of this fusion.\n     return ShapeUtil::EqualIgnoringElementType(first_reduce->shape(),\n@@ -329,12 +329,46 @@ bool IsLoopFusibleAsProducer(const HloInstruction& instr) {\n          (IsUniversallyLoopFusible(instr) ||\n           (instr.opcode() == HloOpcode::kIota ||\n            instr.opcode() == HloOpcode::kConstant ||\n-           // Non-variadic elemental reductions can be fused as producers.\n-           (instr.opcode() == HloOpcode::kReduce &&\n-            !IsReductionFromOrToContiguousDimensions(instr) &&\n-            !instr.shape().IsTuple())));\n+           // Non-variadic reductions can be fused as producers.\n+           (instr.opcode() == HloOpcode::kReduce && !instr.shape().IsTuple())));\n }\n \n+static bool AllSatisfy(const HloInstruction& instr,\n+                       const HloPredicate& predicate) {\n+  if (instr.opcode() != HloOpcode::kFusion) {\n+    return predicate(&instr);\n+  }\n+\n+  return absl::c_all_of(\n+      instr.fused_instructions(), [&](const HloInstruction* i) {\n+        return i->opcode() == HloOpcode::kParameter || predicate(i);\n+      });\n+}\n+\n+namespace {\n+// Whether 'instr' is an intermediate node for reduction fusion.\n+bool IsReduceIntermediate(const HloInstruction* instr) {\n+  if (instr->operand_count() > 1 || instr->user_count() > 1) {\n+    return false;\n+  }\n+\n+  // Only support elementwise ops that don't introduce additional compute.\n+  // More benchmarking and better cost model are needed to enable this for\n+  // more compute ops.\n+  switch (instr->opcode()) {\n+    case HloOpcode::kBitcast:\n+    case HloOpcode::kBitcastConvert:\n+    case HloOpcode::kConvert:\n+      return true;\n+    case HloOpcode::kReshape:\n+      return ShapeUtil::ReshapeIsBitcast(instr->operand(0)->shape(),\n+                                         instr->shape());\n+    default:\n+      return false;\n+  }\n+}\n+}  // namespace\n+\n FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n                                          const HloInstruction& consumer) {\n   if (!IsLoopFusibleAsProducer(producer) &&\n@@ -343,6 +377,16 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n     return \"the producer is not loop-fusible\";\n   }\n \n+  if (IsReductionFromOrToContiguousDimensions(producer)) {\n+    if (!AllSatisfy(consumer, &IsReduceIntermediate)) {\n+      return \"Reductions from/to continuous dims epilogue not fusible\";\n+    }\n+\n+    if (producer.user_count() > 1) {\n+      return \"reduction output fusion only works for single user\";\n+    }\n+  }\n+\n   if (!IsInputFusible(consumer) && !IsLoopFusibleAsConsumer(consumer)) {\n     return \"the consumer is not input-fusible and not loop-fusible\";\n   }\n@@ -760,24 +804,67 @@ static void GetFusionRootsRec(HloInstruction* root,\n   }\n }\n \n-std::vector<HloInstruction*> GetFusionRoots(HloComputation* computation) {\n+std::vector<HloInstruction*> GetFusionRoots(const HloComputation& computation) {\n   std::vector<HloInstruction*> out;\n-  GetFusionRootsRec(computation->root_instruction(), out);\n+  GetFusionRootsRec(computation.root_instruction(), out);\n   return out;\n }\n \n-bool HasAnyTiledTransposeRoot(HloComputation* computation) {\n+bool HasAnyTiledTransposeRoot(const HloComputation& computation) {\n   return absl::c_any_of(GetFusionRoots(computation),\n                         [&](const HloInstruction* instr) {\n                           return FindAnyTiledTranspose(*instr);\n                         });\n }\n \n-bool HasAnyUnnestedReductionRoot(HloComputation* computation) {\n+bool HasAnyUnnestedReductionRoot(const HloComputation& computation) {\n   return absl::c_any_of(\n-      GetFusionRoots(computation), [&](const HloInstruction* instr) {\n-        return IsReductionFromOrToContiguousDimensions(*instr);\n-      });\n+      GetFusionRoots(computation),\n+      [&](const HloInstruction* instr) { return HasRealReductionHero(instr); });\n+}\n+\n+static const HloInstruction* FindNonTrivialReductionHero(\n+    const HloInstruction& instr) {\n+  const HloInstruction* idx = &instr;\n+  while (IsReduceIntermediate(idx) && idx->operand_count() == 1) {\n+    idx = idx->operand(0);\n+  }\n+  if (IsReductionFromOrToContiguousDimensions(*idx)) {\n+    return idx;\n+  }\n+  return nullptr;\n+}\n+\n+const HloInstruction* FindFirstRealReductionHero(const HloComputation& cmp) {\n+  std::vector<HloInstruction*> roots = GetFusionRoots(cmp);\n+  CHECK(!roots.empty());\n+  for (HloInstruction* r : roots) {\n+    const HloInstruction* hero = FindRealReductionHero(r);\n+    if (hero != nullptr) {\n+      return hero;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+const HloInstruction* FindRealReductionHero(const HloInstruction* hlo) {\n+  if (const HloInstruction* rh = FindNonTrivialReductionHero(*hlo)) {\n+    if (rh == hlo ||\n+        (rh->user_count() == 1 &&\n+         ReductionIsRaceFree(hlo->GetModule()->config(),\n+                             GetReductionKindAndContiguousComponents(*rh)))) {\n+      return rh;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+bool HasFirstRealReductionHero(const HloComputation& cmp) {\n+  return FindFirstRealReductionHero(cmp) != nullptr;\n+}\n+\n+bool HasRealReductionHero(const HloInstruction* hlo) {\n+  return FindRealReductionHero(hlo) != nullptr;\n }\n \n }  // namespace gpu\n"
        },
        {
            "name": "gpu_fusible.h",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible.h",
            "patches": [
                {
                    "old_start": 178,
                    "old_length": 14,
                    "new_start": 178,
                    "new_length": 24,
                    "hunk": "@@ -178,14 +178,24 @@ size_t GetOutputSizeOfFusible(const HloInstruction& instr);\n //\n // For input: R1\n // Expected output: [R1]\n-std::vector<HloInstruction*> GetFusionRoots(HloComputation* computation);\n+std::vector<HloInstruction*> GetFusionRoots(const HloComputation& computation);\n \n // Whether there is a fusion root triggering transposition emitter.\n-bool HasAnyTiledTransposeRoot(HloComputation* computation);\n+bool HasAnyTiledTransposeRoot(const HloComputation& computation);\n \n // Returns whether the computation has at least one root triggering unnested\n // reduction emitter.\n-bool HasAnyUnnestedReductionRoot(HloComputation* computation);\n+bool HasAnyUnnestedReductionRoot(const HloComputation& computation);\n+\n+// Finds the first real reduction hero for the fusion.\n+const HloInstruction* FindFirstRealReductionHero(const HloComputation& cmp);\n+// Find the real reduction hero for the given instruction in a fusion.\n+const HloInstruction* FindRealReductionHero(const HloInstruction* hlo);\n+\n+// Whether there exists a real reduction hero for the computation.\n+bool HasFirstRealReductionHero(const HloComputation& cmp);\n+// Whether there exists a real reduction hero for the instruction.\n+bool HasRealReductionHero(const HloInstruction* hlo);\n \n }  // namespace gpu\n }  // namespace xla\n"
                }
            ],
            "whole_deleted": "-std::vector<HloInstruction*> GetFusionRoots(HloComputation* computation);\n-bool HasAnyTiledTransposeRoot(HloComputation* computation);\n-bool HasAnyUnnestedReductionRoot(HloComputation* computation);\n",
            "whole_added": "+std::vector<HloInstruction*> GetFusionRoots(const HloComputation& computation);\n+bool HasAnyTiledTransposeRoot(const HloComputation& computation);\n+bool HasAnyUnnestedReductionRoot(const HloComputation& computation);\n+\n+// Finds the first real reduction hero for the fusion.\n+const HloInstruction* FindFirstRealReductionHero(const HloComputation& cmp);\n+// Find the real reduction hero for the given instruction in a fusion.\n+const HloInstruction* FindRealReductionHero(const HloInstruction* hlo);\n+\n+// Whether there exists a real reduction hero for the computation.\n+bool HasFirstRealReductionHero(const HloComputation& cmp);\n+// Whether there exists a real reduction hero for the instruction.\n+bool HasRealReductionHero(const HloInstruction* hlo);\n",
            "whole_hunk": "@@ -178,14 +178,24 @@ size_t GetOutputSizeOfFusible(const HloInstruction& instr);\n //\n // For input: R1\n // Expected output: [R1]\n-std::vector<HloInstruction*> GetFusionRoots(HloComputation* computation);\n+std::vector<HloInstruction*> GetFusionRoots(const HloComputation& computation);\n \n // Whether there is a fusion root triggering transposition emitter.\n-bool HasAnyTiledTransposeRoot(HloComputation* computation);\n+bool HasAnyTiledTransposeRoot(const HloComputation& computation);\n \n // Returns whether the computation has at least one root triggering unnested\n // reduction emitter.\n-bool HasAnyUnnestedReductionRoot(HloComputation* computation);\n+bool HasAnyUnnestedReductionRoot(const HloComputation& computation);\n+\n+// Finds the first real reduction hero for the fusion.\n+const HloInstruction* FindFirstRealReductionHero(const HloComputation& cmp);\n+// Find the real reduction hero for the given instruction in a fusion.\n+const HloInstruction* FindRealReductionHero(const HloInstruction* hlo);\n+\n+// Whether there exists a real reduction hero for the computation.\n+bool HasFirstRealReductionHero(const HloComputation& cmp);\n+// Whether there exists a real reduction hero for the instruction.\n+bool HasRealReductionHero(const HloInstruction* hlo);\n \n }  // namespace gpu\n }  // namespace xla\n"
        },
        {
            "name": "hlo_fusion_analysis.cc",
            "path": "tensorflow/compiler/xla/service/gpu/hlo_fusion_analysis.cc",
            "patches": [
                {
                    "old_start": 272,
                    "old_length": 7,
                    "new_start": 272,
                    "new_length": 7,
                    "hunk": "@@ -272,7 +272,7 @@ StatusOr<HloFusionAnalysis> HloFusionAnalysis::Create(\n   TF_ASSIGN_OR_RETURN(auto backend_config,\n                       fusion->backend_config<FusionBackendConfig>());\n \n-  auto hlo_roots = GetFusionRoots(fusion->fused_instructions_computation());\n+  auto hlo_roots = GetFusionRoots(*fusion->fused_instructions_computation());\n   std::optional<TransposeDescription> tiled_transpose_hero =\n       FindConsistentTransposeHero(hlo_roots);\n \n"
                },
                {
                    "old_start": 296,
                    "old_length": 9,
                    "new_start": 296,
                    "new_length": 10,
                    "hunk": "@@ -296,9 +296,10 @@ HloFusionAnalysis::EmitterFusionKind HloFusionAnalysis::GetEmitterFusionKind()\n #endif\n \n   HloComputation* fused_computation = fusion_->fused_instructions_computation();\n-  if (HasAnyUnnestedReductionRoot(fused_computation)) {\n+  if (HasFirstRealReductionHero(*fused_computation)) {\n     return EmitterFusionKind::kReduction;\n   }\n+\n   // We expect that the last dimension is swapped with a different dimension.\n   if (HasConsistentTransposeHeros() && tiled_transpose_->permutation[2] != 2) {\n     return EmitterFusionKind::kTranspose;\n"
                },
                {
                    "old_start": 389,
                    "old_length": 14,
                    "new_start": 390,
                    "new_length": 10,
                    "hunk": "@@ -389,14 +390,10 @@ namespace {\n // We always use the first reduce root that triggers unnested reduction emitter\n // as the hero reduction, since all the reductions are required to have the same\n // shape and layout as verified by `IsFusedReductionOutputConsistent()`.\n-HloInstruction* FindHeroReduction(const std::vector<HloInstruction*>& roots) {\n-  auto it = absl::c_find_if(roots, [](HloInstruction* instr) {\n-    return IsReductionFromOrToContiguousDimensions(*instr);\n-  });\n-  if (it == roots.end()) {\n-    return nullptr;\n-  }\n-  return *it;\n+const HloInstruction* FindHeroReduction(const HloComputation& computation) {\n+  const HloInstruction* first_reduce = FindFirstRealReductionHero(computation);\n+  CHECK_NE(first_reduce, nullptr);\n+  return first_reduce;\n }\n }  // namespace\n \n"
                },
                {
                    "old_start": 405,
                    "old_length": 8,
                    "new_start": 402,
                    "new_length": 8,
                    "hunk": "@@ -405,8 +402,8 @@ const ReductionCodegenInfo* HloFusionAnalysis::GetReductionCodegenInfo() {\n     return &reduction_codegen_info_.value();\n   }\n \n-  HloInstruction* hero_reduction = FindHeroReduction(fusion_roots());\n-  CHECK_NE(hero_reduction, nullptr);\n+  const HloInstruction* hero_reduction =\n+      FindHeroReduction(*fused_computation());\n \n   auto reduction_codegen_info = ComputeReductionCodegenInfo(hero_reduction);\n   reduction_codegen_info_.emplace(std::move(reduction_codegen_info));\n"
                },
                {
                    "old_start": 580,
                    "old_length": 7,
                    "new_start": 577,
                    "new_length": 7,
                    "hunk": "@@ -580,7 +577,7 @@ HloFusionAnalysis::GroupDisjointReductions() const {\n \n   for (HloInstruction* root : fusion_roots()) {\n     disjoint_sets[root].Get() = root;\n-    if (!IsReductionFromOrToContiguousDimensions(*root)) {\n+    if (!HasRealReductionHero(root)) {\n       if (!first_non_reduction_root) {\n         first_non_reduction_root = root;\n       } else {\n"
                },
                {
                    "old_start": 595,
                    "old_length": 8,
                    "new_start": 592,
                    "new_length": 8,
                    "hunk": "@@ -595,8 +592,8 @@ HloFusionAnalysis::GroupDisjointReductions() const {\n     std::vector<HloInstruction*> reached_output_ids;\n     bool added_to_reduce = false;\n     for (HloInstruction* output : fusion_roots()) {\n-      if (IsReductionFromOrToContiguousDimensions(*output) &&\n-          (hlo_query::IsBroadcastedConstantOrScalar(*instr))) {\n+      bool has_real_hero = HasRealReductionHero(output);\n+      if (has_real_hero && (hlo_query::IsBroadcastedConstantOrScalar(*instr))) {\n         if (added_to_reduce) {\n           // Do not group more than one output reduce instructions through\n           // broadcasted constants or scalars, as the recomputation should be\n"
                },
                {
                    "old_start": 611,
                    "old_length": 7,
                    "new_start": 608,
                    "new_length": 7,
                    "hunk": "@@ -611,7 +608,7 @@ HloFusionAnalysis::GroupDisjointReductions() const {\n         VLOG(3) << \"Reaching \" << output->ToString() << \" from \"\n                 << instr->ToString();\n         reached_output_ids.push_back(output);\n-        if (IsReductionFromOrToContiguousDimensions(*output)) {\n+        if (has_real_hero) {\n           added_to_reduce = true;\n         }\n       }\n"
                },
                {
                    "old_start": 730,
                    "old_length": 7,
                    "new_start": 727,
                    "new_length": 7,
                    "hunk": "@@ -730,7 +727,7 @@ int HloFusionAnalysis::CalculateVirtualThreadScalingFactorForReduction(\n }\n \n ReductionCodegenInfo HloFusionAnalysis::ComputeReductionCodegenInfo(\n-    HloInstruction* hero_reduction) const {\n+    const HloInstruction* hero_reduction) const {\n   Shape input_shape = hero_reduction->operand(0)->shape();\n   ReductionDimensions reduction_dimensions =\n       GetReductionKindAndContiguousComponents(*hero_reduction);\n"
                }
            ],
            "whole_deleted": "-  auto hlo_roots = GetFusionRoots(fusion->fused_instructions_computation());\n-  if (HasAnyUnnestedReductionRoot(fused_computation)) {\n-HloInstruction* FindHeroReduction(const std::vector<HloInstruction*>& roots) {\n-  auto it = absl::c_find_if(roots, [](HloInstruction* instr) {\n-    return IsReductionFromOrToContiguousDimensions(*instr);\n-  });\n-  if (it == roots.end()) {\n-    return nullptr;\n-  }\n-  return *it;\n-  HloInstruction* hero_reduction = FindHeroReduction(fusion_roots());\n-  CHECK_NE(hero_reduction, nullptr);\n-    if (!IsReductionFromOrToContiguousDimensions(*root)) {\n-      if (IsReductionFromOrToContiguousDimensions(*output) &&\n-          (hlo_query::IsBroadcastedConstantOrScalar(*instr))) {\n-        if (IsReductionFromOrToContiguousDimensions(*output)) {\n-    HloInstruction* hero_reduction) const {\n",
            "whole_added": "+  auto hlo_roots = GetFusionRoots(*fusion->fused_instructions_computation());\n+  if (HasFirstRealReductionHero(*fused_computation)) {\n+\n+const HloInstruction* FindHeroReduction(const HloComputation& computation) {\n+  const HloInstruction* first_reduce = FindFirstRealReductionHero(computation);\n+  CHECK_NE(first_reduce, nullptr);\n+  return first_reduce;\n+  const HloInstruction* hero_reduction =\n+      FindHeroReduction(*fused_computation());\n+    if (!HasRealReductionHero(root)) {\n+      bool has_real_hero = HasRealReductionHero(output);\n+      if (has_real_hero && (hlo_query::IsBroadcastedConstantOrScalar(*instr))) {\n+        if (has_real_hero) {\n+    const HloInstruction* hero_reduction) const {\n",
            "whole_hunk": "@@ -272,7 +272,7 @@ StatusOr<HloFusionAnalysis> HloFusionAnalysis::Create(\n   TF_ASSIGN_OR_RETURN(auto backend_config,\n                       fusion->backend_config<FusionBackendConfig>());\n \n-  auto hlo_roots = GetFusionRoots(fusion->fused_instructions_computation());\n+  auto hlo_roots = GetFusionRoots(*fusion->fused_instructions_computation());\n   std::optional<TransposeDescription> tiled_transpose_hero =\n       FindConsistentTransposeHero(hlo_roots);\n \n@@ -296,9 +296,10 @@ HloFusionAnalysis::EmitterFusionKind HloFusionAnalysis::GetEmitterFusionKind()\n #endif\n \n   HloComputation* fused_computation = fusion_->fused_instructions_computation();\n-  if (HasAnyUnnestedReductionRoot(fused_computation)) {\n+  if (HasFirstRealReductionHero(*fused_computation)) {\n     return EmitterFusionKind::kReduction;\n   }\n+\n   // We expect that the last dimension is swapped with a different dimension.\n   if (HasConsistentTransposeHeros() && tiled_transpose_->permutation[2] != 2) {\n     return EmitterFusionKind::kTranspose;\n@@ -389,14 +390,10 @@ namespace {\n // We always use the first reduce root that triggers unnested reduction emitter\n // as the hero reduction, since all the reductions are required to have the same\n // shape and layout as verified by `IsFusedReductionOutputConsistent()`.\n-HloInstruction* FindHeroReduction(const std::vector<HloInstruction*>& roots) {\n-  auto it = absl::c_find_if(roots, [](HloInstruction* instr) {\n-    return IsReductionFromOrToContiguousDimensions(*instr);\n-  });\n-  if (it == roots.end()) {\n-    return nullptr;\n-  }\n-  return *it;\n+const HloInstruction* FindHeroReduction(const HloComputation& computation) {\n+  const HloInstruction* first_reduce = FindFirstRealReductionHero(computation);\n+  CHECK_NE(first_reduce, nullptr);\n+  return first_reduce;\n }\n }  // namespace\n \n@@ -405,8 +402,8 @@ const ReductionCodegenInfo* HloFusionAnalysis::GetReductionCodegenInfo() {\n     return &reduction_codegen_info_.value();\n   }\n \n-  HloInstruction* hero_reduction = FindHeroReduction(fusion_roots());\n-  CHECK_NE(hero_reduction, nullptr);\n+  const HloInstruction* hero_reduction =\n+      FindHeroReduction(*fused_computation());\n \n   auto reduction_codegen_info = ComputeReductionCodegenInfo(hero_reduction);\n   reduction_codegen_info_.emplace(std::move(reduction_codegen_info));\n@@ -580,7 +577,7 @@ HloFusionAnalysis::GroupDisjointReductions() const {\n \n   for (HloInstruction* root : fusion_roots()) {\n     disjoint_sets[root].Get() = root;\n-    if (!IsReductionFromOrToContiguousDimensions(*root)) {\n+    if (!HasRealReductionHero(root)) {\n       if (!first_non_reduction_root) {\n         first_non_reduction_root = root;\n       } else {\n@@ -595,8 +592,8 @@ HloFusionAnalysis::GroupDisjointReductions() const {\n     std::vector<HloInstruction*> reached_output_ids;\n     bool added_to_reduce = false;\n     for (HloInstruction* output : fusion_roots()) {\n-      if (IsReductionFromOrToContiguousDimensions(*output) &&\n-          (hlo_query::IsBroadcastedConstantOrScalar(*instr))) {\n+      bool has_real_hero = HasRealReductionHero(output);\n+      if (has_real_hero && (hlo_query::IsBroadcastedConstantOrScalar(*instr))) {\n         if (added_to_reduce) {\n           // Do not group more than one output reduce instructions through\n           // broadcasted constants or scalars, as the recomputation should be\n@@ -611,7 +608,7 @@ HloFusionAnalysis::GroupDisjointReductions() const {\n         VLOG(3) << \"Reaching \" << output->ToString() << \" from \"\n                 << instr->ToString();\n         reached_output_ids.push_back(output);\n-        if (IsReductionFromOrToContiguousDimensions(*output)) {\n+        if (has_real_hero) {\n           added_to_reduce = true;\n         }\n       }\n@@ -730,7 +727,7 @@ int HloFusionAnalysis::CalculateVirtualThreadScalingFactorForReduction(\n }\n \n ReductionCodegenInfo HloFusionAnalysis::ComputeReductionCodegenInfo(\n-    HloInstruction* hero_reduction) const {\n+    const HloInstruction* hero_reduction) const {\n   Shape input_shape = hero_reduction->operand(0)->shape();\n   ReductionDimensions reduction_dimensions =\n       GetReductionKindAndContiguousComponents(*hero_reduction);\n"
        },
        {
            "name": "hlo_fusion_analysis.h",
            "path": "tensorflow/compiler/xla/service/gpu/hlo_fusion_analysis.h",
            "patches": [
                {
                    "old_start": 105,
                    "old_length": 7,
                    "new_start": 105,
                    "new_length": 7,
                    "hunk": "@@ -105,7 +105,7 @@ class HloFusionAnalysis {\n   int CalculateVirtualThreadScalingFactorForReduction(\n       const ReductionDimensions& reduction_dimensions) const;\n   ReductionCodegenInfo ComputeReductionCodegenInfo(\n-      HloInstruction* hero_reduction) const;\n+      const HloInstruction* hero_reduction) const;\n   bool HasConsistentTransposeHeros() const;\n \n   const HloFusionInstruction* fusion_;\n"
                }
            ],
            "whole_deleted": "-      HloInstruction* hero_reduction) const;\n",
            "whole_added": "+      const HloInstruction* hero_reduction) const;\n",
            "whole_hunk": "@@ -105,7 +105,7 @@ class HloFusionAnalysis {\n   int CalculateVirtualThreadScalingFactorForReduction(\n       const ReductionDimensions& reduction_dimensions) const;\n   ReductionCodegenInfo ComputeReductionCodegenInfo(\n-      HloInstruction* hero_reduction) const;\n+      const HloInstruction* hero_reduction) const;\n   bool HasConsistentTransposeHeros() const;\n \n   const HloFusionInstruction* fusion_;\n"
        },
        {
            "name": "instruction_fusion_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/instruction_fusion_test.cc",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 6,
                    "new_start": 17,
                    "new_length": 7,
                    "hunk": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <memory>\n \n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/hlo/utils/hlo_matchers.h\"\n #include \"tensorflow/compiler/xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"tensorflow/compiler/xla/service/gpu/gpu_fusible.h\"\n"
                },
                {
                    "old_start": 782,
                    "old_length": 5,
                    "new_start": 783,
                    "new_length": 53,
                    "hunk": "@@ -782,5 +783,53 @@ TEST_F(InstructionFusionTest, IotaIntoVariadicReduction) {\n       op::Reduce(op::Parameter(), op::Iota(), op::Constant(), op::Constant()));\n }\n \n+TEST_F(InstructionFusionTest, InputReductionFusion) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule test_module\n+    add.clone.13 {\n+      x.27 = f32[] parameter(0)\n+      y.27 = f32[] parameter(1)\n+      ROOT add.1036 = f32[] add(x.27, y.27)\n+    }\n+    add.clone.14 {\n+      x.28 = f32[] parameter(0)\n+      y.28 = f32[] parameter(1)\n+      ROOT add.1037 = f32[] add(x.28, y.28)\n+    }\n+    add {\n+      x = bf16[] parameter(0)\n+      convert.448 = f32[] convert(x)\n+      y = bf16[] parameter(1)\n+      convert.449 = f32[] convert(y)\n+      add.597 = f32[] add(convert.448, convert.449)\n+      ROOT convert.450 = bf16[] convert(add.597)\n+    }\n+    ENTRY FuseSmallReduction {\n+      param_2.7 = bf16[8,16,64,2048]{3,2,1,0} parameter(2)\n+      convert.1395 = f32[8,16,64,2048]{3,2,1,0} convert(param_2.7)\n+      param_0.85 = bf16[8,16,64,2048]{3,2,1,0} parameter(0)\n+      convert.1393 = f32[8,16,64,2048]{3,2,1,0} convert(param_0.85)\n+      multiply.1652 = f32[8,16,64,2048]{3,2,1,0} multiply(convert.1395, convert.1393)\n+      convert.1392 = bf16[8,16,64,2048]{3,2,1,0} convert(multiply.1652)\n+      bitcast.15934 = bf16[128,64,2048]{2,1,0} bitcast(convert.1392)\n+      convert.1391 = f32[128,64,2048]{2,1,0} convert(bitcast.15934)\n+      param_1.15 = bf16[] parameter(1)\n+      convert.1394 = f32[] convert(param_1.15)\n+      reduce.462 = f32[128,64]{1,0} reduce(convert.1391, convert.1394), dimensions={2}, to_apply=add.clone.13\n+      reduce.121 = f32[64]{0} reduce(reduce.462, convert.1394), dimensions={0}, to_apply=add.clone.14\n+      ROOT convert.890 = bf16[64]{0} convert(reduce.121)\n+    })\")\n+                    .value();\n+\n+  EXPECT_TRUE(duplicating_instruction_fusion_.Run(module.get()).value());\n+\n+  HloInstruction* fused_convert_fusion =\n+      module->entry_computation()->root_instruction();\n+\n+  ASSERT_THAT(fused_convert_fusion, op::Fusion());\n+  SCOPED_TRACE(module->ToString());\n+  EXPECT_EQ(fused_convert_fusion->fusion_kind(),\n+            HloInstruction::FusionKind::kInput);\n+}\n }  // namespace gpu\n }  // namespace xla\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n+TEST_F(InstructionFusionTest, InputReductionFusion) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule test_module\n+    add.clone.13 {\n+      x.27 = f32[] parameter(0)\n+      y.27 = f32[] parameter(1)\n+      ROOT add.1036 = f32[] add(x.27, y.27)\n+    }\n+    add.clone.14 {\n+      x.28 = f32[] parameter(0)\n+      y.28 = f32[] parameter(1)\n+      ROOT add.1037 = f32[] add(x.28, y.28)\n+    }\n+    add {\n+      x = bf16[] parameter(0)\n+      convert.448 = f32[] convert(x)\n+      y = bf16[] parameter(1)\n+      convert.449 = f32[] convert(y)\n+      add.597 = f32[] add(convert.448, convert.449)\n+      ROOT convert.450 = bf16[] convert(add.597)\n+    }\n+    ENTRY FuseSmallReduction {\n+      param_2.7 = bf16[8,16,64,2048]{3,2,1,0} parameter(2)\n+      convert.1395 = f32[8,16,64,2048]{3,2,1,0} convert(param_2.7)\n+      param_0.85 = bf16[8,16,64,2048]{3,2,1,0} parameter(0)\n+      convert.1393 = f32[8,16,64,2048]{3,2,1,0} convert(param_0.85)\n+      multiply.1652 = f32[8,16,64,2048]{3,2,1,0} multiply(convert.1395, convert.1393)\n+      convert.1392 = bf16[8,16,64,2048]{3,2,1,0} convert(multiply.1652)\n+      bitcast.15934 = bf16[128,64,2048]{2,1,0} bitcast(convert.1392)\n+      convert.1391 = f32[128,64,2048]{2,1,0} convert(bitcast.15934)\n+      param_1.15 = bf16[] parameter(1)\n+      convert.1394 = f32[] convert(param_1.15)\n+      reduce.462 = f32[128,64]{1,0} reduce(convert.1391, convert.1394), dimensions={2}, to_apply=add.clone.13\n+      reduce.121 = f32[64]{0} reduce(reduce.462, convert.1394), dimensions={0}, to_apply=add.clone.14\n+      ROOT convert.890 = bf16[64]{0} convert(reduce.121)\n+    })\")\n+                    .value();\n+\n+  EXPECT_TRUE(duplicating_instruction_fusion_.Run(module.get()).value());\n+\n+  HloInstruction* fused_convert_fusion =\n+      module->entry_computation()->root_instruction();\n+\n+  ASSERT_THAT(fused_convert_fusion, op::Fusion());\n+  SCOPED_TRACE(module->ToString());\n+  EXPECT_EQ(fused_convert_fusion->fusion_kind(),\n+            HloInstruction::FusionKind::kInput);\n+}\n",
            "whole_hunk": "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include <memory>\n \n+#include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/hlo/utils/hlo_matchers.h\"\n #include \"tensorflow/compiler/xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"tensorflow/compiler/xla/service/gpu/gpu_fusible.h\"\n@@ -782,5 +783,53 @@ TEST_F(InstructionFusionTest, IotaIntoVariadicReduction) {\n       op::Reduce(op::Parameter(), op::Iota(), op::Constant(), op::Constant()));\n }\n \n+TEST_F(InstructionFusionTest, InputReductionFusion) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule test_module\n+    add.clone.13 {\n+      x.27 = f32[] parameter(0)\n+      y.27 = f32[] parameter(1)\n+      ROOT add.1036 = f32[] add(x.27, y.27)\n+    }\n+    add.clone.14 {\n+      x.28 = f32[] parameter(0)\n+      y.28 = f32[] parameter(1)\n+      ROOT add.1037 = f32[] add(x.28, y.28)\n+    }\n+    add {\n+      x = bf16[] parameter(0)\n+      convert.448 = f32[] convert(x)\n+      y = bf16[] parameter(1)\n+      convert.449 = f32[] convert(y)\n+      add.597 = f32[] add(convert.448, convert.449)\n+      ROOT convert.450 = bf16[] convert(add.597)\n+    }\n+    ENTRY FuseSmallReduction {\n+      param_2.7 = bf16[8,16,64,2048]{3,2,1,0} parameter(2)\n+      convert.1395 = f32[8,16,64,2048]{3,2,1,0} convert(param_2.7)\n+      param_0.85 = bf16[8,16,64,2048]{3,2,1,0} parameter(0)\n+      convert.1393 = f32[8,16,64,2048]{3,2,1,0} convert(param_0.85)\n+      multiply.1652 = f32[8,16,64,2048]{3,2,1,0} multiply(convert.1395, convert.1393)\n+      convert.1392 = bf16[8,16,64,2048]{3,2,1,0} convert(multiply.1652)\n+      bitcast.15934 = bf16[128,64,2048]{2,1,0} bitcast(convert.1392)\n+      convert.1391 = f32[128,64,2048]{2,1,0} convert(bitcast.15934)\n+      param_1.15 = bf16[] parameter(1)\n+      convert.1394 = f32[] convert(param_1.15)\n+      reduce.462 = f32[128,64]{1,0} reduce(convert.1391, convert.1394), dimensions={2}, to_apply=add.clone.13\n+      reduce.121 = f32[64]{0} reduce(reduce.462, convert.1394), dimensions={0}, to_apply=add.clone.14\n+      ROOT convert.890 = bf16[64]{0} convert(reduce.121)\n+    })\")\n+                    .value();\n+\n+  EXPECT_TRUE(duplicating_instruction_fusion_.Run(module.get()).value());\n+\n+  HloInstruction* fused_convert_fusion =\n+      module->entry_computation()->root_instruction();\n+\n+  ASSERT_THAT(fused_convert_fusion, op::Fusion());\n+  SCOPED_TRACE(module->ToString());\n+  EXPECT_EQ(fused_convert_fusion->fusion_kind(),\n+            HloInstruction::FusionKind::kInput);\n+}\n }  // namespace gpu\n }  // namespace xla\n"
        },
        {
            "name": "kernel_mapping_scheme.h",
            "path": "tensorflow/compiler/xla/service/gpu/kernel_mapping_scheme.h",
            "patches": [
                {
                    "old_start": 173,
                    "old_length": 7,
                    "new_start": 173,
                    "new_length": 7,
                    "hunk": "@@ -173,7 +173,7 @@ class ReductionCodegenInfo {\n   explicit ReductionCodegenInfo(TilingScheme mapping_scheme,\n                                 int num_partial_results, bool is_row_reduction,\n                                 bool is_race_free, IndexGroups index_groups,\n-                                HloInstruction* first_reduce)\n+                                const HloInstruction* first_reduce)\n       : tiling_scheme_(mapping_scheme),\n         num_partial_results_(num_partial_results),\n         is_row_reduction_(is_row_reduction),\n"
                },
                {
                    "old_start": 203,
                    "old_length": 7,
                    "new_start": 203,
                    "new_length": 7,
                    "hunk": "@@ -203,7 +203,7 @@ class ReductionCodegenInfo {\n   bool is_row_reduction_;\n   bool is_race_free_;\n   IndexGroups index_groups_;\n-  HloInstruction* first_reduce_;\n+  const HloInstruction* first_reduce_;\n };\n \n class ReductionCodegenState {"
                }
            ],
            "whole_deleted": "-                                HloInstruction* first_reduce)\n-  HloInstruction* first_reduce_;\n",
            "whole_added": "+                                const HloInstruction* first_reduce)\n+  const HloInstruction* first_reduce_;\n",
            "whole_hunk": "@@ -173,7 +173,7 @@ class ReductionCodegenInfo {\n   explicit ReductionCodegenInfo(TilingScheme mapping_scheme,\n                                 int num_partial_results, bool is_row_reduction,\n                                 bool is_race_free, IndexGroups index_groups,\n-                                HloInstruction* first_reduce)\n+                                const HloInstruction* first_reduce)\n       : tiling_scheme_(mapping_scheme),\n         num_partial_results_(num_partial_results),\n         is_row_reduction_(is_row_reduction),\n@@ -203,7 +203,7 @@ class ReductionCodegenInfo {\n   bool is_row_reduction_;\n   bool is_race_free_;\n   IndexGroups index_groups_;\n-  HloInstruction* first_reduce_;\n+  const HloInstruction* first_reduce_;\n };\n \n class ReductionCodegenState {"
        }
    ]
},
{
    "Id": 148,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fc640c7c9ef550ea27d1ba417ea5febdf97fcff2",
    "date": "2024-02-22T18:04:33-08:00",
    "message": "[PJRT C API] Add a version check for GetCompiledMemoryStats to return an error instead of crashing for old plugins without this method.\n\nPiperOrigin-RevId: 609560110",
    "label": "NO",
    "changes": [
        {
            "name": "pjrt_c_api_helpers.cc",
            "path": "third_party/xla/xla/pjrt/c/pjrt_c_api_helpers.cc",
            "patches": [
                {
                    "old_start": 944,
                    "old_length": 6,
                    "new_start": 944,
                    "new_length": 13,
                    "hunk": "@@ -944,6 +944,13 @@ absl::Span<PJRT_DeviceDescription* const> DeviceDescriptions(\n \n absl::StatusOr<xla::CompiledMemoryStats> GetCompiledMemoryStats(\n     const PJRT_Api* api, PJRT_Executable* executable) {\n+  // TODO(jieying): To be removed after 03/2024.\n+  if (api->pjrt_api_version.major_version == 0 &&\n+      api->pjrt_api_version.minor_version < 40) {\n+    return absl::UnimplementedError(\n+        \"GetCompiledMemoryStats requires a plugin with PJRT C API version >= \"\n+        \"0.40\");\n+  }\n   PJRT_Executable_GetCompiledMemoryStats_Args args;\n   args.struct_size = PJRT_Executable_GetCompiledMemoryStats_Args_STRUCT_SIZE;\n   args.extension_start = nullptr;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // TODO(jieying): To be removed after 03/2024.\n+  if (api->pjrt_api_version.major_version == 0 &&\n+      api->pjrt_api_version.minor_version < 40) {\n+    return absl::UnimplementedError(\n+        \"GetCompiledMemoryStats requires a plugin with PJRT C API version >= \"\n+        \"0.40\");\n+  }\n",
            "whole_hunk": "@@ -944,6 +944,13 @@ absl::Span<PJRT_DeviceDescription* const> DeviceDescriptions(\n \n absl::StatusOr<xla::CompiledMemoryStats> GetCompiledMemoryStats(\n     const PJRT_Api* api, PJRT_Executable* executable) {\n+  // TODO(jieying): To be removed after 03/2024.\n+  if (api->pjrt_api_version.major_version == 0 &&\n+      api->pjrt_api_version.minor_version < 40) {\n+    return absl::UnimplementedError(\n+        \"GetCompiledMemoryStats requires a plugin with PJRT C API version >= \"\n+        \"0.40\");\n+  }\n   PJRT_Executable_GetCompiledMemoryStats_Args args;\n   args.struct_size = PJRT_Executable_GetCompiledMemoryStats_Args_STRUCT_SIZE;\n   args.extension_start = nullptr;"
        }
    ]
},
{
    "Id": 120,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7ad5a7612918285ddca49633d63c462262bd3dc9",
    "date": "2024-03-09T09:28:26-08:00",
    "message": "Fix SegFault in Python InterpreterWrapper\n\nIf `InterpreterWrapper::TensorSparsityParameters` encounters Tensors which do not have a `block_map`, a `nullptr` is dereferenced causing AccViol/SegFault.\n\nAdd a check for `nullptr`.",
    "label": "YES",
    "changes": [
        {
            "name": "interpreter_wrapper.cc",
            "path": "tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc",
            "patches": [
                {
                    "old_start": 135,
                    "old_length": 9,
                    "new_start": 135,
                    "new_length": 11,
                    "hunk": "@@ -135,9 +135,11 @@ PyObject* PyDictFromSparsityParam(const TfLiteSparsity& param) {\n   PyDict_SetItemString(result, \"traversal_order\",\n                        PyArrayFromIntVector(param.traversal_order->data,\n                                             param.traversal_order->size));\n-  PyDict_SetItemString(\n-      result, \"block_map\",\n-      PyArrayFromIntVector(param.block_map->data, param.block_map->size));\n+  if (param.block_map != nullptr) {\n+    PyDict_SetItemString(\n+        result, \"block_map\",\n+        PyArrayFromIntVector(param.block_map->data, param.block_map->size));\n+  }\n   PyObject* dim_metadata = PyList_New(param.dim_metadata_size);\n   for (int i = 0; i < param.dim_metadata_size; i++) {\n     PyObject* dim_metadata_i = PyDict_New();"
                }
            ],
            "whole_deleted": "-  PyDict_SetItemString(\n-      result, \"block_map\",\n-      PyArrayFromIntVector(param.block_map->data, param.block_map->size));\n",
            "whole_added": "+  if (param.block_map != nullptr) {\n+    PyDict_SetItemString(\n+        result, \"block_map\",\n+        PyArrayFromIntVector(param.block_map->data, param.block_map->size));\n+  }\n",
            "whole_hunk": "@@ -135,9 +135,11 @@ PyObject* PyDictFromSparsityParam(const TfLiteSparsity& param) {\n   PyDict_SetItemString(result, \"traversal_order\",\n                        PyArrayFromIntVector(param.traversal_order->data,\n                                             param.traversal_order->size));\n-  PyDict_SetItemString(\n-      result, \"block_map\",\n-      PyArrayFromIntVector(param.block_map->data, param.block_map->size));\n+  if (param.block_map != nullptr) {\n+    PyDict_SetItemString(\n+        result, \"block_map\",\n+        PyArrayFromIntVector(param.block_map->data, param.block_map->size));\n+  }\n   PyObject* dim_metadata = PyList_New(param.dim_metadata_size);\n   for (int i = 0; i < param.dim_metadata_size; i++) {\n     PyObject* dim_metadata_i = PyDict_New();"
        }
    ]
},
{
    "Id": 302,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/512f4b101762dba010e75f93b84cb096b075992c",
    "date": "2023-08-30T00:10:55-07:00",
    "message": "Update `CalibrationWrapper` to support models larger than 2GB using offset buffers.\n\nAdd implementations to handle the case where the calibration wrapper accepts a TFLite flatbuffer model that uses offset buffers.\nOffset buffers are enabled by the TFLite converter's `_experimental_use_buffer_offset` flag.\nWhenever the model's base buffer (that contains the serialized flatbuffer) is modified, the offset values to the offset buffer of the tensor buffers should be updated.\n\nPiperOrigin-RevId: 561251067",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/lite/BUILD",
            "patches": [
                {
                    "old_start": 194,
                    "old_length": 9,
                    "new_start": 194,
                    "new_length": 9,
                    "hunk": "@@ -194,9 +194,9 @@ cc_test(\n     ],\n     deps = [\n         \":arena_planner_with_profiler\",\n+        \":builtin_ops\",\n         \":graph_info\",\n         \"//tensorflow/core:tflite_portable_logging\",\n-        \"//tensorflow/lite:builtin_ops\",\n         \"//tensorflow/lite/core/c:common\",\n         \"//tensorflow/lite/testing:util\",\n         \"@com_google_googletest//:gtest_main\",\n"
                },
                {
                    "old_start": 748,
                    "old_length": 7,
                    "new_start": 748,
                    "new_length": 7,
                    "hunk": "@@ -748,7 +748,7 @@ cc_library(\n         \"//visibility:public\",\n     ],\n     deps = [\n-        \"//tensorflow/lite:stderr_reporter\",\n+        \":stderr_reporter\",\n         \"//tensorflow/lite/core/api:error_reporter\",\n     ],\n )\n"
                },
                {
                    "old_start": 778,
                    "old_length": 7,
                    "new_start": 778,
                    "new_length": 7,
                    "hunk": "@@ -778,7 +778,7 @@ cc_library(\n         \"//visibility:public\",\n     ],\n     deps = [\n-        \"//tensorflow/lite:mutable_op_resolver\",\n+        \":mutable_op_resolver\",\n         \"//tensorflow/lite/core/api:op_resolver\",\n     ],\n )\n"
                }
            ],
            "whole_deleted": "-        \"//tensorflow/lite:builtin_ops\",\n-        \"//tensorflow/lite:stderr_reporter\",\n-        \"//tensorflow/lite:mutable_op_resolver\",\n",
            "whole_added": "+        \":builtin_ops\",\n+        \":stderr_reporter\",\n+        \":mutable_op_resolver\",\n",
            "whole_hunk": "@@ -194,9 +194,9 @@ cc_test(\n     ],\n     deps = [\n         \":arena_planner_with_profiler\",\n+        \":builtin_ops\",\n         \":graph_info\",\n         \"//tensorflow/core:tflite_portable_logging\",\n-        \"//tensorflow/lite:builtin_ops\",\n         \"//tensorflow/lite/core/c:common\",\n         \"//tensorflow/lite/testing:util\",\n         \"@com_google_googletest//:gtest_main\",\n@@ -748,7 +748,7 @@ cc_library(\n         \"//visibility:public\",\n     ],\n     deps = [\n-        \"//tensorflow/lite:stderr_reporter\",\n+        \":stderr_reporter\",\n         \"//tensorflow/lite/core/api:error_reporter\",\n     ],\n )\n@@ -778,7 +778,7 @@ cc_library(\n         \"//visibility:public\",\n     ],\n     deps = [\n-        \"//tensorflow/lite:mutable_op_resolver\",\n+        \":mutable_op_resolver\",\n         \"//tensorflow/lite/core/api:op_resolver\",\n     ],\n )\n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/lite/python/optimize/BUILD",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 12,
                    "new_start": 20,
                    "new_length": 14,
                    "hunk": "@@ -20,12 +20,14 @@ cc_library(\n         \"//tensorflow/lite/python/interpreter_wrapper:numpy\",\n         \"//tensorflow/lite/python/interpreter_wrapper:python_error_reporter\",\n         \"//tensorflow/lite/python/interpreter_wrapper:python_utils\",\n+        \"//tensorflow/lite/schema:schema_fbs_with_mutable\",\n         \"//tensorflow/lite/tools/optimize:quantization_wrapper_utils\",\n         \"//tensorflow/lite/tools/optimize:quantize_model\",\n         \"//tensorflow/lite/tools/optimize/calibration:calibration_reader\",\n         \"//tensorflow/lite/tools/optimize/calibration:calibrator_lib\",\n         \"//third_party/python_runtime:headers\",  # buildcleaner: keep\n-        \"@com_google_absl//absl/memory\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:optional\",\n     ],\n"
                },
                {
                    "old_start": 77,
                    "old_length": 6,
                    "new_start": 79,
                    "new_length": 10,
                    "hunk": "@@ -77,6 +79,10 @@ py_strict_test(\n     tags = [\"no_oss\"],\n     deps = [\n         \":calibrator\",\n+        \"//tensorflow:tensorflow_py_no_contrib\",\n+        \"//tensorflow/lite/python:lite\",\n+        \"//tensorflow/lite/python:schema_py\",\n+        \"//tensorflow/lite/tools:flatbuffer_utils\",\n         \"//tensorflow/python/framework:dtypes\",\n         \"//tensorflow/python/framework:test_lib\",\n         \"//tensorflow/python/platform:client_testlib\",\n"
                }
            ],
            "whole_deleted": "-        \"@com_google_absl//absl/memory\",\n",
            "whole_added": "+        \"//tensorflow/lite/schema:schema_fbs_with_mutable\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/strings\",\n+        \"//tensorflow:tensorflow_py_no_contrib\",\n+        \"//tensorflow/lite/python:lite\",\n+        \"//tensorflow/lite/python:schema_py\",\n+        \"//tensorflow/lite/tools:flatbuffer_utils\",\n",
            "whole_hunk": "@@ -20,12 +20,14 @@ cc_library(\n         \"//tensorflow/lite/python/interpreter_wrapper:numpy\",\n         \"//tensorflow/lite/python/interpreter_wrapper:python_error_reporter\",\n         \"//tensorflow/lite/python/interpreter_wrapper:python_utils\",\n+        \"//tensorflow/lite/schema:schema_fbs_with_mutable\",\n         \"//tensorflow/lite/tools/optimize:quantization_wrapper_utils\",\n         \"//tensorflow/lite/tools/optimize:quantize_model\",\n         \"//tensorflow/lite/tools/optimize/calibration:calibration_reader\",\n         \"//tensorflow/lite/tools/optimize/calibration:calibrator_lib\",\n         \"//third_party/python_runtime:headers\",  # buildcleaner: keep\n-        \"@com_google_absl//absl/memory\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:optional\",\n     ],\n@@ -77,6 +79,10 @@ py_strict_test(\n     tags = [\"no_oss\"],\n     deps = [\n         \":calibrator\",\n+        \"//tensorflow:tensorflow_py_no_contrib\",\n+        \"//tensorflow/lite/python:lite\",\n+        \"//tensorflow/lite/python:schema_py\",\n+        \"//tensorflow/lite/tools:flatbuffer_utils\",\n         \"//tensorflow/python/framework:dtypes\",\n         \"//tensorflow/python/framework:test_lib\",\n         \"//tensorflow/python/platform:client_testlib\",\n"
        },
        {
            "name": "calibration_wrapper.cc",
            "path": "tensorflow/lite/python/optimize/calibration_wrapper.cc",
            "patches": [
                {
                    "old_start": 12,
                    "old_length": 22,
                    "new_start": 12,
                    "new_length": 48,
                    "hunk": "@@ -12,22 +12,48 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n-#include \"tensorflow/lite/python/optimize/calibration_wrapper.h\"\n+// clang-format off\n+// This #include needs to precede the inclusion of any other TF Lite header\n+// file that might depend on the non-mutable schema_generated.h, directly,\n+// e.g. core/api/op_resolver.h, or indirectly, e.g. core/subgraph.h.\n+// That's because \"tensorflow/lite/schema/mutable/schema_generated.h\"\n+// and \"tensorflow/lite/schema/schema_generated.h\" both use the same\n+// header guard macro (FLATBUFFERS_GENERATED_SCHEMA_TFLITE_H_), but have\n+// different contents (the former is a superset of the latter). In particular\n+// the one in mutable/ is built with the \"--gen-mutable\" and \"--gen-object-api\"\n+// flags to the flatbuffer schema compiler which cause some additional\n+// (non-virtual) accessor methods and API functions to be declared.\n+// The code here uses those methods, so we need to make sure that we get\n+// the mutable variant of this header.\n+#include \"tensorflow/lite/schema/mutable/schema_generated.h\"\n \n+#include \"tensorflow/lite/python/optimize/calibration_wrapper.h\"\n+// clang-format on\n+\n+// NOLINTBEGIN\n+// Nolint disables warnings about header file ordering caused by\n+// `mutable/schema_generated.h`.\n+#include <algorithm>\n+#include <cstddef>\n+#include <cstdint>\n #include <functional>\n+#include <limits>\n #include <memory>\n #include <optional>\n-#include <sstream>\n #include <string>\n #include <utility>\n+#include <vector>\n+// NOLINTEND\n \n-#include \"absl/memory/memory.h\"\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/types/optional.h\"\n #include \"tensorflow/lite/core/c/common.h\"\n #include \"tensorflow/lite/core/interpreter.h\"\n #include \"tensorflow/lite/core/kernels/register.h\"\n-#include \"tensorflow/lite/core/model.h\"\n+#include \"tensorflow/lite/core/model_builder.h\"\n #include \"tensorflow/lite/python/interpreter_wrapper/numpy.h\"\n #include \"tensorflow/lite/python/interpreter_wrapper/python_error_reporter.h\"\n #include \"tensorflow/lite/python/interpreter_wrapper/python_utils.h\"\n"
                },
                {
                    "old_start": 55,
                    "old_length": 13,
                    "new_start": 80,
                    "new_length": 13,
                    "hunk": "@@ -55,13 +80,13 @@ namespace {\n \n using python_utils::PyDecrefDeleter;\n \n-std::unique_ptr<tflite::ModelT> CreateMutableModel(const tflite::Model& model) {\n-  auto copied_model = std::make_unique<tflite::ModelT>();\n+std::unique_ptr<ModelT> CreateMutableModel(const Model& model) {\n+  auto copied_model = std::make_unique<ModelT>();\n   model.UnPackTo(copied_model.get(), nullptr);\n   return copied_model;\n }\n \n-bool NoOpModel(const tflite::FlatBufferModel& model) {\n+bool NoOpModel(const FlatBufferModel& model) {\n   return model->subgraphs()->size() == 1 &&\n          (!model->subgraphs()->begin()->operators() ||\n           model->subgraphs()->begin()->operators()->size() == 0);\n"
                },
                {
                    "old_start": 156,
                    "old_length": 6,
                    "new_start": 181,
                    "new_length": 83,
                    "hunk": "@@ -156,6 +181,83 @@ std::optional<std::vector<int>> ConvertInputShapeToVector(\n   return dims;\n }\n \n+// A buffer offset of 0 means the buffer data is stored along with\n+// `tflite::Tensor`, not in the offset buffer. The flatbuffer exporter always\n+// sets the offset value to 1 to avoid the confusion btw. unset value and\n+// default value, but it is also not a valid offset value, so the valid value\n+// should always be greater than 1.\n+bool IsValidBufferOffset(const int64_t val) { return val > 1; }\n+\n+// Finds the starting position of the offset buffer within `model_buffer` if the\n+// `model_buffer` can be split into base buffer and offset buffer. Returns\n+// `std::nullopt` iff offset buffer is not used or there were no buffers with\n+// valid offset. Assumes `model_buffer` is valid.\n+std::optional<int64_t> GetOffsetBufferStartPosition(\n+    const absl::string_view model_buffer) {\n+  const Model& model = *GetModel(model_buffer.data());\n+\n+  if (!FlatBufferModel::CheckBufferOutsideModel(&model)) {\n+    // Means the offset buffer is not used, e.g.\n+    // `_experimental_use_buffer_offset` is not set.\n+    return std::nullopt;\n+  }\n+\n+  const int64_t int64_max = std::numeric_limits<int64_t>::max();\n+  const int64_t min_offset = absl::c_accumulate(\n+      *model.buffers(), /*init=*/int64_max,\n+      /*binary_op=*/[](const int64_t acc, const Buffer* buffer) -> int64_t {\n+        const int64_t buffer_offset = buffer->offset();\n+        return IsValidBufferOffset(buffer_offset) ? std::min(acc, buffer_offset)\n+                                                  : acc;\n+      });\n+  if (min_offset == int64_max) {\n+    // Means there were no buffers with valid offset.\n+    return std::nullopt;\n+  }\n+  return min_offset;\n+}\n+\n+// Splits the model buffer into base buffer and offset buffer. Offset buffer may\n+// exist when `_experimental_use_buffer_offset` is set.\n+std::pair<absl::string_view, absl::string_view> SplitOffsetBuffer(\n+    const absl::string_view model_buffer) {\n+  const std::optional<int64_t> offset_buffer_pos =\n+      GetOffsetBufferStartPosition(model_buffer);\n+  if (offset_buffer_pos == std::nullopt) {\n+    return {model_buffer, absl::string_view(model_buffer.data(), /*len=*/0)};\n+  }\n+\n+  const absl::string_view base_buffer(model_buffer.data(), *offset_buffer_pos);\n+\n+  const int64_t offset_buffer_length = model_buffer.size() - *offset_buffer_pos;\n+  const absl::string_view offset_buffer(\n+      model_buffer.data() + *offset_buffer_pos, offset_buffer_length);\n+\n+  return {base_buffer, offset_buffer};\n+}\n+\n+// Merges `base_buffer` with the `offset_buffer` that contains the actual tensor\n+// buffer data.\n+std::string MergeOffsetBuffer(const absl::string_view base_buffer,\n+                              const absl::string_view offset_buffer) {\n+  return absl::StrCat(base_buffer, offset_buffer);\n+}\n+\n+// Updates buffer offsets in `base_buffer` by `offset_diff`.\n+std::string UpdateBufferOffsets(const absl::string_view base_buffer,\n+                                const int64_t offset_diff) {\n+  std::string result_buffer(base_buffer);\n+\n+  Model* mutable_model = GetMutableModel(result_buffer.data());\n+  for (Buffer* buffer : *mutable_model->mutable_buffers()) {\n+    if (const int64_t offset = buffer->offset(); IsValidBufferOffset(offset)) {\n+      buffer->mutate_offset(offset + offset_diff);\n+    }\n+  }\n+\n+  return result_buffer;\n+}\n+\n }  // namespace\n \n PyObject* AddIntermediateTensors(PyObject* data) {\n"
                },
                {
                    "old_start": 175,
                    "old_length": 6,
                    "new_start": 278,
                    "new_length": 10,
                    "hunk": "@@ -175,6 +278,10 @@ PyObject* AddIntermediateTensors(PyObject* data) {\n     PyErr_Format(PyExc_ValueError, \"Invalid model\");\n     return nullptr;\n   }\n+\n+  const auto [base_buffer, offset_buffer] =\n+      SplitOffsetBuffer(/*model_buffer=*/absl::string_view(buf, length));\n+\n   flatbuffers::FlatBufferBuilder builder;\n   auto tflite_model = CreateMutableModel(*model->GetModel());\n   if (optimize::AddIntermediateTensorsToFusedOp(&builder, tflite_model.get()) !=\n"
                },
                {
                    "old_start": 183,
                    "old_length": 15,
                    "new_start": 290,
                    "new_length": 26,
                    "hunk": "@@ -183,15 +290,26 @@ PyObject* AddIntermediateTensors(PyObject* data) {\n     return nullptr;\n   }\n \n-  if (builder.GetSize()) {\n-    return python_utils::ConvertToPyString(\n-        reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n-        builder.GetSize());\n-  } else {\n+  const int64_t result_base_buffer_size = builder.GetSize();\n+  if (result_base_buffer_size == 0) {\n     // When AddIntermediateTensorsToFusedOp early returns, return the model as\n     // it is.\n     return python_utils::ConvertToPyString(buf, length);\n   }\n+\n+  const int64_t offset_diff =\n+      result_base_buffer_size - static_cast<int64_t>(base_buffer.size());\n+  const std::string updated_result_base_buffer = UpdateBufferOffsets(\n+      /*base_buffer=*/absl::string_view(\n+          reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n+          builder.GetSize()),\n+      offset_diff);\n+\n+  const std::string result_buffer =\n+      MergeOffsetBuffer(updated_result_base_buffer, offset_buffer);\n+\n+  return python_utils::ConvertToPyString(result_buffer.data(),\n+                                         result_buffer.size());\n }\n \n CalibrationWrapper::CalibrationWrapper(\n"
                },
                {
                    "old_start": 547,
                    "old_length": 14,
                    "new_start": 665,
                    "new_length": 30,
                    "hunk": "@@ -547,14 +665,30 @@ PyObject* CalibrationWrapper::SetTensor(int index, PyObject* value) {\n }\n \n PyObject* CalibrationWrapper::Calibrate() {\n+  const auto [base_buffer, offset_buffer] =\n+      SplitOffsetBuffer(/*model_buffer=*/absl::string_view(\n+          reinterpret_cast<const char*>(model_->allocation()->base()),\n+          model_->allocation()->bytes()));\n+\n   auto tflite_model = CreateMutableModel(*model_->GetModel());\n   reader_->AddCalibrationToModel(tflite_model.get(), /*update=*/false);\n   flatbuffers::FlatBufferBuilder builder;\n-  auto loc = tflite::Model::Pack(builder, tflite_model.get());\n-  tflite::FinishModelBuffer(builder, loc);\n-  return python_utils::ConvertToPyString(\n-      reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n-      builder.GetSize());\n+  auto loc = Model::Pack(builder, tflite_model.get());\n+  FinishModelBuffer(builder, loc);\n+\n+  const int64_t result_base_buffer_size = builder.GetSize();\n+  const int64_t offset_diff =\n+      result_base_buffer_size - static_cast<int64_t>(base_buffer.size());\n+  const std::string updated_result_base_buffer = UpdateBufferOffsets(\n+      /*base_buffer=*/absl::string_view(\n+          reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n+          result_base_buffer_size),\n+      offset_diff);\n+  const std::string result_buffer =\n+      MergeOffsetBuffer(updated_result_base_buffer, offset_buffer);\n+\n+  return python_utils::ConvertToPyString(result_buffer.data(),\n+                                         result_buffer.size());\n }\n \n PyObject* CalibrationWrapper::QuantizeModel(int input_py_type,\n"
                }
            ],
            "whole_deleted": "-#include \"tensorflow/lite/python/optimize/calibration_wrapper.h\"\n-#include <sstream>\n-#include \"absl/memory/memory.h\"\n-#include \"tensorflow/lite/core/model.h\"\n-std::unique_ptr<tflite::ModelT> CreateMutableModel(const tflite::Model& model) {\n-  auto copied_model = std::make_unique<tflite::ModelT>();\n-bool NoOpModel(const tflite::FlatBufferModel& model) {\n-  if (builder.GetSize()) {\n-    return python_utils::ConvertToPyString(\n-        reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n-        builder.GetSize());\n-  } else {\n-  auto loc = tflite::Model::Pack(builder, tflite_model.get());\n-  tflite::FinishModelBuffer(builder, loc);\n-  return python_utils::ConvertToPyString(\n-      reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n-      builder.GetSize());\n",
            "whole_added": "+// clang-format off\n+// This #include needs to precede the inclusion of any other TF Lite header\n+// file that might depend on the non-mutable schema_generated.h, directly,\n+// e.g. core/api/op_resolver.h, or indirectly, e.g. core/subgraph.h.\n+// That's because \"tensorflow/lite/schema/mutable/schema_generated.h\"\n+// and \"tensorflow/lite/schema/schema_generated.h\" both use the same\n+// header guard macro (FLATBUFFERS_GENERATED_SCHEMA_TFLITE_H_), but have\n+// different contents (the former is a superset of the latter). In particular\n+// the one in mutable/ is built with the \"--gen-mutable\" and \"--gen-object-api\"\n+// flags to the flatbuffer schema compiler which cause some additional\n+// (non-virtual) accessor methods and API functions to be declared.\n+// The code here uses those methods, so we need to make sure that we get\n+// the mutable variant of this header.\n+#include \"tensorflow/lite/schema/mutable/schema_generated.h\"\n+#include \"tensorflow/lite/python/optimize/calibration_wrapper.h\"\n+// clang-format on\n+\n+// NOLINTBEGIN\n+// Nolint disables warnings about header file ordering caused by\n+// `mutable/schema_generated.h`.\n+#include <algorithm>\n+#include <cstddef>\n+#include <cstdint>\n+#include <limits>\n+#include <vector>\n+// NOLINTEND\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"tensorflow/lite/core/model_builder.h\"\n+std::unique_ptr<ModelT> CreateMutableModel(const Model& model) {\n+  auto copied_model = std::make_unique<ModelT>();\n+bool NoOpModel(const FlatBufferModel& model) {\n+// A buffer offset of 0 means the buffer data is stored along with\n+// `tflite::Tensor`, not in the offset buffer. The flatbuffer exporter always\n+// sets the offset value to 1 to avoid the confusion btw. unset value and\n+// default value, but it is also not a valid offset value, so the valid value\n+// should always be greater than 1.\n+bool IsValidBufferOffset(const int64_t val) { return val > 1; }\n+\n+// Finds the starting position of the offset buffer within `model_buffer` if the\n+// `model_buffer` can be split into base buffer and offset buffer. Returns\n+// `std::nullopt` iff offset buffer is not used or there were no buffers with\n+// valid offset. Assumes `model_buffer` is valid.\n+std::optional<int64_t> GetOffsetBufferStartPosition(\n+    const absl::string_view model_buffer) {\n+  const Model& model = *GetModel(model_buffer.data());\n+\n+  if (!FlatBufferModel::CheckBufferOutsideModel(&model)) {\n+    // Means the offset buffer is not used, e.g.\n+    // `_experimental_use_buffer_offset` is not set.\n+    return std::nullopt;\n+  }\n+\n+  const int64_t int64_max = std::numeric_limits<int64_t>::max();\n+  const int64_t min_offset = absl::c_accumulate(\n+      *model.buffers(), /*init=*/int64_max,\n+      /*binary_op=*/[](const int64_t acc, const Buffer* buffer) -> int64_t {\n+        const int64_t buffer_offset = buffer->offset();\n+        return IsValidBufferOffset(buffer_offset) ? std::min(acc, buffer_offset)\n+                                                  : acc;\n+      });\n+  if (min_offset == int64_max) {\n+    // Means there were no buffers with valid offset.\n+    return std::nullopt;\n+  }\n+  return min_offset;\n+}\n+\n+// Splits the model buffer into base buffer and offset buffer. Offset buffer may\n+// exist when `_experimental_use_buffer_offset` is set.\n+std::pair<absl::string_view, absl::string_view> SplitOffsetBuffer(\n+    const absl::string_view model_buffer) {\n+  const std::optional<int64_t> offset_buffer_pos =\n+      GetOffsetBufferStartPosition(model_buffer);\n+  if (offset_buffer_pos == std::nullopt) {\n+    return {model_buffer, absl::string_view(model_buffer.data(), /*len=*/0)};\n+  }\n+\n+  const absl::string_view base_buffer(model_buffer.data(), *offset_buffer_pos);\n+\n+  const int64_t offset_buffer_length = model_buffer.size() - *offset_buffer_pos;\n+  const absl::string_view offset_buffer(\n+      model_buffer.data() + *offset_buffer_pos, offset_buffer_length);\n+\n+  return {base_buffer, offset_buffer};\n+}\n+\n+// Merges `base_buffer` with the `offset_buffer` that contains the actual tensor\n+// buffer data.\n+std::string MergeOffsetBuffer(const absl::string_view base_buffer,\n+                              const absl::string_view offset_buffer) {\n+  return absl::StrCat(base_buffer, offset_buffer);\n+}\n+\n+// Updates buffer offsets in `base_buffer` by `offset_diff`.\n+std::string UpdateBufferOffsets(const absl::string_view base_buffer,\n+                                const int64_t offset_diff) {\n+  std::string result_buffer(base_buffer);\n+\n+  Model* mutable_model = GetMutableModel(result_buffer.data());\n+  for (Buffer* buffer : *mutable_model->mutable_buffers()) {\n+    if (const int64_t offset = buffer->offset(); IsValidBufferOffset(offset)) {\n+      buffer->mutate_offset(offset + offset_diff);\n+    }\n+  }\n+\n+  return result_buffer;\n+}\n+\n+\n+  const auto [base_buffer, offset_buffer] =\n+      SplitOffsetBuffer(/*model_buffer=*/absl::string_view(buf, length));\n+\n+  const int64_t result_base_buffer_size = builder.GetSize();\n+  if (result_base_buffer_size == 0) {\n+\n+  const int64_t offset_diff =\n+      result_base_buffer_size - static_cast<int64_t>(base_buffer.size());\n+  const std::string updated_result_base_buffer = UpdateBufferOffsets(\n+      /*base_buffer=*/absl::string_view(\n+          reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n+          builder.GetSize()),\n+      offset_diff);\n+\n+  const std::string result_buffer =\n+      MergeOffsetBuffer(updated_result_base_buffer, offset_buffer);\n+\n+  return python_utils::ConvertToPyString(result_buffer.data(),\n+                                         result_buffer.size());\n+  const auto [base_buffer, offset_buffer] =\n+      SplitOffsetBuffer(/*model_buffer=*/absl::string_view(\n+          reinterpret_cast<const char*>(model_->allocation()->base()),\n+          model_->allocation()->bytes()));\n+\n+  auto loc = Model::Pack(builder, tflite_model.get());\n+  FinishModelBuffer(builder, loc);\n+\n+  const int64_t result_base_buffer_size = builder.GetSize();\n+  const int64_t offset_diff =\n+      result_base_buffer_size - static_cast<int64_t>(base_buffer.size());\n+  const std::string updated_result_base_buffer = UpdateBufferOffsets(\n+      /*base_buffer=*/absl::string_view(\n+          reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n+          result_base_buffer_size),\n+      offset_diff);\n+  const std::string result_buffer =\n+      MergeOffsetBuffer(updated_result_base_buffer, offset_buffer);\n+\n+  return python_utils::ConvertToPyString(result_buffer.data(),\n+                                         result_buffer.size());\n",
            "whole_hunk": "@@ -12,22 +12,48 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n-#include \"tensorflow/lite/python/optimize/calibration_wrapper.h\"\n+// clang-format off\n+// This #include needs to precede the inclusion of any other TF Lite header\n+// file that might depend on the non-mutable schema_generated.h, directly,\n+// e.g. core/api/op_resolver.h, or indirectly, e.g. core/subgraph.h.\n+// That's because \"tensorflow/lite/schema/mutable/schema_generated.h\"\n+// and \"tensorflow/lite/schema/schema_generated.h\" both use the same\n+// header guard macro (FLATBUFFERS_GENERATED_SCHEMA_TFLITE_H_), but have\n+// different contents (the former is a superset of the latter). In particular\n+// the one in mutable/ is built with the \"--gen-mutable\" and \"--gen-object-api\"\n+// flags to the flatbuffer schema compiler which cause some additional\n+// (non-virtual) accessor methods and API functions to be declared.\n+// The code here uses those methods, so we need to make sure that we get\n+// the mutable variant of this header.\n+#include \"tensorflow/lite/schema/mutable/schema_generated.h\"\n \n+#include \"tensorflow/lite/python/optimize/calibration_wrapper.h\"\n+// clang-format on\n+\n+// NOLINTBEGIN\n+// Nolint disables warnings about header file ordering caused by\n+// `mutable/schema_generated.h`.\n+#include <algorithm>\n+#include <cstddef>\n+#include <cstdint>\n #include <functional>\n+#include <limits>\n #include <memory>\n #include <optional>\n-#include <sstream>\n #include <string>\n #include <utility>\n+#include <vector>\n+// NOLINTEND\n \n-#include \"absl/memory/memory.h\"\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/types/optional.h\"\n #include \"tensorflow/lite/core/c/common.h\"\n #include \"tensorflow/lite/core/interpreter.h\"\n #include \"tensorflow/lite/core/kernels/register.h\"\n-#include \"tensorflow/lite/core/model.h\"\n+#include \"tensorflow/lite/core/model_builder.h\"\n #include \"tensorflow/lite/python/interpreter_wrapper/numpy.h\"\n #include \"tensorflow/lite/python/interpreter_wrapper/python_error_reporter.h\"\n #include \"tensorflow/lite/python/interpreter_wrapper/python_utils.h\"\n@@ -55,13 +80,13 @@ namespace {\n \n using python_utils::PyDecrefDeleter;\n \n-std::unique_ptr<tflite::ModelT> CreateMutableModel(const tflite::Model& model) {\n-  auto copied_model = std::make_unique<tflite::ModelT>();\n+std::unique_ptr<ModelT> CreateMutableModel(const Model& model) {\n+  auto copied_model = std::make_unique<ModelT>();\n   model.UnPackTo(copied_model.get(), nullptr);\n   return copied_model;\n }\n \n-bool NoOpModel(const tflite::FlatBufferModel& model) {\n+bool NoOpModel(const FlatBufferModel& model) {\n   return model->subgraphs()->size() == 1 &&\n          (!model->subgraphs()->begin()->operators() ||\n           model->subgraphs()->begin()->operators()->size() == 0);\n@@ -156,6 +181,83 @@ std::optional<std::vector<int>> ConvertInputShapeToVector(\n   return dims;\n }\n \n+// A buffer offset of 0 means the buffer data is stored along with\n+// `tflite::Tensor`, not in the offset buffer. The flatbuffer exporter always\n+// sets the offset value to 1 to avoid the confusion btw. unset value and\n+// default value, but it is also not a valid offset value, so the valid value\n+// should always be greater than 1.\n+bool IsValidBufferOffset(const int64_t val) { return val > 1; }\n+\n+// Finds the starting position of the offset buffer within `model_buffer` if the\n+// `model_buffer` can be split into base buffer and offset buffer. Returns\n+// `std::nullopt` iff offset buffer is not used or there were no buffers with\n+// valid offset. Assumes `model_buffer` is valid.\n+std::optional<int64_t> GetOffsetBufferStartPosition(\n+    const absl::string_view model_buffer) {\n+  const Model& model = *GetModel(model_buffer.data());\n+\n+  if (!FlatBufferModel::CheckBufferOutsideModel(&model)) {\n+    // Means the offset buffer is not used, e.g.\n+    // `_experimental_use_buffer_offset` is not set.\n+    return std::nullopt;\n+  }\n+\n+  const int64_t int64_max = std::numeric_limits<int64_t>::max();\n+  const int64_t min_offset = absl::c_accumulate(\n+      *model.buffers(), /*init=*/int64_max,\n+      /*binary_op=*/[](const int64_t acc, const Buffer* buffer) -> int64_t {\n+        const int64_t buffer_offset = buffer->offset();\n+        return IsValidBufferOffset(buffer_offset) ? std::min(acc, buffer_offset)\n+                                                  : acc;\n+      });\n+  if (min_offset == int64_max) {\n+    // Means there were no buffers with valid offset.\n+    return std::nullopt;\n+  }\n+  return min_offset;\n+}\n+\n+// Splits the model buffer into base buffer and offset buffer. Offset buffer may\n+// exist when `_experimental_use_buffer_offset` is set.\n+std::pair<absl::string_view, absl::string_view> SplitOffsetBuffer(\n+    const absl::string_view model_buffer) {\n+  const std::optional<int64_t> offset_buffer_pos =\n+      GetOffsetBufferStartPosition(model_buffer);\n+  if (offset_buffer_pos == std::nullopt) {\n+    return {model_buffer, absl::string_view(model_buffer.data(), /*len=*/0)};\n+  }\n+\n+  const absl::string_view base_buffer(model_buffer.data(), *offset_buffer_pos);\n+\n+  const int64_t offset_buffer_length = model_buffer.size() - *offset_buffer_pos;\n+  const absl::string_view offset_buffer(\n+      model_buffer.data() + *offset_buffer_pos, offset_buffer_length);\n+\n+  return {base_buffer, offset_buffer};\n+}\n+\n+// Merges `base_buffer` with the `offset_buffer` that contains the actual tensor\n+// buffer data.\n+std::string MergeOffsetBuffer(const absl::string_view base_buffer,\n+                              const absl::string_view offset_buffer) {\n+  return absl::StrCat(base_buffer, offset_buffer);\n+}\n+\n+// Updates buffer offsets in `base_buffer` by `offset_diff`.\n+std::string UpdateBufferOffsets(const absl::string_view base_buffer,\n+                                const int64_t offset_diff) {\n+  std::string result_buffer(base_buffer);\n+\n+  Model* mutable_model = GetMutableModel(result_buffer.data());\n+  for (Buffer* buffer : *mutable_model->mutable_buffers()) {\n+    if (const int64_t offset = buffer->offset(); IsValidBufferOffset(offset)) {\n+      buffer->mutate_offset(offset + offset_diff);\n+    }\n+  }\n+\n+  return result_buffer;\n+}\n+\n }  // namespace\n \n PyObject* AddIntermediateTensors(PyObject* data) {\n@@ -175,6 +278,10 @@ PyObject* AddIntermediateTensors(PyObject* data) {\n     PyErr_Format(PyExc_ValueError, \"Invalid model\");\n     return nullptr;\n   }\n+\n+  const auto [base_buffer, offset_buffer] =\n+      SplitOffsetBuffer(/*model_buffer=*/absl::string_view(buf, length));\n+\n   flatbuffers::FlatBufferBuilder builder;\n   auto tflite_model = CreateMutableModel(*model->GetModel());\n   if (optimize::AddIntermediateTensorsToFusedOp(&builder, tflite_model.get()) !=\n@@ -183,15 +290,26 @@ PyObject* AddIntermediateTensors(PyObject* data) {\n     return nullptr;\n   }\n \n-  if (builder.GetSize()) {\n-    return python_utils::ConvertToPyString(\n-        reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n-        builder.GetSize());\n-  } else {\n+  const int64_t result_base_buffer_size = builder.GetSize();\n+  if (result_base_buffer_size == 0) {\n     // When AddIntermediateTensorsToFusedOp early returns, return the model as\n     // it is.\n     return python_utils::ConvertToPyString(buf, length);\n   }\n+\n+  const int64_t offset_diff =\n+      result_base_buffer_size - static_cast<int64_t>(base_buffer.size());\n+  const std::string updated_result_base_buffer = UpdateBufferOffsets(\n+      /*base_buffer=*/absl::string_view(\n+          reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n+          builder.GetSize()),\n+      offset_diff);\n+\n+  const std::string result_buffer =\n+      MergeOffsetBuffer(updated_result_base_buffer, offset_buffer);\n+\n+  return python_utils::ConvertToPyString(result_buffer.data(),\n+                                         result_buffer.size());\n }\n \n CalibrationWrapper::CalibrationWrapper(\n@@ -547,14 +665,30 @@ PyObject* CalibrationWrapper::SetTensor(int index, PyObject* value) {\n }\n \n PyObject* CalibrationWrapper::Calibrate() {\n+  const auto [base_buffer, offset_buffer] =\n+      SplitOffsetBuffer(/*model_buffer=*/absl::string_view(\n+          reinterpret_cast<const char*>(model_->allocation()->base()),\n+          model_->allocation()->bytes()));\n+\n   auto tflite_model = CreateMutableModel(*model_->GetModel());\n   reader_->AddCalibrationToModel(tflite_model.get(), /*update=*/false);\n   flatbuffers::FlatBufferBuilder builder;\n-  auto loc = tflite::Model::Pack(builder, tflite_model.get());\n-  tflite::FinishModelBuffer(builder, loc);\n-  return python_utils::ConvertToPyString(\n-      reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n-      builder.GetSize());\n+  auto loc = Model::Pack(builder, tflite_model.get());\n+  FinishModelBuffer(builder, loc);\n+\n+  const int64_t result_base_buffer_size = builder.GetSize();\n+  const int64_t offset_diff =\n+      result_base_buffer_size - static_cast<int64_t>(base_buffer.size());\n+  const std::string updated_result_base_buffer = UpdateBufferOffsets(\n+      /*base_buffer=*/absl::string_view(\n+          reinterpret_cast<const char*>(builder.GetCurrentBufferPointer()),\n+          result_base_buffer_size),\n+      offset_diff);\n+  const std::string result_buffer =\n+      MergeOffsetBuffer(updated_result_base_buffer, offset_buffer);\n+\n+  return python_utils::ConvertToPyString(result_buffer.data(),\n+                                         result_buffer.size());\n }\n \n PyObject* CalibrationWrapper::QuantizeModel(int input_py_type,\n"
        },
        {
            "name": "calibrator_test.py",
            "path": "tensorflow/lite/python/optimize/calibrator_test.py",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 14,
                    "new_start": 16,
                    "new_length": 39,
                    "hunk": "@@ -16,14 +16,39 @@\n \n from absl.testing import parameterized\n import numpy as np\n+import tensorflow as tf\n \n+from tensorflow.lite.python import lite\n+from tensorflow.lite.python import schema_py_generated as schema_fb\n from tensorflow.lite.python.optimize import calibrator as _calibrator\n+from tensorflow.lite.tools import flatbuffer_utils\n from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import test_util\n from tensorflow.python.platform import resource_loader\n from tensorflow.python.platform import test\n \n \n+def _uses_buffer_offset(model: schema_fb.ModelT) -> bool:\n+  \"\"\"Determines whether the model is using an offset buffer.\n+\n+  Args:\n+    model: A TFLite model.\n+\n+  Returns:\n+    True iff the model is using offset buffers. Offset buffers are enabled by\n+    the flag `_experimental_use_buffer_offset`.\n+  \"\"\"\n+  if not model.metadata:\n+    return False\n+\n+  return any(\n+      map(\n+          lambda metadata: metadata.name.decode('utf-8') == 'buffer_location',\n+          model.metadata,\n+      )\n+  )\n+\n+\n class CalibratorTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n \n   @parameterized.named_parameters(\n"
                },
                {
                    "old_start": 219,
                    "old_length": 6,
                    "new_start": 244,
                    "new_length": 68,
                    "hunk": "@@ -219,6 +244,68 @@ class CalibratorTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n     added_model = _calibrator.add_intermediate_tensors(model)\n     self.assertIsNotNone(added_model)\n \n+  def test_calibrate_model_with_offset_buffer(self):\n+    # Define a simple model to run calibration with.\n+    class MatMulModel(tf.Module):\n+\n+      def __init__(self):\n+        # Use ones for predictable calibration results.\n+        self.filter = np.ones((4, 3)).astype(np.float32)\n+\n+      @tf.function(\n+          input_signature=[tf.TensorSpec(shape=(1, 4), dtype=dtypes.float32)]\n+      )\n+      def __call__(self, input_tensor: tf.Tensor) -> tf.Tensor:\n+        output_tensor = tf.linalg.matmul(input_tensor, self.filter)\n+        return {'output': output_tensor}\n+\n+    model = MatMulModel()\n+    saved_model_path = self.create_tempdir().full_path\n+    tf.saved_model.save(model, saved_model_path)\n+\n+    converter = lite.TFLiteConverter.from_saved_model(saved_model_path)\n+    # Enable the use of buffer offsets.\n+    # pylint: disable=protected-access\n+    converter._experimental_use_buffer_offset = True\n+    # pylint: enable=protected-access\n+    converter.exclude_conversion_metadata = True\n+\n+    model_serialized = converter.convert()\n+\n+    model = flatbuffer_utils.convert_bytearray_to_object(model_serialized)\n+    self.assertTrue(_uses_buffer_offset(model))\n+\n+    quantizer = _calibrator.Calibrator(model_serialized)\n+\n+    # Input generator for the model.\n+    def input_gen():\n+      for _ in range(2):\n+        yield [np.array([1.0, 1.0, 1.0, 1.0], dtype=np.float32)]\n+\n+    calibrated_model_serialized = quantizer.calibrate(input_gen)\n+    self.assertIsNotNone(calibrated_model_serialized)\n+\n+    calibrated_model = flatbuffer_utils.convert_bytearray_to_object(\n+        calibrated_model_serialized\n+    )\n+    self.assertTrue(_uses_buffer_offset(calibrated_model))\n+\n+    # Confirm that the tensors are correctly calibrated.\n+    subgraph = calibrated_model.subgraphs[0]\n+\n+    matmul_input_tensor = subgraph.tensors[0]\n+    self.assertAllClose(matmul_input_tensor.quantization.min, [1.0])\n+    self.assertAllClose(matmul_input_tensor.quantization.max, [1.0])\n+\n+    matmul_filter_tensor = subgraph.tensors[1]\n+    self.assertAllClose(matmul_filter_tensor.quantization.min, [1.0])\n+    self.assertAllClose(matmul_filter_tensor.quantization.max, [1.0])\n+\n+    # The matmul is performed with all ones so the output is expected to be 4s.\n+    matmul_output_tensor = subgraph.tensors[2]\n+    self.assertAllClose(matmul_output_tensor.quantization.min, [4.0])\n+    self.assertAllClose(matmul_output_tensor.quantization.max, [4.0])\n+\n \n if __name__ == '__main__':\n   test.main()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+import tensorflow as tf\n+from tensorflow.lite.python import lite\n+from tensorflow.lite.python import schema_py_generated as schema_fb\n+from tensorflow.lite.tools import flatbuffer_utils\n+def _uses_buffer_offset(model: schema_fb.ModelT) -> bool:\n+  \"\"\"Determines whether the model is using an offset buffer.\n+\n+  Args:\n+    model: A TFLite model.\n+\n+  Returns:\n+    True iff the model is using offset buffers. Offset buffers are enabled by\n+    the flag `_experimental_use_buffer_offset`.\n+  \"\"\"\n+  if not model.metadata:\n+    return False\n+\n+  return any(\n+      map(\n+          lambda metadata: metadata.name.decode('utf-8') == 'buffer_location',\n+          model.metadata,\n+      )\n+  )\n+\n+\n+  def test_calibrate_model_with_offset_buffer(self):\n+    # Define a simple model to run calibration with.\n+    class MatMulModel(tf.Module):\n+\n+      def __init__(self):\n+        # Use ones for predictable calibration results.\n+        self.filter = np.ones((4, 3)).astype(np.float32)\n+\n+      @tf.function(\n+          input_signature=[tf.TensorSpec(shape=(1, 4), dtype=dtypes.float32)]\n+      )\n+      def __call__(self, input_tensor: tf.Tensor) -> tf.Tensor:\n+        output_tensor = tf.linalg.matmul(input_tensor, self.filter)\n+        return {'output': output_tensor}\n+\n+    model = MatMulModel()\n+    saved_model_path = self.create_tempdir().full_path\n+    tf.saved_model.save(model, saved_model_path)\n+\n+    converter = lite.TFLiteConverter.from_saved_model(saved_model_path)\n+    # Enable the use of buffer offsets.\n+    # pylint: disable=protected-access\n+    converter._experimental_use_buffer_offset = True\n+    # pylint: enable=protected-access\n+    converter.exclude_conversion_metadata = True\n+\n+    model_serialized = converter.convert()\n+\n+    model = flatbuffer_utils.convert_bytearray_to_object(model_serialized)\n+    self.assertTrue(_uses_buffer_offset(model))\n+\n+    quantizer = _calibrator.Calibrator(model_serialized)\n+\n+    # Input generator for the model.\n+    def input_gen():\n+      for _ in range(2):\n+        yield [np.array([1.0, 1.0, 1.0, 1.0], dtype=np.float32)]\n+\n+    calibrated_model_serialized = quantizer.calibrate(input_gen)\n+    self.assertIsNotNone(calibrated_model_serialized)\n+\n+    calibrated_model = flatbuffer_utils.convert_bytearray_to_object(\n+        calibrated_model_serialized\n+    )\n+    self.assertTrue(_uses_buffer_offset(calibrated_model))\n+\n+    # Confirm that the tensors are correctly calibrated.\n+    subgraph = calibrated_model.subgraphs[0]\n+\n+    matmul_input_tensor = subgraph.tensors[0]\n+    self.assertAllClose(matmul_input_tensor.quantization.min, [1.0])\n+    self.assertAllClose(matmul_input_tensor.quantization.max, [1.0])\n+\n+    matmul_filter_tensor = subgraph.tensors[1]\n+    self.assertAllClose(matmul_filter_tensor.quantization.min, [1.0])\n+    self.assertAllClose(matmul_filter_tensor.quantization.max, [1.0])\n+\n+    # The matmul is performed with all ones so the output is expected to be 4s.\n+    matmul_output_tensor = subgraph.tensors[2]\n+    self.assertAllClose(matmul_output_tensor.quantization.min, [4.0])\n+    self.assertAllClose(matmul_output_tensor.quantization.max, [4.0])\n+\n",
            "whole_hunk": "@@ -16,14 +16,39 @@\n \n from absl.testing import parameterized\n import numpy as np\n+import tensorflow as tf\n \n+from tensorflow.lite.python import lite\n+from tensorflow.lite.python import schema_py_generated as schema_fb\n from tensorflow.lite.python.optimize import calibrator as _calibrator\n+from tensorflow.lite.tools import flatbuffer_utils\n from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import test_util\n from tensorflow.python.platform import resource_loader\n from tensorflow.python.platform import test\n \n \n+def _uses_buffer_offset(model: schema_fb.ModelT) -> bool:\n+  \"\"\"Determines whether the model is using an offset buffer.\n+\n+  Args:\n+    model: A TFLite model.\n+\n+  Returns:\n+    True iff the model is using offset buffers. Offset buffers are enabled by\n+    the flag `_experimental_use_buffer_offset`.\n+  \"\"\"\n+  if not model.metadata:\n+    return False\n+\n+  return any(\n+      map(\n+          lambda metadata: metadata.name.decode('utf-8') == 'buffer_location',\n+          model.metadata,\n+      )\n+  )\n+\n+\n class CalibratorTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n \n   @parameterized.named_parameters(\n@@ -219,6 +244,68 @@ class CalibratorTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n     added_model = _calibrator.add_intermediate_tensors(model)\n     self.assertIsNotNone(added_model)\n \n+  def test_calibrate_model_with_offset_buffer(self):\n+    # Define a simple model to run calibration with.\n+    class MatMulModel(tf.Module):\n+\n+      def __init__(self):\n+        # Use ones for predictable calibration results.\n+        self.filter = np.ones((4, 3)).astype(np.float32)\n+\n+      @tf.function(\n+          input_signature=[tf.TensorSpec(shape=(1, 4), dtype=dtypes.float32)]\n+      )\n+      def __call__(self, input_tensor: tf.Tensor) -> tf.Tensor:\n+        output_tensor = tf.linalg.matmul(input_tensor, self.filter)\n+        return {'output': output_tensor}\n+\n+    model = MatMulModel()\n+    saved_model_path = self.create_tempdir().full_path\n+    tf.saved_model.save(model, saved_model_path)\n+\n+    converter = lite.TFLiteConverter.from_saved_model(saved_model_path)\n+    # Enable the use of buffer offsets.\n+    # pylint: disable=protected-access\n+    converter._experimental_use_buffer_offset = True\n+    # pylint: enable=protected-access\n+    converter.exclude_conversion_metadata = True\n+\n+    model_serialized = converter.convert()\n+\n+    model = flatbuffer_utils.convert_bytearray_to_object(model_serialized)\n+    self.assertTrue(_uses_buffer_offset(model))\n+\n+    quantizer = _calibrator.Calibrator(model_serialized)\n+\n+    # Input generator for the model.\n+    def input_gen():\n+      for _ in range(2):\n+        yield [np.array([1.0, 1.0, 1.0, 1.0], dtype=np.float32)]\n+\n+    calibrated_model_serialized = quantizer.calibrate(input_gen)\n+    self.assertIsNotNone(calibrated_model_serialized)\n+\n+    calibrated_model = flatbuffer_utils.convert_bytearray_to_object(\n+        calibrated_model_serialized\n+    )\n+    self.assertTrue(_uses_buffer_offset(calibrated_model))\n+\n+    # Confirm that the tensors are correctly calibrated.\n+    subgraph = calibrated_model.subgraphs[0]\n+\n+    matmul_input_tensor = subgraph.tensors[0]\n+    self.assertAllClose(matmul_input_tensor.quantization.min, [1.0])\n+    self.assertAllClose(matmul_input_tensor.quantization.max, [1.0])\n+\n+    matmul_filter_tensor = subgraph.tensors[1]\n+    self.assertAllClose(matmul_filter_tensor.quantization.min, [1.0])\n+    self.assertAllClose(matmul_filter_tensor.quantization.max, [1.0])\n+\n+    # The matmul is performed with all ones so the output is expected to be 4s.\n+    matmul_output_tensor = subgraph.tensors[2]\n+    self.assertAllClose(matmul_output_tensor.quantization.min, [4.0])\n+    self.assertAllClose(matmul_output_tensor.quantization.max, [4.0])\n+\n \n if __name__ == '__main__':\n   test.main()"
        }
    ]
},
{
    "Id": 211,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/bb9b9f32bd08bc5660343cdc09f4b467d7297e3c",
    "date": "2024-01-04T17:05:02-08:00",
    "message": "Change a recently introduced DCHECK from O(N) cost to O(1).\n\nA recent change added a DCHECK that took O(N) time for an\ninstruction with N users every time a user was added. This\nbecame quadratic and caused timeouts in some tests that\nenable assertion checking. Made the DCHECK O(1) to fix it.\n\nPiperOrigin-RevId: 595834589",
    "label": "YES",
    "changes": [
        {
            "name": "hlo_instruction.cc",
            "path": "third_party/xla/xla/hlo/ir/hlo_instruction.cc",
            "patches": [
                {
                    "old_start": 206,
                    "old_length": 17,
                    "new_start": 206,
                    "new_length": 9,
                    "hunk": "@@ -206,17 +206,9 @@ void HloInstruction::Users::RebuildMap() {\n \n bool HloInstruction::Users::CheckInvariants() {\n   if (user_map_ != nullptr) {\n-    int64_t index = 0;\n-    for (const HloInstruction* u : users_) {\n-      CHECK(user_map_->contains(u));\n-      CHECK_EQ((*user_map_)[u], index);\n-      index++;\n-    }\n-    for (auto [u, index] : *user_map_) {\n-      CHECK_GE(index, 0);\n-      CHECK_LT(index, users_.size());\n-      CHECK_EQ(users_[index], u);\n-    }\n+    // Avoid quadratic behavior by doing a quick and dirty check on\n+    // size instead of actually comparing mapped indices.\n+    CHECK_EQ(users_.size(), user_map_->size());\n   }\n   return true;\n }"
                }
            ],
            "whole_deleted": "-    int64_t index = 0;\n-    for (const HloInstruction* u : users_) {\n-      CHECK(user_map_->contains(u));\n-      CHECK_EQ((*user_map_)[u], index);\n-      index++;\n-    }\n-    for (auto [u, index] : *user_map_) {\n-      CHECK_GE(index, 0);\n-      CHECK_LT(index, users_.size());\n-      CHECK_EQ(users_[index], u);\n-    }\n",
            "whole_added": "+    // Avoid quadratic behavior by doing a quick and dirty check on\n+    // size instead of actually comparing mapped indices.\n+    CHECK_EQ(users_.size(), user_map_->size());\n",
            "whole_hunk": "@@ -206,17 +206,9 @@ void HloInstruction::Users::RebuildMap() {\n \n bool HloInstruction::Users::CheckInvariants() {\n   if (user_map_ != nullptr) {\n-    int64_t index = 0;\n-    for (const HloInstruction* u : users_) {\n-      CHECK(user_map_->contains(u));\n-      CHECK_EQ((*user_map_)[u], index);\n-      index++;\n-    }\n-    for (auto [u, index] : *user_map_) {\n-      CHECK_GE(index, 0);\n-      CHECK_LT(index, users_.size());\n-      CHECK_EQ(users_[index], u);\n-    }\n+    // Avoid quadratic behavior by doing a quick and dirty check on\n+    // size instead of actually comparing mapped indices.\n+    CHECK_EQ(users_.size(), user_map_->size());\n   }\n   return true;\n }"
        }
    ]
},
{
    "Id": 630,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/63410d7f0c444d9081ff674bef024cfbfc939027",
    "date": "2022-11-23T23:56:06-08:00",
    "message": "Minor fix: Support q/dq ops after a weight when checking if the weight is constant\n\nPiperOrigin-RevId: 490665558",
    "label": "YES",
    "changes": [
        {
            "name": "lift_quantizable_spots_as_functions.cc",
            "path": "tensorflow/compiler/mlir/quantization/tensorflow/passes/lift_quantizable_spots_as_functions.cc",
            "patches": [
                {
                    "old_start": 155,
                    "old_length": 7,
                    "new_start": 155,
                    "new_length": 21,
                    "hunk": "@@ -155,7 +155,21 @@ class CheckQuantizableOps\n     for (auto iter : spec->coeff_op_quant_dim) {\n       Operation* preceding_op = call_op.getOperand(iter.first).getDefiningOp();\n       // The XLA opset only supports constant filter/weight at the moment.\n-      if (!preceding_op || !preceding_op->hasTrait<OpTrait::ConstantLike>()) {\n+      bool is_weight_constant =\n+          preceding_op && preceding_op->hasTrait<OpTrait::ConstantLike>();\n+\n+      // There might be q/dq ops after the filter/weight.\n+      if (auto dq_op = llvm::dyn_cast_or_null<quantfork::DequantizeCastOp>(\n+              preceding_op)) {\n+        if (auto q_op = llvm::dyn_cast_or_null<quantfork::QuantizeCastOp>(\n+                dq_op.getArg().getDefiningOp())) {\n+          Operation* q_op_input = q_op.getArg().getDefiningOp();\n+          is_weight_constant =\n+              q_op_input && q_op_input->hasTrait<OpTrait::ConstantLike>();\n+        }\n+      }\n+\n+      if (!is_weight_constant) {\n         return tensorflow::errors::Unknown(\n             \"Non-constant weights are not supported at the moment.\");\n       }\n"
                }
            ],
            "whole_deleted": "-      if (!preceding_op || !preceding_op->hasTrait<OpTrait::ConstantLike>()) {\n",
            "whole_added": "+      bool is_weight_constant =\n+          preceding_op && preceding_op->hasTrait<OpTrait::ConstantLike>();\n+\n+      // There might be q/dq ops after the filter/weight.\n+      if (auto dq_op = llvm::dyn_cast_or_null<quantfork::DequantizeCastOp>(\n+              preceding_op)) {\n+        if (auto q_op = llvm::dyn_cast_or_null<quantfork::QuantizeCastOp>(\n+                dq_op.getArg().getDefiningOp())) {\n+          Operation* q_op_input = q_op.getArg().getDefiningOp();\n+          is_weight_constant =\n+              q_op_input && q_op_input->hasTrait<OpTrait::ConstantLike>();\n+        }\n+      }\n+\n+      if (!is_weight_constant) {\n",
            "whole_hunk": "@@ -155,7 +155,21 @@ class CheckQuantizableOps\n     for (auto iter : spec->coeff_op_quant_dim) {\n       Operation* preceding_op = call_op.getOperand(iter.first).getDefiningOp();\n       // The XLA opset only supports constant filter/weight at the moment.\n-      if (!preceding_op || !preceding_op->hasTrait<OpTrait::ConstantLike>()) {\n+      bool is_weight_constant =\n+          preceding_op && preceding_op->hasTrait<OpTrait::ConstantLike>();\n+\n+      // There might be q/dq ops after the filter/weight.\n+      if (auto dq_op = llvm::dyn_cast_or_null<quantfork::DequantizeCastOp>(\n+              preceding_op)) {\n+        if (auto q_op = llvm::dyn_cast_or_null<quantfork::QuantizeCastOp>(\n+                dq_op.getArg().getDefiningOp())) {\n+          Operation* q_op_input = q_op.getArg().getDefiningOp();\n+          is_weight_constant =\n+              q_op_input && q_op_input->hasTrait<OpTrait::ConstantLike>();\n+        }\n+      }\n+\n+      if (!is_weight_constant) {\n         return tensorflow::errors::Unknown(\n             \"Non-constant weights are not supported at the moment.\");\n       }\n"
        },
        {
            "name": "lift_quantizable_spots_as_functions_xla.mlir",
            "path": "tensorflow/compiler/mlir/quantization/tensorflow/tests/lift_quantizable_spots_as_functions_xla.mlir",
            "patches": [
                {
                    "old_start": 71,
                    "old_length": 3,
                    "new_start": 71,
                    "new_length": 33,
                    "hunk": "@@ -71,3 +71,33 @@ func.func @conv_with_dynamic_channel_dim(%arg0: tensor<1x3x4x?xf32>) -> tensor<*\n // Check that the `attr_map` attribute has been removed.\n // CHECK-NOT: attr_map\n // CHECK-SAME: data_format = \"NHWC\", dilations = [1, 1, 2, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 2, 1]\n+\n+// -----\n+\n+func.func @const_filter_with_q_dq(%arg0: tensor<1x3x4x3xf32>) -> (tensor<1x3x2x2xf32>) {\n+  %cst = \"tf.Const\"() {device = \"\", value = dense<[[[[-0.308480561, 0.122108772], [-0.0622722618, 0.285358578], [0.279975802, -0.227407396]], [[-0.223535746, 0.301872164], [0.45813936, 0.375932634], [-0.142182723, 9.95125505E-4]], [[0.183462933, 0.212702021], [-0.129749238, 0.0611961856], [0.00308316527, -0.486231536]]], [[[0.272826612, 0.382641196], [-0.135114014, 0.115396179], [-0.424618751, -1.311760e-01]], [[0.433140099, 0.15137814], [-0.102797419, 0.288730145], [-0.183163881, 0.0680986494]], [[0.369127393, -0.0638265759], [0.302147657, -0.35623318], [0.204260975, 0.204581305]]]]> : tensor<2x3x3x2xf32>} : () -> tensor<2x3x3x2xf32>\n+  %cst_0 = \"tf.Const\"() {device = \"\", value = dense<[1.000000e-01, 2.000000e-01]> : tensor<2xf32>} : () -> tensor<2xf32>\n+  %0 = \"quantfork.qcast\"(%arg0) : (tensor<1x3x4x3xf32>) -> tensor<1x3x4x3x!quant.uniform<i8:f32, 0.0011764706057660721:-43>>\n+  %1 = \"quantfork.dcast\"(%0) : (tensor<1x3x4x3x!quant.uniform<i8:f32, 0.0011764706057660721:-43>>) -> tensor<1x3x4x3xf32>\n+  %q_w = \"quantfork.qcast\"(%cst) : (tensor<2x3x3x2xf32>) -> tensor<2x3x3x2x!quant.uniform<i8:f32, 0.0125:-24>>\n+  %dq_w = \"quantfork.dcast\"(%q_w) : (tensor<2x3x3x2x!quant.uniform<i8:f32, 0.0125:-24>>) -> tensor<2x3x3x2xf32>\n+  %2 = \"tf.Conv2D\"(%1, %dq_w) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 2, 1], use_cudnn_on_gpu = true} : (tensor<1x3x4x3xf32>, tensor<2x3x3x2xf32>) -> tensor<1x3x2x2xf32>\n+  %3 = \"tf.BiasAdd\"(%2, %cst_0) {data_format = \"NHWC\", device = \"\"} : (tensor<1x3x2x2xf32>, tensor<2xf32>) -> tensor<1x3x2x2xf32>\n+  %4 = \"tf.Relu\"(%3) {device = \"\"} : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2xf32>\n+  %5 = \"quantfork.qcast\"(%4) : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2x!quant.uniform<i8:f32, 0.0027450981093387976:-19>>\n+  %6 = \"quantfork.dcast\"(%5) : (tensor<1x3x2x2x!quant.uniform<i8:f32, 0.0027450981093387976:-19>>) -> tensor<1x3x2x2xf32>\n+  %7 = \"tf.Identity\"(%6) {device = \"\"} : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2xf32>\n+  %8 = \"tf.Identity\"(%7) {device = \"\"} : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2xf32>\n+  return %8 : tensor<1x3x2x2xf32>\n+}\n+\n+// CHECK-LABEL: func @const_filter_with_q_dq\n+// CHECK-DAG: %[[WEIGHT:.*]] = \"tf.Const\"() {{.*}} : () -> tensor<2x3x3x2xf32>\n+// CHECK-DAG: %[[BIAS:.*]] = \"tf.Const\"() {device = \"\", value = dense<[1.000000e-01, 2.000000e-01]> : tensor<2xf32>}\n+// CHECK: %[[Q_W:.*]] = \"quantfork.qcast\"(%[[WEIGHT]])\n+// CHECK: %[[DQ_W:.*]] = \"quantfork.dcast\"(%[[Q_W]])\n+// CHECK: %[[PARTITIONEDCALL_0:.*]] = \"tf.PartitionedCall\"({{.*}}, %[[DQ_W]], %[[BIAS]])\n+// CHECK-SAME: _tfl_quant_trait = \"fully_quantizable\"\n+// CHECK-SAME: f = @composite_conv2d_with_bias_and_relu_fn_1\n+\n+// CHECK-LABEL: func private @composite_conv2d_with_bias_and_relu_fn_1\n\\ No newline at end of file\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+// -----\n+\n+func.func @const_filter_with_q_dq(%arg0: tensor<1x3x4x3xf32>) -> (tensor<1x3x2x2xf32>) {\n+  %cst = \"tf.Const\"() {device = \"\", value = dense<[[[[-0.308480561, 0.122108772], [-0.0622722618, 0.285358578], [0.279975802, -0.227407396]], [[-0.223535746, 0.301872164], [0.45813936, 0.375932634], [-0.142182723, 9.95125505E-4]], [[0.183462933, 0.212702021], [-0.129749238, 0.0611961856], [0.00308316527, -0.486231536]]], [[[0.272826612, 0.382641196], [-0.135114014, 0.115396179], [-0.424618751, -1.311760e-01]], [[0.433140099, 0.15137814], [-0.102797419, 0.288730145], [-0.183163881, 0.0680986494]], [[0.369127393, -0.0638265759], [0.302147657, -0.35623318], [0.204260975, 0.204581305]]]]> : tensor<2x3x3x2xf32>} : () -> tensor<2x3x3x2xf32>\n+  %cst_0 = \"tf.Const\"() {device = \"\", value = dense<[1.000000e-01, 2.000000e-01]> : tensor<2xf32>} : () -> tensor<2xf32>\n+  %0 = \"quantfork.qcast\"(%arg0) : (tensor<1x3x4x3xf32>) -> tensor<1x3x4x3x!quant.uniform<i8:f32, 0.0011764706057660721:-43>>\n+  %1 = \"quantfork.dcast\"(%0) : (tensor<1x3x4x3x!quant.uniform<i8:f32, 0.0011764706057660721:-43>>) -> tensor<1x3x4x3xf32>\n+  %q_w = \"quantfork.qcast\"(%cst) : (tensor<2x3x3x2xf32>) -> tensor<2x3x3x2x!quant.uniform<i8:f32, 0.0125:-24>>\n+  %dq_w = \"quantfork.dcast\"(%q_w) : (tensor<2x3x3x2x!quant.uniform<i8:f32, 0.0125:-24>>) -> tensor<2x3x3x2xf32>\n+  %2 = \"tf.Conv2D\"(%1, %dq_w) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 2, 1], use_cudnn_on_gpu = true} : (tensor<1x3x4x3xf32>, tensor<2x3x3x2xf32>) -> tensor<1x3x2x2xf32>\n+  %3 = \"tf.BiasAdd\"(%2, %cst_0) {data_format = \"NHWC\", device = \"\"} : (tensor<1x3x2x2xf32>, tensor<2xf32>) -> tensor<1x3x2x2xf32>\n+  %4 = \"tf.Relu\"(%3) {device = \"\"} : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2xf32>\n+  %5 = \"quantfork.qcast\"(%4) : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2x!quant.uniform<i8:f32, 0.0027450981093387976:-19>>\n+  %6 = \"quantfork.dcast\"(%5) : (tensor<1x3x2x2x!quant.uniform<i8:f32, 0.0027450981093387976:-19>>) -> tensor<1x3x2x2xf32>\n+  %7 = \"tf.Identity\"(%6) {device = \"\"} : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2xf32>\n+  %8 = \"tf.Identity\"(%7) {device = \"\"} : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2xf32>\n+  return %8 : tensor<1x3x2x2xf32>\n+}\n+\n+// CHECK-LABEL: func @const_filter_with_q_dq\n+// CHECK-DAG: %[[WEIGHT:.*]] = \"tf.Const\"() {{.*}} : () -> tensor<2x3x3x2xf32>\n+// CHECK-DAG: %[[BIAS:.*]] = \"tf.Const\"() {device = \"\", value = dense<[1.000000e-01, 2.000000e-01]> : tensor<2xf32>}\n+// CHECK: %[[Q_W:.*]] = \"quantfork.qcast\"(%[[WEIGHT]])\n+// CHECK: %[[DQ_W:.*]] = \"quantfork.dcast\"(%[[Q_W]])\n+// CHECK: %[[PARTITIONEDCALL_0:.*]] = \"tf.PartitionedCall\"({{.*}}, %[[DQ_W]], %[[BIAS]])\n+// CHECK-SAME: _tfl_quant_trait = \"fully_quantizable\"\n+// CHECK-SAME: f = @composite_conv2d_with_bias_and_relu_fn_1\n+\n+// CHECK-LABEL: func private @composite_conv2d_with_bias_and_relu_fn_1\n",
            "whole_hunk": "@@ -71,3 +71,33 @@ func.func @conv_with_dynamic_channel_dim(%arg0: tensor<1x3x4x?xf32>) -> tensor<*\n // Check that the `attr_map` attribute has been removed.\n // CHECK-NOT: attr_map\n // CHECK-SAME: data_format = \"NHWC\", dilations = [1, 1, 2, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 2, 1]\n+\n+// -----\n+\n+func.func @const_filter_with_q_dq(%arg0: tensor<1x3x4x3xf32>) -> (tensor<1x3x2x2xf32>) {\n+  %cst = \"tf.Const\"() {device = \"\", value = dense<[[[[-0.308480561, 0.122108772], [-0.0622722618, 0.285358578], [0.279975802, -0.227407396]], [[-0.223535746, 0.301872164], [0.45813936, 0.375932634], [-0.142182723, 9.95125505E-4]], [[0.183462933, 0.212702021], [-0.129749238, 0.0611961856], [0.00308316527, -0.486231536]]], [[[0.272826612, 0.382641196], [-0.135114014, 0.115396179], [-0.424618751, -1.311760e-01]], [[0.433140099, 0.15137814], [-0.102797419, 0.288730145], [-0.183163881, 0.0680986494]], [[0.369127393, -0.0638265759], [0.302147657, -0.35623318], [0.204260975, 0.204581305]]]]> : tensor<2x3x3x2xf32>} : () -> tensor<2x3x3x2xf32>\n+  %cst_0 = \"tf.Const\"() {device = \"\", value = dense<[1.000000e-01, 2.000000e-01]> : tensor<2xf32>} : () -> tensor<2xf32>\n+  %0 = \"quantfork.qcast\"(%arg0) : (tensor<1x3x4x3xf32>) -> tensor<1x3x4x3x!quant.uniform<i8:f32, 0.0011764706057660721:-43>>\n+  %1 = \"quantfork.dcast\"(%0) : (tensor<1x3x4x3x!quant.uniform<i8:f32, 0.0011764706057660721:-43>>) -> tensor<1x3x4x3xf32>\n+  %q_w = \"quantfork.qcast\"(%cst) : (tensor<2x3x3x2xf32>) -> tensor<2x3x3x2x!quant.uniform<i8:f32, 0.0125:-24>>\n+  %dq_w = \"quantfork.dcast\"(%q_w) : (tensor<2x3x3x2x!quant.uniform<i8:f32, 0.0125:-24>>) -> tensor<2x3x3x2xf32>\n+  %2 = \"tf.Conv2D\"(%1, %dq_w) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 2, 1], use_cudnn_on_gpu = true} : (tensor<1x3x4x3xf32>, tensor<2x3x3x2xf32>) -> tensor<1x3x2x2xf32>\n+  %3 = \"tf.BiasAdd\"(%2, %cst_0) {data_format = \"NHWC\", device = \"\"} : (tensor<1x3x2x2xf32>, tensor<2xf32>) -> tensor<1x3x2x2xf32>\n+  %4 = \"tf.Relu\"(%3) {device = \"\"} : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2xf32>\n+  %5 = \"quantfork.qcast\"(%4) : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2x!quant.uniform<i8:f32, 0.0027450981093387976:-19>>\n+  %6 = \"quantfork.dcast\"(%5) : (tensor<1x3x2x2x!quant.uniform<i8:f32, 0.0027450981093387976:-19>>) -> tensor<1x3x2x2xf32>\n+  %7 = \"tf.Identity\"(%6) {device = \"\"} : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2xf32>\n+  %8 = \"tf.Identity\"(%7) {device = \"\"} : (tensor<1x3x2x2xf32>) -> tensor<1x3x2x2xf32>\n+  return %8 : tensor<1x3x2x2xf32>\n+}\n+\n+// CHECK-LABEL: func @const_filter_with_q_dq\n+// CHECK-DAG: %[[WEIGHT:.*]] = \"tf.Const\"() {{.*}} : () -> tensor<2x3x3x2xf32>\n+// CHECK-DAG: %[[BIAS:.*]] = \"tf.Const\"() {device = \"\", value = dense<[1.000000e-01, 2.000000e-01]> : tensor<2xf32>}\n+// CHECK: %[[Q_W:.*]] = \"quantfork.qcast\"(%[[WEIGHT]])\n+// CHECK: %[[DQ_W:.*]] = \"quantfork.dcast\"(%[[Q_W]])\n+// CHECK: %[[PARTITIONEDCALL_0:.*]] = \"tf.PartitionedCall\"({{.*}}, %[[DQ_W]], %[[BIAS]])\n+// CHECK-SAME: _tfl_quant_trait = \"fully_quantizable\"\n+// CHECK-SAME: f = @composite_conv2d_with_bias_and_relu_fn_1\n+\n+// CHECK-LABEL: func private @composite_conv2d_with_bias_and_relu_fn_1\n\\ No newline at end of file\n"
        }
    ]
},
{
    "Id": 97,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/5fc08bdd5eef1e5a42d97b10c53536c6cdaf5425",
    "date": "2024-03-21T14:41:30-07:00",
    "message": "Add a capacity check when scheduling a low priority task\n\nPiperOrigin-RevId: 617961589",
    "label": "NO",
    "changes": [
        {
            "name": "batch_kernels_test.cc",
            "path": "tensorflow/core/kernels/batch_kernels_test.cc",
            "patches": [
                {
                    "old_start": 117,
                    "old_length": 28,
                    "new_start": 117,
                    "new_length": 29,
                    "hunk": "@@ -117,28 +117,29 @@ class BatchFunctionTestState : public SharedBatchFunctionTestState {\n \n     std::vector<NodeDefBuilder::NodeOut> inputs(\n         {NodeDefBuilder::NodeOut({\"n1\", 0, DataType::DT_INT64})});\n-    TF_RETURN_IF_ERROR(NodeDefBuilder(\"BatchTPUInput\", \"BatchFunction\")\n-                           .Attr(\"max_batch_size\", 4)\n-                           .Attr(\"num_batch_threads\", 4)\n-                           .Attr(\"allowed_batch_sizes\", {4})\n-                           .Attr(\"batch_timeout_micros\", 5000000)\n-                           .Attr(\"max_enqueued_batches\", 10)\n-                           .Attr(\"low_priority_max_batch_size\",\n-                                 enable_low_priority_queue ? 64 : 0)\n-                           .Attr(\"low_priority_batch_timeout_micros\",\n-                                 enable_low_priority_queue ? 50000000 : 0)\n-                           .Attr(\"low_priority_allowed_batch_sizes\",\n-                                 enable_low_priority_queue ? std::vector<int>{1}\n-                                                           : std::vector<int>())\n-                           .Attr(\"low_priority_max_enqueued_batches\",\n-                                 enable_low_priority_queue ? 100 : 0)\n-                           .Attr(\"Tin\", {DataType::DT_INT64})\n-                           .Input(inputs)\n-                           .Attr(\"Tcaptured\", std::vector<DataType>{})\n-                           .Input(std::vector<NodeDefBuilder::NodeOut>{})\n-                           .Attr(\"Tout\", std::vector<DataType>{DT_INT64})\n-                           .Attr(\"f\", f)\n-                           .Finalize(node_def()));\n+    TF_RETURN_IF_ERROR(\n+        NodeDefBuilder(\"BatchTPUInput\", \"BatchFunction\")\n+            .Attr(\"max_batch_size\", 4)\n+            .Attr(\"num_batch_threads\", 4)\n+            .Attr(\"allowed_batch_sizes\", {4})\n+            .Attr(\"batch_timeout_micros\", 5000000)\n+            .Attr(\"max_enqueued_batches\", 10)\n+            .Attr(\"low_priority_max_batch_size\",\n+                  enable_low_priority_queue ? 64 : 0)\n+            .Attr(\"low_priority_batch_timeout_micros\",\n+                  enable_low_priority_queue ? 50000000 : 0)\n+            .Attr(\"low_priority_allowed_batch_sizes\", enable_low_priority_queue\n+                                                          ? std::vector<int>{64}\n+                                                          : std::vector<int>())\n+            .Attr(\"low_priority_max_enqueued_batches\",\n+                  enable_low_priority_queue ? 100 : 0)\n+            .Attr(\"Tin\", {DataType::DT_INT64})\n+            .Input(inputs)\n+            .Attr(\"Tcaptured\", std::vector<DataType>{})\n+            .Input(std::vector<NodeDefBuilder::NodeOut>{})\n+            .Attr(\"Tout\", std::vector<DataType>{DT_INT64})\n+            .Attr(\"f\", f)\n+            .Finalize(node_def()));\n     return OpsTestBase::InitOp();\n   }\n \n"
                }
            ],
            "whole_deleted": "-    TF_RETURN_IF_ERROR(NodeDefBuilder(\"BatchTPUInput\", \"BatchFunction\")\n-                           .Attr(\"max_batch_size\", 4)\n-                           .Attr(\"num_batch_threads\", 4)\n-                           .Attr(\"allowed_batch_sizes\", {4})\n-                           .Attr(\"batch_timeout_micros\", 5000000)\n-                           .Attr(\"max_enqueued_batches\", 10)\n-                           .Attr(\"low_priority_max_batch_size\",\n-                                 enable_low_priority_queue ? 64 : 0)\n-                           .Attr(\"low_priority_batch_timeout_micros\",\n-                                 enable_low_priority_queue ? 50000000 : 0)\n-                           .Attr(\"low_priority_allowed_batch_sizes\",\n-                                 enable_low_priority_queue ? std::vector<int>{1}\n-                                                           : std::vector<int>())\n-                           .Attr(\"low_priority_max_enqueued_batches\",\n-                                 enable_low_priority_queue ? 100 : 0)\n-                           .Attr(\"Tin\", {DataType::DT_INT64})\n-                           .Input(inputs)\n-                           .Attr(\"Tcaptured\", std::vector<DataType>{})\n-                           .Input(std::vector<NodeDefBuilder::NodeOut>{})\n-                           .Attr(\"Tout\", std::vector<DataType>{DT_INT64})\n-                           .Attr(\"f\", f)\n-                           .Finalize(node_def()));\n",
            "whole_added": "+    TF_RETURN_IF_ERROR(\n+        NodeDefBuilder(\"BatchTPUInput\", \"BatchFunction\")\n+            .Attr(\"max_batch_size\", 4)\n+            .Attr(\"num_batch_threads\", 4)\n+            .Attr(\"allowed_batch_sizes\", {4})\n+            .Attr(\"batch_timeout_micros\", 5000000)\n+            .Attr(\"max_enqueued_batches\", 10)\n+            .Attr(\"low_priority_max_batch_size\",\n+                  enable_low_priority_queue ? 64 : 0)\n+            .Attr(\"low_priority_batch_timeout_micros\",\n+                  enable_low_priority_queue ? 50000000 : 0)\n+            .Attr(\"low_priority_allowed_batch_sizes\", enable_low_priority_queue\n+                                                          ? std::vector<int>{64}\n+                                                          : std::vector<int>())\n+            .Attr(\"low_priority_max_enqueued_batches\",\n+                  enable_low_priority_queue ? 100 : 0)\n+            .Attr(\"Tin\", {DataType::DT_INT64})\n+            .Input(inputs)\n+            .Attr(\"Tcaptured\", std::vector<DataType>{})\n+            .Input(std::vector<NodeDefBuilder::NodeOut>{})\n+            .Attr(\"Tout\", std::vector<DataType>{DT_INT64})\n+            .Attr(\"f\", f)\n+            .Finalize(node_def()));\n",
            "whole_hunk": "@@ -117,28 +117,29 @@ class BatchFunctionTestState : public SharedBatchFunctionTestState {\n \n     std::vector<NodeDefBuilder::NodeOut> inputs(\n         {NodeDefBuilder::NodeOut({\"n1\", 0, DataType::DT_INT64})});\n-    TF_RETURN_IF_ERROR(NodeDefBuilder(\"BatchTPUInput\", \"BatchFunction\")\n-                           .Attr(\"max_batch_size\", 4)\n-                           .Attr(\"num_batch_threads\", 4)\n-                           .Attr(\"allowed_batch_sizes\", {4})\n-                           .Attr(\"batch_timeout_micros\", 5000000)\n-                           .Attr(\"max_enqueued_batches\", 10)\n-                           .Attr(\"low_priority_max_batch_size\",\n-                                 enable_low_priority_queue ? 64 : 0)\n-                           .Attr(\"low_priority_batch_timeout_micros\",\n-                                 enable_low_priority_queue ? 50000000 : 0)\n-                           .Attr(\"low_priority_allowed_batch_sizes\",\n-                                 enable_low_priority_queue ? std::vector<int>{1}\n-                                                           : std::vector<int>())\n-                           .Attr(\"low_priority_max_enqueued_batches\",\n-                                 enable_low_priority_queue ? 100 : 0)\n-                           .Attr(\"Tin\", {DataType::DT_INT64})\n-                           .Input(inputs)\n-                           .Attr(\"Tcaptured\", std::vector<DataType>{})\n-                           .Input(std::vector<NodeDefBuilder::NodeOut>{})\n-                           .Attr(\"Tout\", std::vector<DataType>{DT_INT64})\n-                           .Attr(\"f\", f)\n-                           .Finalize(node_def()));\n+    TF_RETURN_IF_ERROR(\n+        NodeDefBuilder(\"BatchTPUInput\", \"BatchFunction\")\n+            .Attr(\"max_batch_size\", 4)\n+            .Attr(\"num_batch_threads\", 4)\n+            .Attr(\"allowed_batch_sizes\", {4})\n+            .Attr(\"batch_timeout_micros\", 5000000)\n+            .Attr(\"max_enqueued_batches\", 10)\n+            .Attr(\"low_priority_max_batch_size\",\n+                  enable_low_priority_queue ? 64 : 0)\n+            .Attr(\"low_priority_batch_timeout_micros\",\n+                  enable_low_priority_queue ? 50000000 : 0)\n+            .Attr(\"low_priority_allowed_batch_sizes\", enable_low_priority_queue\n+                                                          ? std::vector<int>{64}\n+                                                          : std::vector<int>())\n+            .Attr(\"low_priority_max_enqueued_batches\",\n+                  enable_low_priority_queue ? 100 : 0)\n+            .Attr(\"Tin\", {DataType::DT_INT64})\n+            .Input(inputs)\n+            .Attr(\"Tcaptured\", std::vector<DataType>{})\n+            .Input(std::vector<NodeDefBuilder::NodeOut>{})\n+            .Attr(\"Tout\", std::vector<DataType>{DT_INT64})\n+            .Attr(\"f\", f)\n+            .Finalize(node_def()));\n     return OpsTestBase::InitOp();\n   }\n \n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/core/kernels/batching_util/BUILD",
            "patches": [
                {
                    "old_start": 160,
                    "old_length": 6,
                    "new_start": 160,
                    "new_length": 7,
                    "hunk": "@@ -160,6 +160,7 @@ cc_library(\n         \"//tensorflow/core/profiler/lib:traceme_encode\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/time\",\n         \"@local_tsl//tsl/platform:criticality\",\n         \"@local_tsl//tsl/platform:errors\",\n"
                },
                {
                    "old_start": 180,
                    "old_length": 6,
                    "new_start": 181,
                    "new_length": 7,
                    "hunk": "@@ -180,6 +181,7 @@ cc_library(\n         \"//tensorflow/core/profiler/lib:traceme_encode\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/time\",\n         \"@local_tsl//tsl/platform:criticality\",\n     ],\n"
                },
                {
                    "old_start": 201,
                    "old_length": 6,
                    "new_start": 203,
                    "new_length": 7,
                    "hunk": "@@ -201,6 +203,7 @@ tf_cc_test(\n         \"//tensorflow/core/platform:status_matchers\",\n         \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/container:fixed_array\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/time\",\n     ],\n )\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/status\",\n",
            "whole_hunk": "@@ -160,6 +160,7 @@ cc_library(\n         \"//tensorflow/core/profiler/lib:traceme_encode\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/time\",\n         \"@local_tsl//tsl/platform:criticality\",\n         \"@local_tsl//tsl/platform:errors\",\n@@ -180,6 +181,7 @@ cc_library(\n         \"//tensorflow/core/profiler/lib:traceme_encode\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/time\",\n         \"@local_tsl//tsl/platform:criticality\",\n     ],\n@@ -201,6 +203,7 @@ tf_cc_test(\n         \"//tensorflow/core/platform:status_matchers\",\n         \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/container:fixed_array\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/time\",\n     ],\n )\n"
        },
        {
            "name": "batch_resource_base.cc",
            "path": "tensorflow/core/kernels/batching_util/batch_resource_base.cc",
            "patches": [
                {
                    "old_start": 532,
                    "old_length": 6,
                    "new_start": 532,
                    "new_length": 13,
                    "hunk": "@@ -532,6 +532,13 @@ BatchResourceBase::GetBatcherQueueOptions(\n       low_priority_max_enqueued_batches;\n   batcher_queue_options.low_priority_queue_options.batch_timeout_micros =\n       low_priority_batch_timeout_micros;\n+  if (low_priority_allowed_batch_sizes.empty()) {\n+    batcher_queue_options.low_priority_queue_options.max_execution_batch_size =\n+        low_priority_max_batch_size;\n+  } else {\n+    batcher_queue_options.low_priority_queue_options.max_execution_batch_size =\n+        *low_priority_allowed_batch_sizes.rbegin();\n+  }\n   batcher_queue_options.enable_large_batch_splitting =\n       enable_large_batch_splitting;\n   if (enable_large_batch_splitting) {\n"
                },
                {
                    "old_start": 554,
                    "old_length": 14,
                    "new_start": 561,
                    "new_length": 6,
                    "hunk": "@@ -554,14 +561,6 @@ BatchResourceBase::GetBatcherQueueOptions(\n           .max_execution_batch_size = *allowed_batch_sizes.rbegin();\n       batcher_queue_options.allowed_batch_sizes = allowed_batch_sizes;\n     }\n-    if (low_priority_allowed_batch_sizes.empty()) {\n-      batcher_queue_options.low_priority_queue_options\n-          .max_execution_batch_size = low_priority_max_batch_size;\n-    } else {\n-      batcher_queue_options.low_priority_queue_options\n-          .max_execution_batch_size =\n-          *low_priority_allowed_batch_sizes.rbegin();\n-    }\n   }\n   batcher_queue_options.disable_padding = disable_padding;\n \n"
                }
            ],
            "whole_deleted": "-    if (low_priority_allowed_batch_sizes.empty()) {\n-      batcher_queue_options.low_priority_queue_options\n-          .max_execution_batch_size = low_priority_max_batch_size;\n-    } else {\n-      batcher_queue_options.low_priority_queue_options\n-          .max_execution_batch_size =\n-          *low_priority_allowed_batch_sizes.rbegin();\n-    }\n",
            "whole_added": "+  if (low_priority_allowed_batch_sizes.empty()) {\n+    batcher_queue_options.low_priority_queue_options.max_execution_batch_size =\n+        low_priority_max_batch_size;\n+  } else {\n+    batcher_queue_options.low_priority_queue_options.max_execution_batch_size =\n+        *low_priority_allowed_batch_sizes.rbegin();\n+  }\n",
            "whole_hunk": "@@ -532,6 +532,13 @@ BatchResourceBase::GetBatcherQueueOptions(\n       low_priority_max_enqueued_batches;\n   batcher_queue_options.low_priority_queue_options.batch_timeout_micros =\n       low_priority_batch_timeout_micros;\n+  if (low_priority_allowed_batch_sizes.empty()) {\n+    batcher_queue_options.low_priority_queue_options.max_execution_batch_size =\n+        low_priority_max_batch_size;\n+  } else {\n+    batcher_queue_options.low_priority_queue_options.max_execution_batch_size =\n+        *low_priority_allowed_batch_sizes.rbegin();\n+  }\n   batcher_queue_options.enable_large_batch_splitting =\n       enable_large_batch_splitting;\n   if (enable_large_batch_splitting) {\n@@ -554,14 +561,6 @@ BatchResourceBase::GetBatcherQueueOptions(\n           .max_execution_batch_size = *allowed_batch_sizes.rbegin();\n       batcher_queue_options.allowed_batch_sizes = allowed_batch_sizes;\n     }\n-    if (low_priority_allowed_batch_sizes.empty()) {\n-      batcher_queue_options.low_priority_queue_options\n-          .max_execution_batch_size = low_priority_max_batch_size;\n-    } else {\n-      batcher_queue_options.low_priority_queue_options\n-          .max_execution_batch_size =\n-          *low_priority_allowed_batch_sizes.rbegin();\n-    }\n   }\n   batcher_queue_options.disable_padding = disable_padding;\n \n"
        },
        {
            "name": "shared_batch_scheduler.h",
            "path": "tensorflow/core/kernels/batching_util/shared_batch_scheduler.h",
            "patches": [
                {
                    "old_start": 31,
                    "old_length": 6,
                    "new_start": 31,
                    "new_length": 7,
                    "hunk": "@@ -31,6 +31,7 @@ limitations under the License.\n \n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/time/clock.h\"\n #include \"tensorflow/core/kernels/batching_util/batch_input_task.h\"\n #include \"tensorflow/core/kernels/batching_util/batch_scheduler.h\"\n"
                },
                {
                    "old_start": 475,
                    "old_length": 6,
                    "new_start": 476,
                    "new_length": 15,
                    "hunk": "@@ -475,6 +476,15 @@ class Queue {\n   Status ValidateBatchTaskQueueCapacity(TaskType* task) const\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n+  // Returns an error if the low priority task queue doesn't have capacity for\n+  // this task using the low priority batch options. Since the low priority\n+  // tasks are not batched until they get scheduled, it only checks that a\n+  // single task does not it exceed input batch size limit and the total size of\n+  // the tasks in the queue does not exceed the max batch size * max enqueued\n+  // batch sizes.\n+  Status ValidateLowPriorityTaskQueueCapacity(const TaskType& task) const\n+      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n+\n   // The task size of the last batch in the queue.\n   size_t tail_batch_task_size() const TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n"
                },
                {
                    "old_start": 884,
                    "old_length": 11,
                    "new_start": 894,
                    "new_length": 6,
                    "hunk": "@@ -884,11 +894,6 @@ Queue<TaskType>::~Queue() {\n \n template <typename TaskType>\n Status Queue<TaskType>::Schedule(std::unique_ptr<TaskType>* task) {\n-  if ((*task)->size() > options_.input_batch_size_limit) {\n-    return errors::InvalidArgument(\"Task size \", (*task)->size(),\n-                                   \" is larger than maximum input batch size \",\n-                                   options_.input_batch_size_limit);\n-  }\n   if (options_.enable_lazy_split) {\n     return ScheduleWithLazySplit(std::move(task));\n   }\n"
                },
                {
                    "old_start": 1048,
                    "old_length": 6,
                    "new_start": 1053,
                    "new_length": 7,
                    "hunk": "@@ -1048,6 +1053,7 @@ Status Queue<TaskType>::ScheduleWithoutOrEagerSplit(\n     if (IsLowPriorityTask(task)) {\n       // Insert the task to the low priority task queue instead of the high\n       // priority batch queue below.\n+      TF_RETURN_IF_ERROR(ValidateLowPriorityTaskQueueCapacity(**task));\n       low_priority_tasks_.AddTask(std::move(*task), env_->NowMicros());\n     } else {\n       TF_RETURN_IF_ERROR(ScheduleWithoutOrEagerSplitImpl(task));\n"
                },
                {
                    "old_start": 1084,
                    "old_length": 7,
                    "new_start": 1090,
                    "new_length": 7,
                    "hunk": "@@ -1084,7 +1090,7 @@ size_t Queue<TaskType>::NumEnqueuedTasks() const {\n   for (const auto& batch : GetBatches()) {\n     num_enqueued_tasks += batch->num_tasks();\n   }\n-  return num_enqueued_tasks;\n+  return num_enqueued_tasks + low_priority_tasks_.num_tasks();\n }\n \n template <typename TaskType>\n"
                },
                {
                    "old_start": 1109,
                    "old_length": 6,
                    "new_start": 1115,
                    "new_length": 14,
                    "hunk": "@@ -1109,6 +1115,14 @@ size_t Queue<TaskType>::SchedulingCapacityInternal() const {\n \n template <typename TaskType>\n Status Queue<TaskType>::ValidateBatchTaskQueueCapacity(TaskType* task) const {\n+  // Check if the task size is larger than the batch size limit, regardless of\n+  // the batch capacity.\n+  if (task->size() > options_.input_batch_size_limit) {\n+    return absl::InvalidArgumentError(absl::StrFormat(\n+        \"Task size %d is larger than maximum input batch size %d\", task->size(),\n+        options_.input_batch_size_limit));\n+  }\n+\n   // Queue creation requires that `enable_large_batch_splitting` is true\n   // when `enable_lazy_split` is true, so this covers both eager split and\n   // lazy split.\n"
                },
                {
                    "old_start": 1148,
                    "old_length": 6,
                    "new_start": 1162,
                    "new_length": 36,
                    "hunk": "@@ -1148,6 +1162,36 @@ Status Queue<TaskType>::ValidateBatchTaskQueueCapacity(TaskType* task) const {\n   return absl::OkStatus();\n }\n \n+template <typename TaskType>\n+Status Queue<TaskType>::ValidateLowPriorityTaskQueueCapacity(\n+    const TaskType& task) const {\n+  // Unlike the high priority batch capacity validation where having only\n+  // input_batch_size_limit without max_execution_batch_size is allowed, it\n+  // doesn't have the backward compatibility check and always assume that\n+  // max_execution_batch_size is present.\n+  if (task.size() >\n+      options_.low_priority_queue_options.max_execution_batch_size) {\n+    return absl::UnavailableError(absl::StrFormat(\n+        \"The low priority task queue to which this task was submitted has \"\n+        \"max_execution_batch_size=%d and the task size is %d\",\n+        options_.low_priority_queue_options.max_execution_batch_size,\n+        task.size()));\n+  }\n+  if (low_priority_tasks_.size() + task.size() >\n+      options_.low_priority_queue_options.max_enqueued_batches *\n+          options_.low_priority_queue_options.max_execution_batch_size) {\n+    return absl::UnavailableError(absl::StrFormat(\n+        \"The low priority task queue to which this task was submitted does not \"\n+        \"have the capcity to handle this task; currently the low priority \"\n+        \"queue has %d tasks enqueued and the submitted task size is %d while \"\n+        \"max_enqueued_batches=%d and max_execution_batch_size=%d\",\n+        low_priority_tasks_.size(), task.size(),\n+        options_.low_priority_queue_options.max_enqueued_batches,\n+        options_.low_priority_queue_options.max_execution_batch_size));\n+  }\n+  return absl::OkStatus();\n+}\n+\n template <typename TaskType>\n std::unique_ptr<Batch<TaskType>>\n Queue<TaskType>::ScheduleBatchWithEagerSplit() {\n"
                }
            ],
            "whole_deleted": "-  if ((*task)->size() > options_.input_batch_size_limit) {\n-    return errors::InvalidArgument(\"Task size \", (*task)->size(),\n-                                   \" is larger than maximum input batch size \",\n-                                   options_.input_batch_size_limit);\n-  }\n-  return num_enqueued_tasks;\n",
            "whole_added": "+#include \"absl/strings/str_format.h\"\n+  // Returns an error if the low priority task queue doesn't have capacity for\n+  // this task using the low priority batch options. Since the low priority\n+  // tasks are not batched until they get scheduled, it only checks that a\n+  // single task does not it exceed input batch size limit and the total size of\n+  // the tasks in the queue does not exceed the max batch size * max enqueued\n+  // batch sizes.\n+  Status ValidateLowPriorityTaskQueueCapacity(const TaskType& task) const\n+      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n+\n+      TF_RETURN_IF_ERROR(ValidateLowPriorityTaskQueueCapacity(**task));\n+  return num_enqueued_tasks + low_priority_tasks_.num_tasks();\n+  // Check if the task size is larger than the batch size limit, regardless of\n+  // the batch capacity.\n+  if (task->size() > options_.input_batch_size_limit) {\n+    return absl::InvalidArgumentError(absl::StrFormat(\n+        \"Task size %d is larger than maximum input batch size %d\", task->size(),\n+        options_.input_batch_size_limit));\n+  }\n+\n+template <typename TaskType>\n+Status Queue<TaskType>::ValidateLowPriorityTaskQueueCapacity(\n+    const TaskType& task) const {\n+  // Unlike the high priority batch capacity validation where having only\n+  // input_batch_size_limit without max_execution_batch_size is allowed, it\n+  // doesn't have the backward compatibility check and always assume that\n+  // max_execution_batch_size is present.\n+  if (task.size() >\n+      options_.low_priority_queue_options.max_execution_batch_size) {\n+    return absl::UnavailableError(absl::StrFormat(\n+        \"The low priority task queue to which this task was submitted has \"\n+        \"max_execution_batch_size=%d and the task size is %d\",\n+        options_.low_priority_queue_options.max_execution_batch_size,\n+        task.size()));\n+  }\n+  if (low_priority_tasks_.size() + task.size() >\n+      options_.low_priority_queue_options.max_enqueued_batches *\n+          options_.low_priority_queue_options.max_execution_batch_size) {\n+    return absl::UnavailableError(absl::StrFormat(\n+        \"The low priority task queue to which this task was submitted does not \"\n+        \"have the capcity to handle this task; currently the low priority \"\n+        \"queue has %d tasks enqueued and the submitted task size is %d while \"\n+        \"max_enqueued_batches=%d and max_execution_batch_size=%d\",\n+        low_priority_tasks_.size(), task.size(),\n+        options_.low_priority_queue_options.max_enqueued_batches,\n+        options_.low_priority_queue_options.max_execution_batch_size));\n+  }\n+  return absl::OkStatus();\n+}\n+\n",
            "whole_hunk": "@@ -31,6 +31,7 @@ limitations under the License.\n \n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/time/clock.h\"\n #include \"tensorflow/core/kernels/batching_util/batch_input_task.h\"\n #include \"tensorflow/core/kernels/batching_util/batch_scheduler.h\"\n@@ -475,6 +476,15 @@ class Queue {\n   Status ValidateBatchTaskQueueCapacity(TaskType* task) const\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n+  // Returns an error if the low priority task queue doesn't have capacity for\n+  // this task using the low priority batch options. Since the low priority\n+  // tasks are not batched until they get scheduled, it only checks that a\n+  // single task does not it exceed input batch size limit and the total size of\n+  // the tasks in the queue does not exceed the max batch size * max enqueued\n+  // batch sizes.\n+  Status ValidateLowPriorityTaskQueueCapacity(const TaskType& task) const\n+      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n+\n   // The task size of the last batch in the queue.\n   size_t tail_batch_task_size() const TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n@@ -884,11 +894,6 @@ Queue<TaskType>::~Queue() {\n \n template <typename TaskType>\n Status Queue<TaskType>::Schedule(std::unique_ptr<TaskType>* task) {\n-  if ((*task)->size() > options_.input_batch_size_limit) {\n-    return errors::InvalidArgument(\"Task size \", (*task)->size(),\n-                                   \" is larger than maximum input batch size \",\n-                                   options_.input_batch_size_limit);\n-  }\n   if (options_.enable_lazy_split) {\n     return ScheduleWithLazySplit(std::move(task));\n   }\n@@ -1048,6 +1053,7 @@ Status Queue<TaskType>::ScheduleWithoutOrEagerSplit(\n     if (IsLowPriorityTask(task)) {\n       // Insert the task to the low priority task queue instead of the high\n       // priority batch queue below.\n+      TF_RETURN_IF_ERROR(ValidateLowPriorityTaskQueueCapacity(**task));\n       low_priority_tasks_.AddTask(std::move(*task), env_->NowMicros());\n     } else {\n       TF_RETURN_IF_ERROR(ScheduleWithoutOrEagerSplitImpl(task));\n@@ -1084,7 +1090,7 @@ size_t Queue<TaskType>::NumEnqueuedTasks() const {\n   for (const auto& batch : GetBatches()) {\n     num_enqueued_tasks += batch->num_tasks();\n   }\n-  return num_enqueued_tasks;\n+  return num_enqueued_tasks + low_priority_tasks_.num_tasks();\n }\n \n template <typename TaskType>\n@@ -1109,6 +1115,14 @@ size_t Queue<TaskType>::SchedulingCapacityInternal() const {\n \n template <typename TaskType>\n Status Queue<TaskType>::ValidateBatchTaskQueueCapacity(TaskType* task) const {\n+  // Check if the task size is larger than the batch size limit, regardless of\n+  // the batch capacity.\n+  if (task->size() > options_.input_batch_size_limit) {\n+    return absl::InvalidArgumentError(absl::StrFormat(\n+        \"Task size %d is larger than maximum input batch size %d\", task->size(),\n+        options_.input_batch_size_limit));\n+  }\n+\n   // Queue creation requires that `enable_large_batch_splitting` is true\n   // when `enable_lazy_split` is true, so this covers both eager split and\n   // lazy split.\n@@ -1148,6 +1162,36 @@ Status Queue<TaskType>::ValidateBatchTaskQueueCapacity(TaskType* task) const {\n   return absl::OkStatus();\n }\n \n+template <typename TaskType>\n+Status Queue<TaskType>::ValidateLowPriorityTaskQueueCapacity(\n+    const TaskType& task) const {\n+  // Unlike the high priority batch capacity validation where having only\n+  // input_batch_size_limit without max_execution_batch_size is allowed, it\n+  // doesn't have the backward compatibility check and always assume that\n+  // max_execution_batch_size is present.\n+  if (task.size() >\n+      options_.low_priority_queue_options.max_execution_batch_size) {\n+    return absl::UnavailableError(absl::StrFormat(\n+        \"The low priority task queue to which this task was submitted has \"\n+        \"max_execution_batch_size=%d and the task size is %d\",\n+        options_.low_priority_queue_options.max_execution_batch_size,\n+        task.size()));\n+  }\n+  if (low_priority_tasks_.size() + task.size() >\n+      options_.low_priority_queue_options.max_enqueued_batches *\n+          options_.low_priority_queue_options.max_execution_batch_size) {\n+    return absl::UnavailableError(absl::StrFormat(\n+        \"The low priority task queue to which this task was submitted does not \"\n+        \"have the capcity to handle this task; currently the low priority \"\n+        \"queue has %d tasks enqueued and the submitted task size is %d while \"\n+        \"max_enqueued_batches=%d and max_execution_batch_size=%d\",\n+        low_priority_tasks_.size(), task.size(),\n+        options_.low_priority_queue_options.max_enqueued_batches,\n+        options_.low_priority_queue_options.max_execution_batch_size));\n+  }\n+  return absl::OkStatus();\n+}\n+\n template <typename TaskType>\n std::unique_ptr<Batch<TaskType>>\n Queue<TaskType>::ScheduleBatchWithEagerSplit() {\n"
        },
        {
            "name": "shared_batch_scheduler_test.cc",
            "path": "tensorflow/core/kernels/batching_util/shared_batch_scheduler_test.cc",
            "patches": [
                {
                    "old_start": 25,
                    "old_length": 6,
                    "new_start": 25,
                    "new_length": 7,
                    "hunk": "@@ -25,6 +25,7 @@ limitations under the License.\n \n #include \"absl/base/call_once.h\"\n #include \"absl/container/fixed_array.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/time/time.h\"\n #include \"tensorflow/core/kernels/batching_util/batch_scheduler.h\"\n #include \"tensorflow/core/kernels/batching_util/fake_clock_env.h\"\n"
                },
                {
                    "old_start": 38,
                    "old_length": 6,
                    "new_start": 39,
                    "new_length": 7,
                    "hunk": "@@ -38,6 +39,7 @@ limitations under the License.\n #include \"tensorflow/core/platform/test.h\"\n #include \"tensorflow/core/platform/test_benchmark.h\"\n #include \"tensorflow/core/protobuf/error_codes.pb.h\"\n+#include \"tsl/lib/core/status_test_util.h\"\n #include \"tsl/platform/criticality.h\"\n \n namespace tensorflow {\n"
                },
                {
                    "old_start": 1051,
                    "old_length": 6,
                    "new_start": 1053,
                    "new_length": 102,
                    "hunk": "@@ -1051,6 +1053,102 @@ INSTANTIATE_TEST_SUITE_P(\n \n using SharedBatchSchedulerPriorityTest = SharedBatchSchedulerTest;\n \n+TEST_P(SharedBatchSchedulerPriorityTest,\n+       InvalidLowPriorityTaskWithPriorityQueueEnabled) {\n+  bool queue_callback_called = false;\n+  auto queue_callback = [&queue_callback_called](\n+                            std::unique_ptr<Batch<FakeTask>> batch,\n+                            std::vector<std::unique_ptr<FakeTask>> tasks) {\n+    queue_callback_called = true;\n+  };\n+\n+  {\n+    std::shared_ptr<Scheduler> scheduler =\n+        CreateSharedBatchScheduler(/*num_batch_threads=*/3);\n+\n+    QueueOptions queue_options = CreateQueueOptions(\n+        /*max_execution_batch_size=*/100, /*input_batch_size_limit=*/100,\n+        /*batch_timeout_micros=*/1 * 1000 * 1000, /*max_enqueued_batches=*/2,\n+        /*enable_priority_queue=*/true);\n+    queue_options.low_priority_queue_options.max_execution_batch_size = 1;\n+    queue_options.low_priority_queue_options.batch_timeout_micros =\n+        1 * 1000 * 1000;\n+    queue_options.low_priority_queue_options.input_batch_size_limit = 1;\n+    queue_options.low_priority_queue_options.max_enqueued_batches = 2;\n+    std::unique_ptr<Queue> queue =\n+        CreateQueue(scheduler, queue_options, queue_callback);\n+\n+    EXPECT_THAT(\n+        ScheduleTask(10, queue.get(),\n+                     tsl::criticality::Criticality::kSheddablePlus),\n+        testing::StatusIs(\n+            absl::StatusCode::kUnavailable,\n+            HasSubstr(\n+                \"The low priority task queue to which this task was submitted \"\n+                \"has max_execution_batch_size=1 and the task size is 10\")));\n+  }\n+  EXPECT_FALSE(queue_callback_called);\n+}\n+\n+TEST_P(SharedBatchSchedulerPriorityTest,\n+       InvalidLowPriorityTaskWithQueueFullWithPriorityQueueEnabled) {\n+  Notification processing, proceed;\n+  auto queue_callback = [&processing, &proceed](\n+                            std::unique_ptr<Batch<FakeTask>> batch,\n+                            std::vector<std::unique_ptr<FakeTask>> tasks) {\n+    if (!processing.HasBeenNotified()) {\n+      processing.Notify();\n+    }\n+    proceed.WaitForNotification();\n+  };\n+\n+  std::shared_ptr<Scheduler> scheduler =\n+      CreateSharedBatchScheduler(/*num_batch_threads=*/1);\n+\n+  QueueOptions queue_options = CreateQueueOptions(\n+      /*max_execution_batch_size=*/100, /*input_batch_size_limit=*/100,\n+      /*batch_timeout_micros=*/1 * 1000 * 1000, /*max_enqueued_batches=*/2,\n+      /*enable_priority_queue=*/true);\n+  queue_options.low_priority_queue_options.max_execution_batch_size = 10;\n+  queue_options.low_priority_queue_options.batch_timeout_micros =\n+      1 * 1000 * 1000;\n+  queue_options.low_priority_queue_options.input_batch_size_limit = 10;\n+  queue_options.low_priority_queue_options.max_enqueued_batches = 2;\n+  std::unique_ptr<Queue> queue =\n+      CreateQueue(scheduler, queue_options, queue_callback);\n+\n+  // Schedule one task and block the thread.\n+  TF_ASSERT_OK(ScheduleTask(5, queue.get(),\n+                            tsl::criticality::Criticality::kCriticalPlus));\n+  TF_ASSERT_OK(ScheduleTask(5, queue.get(),\n+                            tsl::criticality::Criticality::kSheddablePlus));\n+  processing.WaitForNotification();\n+  ASSERT_EQ(0, queue->NumEnqueuedTasks());\n+\n+  // Adding tasks up to size 20 should be fine.\n+  TF_ASSERT_OK(ScheduleTask(10, queue.get(),\n+                            tsl::criticality::Criticality::kSheddablePlus));\n+  ASSERT_EQ(1, queue->NumEnqueuedTasks());\n+  TF_ASSERT_OK(ScheduleTask(10, queue.get(),\n+                            tsl::criticality::Criticality::kSheddablePlus));\n+  ASSERT_EQ(2, queue->NumEnqueuedTasks());\n+\n+  // Adding one more task should result in an error.\n+  EXPECT_THAT(\n+      ScheduleTask(1, queue.get(),\n+                   tsl::criticality::Criticality::kSheddablePlus),\n+      testing::StatusIs(\n+          absl::StatusCode::kUnavailable,\n+          HasSubstr(\"The low priority task queue to which this task was \"\n+                    \"submitted does not have the capcity to handle this task; \"\n+                    \"currently the low priority queue has 20 tasks enqueued \"\n+                    \"and the submitted task size is 1 while \"\n+                    \"max_enqueued_batches=2 and max_execution_batch_size=10\")));\n+\n+  // Unblock the thread.\n+  proceed.Notify();\n+}\n+\n TEST_P(SharedBatchSchedulerPriorityTest,\n        CallbackWithTaskVectorOkWithPriorityQueueEnabledWithPrioritySet) {\n   bool queue_callback_called = false;\n"
                },
                {
                    "old_start": 1070,
                    "old_length": 11,
                    "new_start": 1168,
                    "new_length": 16,
                    "hunk": "@@ -1070,11 +1168,16 @@ TEST_P(SharedBatchSchedulerPriorityTest,\n     std::shared_ptr<Scheduler> scheduler =\n         CreateSharedBatchScheduler(/*num_batch_threads=*/3);\n \n-    // Create two queues.\n-    const QueueOptions queue_options = CreateQueueOptions(\n+    // Create a queue with the priority queue enabled.\n+    QueueOptions queue_options = CreateQueueOptions(\n         /*max_execution_batch_size=*/10, /*input_batch_size_limit=*/10,\n         /*batch_timeout_micros=*/1 * 1000 * 1000, /*max_enqueued_batches=*/2,\n         /*enable_priority_queue=*/true);\n+    queue_options.low_priority_queue_options.max_execution_batch_size = 10;\n+    queue_options.low_priority_queue_options.batch_timeout_micros =\n+        1 * 1000 * 1000;\n+    queue_options.low_priority_queue_options.input_batch_size_limit = 10;\n+    queue_options.low_priority_queue_options.max_enqueued_batches = 2;\n     std::unique_ptr<Queue> queue =\n         CreateQueue(scheduler, queue_options, queue_callback);\n "
                }
            ],
            "whole_deleted": "-    // Create two queues.\n-    const QueueOptions queue_options = CreateQueueOptions(\n",
            "whole_added": "+#include \"absl/status/status.h\"\n+#include \"tsl/lib/core/status_test_util.h\"\n+TEST_P(SharedBatchSchedulerPriorityTest,\n+       InvalidLowPriorityTaskWithPriorityQueueEnabled) {\n+  bool queue_callback_called = false;\n+  auto queue_callback = [&queue_callback_called](\n+                            std::unique_ptr<Batch<FakeTask>> batch,\n+                            std::vector<std::unique_ptr<FakeTask>> tasks) {\n+    queue_callback_called = true;\n+  };\n+\n+  {\n+    std::shared_ptr<Scheduler> scheduler =\n+        CreateSharedBatchScheduler(/*num_batch_threads=*/3);\n+\n+    QueueOptions queue_options = CreateQueueOptions(\n+        /*max_execution_batch_size=*/100, /*input_batch_size_limit=*/100,\n+        /*batch_timeout_micros=*/1 * 1000 * 1000, /*max_enqueued_batches=*/2,\n+        /*enable_priority_queue=*/true);\n+    queue_options.low_priority_queue_options.max_execution_batch_size = 1;\n+    queue_options.low_priority_queue_options.batch_timeout_micros =\n+        1 * 1000 * 1000;\n+    queue_options.low_priority_queue_options.input_batch_size_limit = 1;\n+    queue_options.low_priority_queue_options.max_enqueued_batches = 2;\n+    std::unique_ptr<Queue> queue =\n+        CreateQueue(scheduler, queue_options, queue_callback);\n+\n+    EXPECT_THAT(\n+        ScheduleTask(10, queue.get(),\n+                     tsl::criticality::Criticality::kSheddablePlus),\n+        testing::StatusIs(\n+            absl::StatusCode::kUnavailable,\n+            HasSubstr(\n+                \"The low priority task queue to which this task was submitted \"\n+                \"has max_execution_batch_size=1 and the task size is 10\")));\n+  }\n+  EXPECT_FALSE(queue_callback_called);\n+}\n+\n+TEST_P(SharedBatchSchedulerPriorityTest,\n+       InvalidLowPriorityTaskWithQueueFullWithPriorityQueueEnabled) {\n+  Notification processing, proceed;\n+  auto queue_callback = [&processing, &proceed](\n+                            std::unique_ptr<Batch<FakeTask>> batch,\n+                            std::vector<std::unique_ptr<FakeTask>> tasks) {\n+    if (!processing.HasBeenNotified()) {\n+      processing.Notify();\n+    }\n+    proceed.WaitForNotification();\n+  };\n+\n+  std::shared_ptr<Scheduler> scheduler =\n+      CreateSharedBatchScheduler(/*num_batch_threads=*/1);\n+\n+  QueueOptions queue_options = CreateQueueOptions(\n+      /*max_execution_batch_size=*/100, /*input_batch_size_limit=*/100,\n+      /*batch_timeout_micros=*/1 * 1000 * 1000, /*max_enqueued_batches=*/2,\n+      /*enable_priority_queue=*/true);\n+  queue_options.low_priority_queue_options.max_execution_batch_size = 10;\n+  queue_options.low_priority_queue_options.batch_timeout_micros =\n+      1 * 1000 * 1000;\n+  queue_options.low_priority_queue_options.input_batch_size_limit = 10;\n+  queue_options.low_priority_queue_options.max_enqueued_batches = 2;\n+  std::unique_ptr<Queue> queue =\n+      CreateQueue(scheduler, queue_options, queue_callback);\n+\n+  // Schedule one task and block the thread.\n+  TF_ASSERT_OK(ScheduleTask(5, queue.get(),\n+                            tsl::criticality::Criticality::kCriticalPlus));\n+  TF_ASSERT_OK(ScheduleTask(5, queue.get(),\n+                            tsl::criticality::Criticality::kSheddablePlus));\n+  processing.WaitForNotification();\n+  ASSERT_EQ(0, queue->NumEnqueuedTasks());\n+\n+  // Adding tasks up to size 20 should be fine.\n+  TF_ASSERT_OK(ScheduleTask(10, queue.get(),\n+                            tsl::criticality::Criticality::kSheddablePlus));\n+  ASSERT_EQ(1, queue->NumEnqueuedTasks());\n+  TF_ASSERT_OK(ScheduleTask(10, queue.get(),\n+                            tsl::criticality::Criticality::kSheddablePlus));\n+  ASSERT_EQ(2, queue->NumEnqueuedTasks());\n+\n+  // Adding one more task should result in an error.\n+  EXPECT_THAT(\n+      ScheduleTask(1, queue.get(),\n+                   tsl::criticality::Criticality::kSheddablePlus),\n+      testing::StatusIs(\n+          absl::StatusCode::kUnavailable,\n+          HasSubstr(\"The low priority task queue to which this task was \"\n+                    \"submitted does not have the capcity to handle this task; \"\n+                    \"currently the low priority queue has 20 tasks enqueued \"\n+                    \"and the submitted task size is 1 while \"\n+                    \"max_enqueued_batches=2 and max_execution_batch_size=10\")));\n+\n+  // Unblock the thread.\n+  proceed.Notify();\n+}\n+\n+    // Create a queue with the priority queue enabled.\n+    QueueOptions queue_options = CreateQueueOptions(\n+    queue_options.low_priority_queue_options.max_execution_batch_size = 10;\n+    queue_options.low_priority_queue_options.batch_timeout_micros =\n+        1 * 1000 * 1000;\n+    queue_options.low_priority_queue_options.input_batch_size_limit = 10;\n+    queue_options.low_priority_queue_options.max_enqueued_batches = 2;\n",
            "whole_hunk": "@@ -25,6 +25,7 @@ limitations under the License.\n \n #include \"absl/base/call_once.h\"\n #include \"absl/container/fixed_array.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/time/time.h\"\n #include \"tensorflow/core/kernels/batching_util/batch_scheduler.h\"\n #include \"tensorflow/core/kernels/batching_util/fake_clock_env.h\"\n@@ -38,6 +39,7 @@ limitations under the License.\n #include \"tensorflow/core/platform/test.h\"\n #include \"tensorflow/core/platform/test_benchmark.h\"\n #include \"tensorflow/core/protobuf/error_codes.pb.h\"\n+#include \"tsl/lib/core/status_test_util.h\"\n #include \"tsl/platform/criticality.h\"\n \n namespace tensorflow {\n@@ -1051,6 +1053,102 @@ INSTANTIATE_TEST_SUITE_P(\n \n using SharedBatchSchedulerPriorityTest = SharedBatchSchedulerTest;\n \n+TEST_P(SharedBatchSchedulerPriorityTest,\n+       InvalidLowPriorityTaskWithPriorityQueueEnabled) {\n+  bool queue_callback_called = false;\n+  auto queue_callback = [&queue_callback_called](\n+                            std::unique_ptr<Batch<FakeTask>> batch,\n+                            std::vector<std::unique_ptr<FakeTask>> tasks) {\n+    queue_callback_called = true;\n+  };\n+\n+  {\n+    std::shared_ptr<Scheduler> scheduler =\n+        CreateSharedBatchScheduler(/*num_batch_threads=*/3);\n+\n+    QueueOptions queue_options = CreateQueueOptions(\n+        /*max_execution_batch_size=*/100, /*input_batch_size_limit=*/100,\n+        /*batch_timeout_micros=*/1 * 1000 * 1000, /*max_enqueued_batches=*/2,\n+        /*enable_priority_queue=*/true);\n+    queue_options.low_priority_queue_options.max_execution_batch_size = 1;\n+    queue_options.low_priority_queue_options.batch_timeout_micros =\n+        1 * 1000 * 1000;\n+    queue_options.low_priority_queue_options.input_batch_size_limit = 1;\n+    queue_options.low_priority_queue_options.max_enqueued_batches = 2;\n+    std::unique_ptr<Queue> queue =\n+        CreateQueue(scheduler, queue_options, queue_callback);\n+\n+    EXPECT_THAT(\n+        ScheduleTask(10, queue.get(),\n+                     tsl::criticality::Criticality::kSheddablePlus),\n+        testing::StatusIs(\n+            absl::StatusCode::kUnavailable,\n+            HasSubstr(\n+                \"The low priority task queue to which this task was submitted \"\n+                \"has max_execution_batch_size=1 and the task size is 10\")));\n+  }\n+  EXPECT_FALSE(queue_callback_called);\n+}\n+\n+TEST_P(SharedBatchSchedulerPriorityTest,\n+       InvalidLowPriorityTaskWithQueueFullWithPriorityQueueEnabled) {\n+  Notification processing, proceed;\n+  auto queue_callback = [&processing, &proceed](\n+                            std::unique_ptr<Batch<FakeTask>> batch,\n+                            std::vector<std::unique_ptr<FakeTask>> tasks) {\n+    if (!processing.HasBeenNotified()) {\n+      processing.Notify();\n+    }\n+    proceed.WaitForNotification();\n+  };\n+\n+  std::shared_ptr<Scheduler> scheduler =\n+      CreateSharedBatchScheduler(/*num_batch_threads=*/1);\n+\n+  QueueOptions queue_options = CreateQueueOptions(\n+      /*max_execution_batch_size=*/100, /*input_batch_size_limit=*/100,\n+      /*batch_timeout_micros=*/1 * 1000 * 1000, /*max_enqueued_batches=*/2,\n+      /*enable_priority_queue=*/true);\n+  queue_options.low_priority_queue_options.max_execution_batch_size = 10;\n+  queue_options.low_priority_queue_options.batch_timeout_micros =\n+      1 * 1000 * 1000;\n+  queue_options.low_priority_queue_options.input_batch_size_limit = 10;\n+  queue_options.low_priority_queue_options.max_enqueued_batches = 2;\n+  std::unique_ptr<Queue> queue =\n+      CreateQueue(scheduler, queue_options, queue_callback);\n+\n+  // Schedule one task and block the thread.\n+  TF_ASSERT_OK(ScheduleTask(5, queue.get(),\n+                            tsl::criticality::Criticality::kCriticalPlus));\n+  TF_ASSERT_OK(ScheduleTask(5, queue.get(),\n+                            tsl::criticality::Criticality::kSheddablePlus));\n+  processing.WaitForNotification();\n+  ASSERT_EQ(0, queue->NumEnqueuedTasks());\n+\n+  // Adding tasks up to size 20 should be fine.\n+  TF_ASSERT_OK(ScheduleTask(10, queue.get(),\n+                            tsl::criticality::Criticality::kSheddablePlus));\n+  ASSERT_EQ(1, queue->NumEnqueuedTasks());\n+  TF_ASSERT_OK(ScheduleTask(10, queue.get(),\n+                            tsl::criticality::Criticality::kSheddablePlus));\n+  ASSERT_EQ(2, queue->NumEnqueuedTasks());\n+\n+  // Adding one more task should result in an error.\n+  EXPECT_THAT(\n+      ScheduleTask(1, queue.get(),\n+                   tsl::criticality::Criticality::kSheddablePlus),\n+      testing::StatusIs(\n+          absl::StatusCode::kUnavailable,\n+          HasSubstr(\"The low priority task queue to which this task was \"\n+                    \"submitted does not have the capcity to handle this task; \"\n+                    \"currently the low priority queue has 20 tasks enqueued \"\n+                    \"and the submitted task size is 1 while \"\n+                    \"max_enqueued_batches=2 and max_execution_batch_size=10\")));\n+\n+  // Unblock the thread.\n+  proceed.Notify();\n+}\n+\n TEST_P(SharedBatchSchedulerPriorityTest,\n        CallbackWithTaskVectorOkWithPriorityQueueEnabledWithPrioritySet) {\n   bool queue_callback_called = false;\n@@ -1070,11 +1168,16 @@ TEST_P(SharedBatchSchedulerPriorityTest,\n     std::shared_ptr<Scheduler> scheduler =\n         CreateSharedBatchScheduler(/*num_batch_threads=*/3);\n \n-    // Create two queues.\n-    const QueueOptions queue_options = CreateQueueOptions(\n+    // Create a queue with the priority queue enabled.\n+    QueueOptions queue_options = CreateQueueOptions(\n         /*max_execution_batch_size=*/10, /*input_batch_size_limit=*/10,\n         /*batch_timeout_micros=*/1 * 1000 * 1000, /*max_enqueued_batches=*/2,\n         /*enable_priority_queue=*/true);\n+    queue_options.low_priority_queue_options.max_execution_batch_size = 10;\n+    queue_options.low_priority_queue_options.batch_timeout_micros =\n+        1 * 1000 * 1000;\n+    queue_options.low_priority_queue_options.input_batch_size_limit = 10;\n+    queue_options.low_priority_queue_options.max_enqueued_batches = 2;\n     std::unique_ptr<Queue> queue =\n         CreateQueue(scheduler, queue_options, queue_callback);\n "
        }
    ]
},
{
    "Id": 192,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c0a3117d09daf89717441cb6b302439486732500",
    "date": "2024-01-22T03:55:32-08:00",
    "message": "Adjust FindNonTrivialHero check for reductions.\n\nFor reductions, we don't need to check whether there is a non-elementwise\ndirect or indirect user. We only allow unary ops in the reduction epilogue, so\nany non-elementwise user would be coming from a different fusion root. In that\ncase, the reduction would be code generated as loop.\n\nPiperOrigin-RevId: 600410667",
    "label": "NO",
    "changes": [
        {
            "name": "ir_emission_utils.cc",
            "path": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "patches": [
                {
                    "old_start": 1051,
                    "old_length": 6,
                    "new_start": 1051,
                    "new_length": 20,
                    "hunk": "@@ -1051,6 +1051,20 @@ static std::optional<HloInstructionAdaptor> FindTransposeHero(\n     return TraversalResult::kAdvance;\n   };\n   HloBfsConsumersFirstTraversal({root}, fusion, visit);\n+  if (transpose) {\n+    // Make sure that no non-elementwise op is reachable from the transpose.\n+    auto visit = [](HloInstructionAdaptor node) {\n+      return node.instruction().opcode() != HloOpcode::kTuple &&\n+             node.instruction().opcode() != HloOpcode::kParameter &&\n+             !IsIntermediate(&node.instruction(),\n+                             /*allowed_operand_count=*/3);\n+    };\n+    bool has_nontrivial_user = HloAnyOf(transpose->GetUsers(), fusion, visit,\n+                                        /*visit_operands=*/false);\n+    if (has_nontrivial_user) {\n+      return std::nullopt;\n+    }\n+  }\n   return transpose;\n }\n \n"
                },
                {
                    "old_start": 1058,
                    "old_length": 13,
                    "new_start": 1072,
                    "new_length": 13,
                    "hunk": "@@ -1058,13 +1072,13 @@ const HloInstruction& FindNonTrivialHero(const HloInstruction& instr,\n                                          const HloFusionAdaptor& fusion) {\n   HloInstructionAdaptor idx{instr};\n \n-  // Go up the chain of trivial element-wise(+bitcast, -copy) operations. Such\n-  // chains are bound to be quite small, as we restrict the number of users as\n-  // well. Note that no memoization is needed due to user number constraints: we\n+  // Go up the chain of trivial element-wise(+bitcast, -copy) operations. Note\n+  // that no memoization is needed due to number of operands constraints: we\n   // never have to revisit same nodes.\n   auto get_intermediate_arg =\n       [&](HloInstructionAdaptor node) -> std::optional<HloInstructionAdaptor> {\n-    if (IsIntermediate(&node.instruction(), 1, &fusion) &&\n+    if (IsIntermediate(&node.instruction(), /*allowed_operand_count=*/1,\n+                       &fusion) &&\n         fusion.ContainsInstruction(node.GetOperand(0))) {\n       return node.GetOperand(0);\n     }\n"
                },
                {
                    "old_start": 1075,
                    "old_length": 24,
                    "new_start": 1089,
                    "new_length": 11,
                    "hunk": "@@ -1075,24 +1089,11 @@ const HloInstruction& FindNonTrivialHero(const HloInstruction& instr,\n   }\n \n   // Try a bit harder to find a transpose hero. The shared memory transpose\n-  // emitter also works if there are ops with more than 1 operand on the path\n-  // between root and the transpose op, we still want the restriction though\n-  // that each op on the path is elementwise and has only 1 user.\n+  // emitter also works if there are elementwise ops with more than 1 operand on\n+  // the path between root and the transpose op.\n   std::optional<HloInstructionAdaptor> transpose =\n       FindTransposeHero(idx, fusion);\n-  HloInstructionAdaptor hero = transpose ? *transpose : idx;\n-  auto visit = [](HloInstructionAdaptor node) {\n-    return node.instruction().opcode() != HloOpcode::kTuple &&\n-           node.instruction().opcode() != HloOpcode::kParameter &&\n-           !IsIntermediate(&node.instruction(),\n-                           /*allowed_operand_count=*/3);\n-  };\n-  bool has_nontrivial_user =\n-      HloAnyOf(hero.GetUsers(), fusion, visit, /*visit_operands=*/false);\n-  if (has_nontrivial_user) {\n-    return instr;\n-  }\n-  return hero.instruction();\n+  return transpose ? transpose->instruction() : idx.instruction();\n }\n \n const HloInstruction& FindNonTrivialHero(const HloInstruction& instr) {\n"
                }
            ],
            "whole_deleted": "-  // Go up the chain of trivial element-wise(+bitcast, -copy) operations. Such\n-  // chains are bound to be quite small, as we restrict the number of users as\n-  // well. Note that no memoization is needed due to user number constraints: we\n-    if (IsIntermediate(&node.instruction(), 1, &fusion) &&\n-  // emitter also works if there are ops with more than 1 operand on the path\n-  // between root and the transpose op, we still want the restriction though\n-  // that each op on the path is elementwise and has only 1 user.\n-  HloInstructionAdaptor hero = transpose ? *transpose : idx;\n-  auto visit = [](HloInstructionAdaptor node) {\n-    return node.instruction().opcode() != HloOpcode::kTuple &&\n-           node.instruction().opcode() != HloOpcode::kParameter &&\n-           !IsIntermediate(&node.instruction(),\n-                           /*allowed_operand_count=*/3);\n-  };\n-  bool has_nontrivial_user =\n-      HloAnyOf(hero.GetUsers(), fusion, visit, /*visit_operands=*/false);\n-  if (has_nontrivial_user) {\n-    return instr;\n-  }\n-  return hero.instruction();\n",
            "whole_added": "+  if (transpose) {\n+    // Make sure that no non-elementwise op is reachable from the transpose.\n+    auto visit = [](HloInstructionAdaptor node) {\n+      return node.instruction().opcode() != HloOpcode::kTuple &&\n+             node.instruction().opcode() != HloOpcode::kParameter &&\n+             !IsIntermediate(&node.instruction(),\n+                             /*allowed_operand_count=*/3);\n+    };\n+    bool has_nontrivial_user = HloAnyOf(transpose->GetUsers(), fusion, visit,\n+                                        /*visit_operands=*/false);\n+    if (has_nontrivial_user) {\n+      return std::nullopt;\n+    }\n+  }\n+  // Go up the chain of trivial element-wise(+bitcast, -copy) operations. Note\n+  // that no memoization is needed due to number of operands constraints: we\n+    if (IsIntermediate(&node.instruction(), /*allowed_operand_count=*/1,\n+                       &fusion) &&\n+  // emitter also works if there are elementwise ops with more than 1 operand on\n+  // the path between root and the transpose op.\n+  return transpose ? transpose->instruction() : idx.instruction();\n",
            "whole_hunk": "@@ -1051,6 +1051,20 @@ static std::optional<HloInstructionAdaptor> FindTransposeHero(\n     return TraversalResult::kAdvance;\n   };\n   HloBfsConsumersFirstTraversal({root}, fusion, visit);\n+  if (transpose) {\n+    // Make sure that no non-elementwise op is reachable from the transpose.\n+    auto visit = [](HloInstructionAdaptor node) {\n+      return node.instruction().opcode() != HloOpcode::kTuple &&\n+             node.instruction().opcode() != HloOpcode::kParameter &&\n+             !IsIntermediate(&node.instruction(),\n+                             /*allowed_operand_count=*/3);\n+    };\n+    bool has_nontrivial_user = HloAnyOf(transpose->GetUsers(), fusion, visit,\n+                                        /*visit_operands=*/false);\n+    if (has_nontrivial_user) {\n+      return std::nullopt;\n+    }\n+  }\n   return transpose;\n }\n \n@@ -1058,13 +1072,13 @@ const HloInstruction& FindNonTrivialHero(const HloInstruction& instr,\n                                          const HloFusionAdaptor& fusion) {\n   HloInstructionAdaptor idx{instr};\n \n-  // Go up the chain of trivial element-wise(+bitcast, -copy) operations. Such\n-  // chains are bound to be quite small, as we restrict the number of users as\n-  // well. Note that no memoization is needed due to user number constraints: we\n+  // Go up the chain of trivial element-wise(+bitcast, -copy) operations. Note\n+  // that no memoization is needed due to number of operands constraints: we\n   // never have to revisit same nodes.\n   auto get_intermediate_arg =\n       [&](HloInstructionAdaptor node) -> std::optional<HloInstructionAdaptor> {\n-    if (IsIntermediate(&node.instruction(), 1, &fusion) &&\n+    if (IsIntermediate(&node.instruction(), /*allowed_operand_count=*/1,\n+                       &fusion) &&\n         fusion.ContainsInstruction(node.GetOperand(0))) {\n       return node.GetOperand(0);\n     }\n@@ -1075,24 +1089,11 @@ const HloInstruction& FindNonTrivialHero(const HloInstruction& instr,\n   }\n \n   // Try a bit harder to find a transpose hero. The shared memory transpose\n-  // emitter also works if there are ops with more than 1 operand on the path\n-  // between root and the transpose op, we still want the restriction though\n-  // that each op on the path is elementwise and has only 1 user.\n+  // emitter also works if there are elementwise ops with more than 1 operand on\n+  // the path between root and the transpose op.\n   std::optional<HloInstructionAdaptor> transpose =\n       FindTransposeHero(idx, fusion);\n-  HloInstructionAdaptor hero = transpose ? *transpose : idx;\n-  auto visit = [](HloInstructionAdaptor node) {\n-    return node.instruction().opcode() != HloOpcode::kTuple &&\n-           node.instruction().opcode() != HloOpcode::kParameter &&\n-           !IsIntermediate(&node.instruction(),\n-                           /*allowed_operand_count=*/3);\n-  };\n-  bool has_nontrivial_user =\n-      HloAnyOf(hero.GetUsers(), fusion, visit, /*visit_operands=*/false);\n-  if (has_nontrivial_user) {\n-    return instr;\n-  }\n-  return hero.instruction();\n+  return transpose ? transpose->instruction() : idx.instruction();\n }\n \n const HloInstruction& FindNonTrivialHero(const HloInstruction& instr) {\n"
        },
        {
            "name": "ir_emission_utils_test.cc",
            "path": "third_party/xla/xla/service/gpu/ir_emission_utils_test.cc",
            "patches": [
                {
                    "old_start": 260,
                    "old_length": 6,
                    "new_start": 260,
                    "new_length": 46,
                    "hunk": "@@ -260,6 +260,46 @@ TEST_F(IrEmissionUtilsTest, FindReduceHeroEpilogueFusionTwoRootUsers) {\n   EXPECT_EQ(result2.name(), \"reduce.1\");\n }\n \n+TEST_F(IrEmissionUtilsTest, FindReduceHeroEpilogueFusionHeroAlsoUsedAsNonHero) {\n+  const char* hlo = R\"(\n+    HloModule module\n+\n+    Add {\n+      x = f32[] parameter(0)\n+      y = f32[] parameter(1)\n+      ROOT add = f32[] add(x, y)\n+    }\n+\n+    fused_computation {\n+      p0 = f32[4]{0} parameter(0)\n+      zero = f32[] constant(0.0)\n+      reduce.0 = f32[] reduce(f32[4]{0} p0, f32[] zero), dimensions={0}, to_apply=Add\n+      broadcast = f32[4]{0} broadcast(f32[] reduce.0), dimensions={}\n+      reduce.1 = f32[] reduce(f32[4]{0} broadcast, f32[] zero), dimensions={0}, to_apply=Add\n+      bitcast = f32[1]{0} bitcast(f32[] reduce.0)\n+      ROOT tuple.1 = (f32[], f32[4]{0}, f32[1]{0}) tuple(f32[] reduce.1, f32[4]{0} broadcast, f32[1]{0} bitcast)\n+    }\n+\n+    ENTRY main {\n+      Arg0 = f32[4]{0} parameter(0)\n+      ROOT fusion = (f32[], f32[4]{0}, f32[1]{0}) fusion(Arg0), kind=kInput, calls=fused_computation\n+    })\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+\n+  HloInstruction* r = module->entry_computation()->root_instruction();\n+  auto fusion = HloFusionAdaptor::ForInstruction(r);\n+  const auto& result =\n+      FindNonTrivialHero(fusion->GetRoots()[1].instruction(), *fusion);\n+  // reduce.0 is also an operand of broadcast, but it is not a hero for that\n+  // root.\n+  EXPECT_EQ(result.name(), \"broadcast\");\n+  const auto& result2 =\n+      FindNonTrivialHero(fusion->GetRoots()[2].instruction(), *fusion);\n+  EXPECT_EQ(result2.name(), \"reduce.0\");\n+}\n+\n TEST_F(IrEmissionUtilsTest, FindAnyTiledTransposeWithIntermediateBinaryOp) {\n   const char* hlo = R\"(\n HloModule module\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(IrEmissionUtilsTest, FindReduceHeroEpilogueFusionHeroAlsoUsedAsNonHero) {\n+  const char* hlo = R\"(\n+    HloModule module\n+\n+    Add {\n+      x = f32[] parameter(0)\n+      y = f32[] parameter(1)\n+      ROOT add = f32[] add(x, y)\n+    }\n+\n+    fused_computation {\n+      p0 = f32[4]{0} parameter(0)\n+      zero = f32[] constant(0.0)\n+      reduce.0 = f32[] reduce(f32[4]{0} p0, f32[] zero), dimensions={0}, to_apply=Add\n+      broadcast = f32[4]{0} broadcast(f32[] reduce.0), dimensions={}\n+      reduce.1 = f32[] reduce(f32[4]{0} broadcast, f32[] zero), dimensions={0}, to_apply=Add\n+      bitcast = f32[1]{0} bitcast(f32[] reduce.0)\n+      ROOT tuple.1 = (f32[], f32[4]{0}, f32[1]{0}) tuple(f32[] reduce.1, f32[4]{0} broadcast, f32[1]{0} bitcast)\n+    }\n+\n+    ENTRY main {\n+      Arg0 = f32[4]{0} parameter(0)\n+      ROOT fusion = (f32[], f32[4]{0}, f32[1]{0}) fusion(Arg0), kind=kInput, calls=fused_computation\n+    })\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+\n+  HloInstruction* r = module->entry_computation()->root_instruction();\n+  auto fusion = HloFusionAdaptor::ForInstruction(r);\n+  const auto& result =\n+      FindNonTrivialHero(fusion->GetRoots()[1].instruction(), *fusion);\n+  // reduce.0 is also an operand of broadcast, but it is not a hero for that\n+  // root.\n+  EXPECT_EQ(result.name(), \"broadcast\");\n+  const auto& result2 =\n+      FindNonTrivialHero(fusion->GetRoots()[2].instruction(), *fusion);\n+  EXPECT_EQ(result2.name(), \"reduce.0\");\n+}\n+\n",
            "whole_hunk": "@@ -260,6 +260,46 @@ TEST_F(IrEmissionUtilsTest, FindReduceHeroEpilogueFusionTwoRootUsers) {\n   EXPECT_EQ(result2.name(), \"reduce.1\");\n }\n \n+TEST_F(IrEmissionUtilsTest, FindReduceHeroEpilogueFusionHeroAlsoUsedAsNonHero) {\n+  const char* hlo = R\"(\n+    HloModule module\n+\n+    Add {\n+      x = f32[] parameter(0)\n+      y = f32[] parameter(1)\n+      ROOT add = f32[] add(x, y)\n+    }\n+\n+    fused_computation {\n+      p0 = f32[4]{0} parameter(0)\n+      zero = f32[] constant(0.0)\n+      reduce.0 = f32[] reduce(f32[4]{0} p0, f32[] zero), dimensions={0}, to_apply=Add\n+      broadcast = f32[4]{0} broadcast(f32[] reduce.0), dimensions={}\n+      reduce.1 = f32[] reduce(f32[4]{0} broadcast, f32[] zero), dimensions={0}, to_apply=Add\n+      bitcast = f32[1]{0} bitcast(f32[] reduce.0)\n+      ROOT tuple.1 = (f32[], f32[4]{0}, f32[1]{0}) tuple(f32[] reduce.1, f32[4]{0} broadcast, f32[1]{0} bitcast)\n+    }\n+\n+    ENTRY main {\n+      Arg0 = f32[4]{0} parameter(0)\n+      ROOT fusion = (f32[], f32[4]{0}, f32[1]{0}) fusion(Arg0), kind=kInput, calls=fused_computation\n+    })\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(hlo));\n+\n+  HloInstruction* r = module->entry_computation()->root_instruction();\n+  auto fusion = HloFusionAdaptor::ForInstruction(r);\n+  const auto& result =\n+      FindNonTrivialHero(fusion->GetRoots()[1].instruction(), *fusion);\n+  // reduce.0 is also an operand of broadcast, but it is not a hero for that\n+  // root.\n+  EXPECT_EQ(result.name(), \"broadcast\");\n+  const auto& result2 =\n+      FindNonTrivialHero(fusion->GetRoots()[2].instruction(), *fusion);\n+  EXPECT_EQ(result2.name(), \"reduce.0\");\n+}\n+\n TEST_F(IrEmissionUtilsTest, FindAnyTiledTransposeWithIntermediateBinaryOp) {\n   const char* hlo = R\"(\n HloModule module\n"
        },
        {
            "name": "multioutput_fusion_test.cc",
            "path": "third_party/xla/xla/tests/multioutput_fusion_test.cc",
            "patches": [
                {
                    "old_start": 494,
                    "old_length": 6,
                    "new_start": 494,
                    "new_length": 30,
                    "hunk": "@@ -494,6 +494,30 @@ ENTRY main.7749 {\n   EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));\n }\n \n+XLA_TEST_F(MultiOutputFusionTest,\n+           MultiOutputReduceWithEpilogueHeroAlsoUsedAsNonHero) {\n+  // reduce.8 is used by bitcast as reduce hero and by broadcast as non-hero.\n+  const std::string testcase = absl::StrCat(kScalarOps, R\"(\n+fused_computation {\n+  %param_0.6 = f32[4]{0} parameter(0)\n+  %zero = f32[] constant(0.0)\n+  %reduce.8 = f32[] reduce(f32[4]{0} %param_0.6, f32[] %zero), dimensions={0}, to_apply=Add\n+  %broadcast = f32[4]{0} broadcast(f32[] %reduce.8), dimensions={}\n+  %compare = pred[4]{0} compare(f32[4]{0} %param_0.6, f32[4]{0} %broadcast), direction=EQ\n+  %convert = f32[4]{0} convert(pred[4]{0} %compare)\n+  %reduce.19.1 = f32[] reduce(f32[4]{0} %convert, f32[] %zero), dimensions={0}, to_apply=Add\n+  %bitcast = f32[1]{0} bitcast(f32[] %reduce.8)\n+  ROOT %tuple.1 = (f32[], f32[4]{0}, f32[1]{0}) tuple(f32[] %reduce.19.1, f32[4]{0} %convert, f32[1]{0} %bitcast)\n+}\n+\n+ENTRY main {\n+  Arg0 = f32[4]{0} parameter(0)\n+  ROOT fusion = (f32[], f32[4]{0}, f32[1]{0}) fusion(Arg0), kind=kInput, calls=fused_computation\n+})\");\n+  auto module = ParseAndReturnVerifiedModule(testcase).value();\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));\n+}\n+\n XLA_TEST_F(MultiOutputFusionTest,\n            MultiOutputTransposeFusionHeroWithMultipleRootUsers) {\n   const std::string testcase = R\"("
                }
            ],
            "whole_deleted": "",
            "whole_added": "+XLA_TEST_F(MultiOutputFusionTest,\n+           MultiOutputReduceWithEpilogueHeroAlsoUsedAsNonHero) {\n+  // reduce.8 is used by bitcast as reduce hero and by broadcast as non-hero.\n+  const std::string testcase = absl::StrCat(kScalarOps, R\"(\n+fused_computation {\n+  %param_0.6 = f32[4]{0} parameter(0)\n+  %zero = f32[] constant(0.0)\n+  %reduce.8 = f32[] reduce(f32[4]{0} %param_0.6, f32[] %zero), dimensions={0}, to_apply=Add\n+  %broadcast = f32[4]{0} broadcast(f32[] %reduce.8), dimensions={}\n+  %compare = pred[4]{0} compare(f32[4]{0} %param_0.6, f32[4]{0} %broadcast), direction=EQ\n+  %convert = f32[4]{0} convert(pred[4]{0} %compare)\n+  %reduce.19.1 = f32[] reduce(f32[4]{0} %convert, f32[] %zero), dimensions={0}, to_apply=Add\n+  %bitcast = f32[1]{0} bitcast(f32[] %reduce.8)\n+  ROOT %tuple.1 = (f32[], f32[4]{0}, f32[1]{0}) tuple(f32[] %reduce.19.1, f32[4]{0} %convert, f32[1]{0} %bitcast)\n+}\n+\n+ENTRY main {\n+  Arg0 = f32[4]{0} parameter(0)\n+  ROOT fusion = (f32[], f32[4]{0}, f32[1]{0}) fusion(Arg0), kind=kInput, calls=fused_computation\n+})\");\n+  auto module = ParseAndReturnVerifiedModule(testcase).value();\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));\n+}\n+\n",
            "whole_hunk": "@@ -494,6 +494,30 @@ ENTRY main.7749 {\n   EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));\n }\n \n+XLA_TEST_F(MultiOutputFusionTest,\n+           MultiOutputReduceWithEpilogueHeroAlsoUsedAsNonHero) {\n+  // reduce.8 is used by bitcast as reduce hero and by broadcast as non-hero.\n+  const std::string testcase = absl::StrCat(kScalarOps, R\"(\n+fused_computation {\n+  %param_0.6 = f32[4]{0} parameter(0)\n+  %zero = f32[] constant(0.0)\n+  %reduce.8 = f32[] reduce(f32[4]{0} %param_0.6, f32[] %zero), dimensions={0}, to_apply=Add\n+  %broadcast = f32[4]{0} broadcast(f32[] %reduce.8), dimensions={}\n+  %compare = pred[4]{0} compare(f32[4]{0} %param_0.6, f32[4]{0} %broadcast), direction=EQ\n+  %convert = f32[4]{0} convert(pred[4]{0} %compare)\n+  %reduce.19.1 = f32[] reduce(f32[4]{0} %convert, f32[] %zero), dimensions={0}, to_apply=Add\n+  %bitcast = f32[1]{0} bitcast(f32[] %reduce.8)\n+  ROOT %tuple.1 = (f32[], f32[4]{0}, f32[1]{0}) tuple(f32[] %reduce.19.1, f32[4]{0} %convert, f32[1]{0} %bitcast)\n+}\n+\n+ENTRY main {\n+  Arg0 = f32[4]{0} parameter(0)\n+  ROOT fusion = (f32[], f32[4]{0}, f32[1]{0}) fusion(Arg0), kind=kInput, calls=fused_computation\n+})\");\n+  auto module = ParseAndReturnVerifiedModule(testcase).value();\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));\n+}\n+\n XLA_TEST_F(MultiOutputFusionTest,\n            MultiOutputTransposeFusionHeroWithMultipleRootUsers) {\n   const std::string testcase = R\"("
        }
    ]
},
{
    "Id": 393,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b97f0f708cb9a7039a12c605391c6ec90f767f49",
    "date": "2023-06-21T11:10:38-07:00",
    "message": "Add check to see if key exists before calling at.\n\nPiperOrigin-RevId: 542306890",
    "label": "YES",
    "changes": [
        {
            "name": "op_stats_to_pod_stats.cc",
            "path": "tensorflow/core/profiler/convert/op_stats_to_pod_stats.cc",
            "patches": [
                {
                    "old_start": 87,
                    "old_length": 6,
                    "new_start": 87,
                    "new_length": 10,
                    "hunk": "@@ -87,6 +87,10 @@ PodStatsDatabase ConvertOpStatsToPodStats(const OpStats& op_stats) {\n \n   for (const auto& step_sequence : op_stats.step_db().step_sequence()) {\n     for (const auto& entry : step_sequence.step_info_per_core()) {\n+      if (!core_id_map.contains(entry.first)) {\n+        LOG(WARNING) << \"core_id_map does not contain \" << entry.first;\n+        continue;\n+      }\n       const CoreDetails& details = core_id_map.at(entry.first);\n       *pod_stats_db.add_pod_stats_record() =\n           CreatePodStatsRecord(details.hostname(), entry.second);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      if (!core_id_map.contains(entry.first)) {\n+        LOG(WARNING) << \"core_id_map does not contain \" << entry.first;\n+        continue;\n+      }\n",
            "whole_hunk": "@@ -87,6 +87,10 @@ PodStatsDatabase ConvertOpStatsToPodStats(const OpStats& op_stats) {\n \n   for (const auto& step_sequence : op_stats.step_db().step_sequence()) {\n     for (const auto& entry : step_sequence.step_info_per_core()) {\n+      if (!core_id_map.contains(entry.first)) {\n+        LOG(WARNING) << \"core_id_map does not contain \" << entry.first;\n+        continue;\n+      }\n       const CoreDetails& details = core_id_map.at(entry.first);\n       *pod_stats_db.add_pod_stats_record() =\n           CreatePodStatsRecord(details.hostname(), entry.second);"
        }
    ]
},
{
    "Id": 90,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9ceb4517a93c612c3b90c72a0cf54b3bad1bde97",
    "date": "2024-03-27T16:58:42-07:00",
    "message": "PR #10668: [XLA:GPU] Fix cuDNN FMHA rewriter sequence length checks and Bump up minimum flash attn cuDNN version to 8.9.4\n\nImported from GitHub PR https://github.com/openxla/xla/pull/10668\n\n* If only seqlen_q or seqlen_kv is larger than 512, it will be lowered to fused attn instead of flash attn. This is incorrect as fused attn does not support this case. Add checks to lower to flash attn if one seqlen is larger than 512.\n* While fixing the seqlen checks, also relax the seqlen/head dim constraints to support any seqlen % 2 == 0, head_dim <= 128 and head_dim % 8 = 0. (Since 8.9.4).\n* Consolidate multiple get seqlen/head dim from fused attn/flash attn checks into one `getBHSD` function and rewrite both fused attn/flash attn checks to make it easier to understand.\n* Bump up minimum cuDNN version require for flash attn from 8.9.3 to 8.9.4. (More seqlen/head dim support and cross attn support).\n* Remove cross attn checks from rewriter and rewriter test since it is now default supported with 8.9.4.\n* Add one testcase to cover the first bullet point.\nCopybara import of the project:\n\n--\nbfdce45d669fe899dd845f497df783f014558384 by cjkkkk <ske@nvidia.com>:\n\nrewrite seqlen checks\n\n--\n8a0e8b465cc1790953b1dc5f8740adfa7cf1124a by cjkkkk <ske@nvidia.com>:\n\nreturn vector directly\n\n--\n72abf544d2f962c5554fe00ffea8d3167a9cde81 by cjkkkk <ske@nvidia.com>:\n\nuse struct for qkv_layout && add () for extra &&\n\n--\n0cac35422315180fcc9b04372916e061ebe17754 by cjkkkk <ske@nvidia.com>:\n\nfix head dim check with cuDNN < 8.9.6\n\nMerging this change closes #10668\n\nPiperOrigin-RevId: 619703747",
    "label": "YES",
    "changes": [
        {
            "name": "cudnn_fused_mha_rewriter.cc",
            "path": "third_party/xla/xla/service/gpu/cudnn_fused_mha_rewriter.cc",
            "patches": [
                {
                    "old_start": 315,
                    "old_length": 19,
                    "new_start": 315,
                    "new_length": 6,
                    "hunk": "@@ -315,19 +315,6 @@ bool IsSupportedPrimitiveType(const HloInstruction* bmm) {\n   return dtype == BF16 || dtype == F16;\n }\n \n-bool IsContractingDimSupported(absl::Span<const int64_t> contracting_dims) {\n-  return absl::c_all_of(contracting_dims,\n-                        [](int64_t dim) { return dim == 64; });\n-}\n-\n-bool IsNonContractingDimSupported(\n-    const std::vector<int64_t>& non_contracting_dims, bool is_training) {\n-  // For training, cuDNN require non_contracting_dim to be Divisible by 64\n-  return absl::c_all_of(non_contracting_dims, [&](int64_t dim) {\n-    return dim <= 512 && (!is_training || dim % 64 == 0);\n-  });\n-}\n-\n std::vector<int64_t> GetDimensionVector(absl::Span<const int64_t> dimensions,\n                                         absl::Span<const int64_t> dim_nums) {\n   std::vector<int64_t> vec(dim_nums.size());\n"
                },
                {
                    "old_start": 337,
                    "old_length": 150,
                    "new_start": 324,
                    "new_length": 163,
                    "hunk": "@@ -337,150 +324,163 @@ std::vector<int64_t> GetDimensionVector(absl::Span<const int64_t> dimensions,\n   return vec;\n }\n \n-absl::StatusOr<bool> IsSupportedBMM1(const HloInstruction* bmm_1,\n-                                     bool is_training) {\n-  const DotDimensionNumbers& dot_dims_bmm1 = bmm_1->dot_dimension_numbers();\n+struct QKVLayout {\n+  int64_t batch;\n+  int64_t num_heads;\n+  int64_t seqlen_q;\n+  int64_t seqlen_kv;\n+  int64_t hidden_dim;\n+};\n+\n+absl::StatusOr<std::optional<QKVLayout>> GetQKVLayout(\n+    HloInstruction* bmm_1, HloInstruction* bmm_2, bool need_canonicalization) {\n+  // get layout from bmm1\n+  const DotDimensionNumbers& bmm1_dnums = bmm_1->dot_dimension_numbers();\n   TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> lhs_non_contracting_dim_nums_bmm1,\n+      std::vector<int64_t> bmm1_s_q_dims,\n       GetNonContractingDims(bmm_1->operand(0)->shape(),\n-                            dot_dims_bmm1.lhs_batch_dimensions(),\n-                            dot_dims_bmm1.lhs_contracting_dimensions()));\n+                            bmm1_dnums.lhs_batch_dimensions(),\n+                            bmm1_dnums.lhs_contracting_dimensions()));\n+\n   TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> rhs_non_contracting_dim_nums_bmm1,\n+      std::vector<int64_t> bmm1_s_kv_dims,\n       GetNonContractingDims(bmm_1->operand(1)->shape(),\n-                            dot_dims_bmm1.rhs_batch_dimensions(),\n-                            dot_dims_bmm1.rhs_contracting_dimensions()));\n-  std::vector<int64_t> lhs_non_contracting_dims_bmm1 =\n+                            bmm1_dnums.rhs_batch_dimensions(),\n+                            bmm1_dnums.rhs_contracting_dimensions()));\n+\n+  std::vector<int64_t> bmm1_bh =\n       GetDimensionVector(bmm_1->operand(0)->shape().dimensions(),\n-                         lhs_non_contracting_dim_nums_bmm1);\n-  std::vector<int64_t> rhs_non_contracting_dims_bmm1 =\n-      GetDimensionVector(bmm_1->operand(1)->shape().dimensions(),\n-                         rhs_non_contracting_dim_nums_bmm1);\n-  // The non contracting dimensions for BMM1 need to be less than or equal to\n-  // 512.\n-  if (!IsNonContractingDimSupported(lhs_non_contracting_dims_bmm1,\n-                                    is_training) ||\n-      !IsNonContractingDimSupported(rhs_non_contracting_dims_bmm1,\n-                                    is_training)) {\n-    if (VLOG_IS_ON(2)) {\n-      VLOG(2) << \"BMM1 lhs_non_contracting_dims: \"\n-              << absl::StrJoin(lhs_non_contracting_dims_bmm1, \",\")\n-              << \" BMM1 rhs_non_contracting_dims: \"\n-              << absl::StrJoin(rhs_non_contracting_dims_bmm1, \",\")\n-              << \" are not supported. The non-contracting dims should be less \"\n-                 \"than 512. This is a criteria for current cuDNN 8.8 support.\";\n-    }\n-    return false;\n-  }\n+                         bmm1_dnums.lhs_batch_dimensions());\n+\n+  std::vector<int64_t> bmm1_s_q = GetDimensionVector(\n+      bmm_1->operand(0)->shape().dimensions(), bmm1_s_q_dims);\n+\n+  std::vector<int64_t> bmm1_s_kv = GetDimensionVector(\n+      bmm_1->operand(1)->shape().dimensions(), bmm1_s_kv_dims);\n \n-  std::vector<int64_t> lhs_contracting_dims_bmm1 =\n+  std::vector<int64_t> bmm1_d =\n       GetDimensionVector(bmm_1->operand(0)->shape().dimensions(),\n-                         dot_dims_bmm1.lhs_contracting_dimensions());\n-  std::vector<int64_t> rhs_contracting_dims_bmm1 =\n-      GetDimensionVector(bmm_1->operand(1)->shape().dimensions(),\n-                         dot_dims_bmm1.rhs_contracting_dimensions());\n-\n-  // The contracting dimensions for BMM1 need to be 64.\n-  if (!IsContractingDimSupported(lhs_contracting_dims_bmm1) ||\n-      !IsContractingDimSupported(rhs_contracting_dims_bmm1)) {\n-    if (VLOG_IS_ON(2)) {\n-      VLOG(2) << \"BMM1 lhs_contracting_dims: \"\n-              << absl::StrJoin(lhs_contracting_dims_bmm1, \",\")\n-              << \" BMM1 rhs_contracting_dims: \"\n-              << absl::StrJoin(rhs_contracting_dims_bmm1, \",\")\n-              << \" are not supported.\";\n-    }\n-    return false;\n-  }\n-  return true;\n-}\n+                         bmm1_dnums.lhs_contracting_dimensions());\n \n-absl::StatusOr<bool> IsSupportedBMM2(const HloInstruction* bmm_2,\n-                                     bool need_canonicalization) {\n-  const DotDimensionNumbers& dot_dims_bmm2 = bmm_2->dot_dimension_numbers();\n-  // need swap lhs and rhs for bmm2 if canonicalization is needed\n-  int operand_index = need_canonicalization ? 0 : 1;\n-  auto batch_dim = need_canonicalization ? dot_dims_bmm2.lhs_batch_dimensions()\n-                                         : dot_dims_bmm2.rhs_batch_dimensions();\n-  auto contracting_dim = need_canonicalization\n-                             ? dot_dims_bmm2.lhs_contracting_dimensions()\n-                             : dot_dims_bmm2.rhs_contracting_dimensions();\n+  TF_RET_CHECK(bmm1_bh.size() == 2);\n+  TF_RET_CHECK(bmm1_s_q.size() == 1);\n+  TF_RET_CHECK(bmm1_s_kv.size() == 1);\n+  TF_RET_CHECK(bmm1_d.size() == 1);\n \n+  // get layout from bmm2\n+  const DotDimensionNumbers& bmm2_dnums = bmm_2->dot_dimension_numbers();\n   TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> non_contracting_dim_nums_bmm2,\n-      GetNonContractingDims(bmm_2->operand(operand_index)->shape(), batch_dim,\n-                            contracting_dim));\n-\n-  std::vector<int64_t> non_contracting_dims_bmm2 =\n-      GetDimensionVector(bmm_2->operand(operand_index)->shape().dimensions(),\n-                         non_contracting_dim_nums_bmm2);\n-  // The non contracting dimension for BMM2 needs to be 64 for the input matrix.\n-  // The input matrix is the second argument to BMM2 i.e, rhs.\n-  if (!absl::c_all_of(non_contracting_dims_bmm2,\n-                      [](int64_t dim) { return dim == 64; })) {\n-    if (VLOG_IS_ON(2)) {\n-      VLOG(2) << \" BMM2 rhs_non_contracting_dims: \"\n-              << absl::StrJoin(non_contracting_dims_bmm2, \",\")\n-              << \" are not supported.\";\n-    }\n-    return false;\n+      std::vector<int64_t> bmm2_lhs_non_contracting_dims,\n+      GetNonContractingDims(bmm_2->operand(0)->shape(),\n+                            bmm2_dnums.lhs_batch_dimensions(),\n+                            bmm2_dnums.lhs_contracting_dimensions()));\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<int64_t> bmm2_rhs_non_contracting_dims,\n+      GetNonContractingDims(bmm_2->operand(1)->shape(),\n+                            bmm2_dnums.rhs_batch_dimensions(),\n+                            bmm2_dnums.rhs_contracting_dimensions()));\n+\n+  std::vector<int64_t> bmm2_bh =\n+      GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                         bmm2_dnums.lhs_batch_dimensions());\n+\n+  std::vector<int64_t> bmm2_s_kv =\n+      GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                         bmm2_dnums.lhs_contracting_dimensions());\n+\n+  std::vector<int64_t> bmm2_s_q =\n+      need_canonicalization\n+          ? GetDimensionVector(bmm_2->operand(1)->shape().dimensions(),\n+                               bmm2_rhs_non_contracting_dims)\n+          : GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                               bmm2_lhs_non_contracting_dims);\n+\n+  std::vector<int64_t> bmm2_d =\n+      need_canonicalization\n+          ? GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                               bmm2_lhs_non_contracting_dims)\n+          : GetDimensionVector(bmm_2->operand(1)->shape().dimensions(),\n+                               bmm2_rhs_non_contracting_dims);\n+\n+  TF_RET_CHECK(bmm2_bh.size() == 2);\n+  TF_RET_CHECK(bmm2_s_q.size() == 1);\n+  TF_RET_CHECK(bmm2_s_kv.size() == 1);\n+  TF_RET_CHECK(bmm2_d.size() == 1);\n+\n+  // check if bhsd is correct between bmm1 and bmm2\n+  if (bmm1_bh[0] != bmm2_bh[0] || bmm1_bh[1] != bmm2_bh[1] ||\n+      bmm1_s_q[0] != bmm2_s_q[0] || bmm1_s_kv[0] != bmm2_s_kv[0] ||\n+      bmm1_d[0] != bmm2_d[0]) {\n+    return std::nullopt;\n   }\n-  return true;\n+\n+  QKVLayout qkv_layout;\n+  qkv_layout.batch = bmm1_bh[0];\n+  qkv_layout.num_heads = bmm1_bh[1];\n+  qkv_layout.seqlen_q = bmm1_s_q[0];\n+  qkv_layout.seqlen_kv = bmm1_s_kv[0];\n+  qkv_layout.hidden_dim = bmm1_d[0];\n+  return qkv_layout;\n }\n \n-absl::StatusOr<bool> IsFlashAttention(\n-    HloInstruction* bmm_1, bool is_causal_mask,\n-    absl::string_view custom_call_name,\n+absl::StatusOr<bool> IsFusedAttention(\n+    QKVLayout qkv_layout, bool is_training,\n     stream_executor::CudaComputeCapability cc,\n     stream_executor::dnn::VersionInfo cudnn_version) {\n-  const DotDimensionNumbers& dnums = bmm_1->dot_dimension_numbers();\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> seq_q_dims,\n-      GetNonContractingDims(bmm_1->operand(0)->shape(),\n-                            dnums.lhs_batch_dimensions(),\n-                            dnums.lhs_contracting_dimensions()));\n-\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> seq_k_dims,\n-      GetNonContractingDims(bmm_1->operand(1)->shape(),\n-                            dnums.rhs_batch_dimensions(),\n-                            dnums.rhs_contracting_dimensions()));\n-\n-  std::vector<int64_t> seq_q =\n-      GetDimensionVector(bmm_1->operand(0)->shape().dimensions(), seq_q_dims);\n+  // otherwise check if it is supported by regular attention\n+  int64_t s_q = qkv_layout.seqlen_q;\n+  int64_t s_kv = qkv_layout.seqlen_kv;\n+  int64_t hidden_dim = qkv_layout.hidden_dim;\n+  bool is_seqlen_supported =\n+      (s_q <= 512 && s_kv <= 512) &&\n+      (!is_training || (s_q % 64 == 0 && s_kv % 64 == 0));\n+  bool is_hidden_dim_supported = hidden_dim == 64;\n+  bool is_fused_attention = is_seqlen_supported && is_hidden_dim_supported;\n+  return is_fused_attention;\n+}\n \n-  std::vector<int64_t> seq_k =\n-      GetDimensionVector(bmm_1->operand(1)->shape().dimensions(), seq_k_dims);\n+absl::StatusOr<bool> IsFlashAttention(\n+    QKVLayout qkv_layout, bool is_training,\n+    stream_executor::CudaComputeCapability cc,\n+    stream_executor::dnn::VersionInfo cudnn_version) {\n+  int64_t s_q = qkv_layout.seqlen_q;\n+  int64_t s_kv = qkv_layout.seqlen_kv;\n+  int64_t hidden_dim = qkv_layout.hidden_dim;\n+  // start with most relaxed constraint\n+  bool is_seqlen_supported = (s_q > 512 || s_kv > 512) &&\n+                             (!is_training || (s_q % 2 == 0 && s_kv % 2 == 0));\n+  bool is_hidden_dim_supported = hidden_dim <= 128 && hidden_dim % 8 == 0;\n+  bool is_flash_attention = is_seqlen_supported && is_hidden_dim_supported;\n+  if (!is_flash_attention) return false;\n+  // going backwards to check compatibility\n+  if ((is_training && (s_q < 64 || s_kv < 64)) &&\n+      !IsComputeCapabilityAndCudnnSupported(\n+          cc, cudnn_version, stream_executor::dnn::VersionInfo(9, 0, 0))) {\n+    VLOG(2) << \"Flash attention training with seq < 64 not supported cuDNN < \"\n+               \"9.0.0.\";\n+    return false;\n+  }\n \n-  std::vector<int64_t> hidden_dim =\n-      GetDimensionVector(bmm_1->operand(0)->shape().dimensions(),\n-                         dnums.lhs_contracting_dimensions());\n-  // for now, seq_q and seq_k should be equal for flash attention to work\n-  // flash attention only supports fixed topology so we check if custom call is\n-  // such topology by checking custom_call_name\n-  TF_RET_CHECK(seq_q.size() == 1);\n-  TF_RET_CHECK(seq_k.size() == 1);\n-  TF_RET_CHECK(hidden_dim.size() == 1);\n-\n-  auto is_seqlen_supported = seq_q[0] > 512 && seq_k[0] > 512 &&\n-                             seq_q[0] % 64 == 0 && seq_k[0] % 64 == 0;\n-  auto is_hidden_dim_supported = hidden_dim[0] == 64 || hidden_dim[0] == 128;\n-  auto is_flash_attention = is_seqlen_supported && is_hidden_dim_supported;\n-  auto is_cross_attention = seq_q[0] != seq_k[0];\n-\n-  // flash attention requires cuDNN 8.9.3 to run non-fused QKV\n-  // once we have fused QKV support, we can relax this contraint\n-  if (is_flash_attention &&\n+  if ((hidden_dim != 64 && hidden_dim != 128) &&\n       !IsComputeCapabilityAndCudnnSupported(\n-          cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 3))) {\n-    VLOG(2) << \"Require cuDNN 8.9.3 to run flash attention.\";\n+          cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 6))) {\n+    VLOG(2) << \"Flash attention head dim != 64 or 128 not supported with cuDNN \"\n+               \"< 8.9.6.\";\n     return false;\n   }\n-  // flash attention cross attention requires cuDNN 8.9.4 to run\n-  if (is_cross_attention &&\n+\n+  if ((is_training && s_kv % 64 != 0) &&\n       !IsComputeCapabilityAndCudnnSupported(\n+          cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 5))) {\n+    VLOG(2) << \"Flash attention training with seq kv % 64 != 0 not supported \"\n+               \"with cuDNN < 8.9.5.\";\n+    return false;\n+  }\n+\n+  if (!IsComputeCapabilityAndCudnnSupported(\n           cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 4))) {\n-    VLOG(2) << \"Require cuDNN 8.9.4 to run flash cross attention.\";\n+    VLOG(2) << \"Require cuDNN 8.9.4 to run flash attention.\";\n     return false;\n   }\n   return is_flash_attention;\n"
                },
                {
                    "old_start": 1239,
                    "old_length": 10,
                    "new_start": 1239,
                    "new_length": 19,
                    "hunk": "@@ -1239,10 +1239,19 @@ absl::StatusOr<bool> IsMHABlockSupported(\n     return false;\n   }\n \n+  // get batch/num heads/sequence length/hidden dim from bmm1 and bmm2\n+  // also make sure they are the same between bmm1 and bmm2\n+  TF_ASSIGN_OR_RETURN(std::optional<QKVLayout> qkv_layout,\n+                      GetQKVLayout(bmm_1, bmm_2, need_canonicalization));\n+  if (!qkv_layout.has_value()) {\n+    VLOG(2) << \"bmm1 and bmm2 have different qkv layout.\";\n+    return false;\n+  }\n+\n   // check if matched attention block is supported by cuDNN flash attention.\n-  TF_ASSIGN_OR_RETURN(is_flash_attention,\n-                      IsFlashAttention(bmm_1, is_causal_mask, custom_call_name,\n-                                       cc, cudnn_version));\n+  TF_ASSIGN_OR_RETURN(\n+      is_flash_attention,\n+      IsFlashAttention(qkv_layout.value(), is_training, cc, cudnn_version));\n   if (is_flash_attention) {\n     if (is_causal_mask) {\n       // if bias is causal mask, needs to remove bias from name\n"
                },
                {
                    "old_start": 1259,
                    "old_length": 14,
                    "new_start": 1268,
                    "new_length": 11,
                    "hunk": "@@ -1259,14 +1268,11 @@ absl::StatusOr<bool> IsMHABlockSupported(\n     }\n     return true;\n   }\n-  // otherwise check if it is supported by regular attention\n-  TF_ASSIGN_OR_RETURN(bool is_bmm1_supported,\n-                      IsSupportedBMM1(bmm_1, is_training));\n-  if (!is_bmm1_supported) return false;\n-  TF_ASSIGN_OR_RETURN(bool is_bmm2_supported,\n-                      IsSupportedBMM2(bmm_2, need_canonicalization));\n-  if (!is_bmm2_supported) return false;\n-  return true;\n+  // check if matched attention block is supported by cuDNN fused attention.\n+  TF_ASSIGN_OR_RETURN(\n+      bool is_fused_attention,\n+      IsFusedAttention(qkv_layout.value(), is_training, cc, cudnn_version));\n+  return is_fused_attention;\n }\n \n absl::StatusOr<HloInstruction*> CanonicalizeBatchedGemmForcuDNNFMHA(\n"
                }
            ],
            "whole_deleted": "-bool IsContractingDimSupported(absl::Span<const int64_t> contracting_dims) {\n-  return absl::c_all_of(contracting_dims,\n-                        [](int64_t dim) { return dim == 64; });\n-}\n-\n-bool IsNonContractingDimSupported(\n-    const std::vector<int64_t>& non_contracting_dims, bool is_training) {\n-  // For training, cuDNN require non_contracting_dim to be Divisible by 64\n-  return absl::c_all_of(non_contracting_dims, [&](int64_t dim) {\n-    return dim <= 512 && (!is_training || dim % 64 == 0);\n-  });\n-}\n-\n-absl::StatusOr<bool> IsSupportedBMM1(const HloInstruction* bmm_1,\n-                                     bool is_training) {\n-  const DotDimensionNumbers& dot_dims_bmm1 = bmm_1->dot_dimension_numbers();\n-      std::vector<int64_t> lhs_non_contracting_dim_nums_bmm1,\n-                            dot_dims_bmm1.lhs_batch_dimensions(),\n-                            dot_dims_bmm1.lhs_contracting_dimensions()));\n-      std::vector<int64_t> rhs_non_contracting_dim_nums_bmm1,\n-                            dot_dims_bmm1.rhs_batch_dimensions(),\n-                            dot_dims_bmm1.rhs_contracting_dimensions()));\n-  std::vector<int64_t> lhs_non_contracting_dims_bmm1 =\n-                         lhs_non_contracting_dim_nums_bmm1);\n-  std::vector<int64_t> rhs_non_contracting_dims_bmm1 =\n-      GetDimensionVector(bmm_1->operand(1)->shape().dimensions(),\n-                         rhs_non_contracting_dim_nums_bmm1);\n-  // The non contracting dimensions for BMM1 need to be less than or equal to\n-  // 512.\n-  if (!IsNonContractingDimSupported(lhs_non_contracting_dims_bmm1,\n-                                    is_training) ||\n-      !IsNonContractingDimSupported(rhs_non_contracting_dims_bmm1,\n-                                    is_training)) {\n-    if (VLOG_IS_ON(2)) {\n-      VLOG(2) << \"BMM1 lhs_non_contracting_dims: \"\n-              << absl::StrJoin(lhs_non_contracting_dims_bmm1, \",\")\n-              << \" BMM1 rhs_non_contracting_dims: \"\n-              << absl::StrJoin(rhs_non_contracting_dims_bmm1, \",\")\n-              << \" are not supported. The non-contracting dims should be less \"\n-                 \"than 512. This is a criteria for current cuDNN 8.8 support.\";\n-    }\n-    return false;\n-  }\n-  std::vector<int64_t> lhs_contracting_dims_bmm1 =\n-                         dot_dims_bmm1.lhs_contracting_dimensions());\n-  std::vector<int64_t> rhs_contracting_dims_bmm1 =\n-      GetDimensionVector(bmm_1->operand(1)->shape().dimensions(),\n-                         dot_dims_bmm1.rhs_contracting_dimensions());\n-\n-  // The contracting dimensions for BMM1 need to be 64.\n-  if (!IsContractingDimSupported(lhs_contracting_dims_bmm1) ||\n-      !IsContractingDimSupported(rhs_contracting_dims_bmm1)) {\n-    if (VLOG_IS_ON(2)) {\n-      VLOG(2) << \"BMM1 lhs_contracting_dims: \"\n-              << absl::StrJoin(lhs_contracting_dims_bmm1, \",\")\n-              << \" BMM1 rhs_contracting_dims: \"\n-              << absl::StrJoin(rhs_contracting_dims_bmm1, \",\")\n-              << \" are not supported.\";\n-    }\n-    return false;\n-  }\n-  return true;\n-}\n-absl::StatusOr<bool> IsSupportedBMM2(const HloInstruction* bmm_2,\n-                                     bool need_canonicalization) {\n-  const DotDimensionNumbers& dot_dims_bmm2 = bmm_2->dot_dimension_numbers();\n-  // need swap lhs and rhs for bmm2 if canonicalization is needed\n-  int operand_index = need_canonicalization ? 0 : 1;\n-  auto batch_dim = need_canonicalization ? dot_dims_bmm2.lhs_batch_dimensions()\n-                                         : dot_dims_bmm2.rhs_batch_dimensions();\n-  auto contracting_dim = need_canonicalization\n-                             ? dot_dims_bmm2.lhs_contracting_dimensions()\n-                             : dot_dims_bmm2.rhs_contracting_dimensions();\n-      std::vector<int64_t> non_contracting_dim_nums_bmm2,\n-      GetNonContractingDims(bmm_2->operand(operand_index)->shape(), batch_dim,\n-                            contracting_dim));\n-\n-  std::vector<int64_t> non_contracting_dims_bmm2 =\n-      GetDimensionVector(bmm_2->operand(operand_index)->shape().dimensions(),\n-                         non_contracting_dim_nums_bmm2);\n-  // The non contracting dimension for BMM2 needs to be 64 for the input matrix.\n-  // The input matrix is the second argument to BMM2 i.e, rhs.\n-  if (!absl::c_all_of(non_contracting_dims_bmm2,\n-                      [](int64_t dim) { return dim == 64; })) {\n-    if (VLOG_IS_ON(2)) {\n-      VLOG(2) << \" BMM2 rhs_non_contracting_dims: \"\n-              << absl::StrJoin(non_contracting_dims_bmm2, \",\")\n-              << \" are not supported.\";\n-    }\n-    return false;\n-  return true;\n-absl::StatusOr<bool> IsFlashAttention(\n-    HloInstruction* bmm_1, bool is_causal_mask,\n-    absl::string_view custom_call_name,\n-  const DotDimensionNumbers& dnums = bmm_1->dot_dimension_numbers();\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> seq_q_dims,\n-      GetNonContractingDims(bmm_1->operand(0)->shape(),\n-                            dnums.lhs_batch_dimensions(),\n-                            dnums.lhs_contracting_dimensions()));\n-\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> seq_k_dims,\n-      GetNonContractingDims(bmm_1->operand(1)->shape(),\n-                            dnums.rhs_batch_dimensions(),\n-                            dnums.rhs_contracting_dimensions()));\n-\n-  std::vector<int64_t> seq_q =\n-      GetDimensionVector(bmm_1->operand(0)->shape().dimensions(), seq_q_dims);\n-  std::vector<int64_t> seq_k =\n-      GetDimensionVector(bmm_1->operand(1)->shape().dimensions(), seq_k_dims);\n-  std::vector<int64_t> hidden_dim =\n-      GetDimensionVector(bmm_1->operand(0)->shape().dimensions(),\n-                         dnums.lhs_contracting_dimensions());\n-  // for now, seq_q and seq_k should be equal for flash attention to work\n-  // flash attention only supports fixed topology so we check if custom call is\n-  // such topology by checking custom_call_name\n-  TF_RET_CHECK(seq_q.size() == 1);\n-  TF_RET_CHECK(seq_k.size() == 1);\n-  TF_RET_CHECK(hidden_dim.size() == 1);\n-\n-  auto is_seqlen_supported = seq_q[0] > 512 && seq_k[0] > 512 &&\n-                             seq_q[0] % 64 == 0 && seq_k[0] % 64 == 0;\n-  auto is_hidden_dim_supported = hidden_dim[0] == 64 || hidden_dim[0] == 128;\n-  auto is_flash_attention = is_seqlen_supported && is_hidden_dim_supported;\n-  auto is_cross_attention = seq_q[0] != seq_k[0];\n-\n-  // flash attention requires cuDNN 8.9.3 to run non-fused QKV\n-  // once we have fused QKV support, we can relax this contraint\n-  if (is_flash_attention &&\n-          cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 3))) {\n-    VLOG(2) << \"Require cuDNN 8.9.3 to run flash attention.\";\n-  // flash attention cross attention requires cuDNN 8.9.4 to run\n-  if (is_cross_attention &&\n-    VLOG(2) << \"Require cuDNN 8.9.4 to run flash cross attention.\";\n-  TF_ASSIGN_OR_RETURN(is_flash_attention,\n-                      IsFlashAttention(bmm_1, is_causal_mask, custom_call_name,\n-                                       cc, cudnn_version));\n-  // otherwise check if it is supported by regular attention\n-  TF_ASSIGN_OR_RETURN(bool is_bmm1_supported,\n-                      IsSupportedBMM1(bmm_1, is_training));\n-  if (!is_bmm1_supported) return false;\n-  TF_ASSIGN_OR_RETURN(bool is_bmm2_supported,\n-                      IsSupportedBMM2(bmm_2, need_canonicalization));\n-  if (!is_bmm2_supported) return false;\n-  return true;\n",
            "whole_added": "+struct QKVLayout {\n+  int64_t batch;\n+  int64_t num_heads;\n+  int64_t seqlen_q;\n+  int64_t seqlen_kv;\n+  int64_t hidden_dim;\n+};\n+\n+absl::StatusOr<std::optional<QKVLayout>> GetQKVLayout(\n+    HloInstruction* bmm_1, HloInstruction* bmm_2, bool need_canonicalization) {\n+  // get layout from bmm1\n+  const DotDimensionNumbers& bmm1_dnums = bmm_1->dot_dimension_numbers();\n+      std::vector<int64_t> bmm1_s_q_dims,\n+                            bmm1_dnums.lhs_batch_dimensions(),\n+                            bmm1_dnums.lhs_contracting_dimensions()));\n+\n+      std::vector<int64_t> bmm1_s_kv_dims,\n+                            bmm1_dnums.rhs_batch_dimensions(),\n+                            bmm1_dnums.rhs_contracting_dimensions()));\n+\n+  std::vector<int64_t> bmm1_bh =\n+                         bmm1_dnums.lhs_batch_dimensions());\n+\n+  std::vector<int64_t> bmm1_s_q = GetDimensionVector(\n+      bmm_1->operand(0)->shape().dimensions(), bmm1_s_q_dims);\n+\n+  std::vector<int64_t> bmm1_s_kv = GetDimensionVector(\n+      bmm_1->operand(1)->shape().dimensions(), bmm1_s_kv_dims);\n+  std::vector<int64_t> bmm1_d =\n+                         bmm1_dnums.lhs_contracting_dimensions());\n+  TF_RET_CHECK(bmm1_bh.size() == 2);\n+  TF_RET_CHECK(bmm1_s_q.size() == 1);\n+  TF_RET_CHECK(bmm1_s_kv.size() == 1);\n+  TF_RET_CHECK(bmm1_d.size() == 1);\n+  // get layout from bmm2\n+  const DotDimensionNumbers& bmm2_dnums = bmm_2->dot_dimension_numbers();\n+      std::vector<int64_t> bmm2_lhs_non_contracting_dims,\n+      GetNonContractingDims(bmm_2->operand(0)->shape(),\n+                            bmm2_dnums.lhs_batch_dimensions(),\n+                            bmm2_dnums.lhs_contracting_dimensions()));\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<int64_t> bmm2_rhs_non_contracting_dims,\n+      GetNonContractingDims(bmm_2->operand(1)->shape(),\n+                            bmm2_dnums.rhs_batch_dimensions(),\n+                            bmm2_dnums.rhs_contracting_dimensions()));\n+\n+  std::vector<int64_t> bmm2_bh =\n+      GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                         bmm2_dnums.lhs_batch_dimensions());\n+\n+  std::vector<int64_t> bmm2_s_kv =\n+      GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                         bmm2_dnums.lhs_contracting_dimensions());\n+\n+  std::vector<int64_t> bmm2_s_q =\n+      need_canonicalization\n+          ? GetDimensionVector(bmm_2->operand(1)->shape().dimensions(),\n+                               bmm2_rhs_non_contracting_dims)\n+          : GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                               bmm2_lhs_non_contracting_dims);\n+\n+  std::vector<int64_t> bmm2_d =\n+      need_canonicalization\n+          ? GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                               bmm2_lhs_non_contracting_dims)\n+          : GetDimensionVector(bmm_2->operand(1)->shape().dimensions(),\n+                               bmm2_rhs_non_contracting_dims);\n+\n+  TF_RET_CHECK(bmm2_bh.size() == 2);\n+  TF_RET_CHECK(bmm2_s_q.size() == 1);\n+  TF_RET_CHECK(bmm2_s_kv.size() == 1);\n+  TF_RET_CHECK(bmm2_d.size() == 1);\n+\n+  // check if bhsd is correct between bmm1 and bmm2\n+  if (bmm1_bh[0] != bmm2_bh[0] || bmm1_bh[1] != bmm2_bh[1] ||\n+      bmm1_s_q[0] != bmm2_s_q[0] || bmm1_s_kv[0] != bmm2_s_kv[0] ||\n+      bmm1_d[0] != bmm2_d[0]) {\n+    return std::nullopt;\n+\n+  QKVLayout qkv_layout;\n+  qkv_layout.batch = bmm1_bh[0];\n+  qkv_layout.num_heads = bmm1_bh[1];\n+  qkv_layout.seqlen_q = bmm1_s_q[0];\n+  qkv_layout.seqlen_kv = bmm1_s_kv[0];\n+  qkv_layout.hidden_dim = bmm1_d[0];\n+  return qkv_layout;\n+absl::StatusOr<bool> IsFusedAttention(\n+    QKVLayout qkv_layout, bool is_training,\n+  // otherwise check if it is supported by regular attention\n+  int64_t s_q = qkv_layout.seqlen_q;\n+  int64_t s_kv = qkv_layout.seqlen_kv;\n+  int64_t hidden_dim = qkv_layout.hidden_dim;\n+  bool is_seqlen_supported =\n+      (s_q <= 512 && s_kv <= 512) &&\n+      (!is_training || (s_q % 64 == 0 && s_kv % 64 == 0));\n+  bool is_hidden_dim_supported = hidden_dim == 64;\n+  bool is_fused_attention = is_seqlen_supported && is_hidden_dim_supported;\n+  return is_fused_attention;\n+}\n+absl::StatusOr<bool> IsFlashAttention(\n+    QKVLayout qkv_layout, bool is_training,\n+    stream_executor::CudaComputeCapability cc,\n+    stream_executor::dnn::VersionInfo cudnn_version) {\n+  int64_t s_q = qkv_layout.seqlen_q;\n+  int64_t s_kv = qkv_layout.seqlen_kv;\n+  int64_t hidden_dim = qkv_layout.hidden_dim;\n+  // start with most relaxed constraint\n+  bool is_seqlen_supported = (s_q > 512 || s_kv > 512) &&\n+                             (!is_training || (s_q % 2 == 0 && s_kv % 2 == 0));\n+  bool is_hidden_dim_supported = hidden_dim <= 128 && hidden_dim % 8 == 0;\n+  bool is_flash_attention = is_seqlen_supported && is_hidden_dim_supported;\n+  if (!is_flash_attention) return false;\n+  // going backwards to check compatibility\n+  if ((is_training && (s_q < 64 || s_kv < 64)) &&\n+      !IsComputeCapabilityAndCudnnSupported(\n+          cc, cudnn_version, stream_executor::dnn::VersionInfo(9, 0, 0))) {\n+    VLOG(2) << \"Flash attention training with seq < 64 not supported cuDNN < \"\n+               \"9.0.0.\";\n+    return false;\n+  }\n+  if ((hidden_dim != 64 && hidden_dim != 128) &&\n+          cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 6))) {\n+    VLOG(2) << \"Flash attention head dim != 64 or 128 not supported with cuDNN \"\n+               \"< 8.9.6.\";\n+\n+  if ((is_training && s_kv % 64 != 0) &&\n+          cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 5))) {\n+    VLOG(2) << \"Flash attention training with seq kv % 64 != 0 not supported \"\n+               \"with cuDNN < 8.9.5.\";\n+    return false;\n+  }\n+\n+  if (!IsComputeCapabilityAndCudnnSupported(\n+    VLOG(2) << \"Require cuDNN 8.9.4 to run flash attention.\";\n+  // get batch/num heads/sequence length/hidden dim from bmm1 and bmm2\n+  // also make sure they are the same between bmm1 and bmm2\n+  TF_ASSIGN_OR_RETURN(std::optional<QKVLayout> qkv_layout,\n+                      GetQKVLayout(bmm_1, bmm_2, need_canonicalization));\n+  if (!qkv_layout.has_value()) {\n+    VLOG(2) << \"bmm1 and bmm2 have different qkv layout.\";\n+    return false;\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(\n+      is_flash_attention,\n+      IsFlashAttention(qkv_layout.value(), is_training, cc, cudnn_version));\n+  // check if matched attention block is supported by cuDNN fused attention.\n+  TF_ASSIGN_OR_RETURN(\n+      bool is_fused_attention,\n+      IsFusedAttention(qkv_layout.value(), is_training, cc, cudnn_version));\n+  return is_fused_attention;\n",
            "whole_hunk": "@@ -315,19 +315,6 @@ bool IsSupportedPrimitiveType(const HloInstruction* bmm) {\n   return dtype == BF16 || dtype == F16;\n }\n \n-bool IsContractingDimSupported(absl::Span<const int64_t> contracting_dims) {\n-  return absl::c_all_of(contracting_dims,\n-                        [](int64_t dim) { return dim == 64; });\n-}\n-\n-bool IsNonContractingDimSupported(\n-    const std::vector<int64_t>& non_contracting_dims, bool is_training) {\n-  // For training, cuDNN require non_contracting_dim to be Divisible by 64\n-  return absl::c_all_of(non_contracting_dims, [&](int64_t dim) {\n-    return dim <= 512 && (!is_training || dim % 64 == 0);\n-  });\n-}\n-\n std::vector<int64_t> GetDimensionVector(absl::Span<const int64_t> dimensions,\n                                         absl::Span<const int64_t> dim_nums) {\n   std::vector<int64_t> vec(dim_nums.size());\n@@ -337,150 +324,163 @@ std::vector<int64_t> GetDimensionVector(absl::Span<const int64_t> dimensions,\n   return vec;\n }\n \n-absl::StatusOr<bool> IsSupportedBMM1(const HloInstruction* bmm_1,\n-                                     bool is_training) {\n-  const DotDimensionNumbers& dot_dims_bmm1 = bmm_1->dot_dimension_numbers();\n+struct QKVLayout {\n+  int64_t batch;\n+  int64_t num_heads;\n+  int64_t seqlen_q;\n+  int64_t seqlen_kv;\n+  int64_t hidden_dim;\n+};\n+\n+absl::StatusOr<std::optional<QKVLayout>> GetQKVLayout(\n+    HloInstruction* bmm_1, HloInstruction* bmm_2, bool need_canonicalization) {\n+  // get layout from bmm1\n+  const DotDimensionNumbers& bmm1_dnums = bmm_1->dot_dimension_numbers();\n   TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> lhs_non_contracting_dim_nums_bmm1,\n+      std::vector<int64_t> bmm1_s_q_dims,\n       GetNonContractingDims(bmm_1->operand(0)->shape(),\n-                            dot_dims_bmm1.lhs_batch_dimensions(),\n-                            dot_dims_bmm1.lhs_contracting_dimensions()));\n+                            bmm1_dnums.lhs_batch_dimensions(),\n+                            bmm1_dnums.lhs_contracting_dimensions()));\n+\n   TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> rhs_non_contracting_dim_nums_bmm1,\n+      std::vector<int64_t> bmm1_s_kv_dims,\n       GetNonContractingDims(bmm_1->operand(1)->shape(),\n-                            dot_dims_bmm1.rhs_batch_dimensions(),\n-                            dot_dims_bmm1.rhs_contracting_dimensions()));\n-  std::vector<int64_t> lhs_non_contracting_dims_bmm1 =\n+                            bmm1_dnums.rhs_batch_dimensions(),\n+                            bmm1_dnums.rhs_contracting_dimensions()));\n+\n+  std::vector<int64_t> bmm1_bh =\n       GetDimensionVector(bmm_1->operand(0)->shape().dimensions(),\n-                         lhs_non_contracting_dim_nums_bmm1);\n-  std::vector<int64_t> rhs_non_contracting_dims_bmm1 =\n-      GetDimensionVector(bmm_1->operand(1)->shape().dimensions(),\n-                         rhs_non_contracting_dim_nums_bmm1);\n-  // The non contracting dimensions for BMM1 need to be less than or equal to\n-  // 512.\n-  if (!IsNonContractingDimSupported(lhs_non_contracting_dims_bmm1,\n-                                    is_training) ||\n-      !IsNonContractingDimSupported(rhs_non_contracting_dims_bmm1,\n-                                    is_training)) {\n-    if (VLOG_IS_ON(2)) {\n-      VLOG(2) << \"BMM1 lhs_non_contracting_dims: \"\n-              << absl::StrJoin(lhs_non_contracting_dims_bmm1, \",\")\n-              << \" BMM1 rhs_non_contracting_dims: \"\n-              << absl::StrJoin(rhs_non_contracting_dims_bmm1, \",\")\n-              << \" are not supported. The non-contracting dims should be less \"\n-                 \"than 512. This is a criteria for current cuDNN 8.8 support.\";\n-    }\n-    return false;\n-  }\n+                         bmm1_dnums.lhs_batch_dimensions());\n+\n+  std::vector<int64_t> bmm1_s_q = GetDimensionVector(\n+      bmm_1->operand(0)->shape().dimensions(), bmm1_s_q_dims);\n+\n+  std::vector<int64_t> bmm1_s_kv = GetDimensionVector(\n+      bmm_1->operand(1)->shape().dimensions(), bmm1_s_kv_dims);\n \n-  std::vector<int64_t> lhs_contracting_dims_bmm1 =\n+  std::vector<int64_t> bmm1_d =\n       GetDimensionVector(bmm_1->operand(0)->shape().dimensions(),\n-                         dot_dims_bmm1.lhs_contracting_dimensions());\n-  std::vector<int64_t> rhs_contracting_dims_bmm1 =\n-      GetDimensionVector(bmm_1->operand(1)->shape().dimensions(),\n-                         dot_dims_bmm1.rhs_contracting_dimensions());\n-\n-  // The contracting dimensions for BMM1 need to be 64.\n-  if (!IsContractingDimSupported(lhs_contracting_dims_bmm1) ||\n-      !IsContractingDimSupported(rhs_contracting_dims_bmm1)) {\n-    if (VLOG_IS_ON(2)) {\n-      VLOG(2) << \"BMM1 lhs_contracting_dims: \"\n-              << absl::StrJoin(lhs_contracting_dims_bmm1, \",\")\n-              << \" BMM1 rhs_contracting_dims: \"\n-              << absl::StrJoin(rhs_contracting_dims_bmm1, \",\")\n-              << \" are not supported.\";\n-    }\n-    return false;\n-  }\n-  return true;\n-}\n+                         bmm1_dnums.lhs_contracting_dimensions());\n \n-absl::StatusOr<bool> IsSupportedBMM2(const HloInstruction* bmm_2,\n-                                     bool need_canonicalization) {\n-  const DotDimensionNumbers& dot_dims_bmm2 = bmm_2->dot_dimension_numbers();\n-  // need swap lhs and rhs for bmm2 if canonicalization is needed\n-  int operand_index = need_canonicalization ? 0 : 1;\n-  auto batch_dim = need_canonicalization ? dot_dims_bmm2.lhs_batch_dimensions()\n-                                         : dot_dims_bmm2.rhs_batch_dimensions();\n-  auto contracting_dim = need_canonicalization\n-                             ? dot_dims_bmm2.lhs_contracting_dimensions()\n-                             : dot_dims_bmm2.rhs_contracting_dimensions();\n+  TF_RET_CHECK(bmm1_bh.size() == 2);\n+  TF_RET_CHECK(bmm1_s_q.size() == 1);\n+  TF_RET_CHECK(bmm1_s_kv.size() == 1);\n+  TF_RET_CHECK(bmm1_d.size() == 1);\n \n+  // get layout from bmm2\n+  const DotDimensionNumbers& bmm2_dnums = bmm_2->dot_dimension_numbers();\n   TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> non_contracting_dim_nums_bmm2,\n-      GetNonContractingDims(bmm_2->operand(operand_index)->shape(), batch_dim,\n-                            contracting_dim));\n-\n-  std::vector<int64_t> non_contracting_dims_bmm2 =\n-      GetDimensionVector(bmm_2->operand(operand_index)->shape().dimensions(),\n-                         non_contracting_dim_nums_bmm2);\n-  // The non contracting dimension for BMM2 needs to be 64 for the input matrix.\n-  // The input matrix is the second argument to BMM2 i.e, rhs.\n-  if (!absl::c_all_of(non_contracting_dims_bmm2,\n-                      [](int64_t dim) { return dim == 64; })) {\n-    if (VLOG_IS_ON(2)) {\n-      VLOG(2) << \" BMM2 rhs_non_contracting_dims: \"\n-              << absl::StrJoin(non_contracting_dims_bmm2, \",\")\n-              << \" are not supported.\";\n-    }\n-    return false;\n+      std::vector<int64_t> bmm2_lhs_non_contracting_dims,\n+      GetNonContractingDims(bmm_2->operand(0)->shape(),\n+                            bmm2_dnums.lhs_batch_dimensions(),\n+                            bmm2_dnums.lhs_contracting_dimensions()));\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<int64_t> bmm2_rhs_non_contracting_dims,\n+      GetNonContractingDims(bmm_2->operand(1)->shape(),\n+                            bmm2_dnums.rhs_batch_dimensions(),\n+                            bmm2_dnums.rhs_contracting_dimensions()));\n+\n+  std::vector<int64_t> bmm2_bh =\n+      GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                         bmm2_dnums.lhs_batch_dimensions());\n+\n+  std::vector<int64_t> bmm2_s_kv =\n+      GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                         bmm2_dnums.lhs_contracting_dimensions());\n+\n+  std::vector<int64_t> bmm2_s_q =\n+      need_canonicalization\n+          ? GetDimensionVector(bmm_2->operand(1)->shape().dimensions(),\n+                               bmm2_rhs_non_contracting_dims)\n+          : GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                               bmm2_lhs_non_contracting_dims);\n+\n+  std::vector<int64_t> bmm2_d =\n+      need_canonicalization\n+          ? GetDimensionVector(bmm_2->operand(0)->shape().dimensions(),\n+                               bmm2_lhs_non_contracting_dims)\n+          : GetDimensionVector(bmm_2->operand(1)->shape().dimensions(),\n+                               bmm2_rhs_non_contracting_dims);\n+\n+  TF_RET_CHECK(bmm2_bh.size() == 2);\n+  TF_RET_CHECK(bmm2_s_q.size() == 1);\n+  TF_RET_CHECK(bmm2_s_kv.size() == 1);\n+  TF_RET_CHECK(bmm2_d.size() == 1);\n+\n+  // check if bhsd is correct between bmm1 and bmm2\n+  if (bmm1_bh[0] != bmm2_bh[0] || bmm1_bh[1] != bmm2_bh[1] ||\n+      bmm1_s_q[0] != bmm2_s_q[0] || bmm1_s_kv[0] != bmm2_s_kv[0] ||\n+      bmm1_d[0] != bmm2_d[0]) {\n+    return std::nullopt;\n   }\n-  return true;\n+\n+  QKVLayout qkv_layout;\n+  qkv_layout.batch = bmm1_bh[0];\n+  qkv_layout.num_heads = bmm1_bh[1];\n+  qkv_layout.seqlen_q = bmm1_s_q[0];\n+  qkv_layout.seqlen_kv = bmm1_s_kv[0];\n+  qkv_layout.hidden_dim = bmm1_d[0];\n+  return qkv_layout;\n }\n \n-absl::StatusOr<bool> IsFlashAttention(\n-    HloInstruction* bmm_1, bool is_causal_mask,\n-    absl::string_view custom_call_name,\n+absl::StatusOr<bool> IsFusedAttention(\n+    QKVLayout qkv_layout, bool is_training,\n     stream_executor::CudaComputeCapability cc,\n     stream_executor::dnn::VersionInfo cudnn_version) {\n-  const DotDimensionNumbers& dnums = bmm_1->dot_dimension_numbers();\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> seq_q_dims,\n-      GetNonContractingDims(bmm_1->operand(0)->shape(),\n-                            dnums.lhs_batch_dimensions(),\n-                            dnums.lhs_contracting_dimensions()));\n-\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<int64_t> seq_k_dims,\n-      GetNonContractingDims(bmm_1->operand(1)->shape(),\n-                            dnums.rhs_batch_dimensions(),\n-                            dnums.rhs_contracting_dimensions()));\n-\n-  std::vector<int64_t> seq_q =\n-      GetDimensionVector(bmm_1->operand(0)->shape().dimensions(), seq_q_dims);\n+  // otherwise check if it is supported by regular attention\n+  int64_t s_q = qkv_layout.seqlen_q;\n+  int64_t s_kv = qkv_layout.seqlen_kv;\n+  int64_t hidden_dim = qkv_layout.hidden_dim;\n+  bool is_seqlen_supported =\n+      (s_q <= 512 && s_kv <= 512) &&\n+      (!is_training || (s_q % 64 == 0 && s_kv % 64 == 0));\n+  bool is_hidden_dim_supported = hidden_dim == 64;\n+  bool is_fused_attention = is_seqlen_supported && is_hidden_dim_supported;\n+  return is_fused_attention;\n+}\n \n-  std::vector<int64_t> seq_k =\n-      GetDimensionVector(bmm_1->operand(1)->shape().dimensions(), seq_k_dims);\n+absl::StatusOr<bool> IsFlashAttention(\n+    QKVLayout qkv_layout, bool is_training,\n+    stream_executor::CudaComputeCapability cc,\n+    stream_executor::dnn::VersionInfo cudnn_version) {\n+  int64_t s_q = qkv_layout.seqlen_q;\n+  int64_t s_kv = qkv_layout.seqlen_kv;\n+  int64_t hidden_dim = qkv_layout.hidden_dim;\n+  // start with most relaxed constraint\n+  bool is_seqlen_supported = (s_q > 512 || s_kv > 512) &&\n+                             (!is_training || (s_q % 2 == 0 && s_kv % 2 == 0));\n+  bool is_hidden_dim_supported = hidden_dim <= 128 && hidden_dim % 8 == 0;\n+  bool is_flash_attention = is_seqlen_supported && is_hidden_dim_supported;\n+  if (!is_flash_attention) return false;\n+  // going backwards to check compatibility\n+  if ((is_training && (s_q < 64 || s_kv < 64)) &&\n+      !IsComputeCapabilityAndCudnnSupported(\n+          cc, cudnn_version, stream_executor::dnn::VersionInfo(9, 0, 0))) {\n+    VLOG(2) << \"Flash attention training with seq < 64 not supported cuDNN < \"\n+               \"9.0.0.\";\n+    return false;\n+  }\n \n-  std::vector<int64_t> hidden_dim =\n-      GetDimensionVector(bmm_1->operand(0)->shape().dimensions(),\n-                         dnums.lhs_contracting_dimensions());\n-  // for now, seq_q and seq_k should be equal for flash attention to work\n-  // flash attention only supports fixed topology so we check if custom call is\n-  // such topology by checking custom_call_name\n-  TF_RET_CHECK(seq_q.size() == 1);\n-  TF_RET_CHECK(seq_k.size() == 1);\n-  TF_RET_CHECK(hidden_dim.size() == 1);\n-\n-  auto is_seqlen_supported = seq_q[0] > 512 && seq_k[0] > 512 &&\n-                             seq_q[0] % 64 == 0 && seq_k[0] % 64 == 0;\n-  auto is_hidden_dim_supported = hidden_dim[0] == 64 || hidden_dim[0] == 128;\n-  auto is_flash_attention = is_seqlen_supported && is_hidden_dim_supported;\n-  auto is_cross_attention = seq_q[0] != seq_k[0];\n-\n-  // flash attention requires cuDNN 8.9.3 to run non-fused QKV\n-  // once we have fused QKV support, we can relax this contraint\n-  if (is_flash_attention &&\n+  if ((hidden_dim != 64 && hidden_dim != 128) &&\n       !IsComputeCapabilityAndCudnnSupported(\n-          cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 3))) {\n-    VLOG(2) << \"Require cuDNN 8.9.3 to run flash attention.\";\n+          cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 6))) {\n+    VLOG(2) << \"Flash attention head dim != 64 or 128 not supported with cuDNN \"\n+               \"< 8.9.6.\";\n     return false;\n   }\n-  // flash attention cross attention requires cuDNN 8.9.4 to run\n-  if (is_cross_attention &&\n+\n+  if ((is_training && s_kv % 64 != 0) &&\n       !IsComputeCapabilityAndCudnnSupported(\n+          cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 5))) {\n+    VLOG(2) << \"Flash attention training with seq kv % 64 != 0 not supported \"\n+               \"with cuDNN < 8.9.5.\";\n+    return false;\n+  }\n+\n+  if (!IsComputeCapabilityAndCudnnSupported(\n           cc, cudnn_version, stream_executor::dnn::VersionInfo(8, 9, 4))) {\n-    VLOG(2) << \"Require cuDNN 8.9.4 to run flash cross attention.\";\n+    VLOG(2) << \"Require cuDNN 8.9.4 to run flash attention.\";\n     return false;\n   }\n   return is_flash_attention;\n@@ -1239,10 +1239,19 @@ absl::StatusOr<bool> IsMHABlockSupported(\n     return false;\n   }\n \n+  // get batch/num heads/sequence length/hidden dim from bmm1 and bmm2\n+  // also make sure they are the same between bmm1 and bmm2\n+  TF_ASSIGN_OR_RETURN(std::optional<QKVLayout> qkv_layout,\n+                      GetQKVLayout(bmm_1, bmm_2, need_canonicalization));\n+  if (!qkv_layout.has_value()) {\n+    VLOG(2) << \"bmm1 and bmm2 have different qkv layout.\";\n+    return false;\n+  }\n+\n   // check if matched attention block is supported by cuDNN flash attention.\n-  TF_ASSIGN_OR_RETURN(is_flash_attention,\n-                      IsFlashAttention(bmm_1, is_causal_mask, custom_call_name,\n-                                       cc, cudnn_version));\n+  TF_ASSIGN_OR_RETURN(\n+      is_flash_attention,\n+      IsFlashAttention(qkv_layout.value(), is_training, cc, cudnn_version));\n   if (is_flash_attention) {\n     if (is_causal_mask) {\n       // if bias is causal mask, needs to remove bias from name\n@@ -1259,14 +1268,11 @@ absl::StatusOr<bool> IsMHABlockSupported(\n     }\n     return true;\n   }\n-  // otherwise check if it is supported by regular attention\n-  TF_ASSIGN_OR_RETURN(bool is_bmm1_supported,\n-                      IsSupportedBMM1(bmm_1, is_training));\n-  if (!is_bmm1_supported) return false;\n-  TF_ASSIGN_OR_RETURN(bool is_bmm2_supported,\n-                      IsSupportedBMM2(bmm_2, need_canonicalization));\n-  if (!is_bmm2_supported) return false;\n-  return true;\n+  // check if matched attention block is supported by cuDNN fused attention.\n+  TF_ASSIGN_OR_RETURN(\n+      bool is_fused_attention,\n+      IsFusedAttention(qkv_layout.value(), is_training, cc, cudnn_version));\n+  return is_fused_attention;\n }\n \n absl::StatusOr<HloInstruction*> CanonicalizeBatchedGemmForcuDNNFMHA(\n"
        },
        {
            "name": "cudnn_fused_mha_rewriter_test.cc",
            "path": "third_party/xla/xla/service/gpu/cudnn_fused_mha_rewriter_test.cc",
            "patches": [
                {
                    "old_start": 90,
                    "old_length": 13,
                    "new_start": 90,
                    "new_length": 6,
                    "hunk": "@@ -90,13 +90,6 @@ class CudnnFusedMhaRewriterTestHloTest : public HloTestBase {\n   }\n \n   se::dnn::VersionInfo GetCudnnVersionWithFlashAttentionSupport() {\n-    // Fake a supported compute capability to run tests,\n-    // we don't run any kernels in these tests so they should be safe\n-    // to run anywhere.\n-    return se::dnn::VersionInfo(8, 9, 3);\n-  }\n-\n-  se::dnn::VersionInfo GetCudnnVersionWithFlashCrossAttentionSupport() {\n     // Fake a supported compute capability to run tests,\n     // we don't run any kernels in these tests so they should be safe\n     // to run anywhere.\n"
                },
                {
                    "old_start": 4361,
                    "old_length": 73,
                    "new_start": 4354,
                    "new_length": 6,
                    "hunk": "@@ -4361,73 +4354,6 @@ ENTRY main.82 {\n   EXPECT_EQ(config.is_causal_mask(), false);\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       FlashAttentionF16Bmm1BiasSoftmaxBmm2PatternCrossAttention) {\n-  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n-  const char* module_str = R\"(\n-HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,2048,64]{3,2,1,0},f16[2,6,64,1024]{3,2,1,0},f16[2,6,1024,64]{3,2,1,0},f16[2,6,2048,1024]{3,2,1,0})->f16[2,6,2048,64]{3,2,1,0}}\n-\n-region_0.7 {\n-  Arg_0.8 = f16[] parameter(0)\n-  Arg_1.9 = f16[] parameter(1)\n-  ROOT maximum = f16[] maximum(Arg_0.8, Arg_1.9)\n-}\n-\n-region_1.19 {\n-  Arg_0.20 = f32[] parameter(0)\n-  Arg_1.21 = f32[] parameter(1)\n-  ROOT add = f32[] add(Arg_0.20, Arg_1.21)\n-}\n-\n-ENTRY main.31 {\n-  Arg_0.1 = f16[2,6,2048,64]{3,2,1,0} parameter(0), sharding={replicated}\n-  Arg_1.2 = f16[2,6,64,1024]{3,2,1,0} parameter(1), sharding={replicated}\n-  dot = f16[2,6,2048,1024]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_contracting_dims={3}, rhs_contracting_dims={2}, lhs_batch_dims={0,1}, rhs_batch_dims={0,1}\n-  Arg_3.4 = f16[2,6,2048,1024]{3,2,1,0} parameter(3), sharding={replicated}\n-  add.1 = f16[2,6,2048,1024]{3,2,1,0} add(dot, Arg_3.4)\n-  constant = f16[] constant(-inf)\n-  reduce.11 = f16[2,6,2048]{2,1,0} reduce(add.1, constant), dimensions={3}, to_apply=region_0.7\n-  broadcast.3 = f16[2,6,2048,1024]{3,2,1,0} broadcast(reduce.11), dimensions={0,1,2}\n-  subtract.1 = f16[2,6,2048,1024]{3,2,1,0} subtract(add.1, broadcast.3)\n-  exponential.1 = f16[2,6,2048,1024]{3,2,1,0} exponential(subtract.1)\n-  convert.1 = f32[2,6,2048,1024]{3,2,1,0} convert(exponential.1)\n-  constant.1 = f32[] constant(0)\n-  reduce.23 = f32[2,6,2048]{2,1,0} reduce(convert.1, constant.1), dimensions={3}, to_apply=region_1.19\n-  convert.2 = f16[2,6,2048]{2,1,0} convert(reduce.23)\n-  broadcast.4 = f16[2,6,2048,1024]{3,2,1,0} broadcast(convert.2), dimensions={0,1,2}\n-  divide = f16[2,6,2048,1024]{3,2,1,0} divide(exponential.1, broadcast.4)\n-  Arg_2.3 = f16[2,6,1024,64]{3,2,1,0} parameter(2), sharding={replicated}\n-  ROOT dot.1 = f16[2,6,2048,64]{3,2,1,0} dot(divide, Arg_2.3), lhs_contracting_dims={3}, rhs_contracting_dims={2}, lhs_batch_dims={0,1}, rhs_batch_dims={0,1}\n-})\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n-  CudnnFusedMHARewriter fusedMhaRewriter{\n-      GetCudaComputeCapability(), GetCudnnVersionWithFlashAttentionSupport()};\n-  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n-\n-  SCOPED_TRACE(m->ToString());\n-  EXPECT_THAT(m->entry_computation()->root_instruction(), GmockMatch(m::Dot()));\n-\n-  CudnnFusedMHARewriter fusedMhaRewriterWithCrossAttention{\n-      GetCudaComputeCapability(),\n-      GetCudnnVersionWithFlashCrossAttentionSupport()};\n-  TF_ASSERT_OK(\n-      RunHloPass(&fusedMhaRewriterWithCrossAttention, m.get()).status());\n-  SCOPED_TRACE(m->ToString());\n-  const HloInstruction* fmha;\n-  EXPECT_THAT(\n-      m->entry_computation()->root_instruction(),\n-      GmockMatch(\n-          m::GetTupleElement(\n-              m::CustomCall(&fmha, {kCudnnfMHAScaleBiasSoftmaxCallTarget}), 0)\n-              .WithShape(F16, {2, 6, 2048, 64})));\n-  TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n-                          fmha->backend_config<GpuBackendConfig>());\n-  const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n-  EXPECT_EQ(config.is_flash_attention(), true);\n-  EXPECT_EQ(config.is_causal_mask(), false);\n-}\n-\n // GPT3 pattern\n TEST_F(CudnnFusedMhaRewriterTestHloTest, FlashAttentionBF16TrainingGPT3_5B) {\n   if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n"
                },
                {
                    "old_start": 5194,
                    "old_length": 6,
                    "new_start": 5120,
                    "new_length": 118,
                    "hunk": "@@ -5194,6 +5120,118 @@ ENTRY main.164_spmd {\n               }))))));\n }\n \n+constexpr absl::string_view hlo_should_lower_to_flash_attention = R\"(\n+HloModule fmha_test, entry_computation_layout={(bf16[16,16,128,64]{3,2,1,0},bf16[16,16,1024,64]{3,2,1,0},bf16[16,16,1024,64]{3,2,1,0})->bf16[16,16,128,64]{3,2,1,0}}\n+ENTRY main.6 {\n+  Arg_0.1 = bf16[16,16,128,64]{3,2,1,0} parameter(0)\n+  Arg_1.2 = bf16[16,16,1024,64]{3,2,1,0} parameter(1)\n+  Arg_2.3 = bf16[16,16,1024,64]{3,2,1,0} parameter(2)\n+  dot.0 = bf16[16,16,128,1024]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n+  ROOT dot.1 = bf16[16,16,128,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n+})\";\n+\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, ShouldLowerToFlashAttention) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m, ParseAndReturnVerifiedModule(hlo_should_lower_to_flash_attention,\n+                                           GetModuleConfig()));\n+  CudnnFusedMHARewriter fusedMhaRewriter{\n+      GetCudaComputeCapability(), GetCudnnVersionWithFlashAttentionSupport()};\n+  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n+  const HloInstruction* fmha;\n+\n+  SCOPED_TRACE(m->ToString());\n+  EXPECT_THAT(\n+      m->entry_computation()->root_instruction(),\n+      GmockMatch(m::GetTupleElement(\n+                     m::CustomCall(&fmha, {kCudnnfMHABmmBmmCallTarget}), 0)\n+                     .WithShape(BF16, {16, 16, 128, 64})));\n+  TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n+                          fmha->backend_config<GpuBackendConfig>());\n+  const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n+  EXPECT_EQ(config.fmha_scale(), 1.0);\n+  EXPECT_EQ(config.dropout_rate(), 0.0);\n+  EXPECT_EQ(config.is_flash_attention(), true);\n+}\n+\n+constexpr absl::string_view hlo_head_dim_not_multiple_of_64 = R\"(\n+HloModule jit__reference, entry_computation_layout={(f16[4,48,1024,16]{3,2,1,0}, f16[4,48,1024,16]{3,2,1,0}, f16[4,48,1024,16]{3,2,1,0})->f16[4,48,1024,16]{3,2,1,0}}\n+\n+region_0.26 {\n+  Arg_0.27 = f32[] parameter(0)\n+  Arg_1.28 = f32[] parameter(1)\n+  ROOT maximum = f32[] maximum(Arg_0.27, Arg_1.28)\n+}\n+\n+region_1.37 {\n+  Arg_0.38 = f32[] parameter(0)\n+  Arg_1.39 = f32[] parameter(1)\n+  ROOT add = f32[] add(Arg_0.38, Arg_1.39)\n+}\n+\n+ENTRY main.49 {\n+  iota.2 = s32[1024,1024]{1,0} iota(), iota_dimension=0\n+  iota.3 = s32[1024,1024]{1,0} iota(), iota_dimension=1\n+  compare = pred[1024,1024]{1,0} compare(iota.2, iota.3), direction=GE\n+  broadcast.4 = pred[4,48,1024,1024]{3,2,1,0} broadcast(compare), dimensions={2,3}\n+  Arg_0.1 = f16[4,48,1024,16]{3,2,1,0} parameter(0)\n+  Arg_1.2 = f16[4,48,1024,16]{3,2,1,0} parameter(1)\n+  dot.9 = f16[4,48,1024,1024]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}\n+  constant.4 = f16[] constant(0.5)\n+  broadcast.6 = f16[4,48,1024,1024]{3,2,1,0} broadcast(constant.4), dimensions={}\n+  multiply = f16[4,48,1024,1024]{3,2,1,0} multiply(dot.9, broadcast.6)\n+  constant = f16[] constant(-inf)\n+  broadcast.7 = f16[4,48,1024,1024]{3,2,1,0} broadcast(constant), dimensions={}\n+  select.1 = f16[4,48,1024,1024]{3,2,1,0} select(broadcast.4, multiply, broadcast.7)\n+  convert.1 = f32[4,48,1024,1024]{3,2,1,0} convert(select.1)\n+  constant.7 = f32[] constant(-inf)\n+  reduce.30 = f32[4,48,1024]{2,1,0} reduce(convert.1, constant.7), dimensions={3}, to_apply=region_0.26\n+  broadcast.8 = f32[4,48,1024,1024]{3,2,1,0} broadcast(reduce.30), dimensions={0,1,2}\n+  subtract = f32[4,48,1024,1024]{3,2,1,0} subtract(convert.1, broadcast.8)\n+  exponential = f32[4,48,1024,1024]{3,2,1,0} exponential(subtract)\n+  constant.6 = f32[] constant(0)\n+  reduce.41 = f32[4,48,1024]{2,1,0} reduce(exponential, constant.6), dimensions={3}, to_apply=region_1.37\n+  broadcast.9 = f32[4,48,1024,1024]{3,2,1,0} broadcast(reduce.41), dimensions={0,1,2}\n+  divide = f32[4,48,1024,1024]{3,2,1,0} divide(exponential, broadcast.9)\n+  convert.2 = f16[4,48,1024,1024]{3,2,1,0} convert(divide)\n+  Arg_2.3 = f16[4,48,1024,16]{3,2,1,0} parameter(2)\n+  ROOT dot.48 = f16[4,48,1024,16]{3,2,1,0} dot(convert.2, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+} // main.49\n+)\";\n+\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, HeadDimNotMultipleOf64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m, ParseAndReturnVerifiedModule(hlo_head_dim_not_multiple_of_64,\n+                                           GetModuleConfig()));\n+  CudnnFusedMHARewriter fusedMhaRewriter{\n+      GetCudaComputeCapability(), GetCudnnVersionWithFlashAttentionSupport()};\n+  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n+\n+  // head dim not a multiple of 64 should not be lowered with cuDNN < 8.9.6\n+  SCOPED_TRACE(m->ToString());\n+  EXPECT_THAT(m->entry_computation()->root_instruction(), GmockMatch(m::Dot()));\n+\n+  // should be lowered with cuDNN >= 8.9.6\n+  CudnnFusedMHARewriter fusedMhaRewriterWithcuDNN8907{\n+      GetCudaComputeCapability(), se::dnn::VersionInfo(8, 9, 7)};\n+  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriterWithcuDNN8907, m.get()).status());\n+  const HloInstruction* fmha;\n+\n+  SCOPED_TRACE(m->ToString());\n+  EXPECT_THAT(\n+      m->entry_computation()->root_instruction(),\n+      GmockMatch(\n+          m::GetTupleElement(\n+              m::CustomCall(&fmha, {kCudnnfMHAScaleMaskSoftmaxCallTarget}), 0)\n+              .WithShape(F16, {4, 48, 1024, 16})));\n+  TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n+                          fmha->backend_config<GpuBackendConfig>());\n+  const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n+  EXPECT_EQ(config.fmha_scale(), 0.5);\n+  EXPECT_EQ(config.dropout_rate(), 0.0);\n+  EXPECT_EQ(config.is_flash_attention(), true);\n+}\n }  // anonymous namespace\n }  // namespace gpu\n }  // namespace xla"
                }
            ],
            "whole_deleted": "-    // Fake a supported compute capability to run tests,\n-    // we don't run any kernels in these tests so they should be safe\n-    // to run anywhere.\n-    return se::dnn::VersionInfo(8, 9, 3);\n-  }\n-\n-  se::dnn::VersionInfo GetCudnnVersionWithFlashCrossAttentionSupport() {\n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       FlashAttentionF16Bmm1BiasSoftmaxBmm2PatternCrossAttention) {\n-  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n-  const char* module_str = R\"(\n-HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,2048,64]{3,2,1,0},f16[2,6,64,1024]{3,2,1,0},f16[2,6,1024,64]{3,2,1,0},f16[2,6,2048,1024]{3,2,1,0})->f16[2,6,2048,64]{3,2,1,0}}\n-\n-region_0.7 {\n-  Arg_0.8 = f16[] parameter(0)\n-  Arg_1.9 = f16[] parameter(1)\n-  ROOT maximum = f16[] maximum(Arg_0.8, Arg_1.9)\n-}\n-\n-region_1.19 {\n-  Arg_0.20 = f32[] parameter(0)\n-  Arg_1.21 = f32[] parameter(1)\n-  ROOT add = f32[] add(Arg_0.20, Arg_1.21)\n-}\n-\n-ENTRY main.31 {\n-  Arg_0.1 = f16[2,6,2048,64]{3,2,1,0} parameter(0), sharding={replicated}\n-  Arg_1.2 = f16[2,6,64,1024]{3,2,1,0} parameter(1), sharding={replicated}\n-  dot = f16[2,6,2048,1024]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_contracting_dims={3}, rhs_contracting_dims={2}, lhs_batch_dims={0,1}, rhs_batch_dims={0,1}\n-  Arg_3.4 = f16[2,6,2048,1024]{3,2,1,0} parameter(3), sharding={replicated}\n-  add.1 = f16[2,6,2048,1024]{3,2,1,0} add(dot, Arg_3.4)\n-  constant = f16[] constant(-inf)\n-  reduce.11 = f16[2,6,2048]{2,1,0} reduce(add.1, constant), dimensions={3}, to_apply=region_0.7\n-  broadcast.3 = f16[2,6,2048,1024]{3,2,1,0} broadcast(reduce.11), dimensions={0,1,2}\n-  subtract.1 = f16[2,6,2048,1024]{3,2,1,0} subtract(add.1, broadcast.3)\n-  exponential.1 = f16[2,6,2048,1024]{3,2,1,0} exponential(subtract.1)\n-  convert.1 = f32[2,6,2048,1024]{3,2,1,0} convert(exponential.1)\n-  constant.1 = f32[] constant(0)\n-  reduce.23 = f32[2,6,2048]{2,1,0} reduce(convert.1, constant.1), dimensions={3}, to_apply=region_1.19\n-  convert.2 = f16[2,6,2048]{2,1,0} convert(reduce.23)\n-  broadcast.4 = f16[2,6,2048,1024]{3,2,1,0} broadcast(convert.2), dimensions={0,1,2}\n-  divide = f16[2,6,2048,1024]{3,2,1,0} divide(exponential.1, broadcast.4)\n-  Arg_2.3 = f16[2,6,1024,64]{3,2,1,0} parameter(2), sharding={replicated}\n-  ROOT dot.1 = f16[2,6,2048,64]{3,2,1,0} dot(divide, Arg_2.3), lhs_contracting_dims={3}, rhs_contracting_dims={2}, lhs_batch_dims={0,1}, rhs_batch_dims={0,1}\n-})\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n-  CudnnFusedMHARewriter fusedMhaRewriter{\n-      GetCudaComputeCapability(), GetCudnnVersionWithFlashAttentionSupport()};\n-  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n-\n-  SCOPED_TRACE(m->ToString());\n-  EXPECT_THAT(m->entry_computation()->root_instruction(), GmockMatch(m::Dot()));\n-\n-  CudnnFusedMHARewriter fusedMhaRewriterWithCrossAttention{\n-      GetCudaComputeCapability(),\n-      GetCudnnVersionWithFlashCrossAttentionSupport()};\n-  TF_ASSERT_OK(\n-      RunHloPass(&fusedMhaRewriterWithCrossAttention, m.get()).status());\n-  SCOPED_TRACE(m->ToString());\n-  const HloInstruction* fmha;\n-  EXPECT_THAT(\n-      m->entry_computation()->root_instruction(),\n-      GmockMatch(\n-          m::GetTupleElement(\n-              m::CustomCall(&fmha, {kCudnnfMHAScaleBiasSoftmaxCallTarget}), 0)\n-              .WithShape(F16, {2, 6, 2048, 64})));\n-  TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n-                          fmha->backend_config<GpuBackendConfig>());\n-  const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n-  EXPECT_EQ(config.is_flash_attention(), true);\n-  EXPECT_EQ(config.is_causal_mask(), false);\n-}\n-\n",
            "whole_added": "+constexpr absl::string_view hlo_should_lower_to_flash_attention = R\"(\n+HloModule fmha_test, entry_computation_layout={(bf16[16,16,128,64]{3,2,1,0},bf16[16,16,1024,64]{3,2,1,0},bf16[16,16,1024,64]{3,2,1,0})->bf16[16,16,128,64]{3,2,1,0}}\n+ENTRY main.6 {\n+  Arg_0.1 = bf16[16,16,128,64]{3,2,1,0} parameter(0)\n+  Arg_1.2 = bf16[16,16,1024,64]{3,2,1,0} parameter(1)\n+  Arg_2.3 = bf16[16,16,1024,64]{3,2,1,0} parameter(2)\n+  dot.0 = bf16[16,16,128,1024]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n+  ROOT dot.1 = bf16[16,16,128,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n+})\";\n+\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, ShouldLowerToFlashAttention) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m, ParseAndReturnVerifiedModule(hlo_should_lower_to_flash_attention,\n+                                           GetModuleConfig()));\n+  CudnnFusedMHARewriter fusedMhaRewriter{\n+      GetCudaComputeCapability(), GetCudnnVersionWithFlashAttentionSupport()};\n+  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n+  const HloInstruction* fmha;\n+\n+  SCOPED_TRACE(m->ToString());\n+  EXPECT_THAT(\n+      m->entry_computation()->root_instruction(),\n+      GmockMatch(m::GetTupleElement(\n+                     m::CustomCall(&fmha, {kCudnnfMHABmmBmmCallTarget}), 0)\n+                     .WithShape(BF16, {16, 16, 128, 64})));\n+  TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n+                          fmha->backend_config<GpuBackendConfig>());\n+  const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n+  EXPECT_EQ(config.fmha_scale(), 1.0);\n+  EXPECT_EQ(config.dropout_rate(), 0.0);\n+  EXPECT_EQ(config.is_flash_attention(), true);\n+}\n+\n+constexpr absl::string_view hlo_head_dim_not_multiple_of_64 = R\"(\n+HloModule jit__reference, entry_computation_layout={(f16[4,48,1024,16]{3,2,1,0}, f16[4,48,1024,16]{3,2,1,0}, f16[4,48,1024,16]{3,2,1,0})->f16[4,48,1024,16]{3,2,1,0}}\n+\n+region_0.26 {\n+  Arg_0.27 = f32[] parameter(0)\n+  Arg_1.28 = f32[] parameter(1)\n+  ROOT maximum = f32[] maximum(Arg_0.27, Arg_1.28)\n+}\n+\n+region_1.37 {\n+  Arg_0.38 = f32[] parameter(0)\n+  Arg_1.39 = f32[] parameter(1)\n+  ROOT add = f32[] add(Arg_0.38, Arg_1.39)\n+}\n+\n+ENTRY main.49 {\n+  iota.2 = s32[1024,1024]{1,0} iota(), iota_dimension=0\n+  iota.3 = s32[1024,1024]{1,0} iota(), iota_dimension=1\n+  compare = pred[1024,1024]{1,0} compare(iota.2, iota.3), direction=GE\n+  broadcast.4 = pred[4,48,1024,1024]{3,2,1,0} broadcast(compare), dimensions={2,3}\n+  Arg_0.1 = f16[4,48,1024,16]{3,2,1,0} parameter(0)\n+  Arg_1.2 = f16[4,48,1024,16]{3,2,1,0} parameter(1)\n+  dot.9 = f16[4,48,1024,1024]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}\n+  constant.4 = f16[] constant(0.5)\n+  broadcast.6 = f16[4,48,1024,1024]{3,2,1,0} broadcast(constant.4), dimensions={}\n+  multiply = f16[4,48,1024,1024]{3,2,1,0} multiply(dot.9, broadcast.6)\n+  constant = f16[] constant(-inf)\n+  broadcast.7 = f16[4,48,1024,1024]{3,2,1,0} broadcast(constant), dimensions={}\n+  select.1 = f16[4,48,1024,1024]{3,2,1,0} select(broadcast.4, multiply, broadcast.7)\n+  convert.1 = f32[4,48,1024,1024]{3,2,1,0} convert(select.1)\n+  constant.7 = f32[] constant(-inf)\n+  reduce.30 = f32[4,48,1024]{2,1,0} reduce(convert.1, constant.7), dimensions={3}, to_apply=region_0.26\n+  broadcast.8 = f32[4,48,1024,1024]{3,2,1,0} broadcast(reduce.30), dimensions={0,1,2}\n+  subtract = f32[4,48,1024,1024]{3,2,1,0} subtract(convert.1, broadcast.8)\n+  exponential = f32[4,48,1024,1024]{3,2,1,0} exponential(subtract)\n+  constant.6 = f32[] constant(0)\n+  reduce.41 = f32[4,48,1024]{2,1,0} reduce(exponential, constant.6), dimensions={3}, to_apply=region_1.37\n+  broadcast.9 = f32[4,48,1024,1024]{3,2,1,0} broadcast(reduce.41), dimensions={0,1,2}\n+  divide = f32[4,48,1024,1024]{3,2,1,0} divide(exponential, broadcast.9)\n+  convert.2 = f16[4,48,1024,1024]{3,2,1,0} convert(divide)\n+  Arg_2.3 = f16[4,48,1024,16]{3,2,1,0} parameter(2)\n+  ROOT dot.48 = f16[4,48,1024,16]{3,2,1,0} dot(convert.2, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+} // main.49\n+)\";\n+\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, HeadDimNotMultipleOf64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m, ParseAndReturnVerifiedModule(hlo_head_dim_not_multiple_of_64,\n+                                           GetModuleConfig()));\n+  CudnnFusedMHARewriter fusedMhaRewriter{\n+      GetCudaComputeCapability(), GetCudnnVersionWithFlashAttentionSupport()};\n+  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n+\n+  // head dim not a multiple of 64 should not be lowered with cuDNN < 8.9.6\n+  SCOPED_TRACE(m->ToString());\n+  EXPECT_THAT(m->entry_computation()->root_instruction(), GmockMatch(m::Dot()));\n+\n+  // should be lowered with cuDNN >= 8.9.6\n+  CudnnFusedMHARewriter fusedMhaRewriterWithcuDNN8907{\n+      GetCudaComputeCapability(), se::dnn::VersionInfo(8, 9, 7)};\n+  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriterWithcuDNN8907, m.get()).status());\n+  const HloInstruction* fmha;\n+\n+  SCOPED_TRACE(m->ToString());\n+  EXPECT_THAT(\n+      m->entry_computation()->root_instruction(),\n+      GmockMatch(\n+          m::GetTupleElement(\n+              m::CustomCall(&fmha, {kCudnnfMHAScaleMaskSoftmaxCallTarget}), 0)\n+              .WithShape(F16, {4, 48, 1024, 16})));\n+  TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n+                          fmha->backend_config<GpuBackendConfig>());\n+  const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n+  EXPECT_EQ(config.fmha_scale(), 0.5);\n+  EXPECT_EQ(config.dropout_rate(), 0.0);\n+  EXPECT_EQ(config.is_flash_attention(), true);\n+}\n",
            "whole_hunk": "@@ -90,13 +90,6 @@ class CudnnFusedMhaRewriterTestHloTest : public HloTestBase {\n   }\n \n   se::dnn::VersionInfo GetCudnnVersionWithFlashAttentionSupport() {\n-    // Fake a supported compute capability to run tests,\n-    // we don't run any kernels in these tests so they should be safe\n-    // to run anywhere.\n-    return se::dnn::VersionInfo(8, 9, 3);\n-  }\n-\n-  se::dnn::VersionInfo GetCudnnVersionWithFlashCrossAttentionSupport() {\n     // Fake a supported compute capability to run tests,\n     // we don't run any kernels in these tests so they should be safe\n     // to run anywhere.\n@@ -4361,73 +4354,6 @@ ENTRY main.82 {\n   EXPECT_EQ(config.is_causal_mask(), false);\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       FlashAttentionF16Bmm1BiasSoftmaxBmm2PatternCrossAttention) {\n-  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n-  const char* module_str = R\"(\n-HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,2048,64]{3,2,1,0},f16[2,6,64,1024]{3,2,1,0},f16[2,6,1024,64]{3,2,1,0},f16[2,6,2048,1024]{3,2,1,0})->f16[2,6,2048,64]{3,2,1,0}}\n-\n-region_0.7 {\n-  Arg_0.8 = f16[] parameter(0)\n-  Arg_1.9 = f16[] parameter(1)\n-  ROOT maximum = f16[] maximum(Arg_0.8, Arg_1.9)\n-}\n-\n-region_1.19 {\n-  Arg_0.20 = f32[] parameter(0)\n-  Arg_1.21 = f32[] parameter(1)\n-  ROOT add = f32[] add(Arg_0.20, Arg_1.21)\n-}\n-\n-ENTRY main.31 {\n-  Arg_0.1 = f16[2,6,2048,64]{3,2,1,0} parameter(0), sharding={replicated}\n-  Arg_1.2 = f16[2,6,64,1024]{3,2,1,0} parameter(1), sharding={replicated}\n-  dot = f16[2,6,2048,1024]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_contracting_dims={3}, rhs_contracting_dims={2}, lhs_batch_dims={0,1}, rhs_batch_dims={0,1}\n-  Arg_3.4 = f16[2,6,2048,1024]{3,2,1,0} parameter(3), sharding={replicated}\n-  add.1 = f16[2,6,2048,1024]{3,2,1,0} add(dot, Arg_3.4)\n-  constant = f16[] constant(-inf)\n-  reduce.11 = f16[2,6,2048]{2,1,0} reduce(add.1, constant), dimensions={3}, to_apply=region_0.7\n-  broadcast.3 = f16[2,6,2048,1024]{3,2,1,0} broadcast(reduce.11), dimensions={0,1,2}\n-  subtract.1 = f16[2,6,2048,1024]{3,2,1,0} subtract(add.1, broadcast.3)\n-  exponential.1 = f16[2,6,2048,1024]{3,2,1,0} exponential(subtract.1)\n-  convert.1 = f32[2,6,2048,1024]{3,2,1,0} convert(exponential.1)\n-  constant.1 = f32[] constant(0)\n-  reduce.23 = f32[2,6,2048]{2,1,0} reduce(convert.1, constant.1), dimensions={3}, to_apply=region_1.19\n-  convert.2 = f16[2,6,2048]{2,1,0} convert(reduce.23)\n-  broadcast.4 = f16[2,6,2048,1024]{3,2,1,0} broadcast(convert.2), dimensions={0,1,2}\n-  divide = f16[2,6,2048,1024]{3,2,1,0} divide(exponential.1, broadcast.4)\n-  Arg_2.3 = f16[2,6,1024,64]{3,2,1,0} parameter(2), sharding={replicated}\n-  ROOT dot.1 = f16[2,6,2048,64]{3,2,1,0} dot(divide, Arg_2.3), lhs_contracting_dims={3}, rhs_contracting_dims={2}, lhs_batch_dims={0,1}, rhs_batch_dims={0,1}\n-})\";\n-\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n-  CudnnFusedMHARewriter fusedMhaRewriter{\n-      GetCudaComputeCapability(), GetCudnnVersionWithFlashAttentionSupport()};\n-  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n-\n-  SCOPED_TRACE(m->ToString());\n-  EXPECT_THAT(m->entry_computation()->root_instruction(), GmockMatch(m::Dot()));\n-\n-  CudnnFusedMHARewriter fusedMhaRewriterWithCrossAttention{\n-      GetCudaComputeCapability(),\n-      GetCudnnVersionWithFlashCrossAttentionSupport()};\n-  TF_ASSERT_OK(\n-      RunHloPass(&fusedMhaRewriterWithCrossAttention, m.get()).status());\n-  SCOPED_TRACE(m->ToString());\n-  const HloInstruction* fmha;\n-  EXPECT_THAT(\n-      m->entry_computation()->root_instruction(),\n-      GmockMatch(\n-          m::GetTupleElement(\n-              m::CustomCall(&fmha, {kCudnnfMHAScaleBiasSoftmaxCallTarget}), 0)\n-              .WithShape(F16, {2, 6, 2048, 64})));\n-  TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n-                          fmha->backend_config<GpuBackendConfig>());\n-  const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n-  EXPECT_EQ(config.is_flash_attention(), true);\n-  EXPECT_EQ(config.is_causal_mask(), false);\n-}\n-\n // GPT3 pattern\n TEST_F(CudnnFusedMhaRewriterTestHloTest, FlashAttentionBF16TrainingGPT3_5B) {\n   if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n@@ -5194,6 +5120,118 @@ ENTRY main.164_spmd {\n               }))))));\n }\n \n+constexpr absl::string_view hlo_should_lower_to_flash_attention = R\"(\n+HloModule fmha_test, entry_computation_layout={(bf16[16,16,128,64]{3,2,1,0},bf16[16,16,1024,64]{3,2,1,0},bf16[16,16,1024,64]{3,2,1,0})->bf16[16,16,128,64]{3,2,1,0}}\n+ENTRY main.6 {\n+  Arg_0.1 = bf16[16,16,128,64]{3,2,1,0} parameter(0)\n+  Arg_1.2 = bf16[16,16,1024,64]{3,2,1,0} parameter(1)\n+  Arg_2.3 = bf16[16,16,1024,64]{3,2,1,0} parameter(2)\n+  dot.0 = bf16[16,16,128,1024]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n+  ROOT dot.1 = bf16[16,16,128,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n+})\";\n+\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, ShouldLowerToFlashAttention) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m, ParseAndReturnVerifiedModule(hlo_should_lower_to_flash_attention,\n+                                           GetModuleConfig()));\n+  CudnnFusedMHARewriter fusedMhaRewriter{\n+      GetCudaComputeCapability(), GetCudnnVersionWithFlashAttentionSupport()};\n+  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n+  const HloInstruction* fmha;\n+\n+  SCOPED_TRACE(m->ToString());\n+  EXPECT_THAT(\n+      m->entry_computation()->root_instruction(),\n+      GmockMatch(m::GetTupleElement(\n+                     m::CustomCall(&fmha, {kCudnnfMHABmmBmmCallTarget}), 0)\n+                     .WithShape(BF16, {16, 16, 128, 64})));\n+  TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n+                          fmha->backend_config<GpuBackendConfig>());\n+  const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n+  EXPECT_EQ(config.fmha_scale(), 1.0);\n+  EXPECT_EQ(config.dropout_rate(), 0.0);\n+  EXPECT_EQ(config.is_flash_attention(), true);\n+}\n+\n+constexpr absl::string_view hlo_head_dim_not_multiple_of_64 = R\"(\n+HloModule jit__reference, entry_computation_layout={(f16[4,48,1024,16]{3,2,1,0}, f16[4,48,1024,16]{3,2,1,0}, f16[4,48,1024,16]{3,2,1,0})->f16[4,48,1024,16]{3,2,1,0}}\n+\n+region_0.26 {\n+  Arg_0.27 = f32[] parameter(0)\n+  Arg_1.28 = f32[] parameter(1)\n+  ROOT maximum = f32[] maximum(Arg_0.27, Arg_1.28)\n+}\n+\n+region_1.37 {\n+  Arg_0.38 = f32[] parameter(0)\n+  Arg_1.39 = f32[] parameter(1)\n+  ROOT add = f32[] add(Arg_0.38, Arg_1.39)\n+}\n+\n+ENTRY main.49 {\n+  iota.2 = s32[1024,1024]{1,0} iota(), iota_dimension=0\n+  iota.3 = s32[1024,1024]{1,0} iota(), iota_dimension=1\n+  compare = pred[1024,1024]{1,0} compare(iota.2, iota.3), direction=GE\n+  broadcast.4 = pred[4,48,1024,1024]{3,2,1,0} broadcast(compare), dimensions={2,3}\n+  Arg_0.1 = f16[4,48,1024,16]{3,2,1,0} parameter(0)\n+  Arg_1.2 = f16[4,48,1024,16]{3,2,1,0} parameter(1)\n+  dot.9 = f16[4,48,1024,1024]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}\n+  constant.4 = f16[] constant(0.5)\n+  broadcast.6 = f16[4,48,1024,1024]{3,2,1,0} broadcast(constant.4), dimensions={}\n+  multiply = f16[4,48,1024,1024]{3,2,1,0} multiply(dot.9, broadcast.6)\n+  constant = f16[] constant(-inf)\n+  broadcast.7 = f16[4,48,1024,1024]{3,2,1,0} broadcast(constant), dimensions={}\n+  select.1 = f16[4,48,1024,1024]{3,2,1,0} select(broadcast.4, multiply, broadcast.7)\n+  convert.1 = f32[4,48,1024,1024]{3,2,1,0} convert(select.1)\n+  constant.7 = f32[] constant(-inf)\n+  reduce.30 = f32[4,48,1024]{2,1,0} reduce(convert.1, constant.7), dimensions={3}, to_apply=region_0.26\n+  broadcast.8 = f32[4,48,1024,1024]{3,2,1,0} broadcast(reduce.30), dimensions={0,1,2}\n+  subtract = f32[4,48,1024,1024]{3,2,1,0} subtract(convert.1, broadcast.8)\n+  exponential = f32[4,48,1024,1024]{3,2,1,0} exponential(subtract)\n+  constant.6 = f32[] constant(0)\n+  reduce.41 = f32[4,48,1024]{2,1,0} reduce(exponential, constant.6), dimensions={3}, to_apply=region_1.37\n+  broadcast.9 = f32[4,48,1024,1024]{3,2,1,0} broadcast(reduce.41), dimensions={0,1,2}\n+  divide = f32[4,48,1024,1024]{3,2,1,0} divide(exponential, broadcast.9)\n+  convert.2 = f16[4,48,1024,1024]{3,2,1,0} convert(divide)\n+  Arg_2.3 = f16[4,48,1024,16]{3,2,1,0} parameter(2)\n+  ROOT dot.48 = f16[4,48,1024,16]{3,2,1,0} dot(convert.2, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+} // main.49\n+)\";\n+\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, HeadDimNotMultipleOf64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m, ParseAndReturnVerifiedModule(hlo_head_dim_not_multiple_of_64,\n+                                           GetModuleConfig()));\n+  CudnnFusedMHARewriter fusedMhaRewriter{\n+      GetCudaComputeCapability(), GetCudnnVersionWithFlashAttentionSupport()};\n+  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n+\n+  // head dim not a multiple of 64 should not be lowered with cuDNN < 8.9.6\n+  SCOPED_TRACE(m->ToString());\n+  EXPECT_THAT(m->entry_computation()->root_instruction(), GmockMatch(m::Dot()));\n+\n+  // should be lowered with cuDNN >= 8.9.6\n+  CudnnFusedMHARewriter fusedMhaRewriterWithcuDNN8907{\n+      GetCudaComputeCapability(), se::dnn::VersionInfo(8, 9, 7)};\n+  TF_ASSERT_OK(RunHloPass(&fusedMhaRewriterWithcuDNN8907, m.get()).status());\n+  const HloInstruction* fmha;\n+\n+  SCOPED_TRACE(m->ToString());\n+  EXPECT_THAT(\n+      m->entry_computation()->root_instruction(),\n+      GmockMatch(\n+          m::GetTupleElement(\n+              m::CustomCall(&fmha, {kCudnnfMHAScaleMaskSoftmaxCallTarget}), 0)\n+              .WithShape(F16, {4, 48, 1024, 16})));\n+  TF_ASSERT_OK_AND_ASSIGN(auto gpu_config,\n+                          fmha->backend_config<GpuBackendConfig>());\n+  const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n+  EXPECT_EQ(config.fmha_scale(), 0.5);\n+  EXPECT_EQ(config.dropout_rate(), 0.0);\n+  EXPECT_EQ(config.is_flash_attention(), true);\n+}\n }  // anonymous namespace\n }  // namespace gpu\n }  // namespace xla"
        }
    ]
},
{
    "Id": 135,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a3ef3ad4db453ca64453ad7c8cc3a219e70887c8",
    "date": "2024-02-29T07:09:59-08:00",
    "message": "Fix checks whether conversions are supported.\n\nPiperOrigin-RevId: 611457501",
    "label": "YES",
    "changes": [
        {
            "name": "concatenate_mlir.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/concatenate_mlir.cc",
            "patches": [
                {
                    "old_start": 55,
                    "old_length": 7,
                    "new_start": 55,
                    "new_length": 8,
                    "hunk": "@@ -55,7 +55,8 @@ namespace gpu {\n     const HloFusionAnalysis& analysis) {\n   if (analysis.fusion_roots().size() != 1) return false;\n \n-  return true;\n+  return mlir_converter::IsHloConversionSupported(\n+      analysis.fusion(), analysis.device_info().gpu_compute_capability());\n }\n \n LaunchDimensions MlirConcatenateFusion::launch_dimensions() const {\n"
                }
            ],
            "whole_deleted": "-  return true;\n",
            "whole_added": "+  return mlir_converter::IsHloConversionSupported(\n+      analysis.fusion(), analysis.device_info().gpu_compute_capability());\n",
            "whole_hunk": "@@ -55,7 +55,8 @@ namespace gpu {\n     const HloFusionAnalysis& analysis) {\n   if (analysis.fusion_roots().size() != 1) return false;\n \n-  return true;\n+  return mlir_converter::IsHloConversionSupported(\n+      analysis.fusion(), analysis.device_info().gpu_compute_capability());\n }\n \n LaunchDimensions MlirConcatenateFusion::launch_dimensions() const {\n"
        },
        {
            "name": "fusions.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/fusions.cc",
            "patches": [
                {
                    "old_start": 149,
                    "old_length": 7,
                    "new_start": 149,
                    "new_length": 7,
                    "hunk": "@@ -149,7 +149,7 @@ absl::StatusOr<std::unique_ptr<FusionInterface>> GetFusionEmitter(\n       if (enable_mlir_emitters &&\n           mlir_converter::IsHloConversionSupported(\n               analysis.fusion(),\n-              fusion_info.analysis().device_info().gpu_compute_capability())) {\n+              analysis.device_info().gpu_compute_capability())) {\n         return std::make_unique<MlirInputSlicesFusion>(analysis);\n       }\n       return std::make_unique<InputSlicesFusion>(analysis);\n"
                },
                {
                    "old_start": 166,
                    "old_length": 7,
                    "new_start": 166,
                    "new_length": 7,
                    "hunk": "@@ -166,7 +166,7 @@ absl::StatusOr<std::unique_ptr<FusionInterface>> GetFusionEmitter(\n       if (enable_mlir_emitters &&\n           mlir_converter::IsHloConversionSupported(\n               analysis.fusion(),\n-              fusion_info.analysis().device_info().gpu_compute_capability())) {\n+              analysis.device_info().gpu_compute_capability())) {\n         return std::make_unique<MlirLoopFusion>(analysis);\n       }\n       return std::make_unique<LoopFusion>(analysis);\n"
                }
            ],
            "whole_deleted": "-              fusion_info.analysis().device_info().gpu_compute_capability())) {\n-              fusion_info.analysis().device_info().gpu_compute_capability())) {\n",
            "whole_added": "+              analysis.device_info().gpu_compute_capability())) {\n+              analysis.device_info().gpu_compute_capability())) {\n",
            "whole_hunk": "@@ -149,7 +149,7 @@ absl::StatusOr<std::unique_ptr<FusionInterface>> GetFusionEmitter(\n       if (enable_mlir_emitters &&\n           mlir_converter::IsHloConversionSupported(\n               analysis.fusion(),\n-              fusion_info.analysis().device_info().gpu_compute_capability())) {\n+              analysis.device_info().gpu_compute_capability())) {\n         return std::make_unique<MlirInputSlicesFusion>(analysis);\n       }\n       return std::make_unique<InputSlicesFusion>(analysis);\n@@ -166,7 +166,7 @@ absl::StatusOr<std::unique_ptr<FusionInterface>> GetFusionEmitter(\n       if (enable_mlir_emitters &&\n           mlir_converter::IsHloConversionSupported(\n               analysis.fusion(),\n-              fusion_info.analysis().device_info().gpu_compute_capability())) {\n+              analysis.device_info().gpu_compute_capability())) {\n         return std::make_unique<MlirLoopFusion>(analysis);\n       }\n       return std::make_unique<LoopFusion>(analysis);\n"
        },
        {
            "name": "transpose_mlir.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/transpose_mlir.cc",
            "patches": [
                {
                    "old_start": 146,
                    "old_length": 7,
                    "new_start": 146,
                    "new_length": 8,
                    "hunk": "@@ -146,7 +146,8 @@ MlirTransposeFusion::MlirTransposeFusion(const HloFusionAnalysis& analysis)\n     // supported right now.\n     if (operand->opcode() == HloOpcode::kParameter) return false;\n   }\n-  return true;\n+  return mlir_converter::IsHloConversionSupported(\n+      analysis.fusion(), analysis.device_info().gpu_compute_capability());\n }\n \n std::optional<IndexingMap> MlirTransposeFusion::ComputeThreadIdToOutputIndexing("
                }
            ],
            "whole_deleted": "-  return true;\n",
            "whole_added": "+  return mlir_converter::IsHloConversionSupported(\n+      analysis.fusion(), analysis.device_info().gpu_compute_capability());\n",
            "whole_hunk": "@@ -146,7 +146,8 @@ MlirTransposeFusion::MlirTransposeFusion(const HloFusionAnalysis& analysis)\n     // supported right now.\n     if (operand->opcode() == HloOpcode::kParameter) return false;\n   }\n-  return true;\n+  return mlir_converter::IsHloConversionSupported(\n+      analysis.fusion(), analysis.device_info().gpu_compute_capability());\n }\n \n std::optional<IndexingMap> MlirTransposeFusion::ComputeThreadIdToOutputIndexing("
        }
    ]
},
{
    "Id": 428,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9a4b6b6bcc7a813162bf0378727950e321aca19c",
    "date": "2023-05-09T14:49:51-04:00",
    "message": "Add stricter type checking for tf.math.real (using is_numeric)",
    "label": "NO",
    "changes": [
        {
            "name": "math_ops.py",
            "path": "tensorflow/python/ops/math_ops.py",
            "patches": [
                {
                    "old_start": 822,
                    "old_length": 7,
                    "new_start": 822,
                    "new_length": 7,
                    "hunk": "@@ -822,7 +822,7 @@ def real(input, name=None):\n     if input.dtype.is_complex:\n       real_dtype = input.dtype.real_dtype\n       return gen_math_ops.real(input, Tout=real_dtype, name=name)\n-    elif tf.debugging.is_numeric_tensor(input):\n+    elif input.dtype.is_numeric:\n       return input\n     else:\n       raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))"
                }
            ],
            "whole_deleted": "-    elif tf.debugging.is_numeric_tensor(input):\n",
            "whole_added": "+    elif input.dtype.is_numeric:\n",
            "whole_hunk": "@@ -822,7 +822,7 @@ def real(input, name=None):\n     if input.dtype.is_complex:\n       real_dtype = input.dtype.real_dtype\n       return gen_math_ops.real(input, Tout=real_dtype, name=name)\n-    elif tf.debugging.is_numeric_tensor(input):\n+    elif input.dtype.is_numeric:\n       return input\n     else:\n       raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))"
        }
    ]
},
{
    "Id": 60,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/64c943c7633c79d97fc7205c8801a054e354b1e1",
    "date": "2024-05-05T23:05:05-07:00",
    "message": "Add GetParameters() and FusionInstruction() methods to fusion adaptors.\n\nAlso let GetRoots() for ProducerConsumer fusion handle the case where producer\nis a multi-output fusion.\n\nPiperOrigin-RevId: 630936590",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 5738,
                    "old_length": 6,
                    "new_start": 5738,
                    "new_length": 8,
                    "hunk": "@@ -5738,6 +5738,8 @@ xla_cc_test(\n     deps = [\n         \":hlo_traversal\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:pattern_matcher\",\n+        \"//xla/service:pattern_matcher_gmock\",\n         \"//xla/tests:hlo_test_base\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//xla/service:pattern_matcher\",\n+        \"//xla/service:pattern_matcher_gmock\",\n",
            "whole_hunk": "@@ -5738,6 +5738,8 @@ xla_cc_test(\n     deps = [\n         \":hlo_traversal\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:pattern_matcher\",\n+        \"//xla/service:pattern_matcher_gmock\",\n         \"//xla/tests:hlo_test_base\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n"
        },
        {
            "name": "hlo_traversal.cc",
            "path": "third_party/xla/xla/service/gpu/hlo_traversal.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_traversal.h\"\n \n #include <algorithm>\n+#include <cstdint>\n #include <functional>\n #include <iterator>\n #include <memory>\n"
                },
                {
                    "old_start": 102,
                    "old_length": 7,
                    "new_start": 103,
                    "new_length": 7,
                    "hunk": "@@ -102,7 +103,7 @@ class SingleInstructionFusion : public internal::HloFusionInstructionAdaptor {\n                                    const HloFusionAdaptor* parent)\n       : instruction_(instruction), parent_(parent) {\n     CHECK_NE(instruction->opcode(), HloOpcode::kFusion)\n-        << \"Use HloFusionFusion\";\n+        << \"Use HloComputationFusion\";\n   }\n \n   bool ContainsInstruction(const HloInstruction* instruction) const override {\n"
                },
                {
                    "old_start": 113,
                    "old_length": 6,
                    "new_start": 114,
                    "new_length": 16,
                    "hunk": "@@ -113,6 +114,16 @@ class SingleInstructionFusion : public internal::HloFusionInstructionAdaptor {\n     return {HloInstructionAdaptor{*instruction_, parent_}};\n   }\n \n+  absl::InlinedVector<const HloInstruction*, 2> GetParameters() const override {\n+    const auto& operands = instruction_->operands();\n+    return absl::InlinedVector<const HloInstruction*, 2>(operands.begin(),\n+                                                         operands.end());\n+  }\n+\n+  const HloInstruction& FusionInstruction() const override {\n+    return *instruction_;\n+  }\n+\n   absl::InlinedVector<HloInstructionAdaptor, 2> MakeInstructionPostOrder()\n       const override {\n     return {HloInstructionAdaptor{*instruction_, parent_}};\n"
                },
                {
                    "old_start": 176,
                    "old_length": 6,
                    "new_start": 187,
                    "new_length": 16,
                    "hunk": "@@ -176,6 +187,16 @@ class HloComputationFusion : public internal::HloFusionInstructionAdaptor {\n     return roots_;\n   }\n \n+  absl::InlinedVector<const HloInstruction*, 2> GetParameters() const override {\n+    const auto& operands = computation_->FusionInstruction()->operands();\n+    return absl::InlinedVector<const HloInstruction*, 2>(operands.begin(),\n+                                                         operands.end());\n+  }\n+\n+  const HloInstruction& FusionInstruction() const override {\n+    return *computation_->FusionInstruction();\n+  }\n+\n   absl::InlinedVector<HloInstructionAdaptor, 2> MakeInstructionPostOrder()\n       const override {\n     auto post_order = computation_->MakeInstructionPostOrder();\n"
                },
                {
                    "old_start": 250,
                    "old_length": 7,
                    "new_start": 271,
                    "new_length": 111,
                    "hunk": "@@ -250,7 +271,111 @@ bool HloFusionAdaptor::ContainsInstruction(\n \n absl::InlinedVector<HloInstructionAdaptor, 2> HloFusionAdaptor::GetRoots()\n     const {\n-  return fusion_instructions_.back()->GetRoots();\n+  auto roots = fusion_instructions_.back()->GetRoots();\n+  if (fusion_instructions_.size() == 1) {\n+    return roots;\n+  }\n+  CHECK_EQ(fusion_instructions_.size(), 2);\n+  auto producer_roots = fusion_instructions_[0]->GetRoots();\n+  const HloInstruction& producer_fusion =\n+      fusion_instructions_[0]->FusionInstruction();\n+  const HloInstruction& consumer_fusion =\n+      fusion_instructions_.back()->FusionInstruction();\n+\n+  // Check whether there are fusion roots that are parameters which will be\n+  // replaced by a producer fusion root.\n+  for (auto& root : roots) {\n+    if (root.opcode() != HloOpcode::kParameter) {\n+      continue;\n+    }\n+    const HloInstruction* operand =\n+        consumer_fusion.operand(root.instruction().parameter_number());\n+    int64_t root_index = 0;\n+    if (operand->opcode() == HloOpcode::kGetTupleElement) {\n+      root_index = operand->tuple_index();\n+      operand = operand->operand(0);\n+    }\n+    if (operand == &producer_fusion) {\n+      root = producer_roots[root_index];\n+    }\n+  }\n+\n+  if (!producer_fusion.IsMultiOutputFusion()) {\n+    return roots;\n+  }\n+\n+  // Also add the roots of the producer fusion if they are used outside of the\n+  // merged fusion computations. Skip roots that are parameters.\n+  absl::flat_hash_set<int64_t> root_indices_with_outside_usage;\n+  for (HloInstruction* instr : producer_fusion.users()) {\n+    bool has_outside_user = false;\n+    int64_t root_index = 0;\n+    if (instr->opcode() == HloOpcode::kGetTupleElement) {\n+      for (HloInstruction* user : instr->users()) {\n+        if (user != &consumer_fusion) {\n+          root_index = instr->tuple_index();\n+          has_outside_user = true;\n+          break;\n+        }\n+      }\n+    } else if (instr != &consumer_fusion) {\n+      has_outside_user = true;\n+    }\n+    if (has_outside_user) {\n+      root_indices_with_outside_usage.insert(root_index);\n+    }\n+  }\n+  for (int64_t i = 0; i < producer_roots.size(); ++i) {\n+    if (!root_indices_with_outside_usage.contains(i)) {\n+      continue;\n+    }\n+    // Also check the special case that the root is a parameter. We never fuse a\n+    // parameter, instead we would rewire users of such a root to the\n+    // corresponding fusion operand.\n+    if (producer_roots[i].opcode() != HloOpcode::kParameter) {\n+      roots.push_back(producer_roots[i]);\n+    }\n+  }\n+  return roots;\n+}\n+\n+absl::InlinedVector<const HloInstruction*, 2> HloFusionAdaptor::GetParameters()\n+    const {\n+  if (fusion_instructions_.size() == 1) {\n+    return fusion_instructions_.back()->GetParameters();\n+  }\n+  CHECK_EQ(fusion_instructions_.size(), 2);\n+  absl::InlinedVector<const HloInstruction*, 2> combined_parameters;\n+  const HloInstruction& producer_fusion =\n+      fusion_instructions_[0]->FusionInstruction();\n+  for (const auto& param : fusion_instructions_.back()->GetParameters()) {\n+    const HloInstruction* operand = param;\n+    if (operand->opcode() == HloOpcode::kGetTupleElement) {\n+      operand = operand->operand(0);\n+    }\n+    // Check whether 'param' is a user of the producer fusion.\n+    if (operand != &producer_fusion) {\n+      combined_parameters.push_back(param);\n+    }\n+  }\n+  absl::flat_hash_set<const HloInstruction*> params(combined_parameters.begin(),\n+                                                    combined_parameters.end());\n+  auto producer_roots = fusion_instructions_[0]->GetRoots();\n+  absl::flat_hash_set<const HloInstruction*> parameters_to_skip;\n+  // Skip parameters that have just have a root user. Those will not be fused.\n+  for (const auto& root : producer_roots) {\n+    if (root.opcode() == HloOpcode::kParameter &&\n+        root.instruction().user_count() <= 1) {\n+      parameters_to_skip.insert(\n+          producer_fusion.operand(root.instruction().parameter_number()));\n+    }\n+  }\n+  for (auto param : fusion_instructions_[0]->GetParameters()) {\n+    if (!parameters_to_skip.contains(param) && params.insert(param).second) {\n+      combined_parameters.push_back(param);\n+    }\n+  }\n+  return combined_parameters;\n }\n \n absl::InlinedVector<HloInstructionAdaptor, 2>\n"
                }
            ],
            "whole_deleted": "-        << \"Use HloFusionFusion\";\n-  return fusion_instructions_.back()->GetRoots();\n",
            "whole_added": "+#include <cstdint>\n+        << \"Use HloComputationFusion\";\n+  absl::InlinedVector<const HloInstruction*, 2> GetParameters() const override {\n+    const auto& operands = instruction_->operands();\n+    return absl::InlinedVector<const HloInstruction*, 2>(operands.begin(),\n+                                                         operands.end());\n+  }\n+\n+  const HloInstruction& FusionInstruction() const override {\n+    return *instruction_;\n+  }\n+\n+  absl::InlinedVector<const HloInstruction*, 2> GetParameters() const override {\n+    const auto& operands = computation_->FusionInstruction()->operands();\n+    return absl::InlinedVector<const HloInstruction*, 2>(operands.begin(),\n+                                                         operands.end());\n+  }\n+\n+  const HloInstruction& FusionInstruction() const override {\n+    return *computation_->FusionInstruction();\n+  }\n+\n+  auto roots = fusion_instructions_.back()->GetRoots();\n+  if (fusion_instructions_.size() == 1) {\n+    return roots;\n+  }\n+  CHECK_EQ(fusion_instructions_.size(), 2);\n+  auto producer_roots = fusion_instructions_[0]->GetRoots();\n+  const HloInstruction& producer_fusion =\n+      fusion_instructions_[0]->FusionInstruction();\n+  const HloInstruction& consumer_fusion =\n+      fusion_instructions_.back()->FusionInstruction();\n+\n+  // Check whether there are fusion roots that are parameters which will be\n+  // replaced by a producer fusion root.\n+  for (auto& root : roots) {\n+    if (root.opcode() != HloOpcode::kParameter) {\n+      continue;\n+    }\n+    const HloInstruction* operand =\n+        consumer_fusion.operand(root.instruction().parameter_number());\n+    int64_t root_index = 0;\n+    if (operand->opcode() == HloOpcode::kGetTupleElement) {\n+      root_index = operand->tuple_index();\n+      operand = operand->operand(0);\n+    }\n+    if (operand == &producer_fusion) {\n+      root = producer_roots[root_index];\n+    }\n+  }\n+\n+  if (!producer_fusion.IsMultiOutputFusion()) {\n+    return roots;\n+  }\n+\n+  // Also add the roots of the producer fusion if they are used outside of the\n+  // merged fusion computations. Skip roots that are parameters.\n+  absl::flat_hash_set<int64_t> root_indices_with_outside_usage;\n+  for (HloInstruction* instr : producer_fusion.users()) {\n+    bool has_outside_user = false;\n+    int64_t root_index = 0;\n+    if (instr->opcode() == HloOpcode::kGetTupleElement) {\n+      for (HloInstruction* user : instr->users()) {\n+        if (user != &consumer_fusion) {\n+          root_index = instr->tuple_index();\n+          has_outside_user = true;\n+          break;\n+        }\n+      }\n+    } else if (instr != &consumer_fusion) {\n+      has_outside_user = true;\n+    }\n+    if (has_outside_user) {\n+      root_indices_with_outside_usage.insert(root_index);\n+    }\n+  }\n+  for (int64_t i = 0; i < producer_roots.size(); ++i) {\n+    if (!root_indices_with_outside_usage.contains(i)) {\n+      continue;\n+    }\n+    // Also check the special case that the root is a parameter. We never fuse a\n+    // parameter, instead we would rewire users of such a root to the\n+    // corresponding fusion operand.\n+    if (producer_roots[i].opcode() != HloOpcode::kParameter) {\n+      roots.push_back(producer_roots[i]);\n+    }\n+  }\n+  return roots;\n+}\n+\n+absl::InlinedVector<const HloInstruction*, 2> HloFusionAdaptor::GetParameters()\n+    const {\n+  if (fusion_instructions_.size() == 1) {\n+    return fusion_instructions_.back()->GetParameters();\n+  }\n+  CHECK_EQ(fusion_instructions_.size(), 2);\n+  absl::InlinedVector<const HloInstruction*, 2> combined_parameters;\n+  const HloInstruction& producer_fusion =\n+      fusion_instructions_[0]->FusionInstruction();\n+  for (const auto& param : fusion_instructions_.back()->GetParameters()) {\n+    const HloInstruction* operand = param;\n+    if (operand->opcode() == HloOpcode::kGetTupleElement) {\n+      operand = operand->operand(0);\n+    }\n+    // Check whether 'param' is a user of the producer fusion.\n+    if (operand != &producer_fusion) {\n+      combined_parameters.push_back(param);\n+    }\n+  }\n+  absl::flat_hash_set<const HloInstruction*> params(combined_parameters.begin(),\n+                                                    combined_parameters.end());\n+  auto producer_roots = fusion_instructions_[0]->GetRoots();\n+  absl::flat_hash_set<const HloInstruction*> parameters_to_skip;\n+  // Skip parameters that have just have a root user. Those will not be fused.\n+  for (const auto& root : producer_roots) {\n+    if (root.opcode() == HloOpcode::kParameter &&\n+        root.instruction().user_count() <= 1) {\n+      parameters_to_skip.insert(\n+          producer_fusion.operand(root.instruction().parameter_number()));\n+    }\n+  }\n+  for (auto param : fusion_instructions_[0]->GetParameters()) {\n+    if (!parameters_to_skip.contains(param) && params.insert(param).second) {\n+      combined_parameters.push_back(param);\n+    }\n+  }\n+  return combined_parameters;\n",
            "whole_hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n #include \"xla/service/gpu/hlo_traversal.h\"\n \n #include <algorithm>\n+#include <cstdint>\n #include <functional>\n #include <iterator>\n #include <memory>\n@@ -102,7 +103,7 @@ class SingleInstructionFusion : public internal::HloFusionInstructionAdaptor {\n                                    const HloFusionAdaptor* parent)\n       : instruction_(instruction), parent_(parent) {\n     CHECK_NE(instruction->opcode(), HloOpcode::kFusion)\n-        << \"Use HloFusionFusion\";\n+        << \"Use HloComputationFusion\";\n   }\n \n   bool ContainsInstruction(const HloInstruction* instruction) const override {\n@@ -113,6 +114,16 @@ class SingleInstructionFusion : public internal::HloFusionInstructionAdaptor {\n     return {HloInstructionAdaptor{*instruction_, parent_}};\n   }\n \n+  absl::InlinedVector<const HloInstruction*, 2> GetParameters() const override {\n+    const auto& operands = instruction_->operands();\n+    return absl::InlinedVector<const HloInstruction*, 2>(operands.begin(),\n+                                                         operands.end());\n+  }\n+\n+  const HloInstruction& FusionInstruction() const override {\n+    return *instruction_;\n+  }\n+\n   absl::InlinedVector<HloInstructionAdaptor, 2> MakeInstructionPostOrder()\n       const override {\n     return {HloInstructionAdaptor{*instruction_, parent_}};\n@@ -176,6 +187,16 @@ class HloComputationFusion : public internal::HloFusionInstructionAdaptor {\n     return roots_;\n   }\n \n+  absl::InlinedVector<const HloInstruction*, 2> GetParameters() const override {\n+    const auto& operands = computation_->FusionInstruction()->operands();\n+    return absl::InlinedVector<const HloInstruction*, 2>(operands.begin(),\n+                                                         operands.end());\n+  }\n+\n+  const HloInstruction& FusionInstruction() const override {\n+    return *computation_->FusionInstruction();\n+  }\n+\n   absl::InlinedVector<HloInstructionAdaptor, 2> MakeInstructionPostOrder()\n       const override {\n     auto post_order = computation_->MakeInstructionPostOrder();\n@@ -250,7 +271,111 @@ bool HloFusionAdaptor::ContainsInstruction(\n \n absl::InlinedVector<HloInstructionAdaptor, 2> HloFusionAdaptor::GetRoots()\n     const {\n-  return fusion_instructions_.back()->GetRoots();\n+  auto roots = fusion_instructions_.back()->GetRoots();\n+  if (fusion_instructions_.size() == 1) {\n+    return roots;\n+  }\n+  CHECK_EQ(fusion_instructions_.size(), 2);\n+  auto producer_roots = fusion_instructions_[0]->GetRoots();\n+  const HloInstruction& producer_fusion =\n+      fusion_instructions_[0]->FusionInstruction();\n+  const HloInstruction& consumer_fusion =\n+      fusion_instructions_.back()->FusionInstruction();\n+\n+  // Check whether there are fusion roots that are parameters which will be\n+  // replaced by a producer fusion root.\n+  for (auto& root : roots) {\n+    if (root.opcode() != HloOpcode::kParameter) {\n+      continue;\n+    }\n+    const HloInstruction* operand =\n+        consumer_fusion.operand(root.instruction().parameter_number());\n+    int64_t root_index = 0;\n+    if (operand->opcode() == HloOpcode::kGetTupleElement) {\n+      root_index = operand->tuple_index();\n+      operand = operand->operand(0);\n+    }\n+    if (operand == &producer_fusion) {\n+      root = producer_roots[root_index];\n+    }\n+  }\n+\n+  if (!producer_fusion.IsMultiOutputFusion()) {\n+    return roots;\n+  }\n+\n+  // Also add the roots of the producer fusion if they are used outside of the\n+  // merged fusion computations. Skip roots that are parameters.\n+  absl::flat_hash_set<int64_t> root_indices_with_outside_usage;\n+  for (HloInstruction* instr : producer_fusion.users()) {\n+    bool has_outside_user = false;\n+    int64_t root_index = 0;\n+    if (instr->opcode() == HloOpcode::kGetTupleElement) {\n+      for (HloInstruction* user : instr->users()) {\n+        if (user != &consumer_fusion) {\n+          root_index = instr->tuple_index();\n+          has_outside_user = true;\n+          break;\n+        }\n+      }\n+    } else if (instr != &consumer_fusion) {\n+      has_outside_user = true;\n+    }\n+    if (has_outside_user) {\n+      root_indices_with_outside_usage.insert(root_index);\n+    }\n+  }\n+  for (int64_t i = 0; i < producer_roots.size(); ++i) {\n+    if (!root_indices_with_outside_usage.contains(i)) {\n+      continue;\n+    }\n+    // Also check the special case that the root is a parameter. We never fuse a\n+    // parameter, instead we would rewire users of such a root to the\n+    // corresponding fusion operand.\n+    if (producer_roots[i].opcode() != HloOpcode::kParameter) {\n+      roots.push_back(producer_roots[i]);\n+    }\n+  }\n+  return roots;\n+}\n+\n+absl::InlinedVector<const HloInstruction*, 2> HloFusionAdaptor::GetParameters()\n+    const {\n+  if (fusion_instructions_.size() == 1) {\n+    return fusion_instructions_.back()->GetParameters();\n+  }\n+  CHECK_EQ(fusion_instructions_.size(), 2);\n+  absl::InlinedVector<const HloInstruction*, 2> combined_parameters;\n+  const HloInstruction& producer_fusion =\n+      fusion_instructions_[0]->FusionInstruction();\n+  for (const auto& param : fusion_instructions_.back()->GetParameters()) {\n+    const HloInstruction* operand = param;\n+    if (operand->opcode() == HloOpcode::kGetTupleElement) {\n+      operand = operand->operand(0);\n+    }\n+    // Check whether 'param' is a user of the producer fusion.\n+    if (operand != &producer_fusion) {\n+      combined_parameters.push_back(param);\n+    }\n+  }\n+  absl::flat_hash_set<const HloInstruction*> params(combined_parameters.begin(),\n+                                                    combined_parameters.end());\n+  auto producer_roots = fusion_instructions_[0]->GetRoots();\n+  absl::flat_hash_set<const HloInstruction*> parameters_to_skip;\n+  // Skip parameters that have just have a root user. Those will not be fused.\n+  for (const auto& root : producer_roots) {\n+    if (root.opcode() == HloOpcode::kParameter &&\n+        root.instruction().user_count() <= 1) {\n+      parameters_to_skip.insert(\n+          producer_fusion.operand(root.instruction().parameter_number()));\n+    }\n+  }\n+  for (auto param : fusion_instructions_[0]->GetParameters()) {\n+    if (!parameters_to_skip.contains(param) && params.insert(param).second) {\n+      combined_parameters.push_back(param);\n+    }\n+  }\n+  return combined_parameters;\n }\n \n absl::InlinedVector<HloInstructionAdaptor, 2>\n"
        },
        {
            "name": "hlo_traversal.h",
            "path": "third_party/xla/xla/service/gpu/hlo_traversal.h",
            "patches": [
                {
                    "old_start": 89,
                    "old_length": 6,
                    "new_start": 89,
                    "new_length": 9,
                    "hunk": "@@ -89,6 +89,9 @@ class HloFusionInstructionAdaptor {\n   // matches the order of the tuple elements of the tuple root of the fusion\n   // computation. We do not deduplicate fusion roots.\n   virtual absl::InlinedVector<HloInstructionAdaptor, 2> GetRoots() const = 0;\n+  virtual absl::InlinedVector<const HloInstruction*, 2> GetParameters()\n+      const = 0;\n+  virtual const HloInstruction& FusionInstruction() const = 0;\n   virtual absl::InlinedVector<HloInstructionAdaptor, 2>\n   MakeInstructionPostOrder() const = 0;\n   virtual std::string ToString() const = 0;\n"
                },
                {
                    "old_start": 101,
                    "old_length": 6,
                    "new_start": 104,
                    "new_length": 7,
                    "hunk": "@@ -101,6 +104,7 @@ class HloFusionAdaptor {\n   bool ContainsInstruction(HloInstructionAdaptor instruction) const;\n   bool ContainsInstruction(const HloInstruction* instruction) const;\n   absl::InlinedVector<HloInstructionAdaptor, 2> GetRoots() const;\n+  absl::InlinedVector<const HloInstruction*, 2> GetParameters() const;\n   absl::InlinedVector<HloInstructionAdaptor, 2> MakeInstructionPostOrder()\n       const;\n   std::string ToString() const;\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  virtual absl::InlinedVector<const HloInstruction*, 2> GetParameters()\n+      const = 0;\n+  virtual const HloInstruction& FusionInstruction() const = 0;\n+  absl::InlinedVector<const HloInstruction*, 2> GetParameters() const;\n",
            "whole_hunk": "@@ -89,6 +89,9 @@ class HloFusionInstructionAdaptor {\n   // matches the order of the tuple elements of the tuple root of the fusion\n   // computation. We do not deduplicate fusion roots.\n   virtual absl::InlinedVector<HloInstructionAdaptor, 2> GetRoots() const = 0;\n+  virtual absl::InlinedVector<const HloInstruction*, 2> GetParameters()\n+      const = 0;\n+  virtual const HloInstruction& FusionInstruction() const = 0;\n   virtual absl::InlinedVector<HloInstructionAdaptor, 2>\n   MakeInstructionPostOrder() const = 0;\n   virtual std::string ToString() const = 0;\n@@ -101,6 +104,7 @@ class HloFusionAdaptor {\n   bool ContainsInstruction(HloInstructionAdaptor instruction) const;\n   bool ContainsInstruction(const HloInstruction* instruction) const;\n   absl::InlinedVector<HloInstructionAdaptor, 2> GetRoots() const;\n+  absl::InlinedVector<const HloInstruction*, 2> GetParameters() const;\n   absl::InlinedVector<HloInstructionAdaptor, 2> MakeInstructionPostOrder()\n       const;\n   std::string ToString() const;\n"
        },
        {
            "name": "hlo_traversal_test.cc",
            "path": "third_party/xla/xla/service/gpu/hlo_traversal_test.cc",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 7,
                    "new_start": 16,
                    "new_length": 6,
                    "hunk": "@@ -16,7 +16,6 @@ limitations under the License.\n \n #include <optional>\n #include <string>\n-#include <utility>\n #include <vector>\n \n #include <gmock/gmock.h>\n"
                },
                {
                    "old_start": 24,
                    "old_length": 12,
                    "new_start": 23,
                    "new_length": 16,
                    "hunk": "@@ -24,12 +23,16 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/service/pattern_matcher.h\"\n+#include \"xla/service/pattern_matcher_gmock.h\"\n #include \"xla/tests/hlo_test_base.h\"\n \n namespace xla {\n namespace gpu {\n namespace {\n \n+namespace m = ::xla::match;\n+\n using ::testing::ElementsAre;\n using ::testing::IsEmpty;\n \n"
                },
                {
                    "old_start": 522,
                    "old_length": 6,
                    "new_start": 525,
                    "new_length": 99,
                    "hunk": "@@ -522,6 +525,99 @@ TEST_F(HloTraversalTest, MakeInstructionsPostOrder_TwoMultiOutputFusions) {\n                                  InstructionAdaptorName(\"reduce.2\")));\n }\n \n+const char kTwoMultiOutputFusions[] = R\"(\n+    HloModule mof\n+    mof_producer {\n+      param0 = f32[10]{0} parameter(0)\n+      param1 = f32[10]{0} parameter(1)\n+      param2 = f32[10]{0} parameter(2)\n+      add = f32[10]{0} add(param0, param1)\n+      sub = f32[10]{0} subtract(param0, param1)\n+      ROOT res = (f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}) tuple(param1, add, sub, param0, param2)\n+    }\n+\n+    mof_consumer {\n+      param0.0 = f32[10]{0} parameter(0)\n+      param1.0 = f32[10]{0} parameter(1)\n+      param2.0 = f32[10]{0} parameter(2)\n+      mul = f32[10]{0} multiply(param0.0, param1.0)\n+      div = f32[10]{0} divide(param0.0, param1.0)\n+      ROOT res = (f32[10]{0}, f32[10]{0}, f32[10]{0}) tuple(mul, div, param2.0)\n+    }\n+\n+    ENTRY main {\n+      p0 = f32[10]{0} parameter(0)\n+      p1 = f32[10]{0} parameter(1)\n+      p2 = f32[10]{0} parameter(2)\n+      producer = (f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}) fusion(p0, p1, p2), kind=kLoop, calls=mof_producer\n+      gte0 = f32[10]{0} get-tuple-element(producer), index=0\n+      gte1 = f32[10]{0} get-tuple-element(producer), index=1\n+      gte2 = f32[10]{0} get-tuple-element(producer), index=2\n+      gte3 = f32[10]{0} get-tuple-element(producer), index=3\n+      gte4 = f32[10]{0} get-tuple-element(producer), index=4\n+      consumer = (f32[10]{0}, f32[10]{0}, f32[10]{0}) fusion(gte1, gte2, gte3), kind=kLoop, calls=mof_consumer\n+      gte5 = f32[10]{0} get-tuple-element(consumer), index=0\n+      gte6 = f32[10]{0} get-tuple-element(consumer), index=1\n+      gte7 = f32[10]{0} get-tuple-element(consumer), index=2\n+      ROOT res = tuple(gte0, gte1, gte3, gte4, gte5, gte6, gte7)\n+    })\";\n+\n+TEST_F(HloTraversalTest, GetParametersMultiOutputFusion) {\n+  auto module = ParseAndReturnVerifiedModule(kTwoMultiOutputFusions).value();\n+  auto producer =\n+      module->entry_computation()->GetInstructionWithName(\"producer\");\n+  auto consumer =\n+      module->entry_computation()->GetInstructionWithName(\"consumer\");\n+  auto fusion_adaptor =\n+      HloFusionAdaptor::ForProducerConsumer(producer, consumer);\n+  auto p0 = module->entry_computation()->GetInstructionWithName(\"p0\");\n+  auto p1 = module->entry_computation()->GetInstructionWithName(\"p1\");\n+  EXPECT_THAT(fusion_adaptor->GetParameters(), ElementsAre(p0, p1));\n+  // Double-check that after performing the actual fusion, we get the same\n+  // parameters.\n+  consumer->MergeFusionInstructionIntoMultiOutput(producer);\n+  EXPECT_THAT(consumer->operands(), ElementsAre(p0, p1));\n+}\n+\n+TEST_F(HloTraversalTest, GetRootsMultiOutputFusion) {\n+  auto module = ParseAndReturnVerifiedModule(kTwoMultiOutputFusions).value();\n+  auto consumer_fusion_instr =\n+      module->entry_computation()->GetInstructionWithName(\"consumer\");\n+  auto producer_fusion_instr =\n+      module->entry_computation()->GetInstructionWithName(\"producer\");\n+  auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(\n+      producer_fusion_instr, consumer_fusion_instr);\n+  auto producer_computation = module->GetComputationWithName(\"mof_producer\");\n+  auto producer = HloFusionAdaptor::ForComputation(producer_computation);\n+  auto consumer_computation = module->GetComputationWithName(\"mof_consumer\");\n+  auto consumer = HloFusionAdaptor::ForComputation(consumer_computation);\n+  EXPECT_THAT(fusion_adaptor->GetRoots(),\n+              ElementsAre(\n+                  HloInstructionAdaptor{\n+                      *consumer_computation->GetInstructionWithName(\"mul\"),\n+                      consumer.get()},\n+                  HloInstructionAdaptor{\n+                      *consumer_computation->GetInstructionWithName(\"div\"),\n+                      consumer.get()},\n+                  HloInstructionAdaptor{\n+                      *producer_computation->GetInstructionWithName(\"param0\"),\n+                      producer.get()},\n+                  HloInstructionAdaptor{\n+                      *producer_computation->GetInstructionWithName(\"add\"),\n+                      producer.get()}));\n+  // Double-check that after performing the actual fusion, we get the same\n+  // roots.\n+  consumer_fusion_instr->MergeFusionInstructionIntoMultiOutput(\n+      producer_fusion_instr);\n+  EXPECT_THAT(consumer_fusion_instr->fused_expression_root(),\n+              GmockMatch(m::Tuple(\n+                  m::Multiply(m::Add(m::Parameter(0), m::Parameter(1)),\n+                              m::Subtract(m::Parameter(0), m::Parameter(1))),\n+                  m::Divide(m::Add(m::Parameter(0), m::Parameter(1)),\n+                            m::Subtract(m::Parameter(0), m::Parameter(1))),\n+                  m::Parameter(0), m::Add(m::Parameter(0), m::Parameter(1)))));\n+}\n+\n TEST_F(HloTraversalTest, HloFindUseChain) {\n   auto module = ParseAndReturnVerifiedModule(R\"(\n     fusion {"
                }
            ],
            "whole_deleted": "-#include <utility>\n",
            "whole_added": "+#include \"xla/service/pattern_matcher.h\"\n+#include \"xla/service/pattern_matcher_gmock.h\"\n+namespace m = ::xla::match;\n+\n+const char kTwoMultiOutputFusions[] = R\"(\n+    HloModule mof\n+    mof_producer {\n+      param0 = f32[10]{0} parameter(0)\n+      param1 = f32[10]{0} parameter(1)\n+      param2 = f32[10]{0} parameter(2)\n+      add = f32[10]{0} add(param0, param1)\n+      sub = f32[10]{0} subtract(param0, param1)\n+      ROOT res = (f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}) tuple(param1, add, sub, param0, param2)\n+    }\n+\n+    mof_consumer {\n+      param0.0 = f32[10]{0} parameter(0)\n+      param1.0 = f32[10]{0} parameter(1)\n+      param2.0 = f32[10]{0} parameter(2)\n+      mul = f32[10]{0} multiply(param0.0, param1.0)\n+      div = f32[10]{0} divide(param0.0, param1.0)\n+      ROOT res = (f32[10]{0}, f32[10]{0}, f32[10]{0}) tuple(mul, div, param2.0)\n+    }\n+\n+    ENTRY main {\n+      p0 = f32[10]{0} parameter(0)\n+      p1 = f32[10]{0} parameter(1)\n+      p2 = f32[10]{0} parameter(2)\n+      producer = (f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}) fusion(p0, p1, p2), kind=kLoop, calls=mof_producer\n+      gte0 = f32[10]{0} get-tuple-element(producer), index=0\n+      gte1 = f32[10]{0} get-tuple-element(producer), index=1\n+      gte2 = f32[10]{0} get-tuple-element(producer), index=2\n+      gte3 = f32[10]{0} get-tuple-element(producer), index=3\n+      gte4 = f32[10]{0} get-tuple-element(producer), index=4\n+      consumer = (f32[10]{0}, f32[10]{0}, f32[10]{0}) fusion(gte1, gte2, gte3), kind=kLoop, calls=mof_consumer\n+      gte5 = f32[10]{0} get-tuple-element(consumer), index=0\n+      gte6 = f32[10]{0} get-tuple-element(consumer), index=1\n+      gte7 = f32[10]{0} get-tuple-element(consumer), index=2\n+      ROOT res = tuple(gte0, gte1, gte3, gte4, gte5, gte6, gte7)\n+    })\";\n+\n+TEST_F(HloTraversalTest, GetParametersMultiOutputFusion) {\n+  auto module = ParseAndReturnVerifiedModule(kTwoMultiOutputFusions).value();\n+  auto producer =\n+      module->entry_computation()->GetInstructionWithName(\"producer\");\n+  auto consumer =\n+      module->entry_computation()->GetInstructionWithName(\"consumer\");\n+  auto fusion_adaptor =\n+      HloFusionAdaptor::ForProducerConsumer(producer, consumer);\n+  auto p0 = module->entry_computation()->GetInstructionWithName(\"p0\");\n+  auto p1 = module->entry_computation()->GetInstructionWithName(\"p1\");\n+  EXPECT_THAT(fusion_adaptor->GetParameters(), ElementsAre(p0, p1));\n+  // Double-check that after performing the actual fusion, we get the same\n+  // parameters.\n+  consumer->MergeFusionInstructionIntoMultiOutput(producer);\n+  EXPECT_THAT(consumer->operands(), ElementsAre(p0, p1));\n+}\n+\n+TEST_F(HloTraversalTest, GetRootsMultiOutputFusion) {\n+  auto module = ParseAndReturnVerifiedModule(kTwoMultiOutputFusions).value();\n+  auto consumer_fusion_instr =\n+      module->entry_computation()->GetInstructionWithName(\"consumer\");\n+  auto producer_fusion_instr =\n+      module->entry_computation()->GetInstructionWithName(\"producer\");\n+  auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(\n+      producer_fusion_instr, consumer_fusion_instr);\n+  auto producer_computation = module->GetComputationWithName(\"mof_producer\");\n+  auto producer = HloFusionAdaptor::ForComputation(producer_computation);\n+  auto consumer_computation = module->GetComputationWithName(\"mof_consumer\");\n+  auto consumer = HloFusionAdaptor::ForComputation(consumer_computation);\n+  EXPECT_THAT(fusion_adaptor->GetRoots(),\n+              ElementsAre(\n+                  HloInstructionAdaptor{\n+                      *consumer_computation->GetInstructionWithName(\"mul\"),\n+                      consumer.get()},\n+                  HloInstructionAdaptor{\n+                      *consumer_computation->GetInstructionWithName(\"div\"),\n+                      consumer.get()},\n+                  HloInstructionAdaptor{\n+                      *producer_computation->GetInstructionWithName(\"param0\"),\n+                      producer.get()},\n+                  HloInstructionAdaptor{\n+                      *producer_computation->GetInstructionWithName(\"add\"),\n+                      producer.get()}));\n+  // Double-check that after performing the actual fusion, we get the same\n+  // roots.\n+  consumer_fusion_instr->MergeFusionInstructionIntoMultiOutput(\n+      producer_fusion_instr);\n+  EXPECT_THAT(consumer_fusion_instr->fused_expression_root(),\n+              GmockMatch(m::Tuple(\n+                  m::Multiply(m::Add(m::Parameter(0), m::Parameter(1)),\n+                              m::Subtract(m::Parameter(0), m::Parameter(1))),\n+                  m::Divide(m::Add(m::Parameter(0), m::Parameter(1)),\n+                            m::Subtract(m::Parameter(0), m::Parameter(1))),\n+                  m::Parameter(0), m::Add(m::Parameter(0), m::Parameter(1)))));\n+}\n+\n",
            "whole_hunk": "@@ -16,7 +16,6 @@ limitations under the License.\n \n #include <optional>\n #include <string>\n-#include <utility>\n #include <vector>\n \n #include <gmock/gmock.h>\n@@ -24,12 +23,16 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/service/pattern_matcher.h\"\n+#include \"xla/service/pattern_matcher_gmock.h\"\n #include \"xla/tests/hlo_test_base.h\"\n \n namespace xla {\n namespace gpu {\n namespace {\n \n+namespace m = ::xla::match;\n+\n using ::testing::ElementsAre;\n using ::testing::IsEmpty;\n \n@@ -522,6 +525,99 @@ TEST_F(HloTraversalTest, MakeInstructionsPostOrder_TwoMultiOutputFusions) {\n                                  InstructionAdaptorName(\"reduce.2\")));\n }\n \n+const char kTwoMultiOutputFusions[] = R\"(\n+    HloModule mof\n+    mof_producer {\n+      param0 = f32[10]{0} parameter(0)\n+      param1 = f32[10]{0} parameter(1)\n+      param2 = f32[10]{0} parameter(2)\n+      add = f32[10]{0} add(param0, param1)\n+      sub = f32[10]{0} subtract(param0, param1)\n+      ROOT res = (f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}) tuple(param1, add, sub, param0, param2)\n+    }\n+\n+    mof_consumer {\n+      param0.0 = f32[10]{0} parameter(0)\n+      param1.0 = f32[10]{0} parameter(1)\n+      param2.0 = f32[10]{0} parameter(2)\n+      mul = f32[10]{0} multiply(param0.0, param1.0)\n+      div = f32[10]{0} divide(param0.0, param1.0)\n+      ROOT res = (f32[10]{0}, f32[10]{0}, f32[10]{0}) tuple(mul, div, param2.0)\n+    }\n+\n+    ENTRY main {\n+      p0 = f32[10]{0} parameter(0)\n+      p1 = f32[10]{0} parameter(1)\n+      p2 = f32[10]{0} parameter(2)\n+      producer = (f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}, f32[10]{0}) fusion(p0, p1, p2), kind=kLoop, calls=mof_producer\n+      gte0 = f32[10]{0} get-tuple-element(producer), index=0\n+      gte1 = f32[10]{0} get-tuple-element(producer), index=1\n+      gte2 = f32[10]{0} get-tuple-element(producer), index=2\n+      gte3 = f32[10]{0} get-tuple-element(producer), index=3\n+      gte4 = f32[10]{0} get-tuple-element(producer), index=4\n+      consumer = (f32[10]{0}, f32[10]{0}, f32[10]{0}) fusion(gte1, gte2, gte3), kind=kLoop, calls=mof_consumer\n+      gte5 = f32[10]{0} get-tuple-element(consumer), index=0\n+      gte6 = f32[10]{0} get-tuple-element(consumer), index=1\n+      gte7 = f32[10]{0} get-tuple-element(consumer), index=2\n+      ROOT res = tuple(gte0, gte1, gte3, gte4, gte5, gte6, gte7)\n+    })\";\n+\n+TEST_F(HloTraversalTest, GetParametersMultiOutputFusion) {\n+  auto module = ParseAndReturnVerifiedModule(kTwoMultiOutputFusions).value();\n+  auto producer =\n+      module->entry_computation()->GetInstructionWithName(\"producer\");\n+  auto consumer =\n+      module->entry_computation()->GetInstructionWithName(\"consumer\");\n+  auto fusion_adaptor =\n+      HloFusionAdaptor::ForProducerConsumer(producer, consumer);\n+  auto p0 = module->entry_computation()->GetInstructionWithName(\"p0\");\n+  auto p1 = module->entry_computation()->GetInstructionWithName(\"p1\");\n+  EXPECT_THAT(fusion_adaptor->GetParameters(), ElementsAre(p0, p1));\n+  // Double-check that after performing the actual fusion, we get the same\n+  // parameters.\n+  consumer->MergeFusionInstructionIntoMultiOutput(producer);\n+  EXPECT_THAT(consumer->operands(), ElementsAre(p0, p1));\n+}\n+\n+TEST_F(HloTraversalTest, GetRootsMultiOutputFusion) {\n+  auto module = ParseAndReturnVerifiedModule(kTwoMultiOutputFusions).value();\n+  auto consumer_fusion_instr =\n+      module->entry_computation()->GetInstructionWithName(\"consumer\");\n+  auto producer_fusion_instr =\n+      module->entry_computation()->GetInstructionWithName(\"producer\");\n+  auto fusion_adaptor = HloFusionAdaptor::ForProducerConsumer(\n+      producer_fusion_instr, consumer_fusion_instr);\n+  auto producer_computation = module->GetComputationWithName(\"mof_producer\");\n+  auto producer = HloFusionAdaptor::ForComputation(producer_computation);\n+  auto consumer_computation = module->GetComputationWithName(\"mof_consumer\");\n+  auto consumer = HloFusionAdaptor::ForComputation(consumer_computation);\n+  EXPECT_THAT(fusion_adaptor->GetRoots(),\n+              ElementsAre(\n+                  HloInstructionAdaptor{\n+                      *consumer_computation->GetInstructionWithName(\"mul\"),\n+                      consumer.get()},\n+                  HloInstructionAdaptor{\n+                      *consumer_computation->GetInstructionWithName(\"div\"),\n+                      consumer.get()},\n+                  HloInstructionAdaptor{\n+                      *producer_computation->GetInstructionWithName(\"param0\"),\n+                      producer.get()},\n+                  HloInstructionAdaptor{\n+                      *producer_computation->GetInstructionWithName(\"add\"),\n+                      producer.get()}));\n+  // Double-check that after performing the actual fusion, we get the same\n+  // roots.\n+  consumer_fusion_instr->MergeFusionInstructionIntoMultiOutput(\n+      producer_fusion_instr);\n+  EXPECT_THAT(consumer_fusion_instr->fused_expression_root(),\n+              GmockMatch(m::Tuple(\n+                  m::Multiply(m::Add(m::Parameter(0), m::Parameter(1)),\n+                              m::Subtract(m::Parameter(0), m::Parameter(1))),\n+                  m::Divide(m::Add(m::Parameter(0), m::Parameter(1)),\n+                            m::Subtract(m::Parameter(0), m::Parameter(1))),\n+                  m::Parameter(0), m::Add(m::Parameter(0), m::Parameter(1)))));\n+}\n+\n TEST_F(HloTraversalTest, HloFindUseChain) {\n   auto module = ParseAndReturnVerifiedModule(R\"(\n     fusion {"
        }
    ]
},
{
    "Id": 155,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/5c6dc6c062ac1d59176d0a8d75b547bb42104421",
    "date": "2024-02-14T21:51:26-08:00",
    "message": "[xla:gpu] Update check in AddressComputationFusionRewriter to process legacy custom calls too\n\nPiperOrigin-RevId: 607206363",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 3331,
                    "old_length": 6,
                    "new_start": 3331,
                    "new_length": 7,
                    "hunk": "@@ -3331,6 +3331,7 @@ cc_library(\n         \":cublas_cudnn\",\n         \":hlo_traversal\",\n         \":ir_emission_utils\",\n+        \"//xla:shape_util\",\n         \"//xla:statusor\",\n         \"//xla:util\",\n         \"//xla/ffi:ffi_api\",\n"
                },
                {
                    "old_start": 3353,
                    "old_length": 7,
                    "new_start": 3354,
                    "new_length": 7,
                    "hunk": "@@ -3353,7 +3354,7 @@ cc_library(\n \n xla_cc_test(\n     name = \"address_computation_fusion_rewriter_test\",\n-    srcs = [\"address_computation_fusion_rewriter_test.cc\"],\n+    srcs = if_cuda_is_configured([\"address_computation_fusion_rewriter_test.cc\"]),\n     deps = [\n         \":address_computation_fusion_rewriter\",\n         \":gpu_device_info_for_tests\",\n"
                },
                {
                    "old_start": 3364,
                    "old_length": 9,
                    "new_start": 3365,
                    "new_length": 11,
                    "hunk": "@@ -3364,9 +3365,11 @@ xla_cc_test(\n         \"//xla/ffi:ffi_api\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_value\",\n+        \"//xla/service:custom_call_target_registry\",\n         \"//xla/service:executable\",\n         \"//xla/service:hlo_memory_scheduler\",\n         \"//xla/service:hlo_module_config\",\n+        \"//xla/stream_executor/gpu:gpu_types_header\",\n         \"//xla/tests:hlo_test_base\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/status\",\n"
                }
            ],
            "whole_deleted": "-    srcs = [\"address_computation_fusion_rewriter_test.cc\"],\n",
            "whole_added": "+        \"//xla:shape_util\",\n+    srcs = if_cuda_is_configured([\"address_computation_fusion_rewriter_test.cc\"]),\n+        \"//xla/service:custom_call_target_registry\",\n+        \"//xla/stream_executor/gpu:gpu_types_header\",\n",
            "whole_hunk": "@@ -3331,6 +3331,7 @@ cc_library(\n         \":cublas_cudnn\",\n         \":hlo_traversal\",\n         \":ir_emission_utils\",\n+        \"//xla:shape_util\",\n         \"//xla:statusor\",\n         \"//xla:util\",\n         \"//xla/ffi:ffi_api\",\n@@ -3353,7 +3354,7 @@ cc_library(\n \n xla_cc_test(\n     name = \"address_computation_fusion_rewriter_test\",\n-    srcs = [\"address_computation_fusion_rewriter_test.cc\"],\n+    srcs = if_cuda_is_configured([\"address_computation_fusion_rewriter_test.cc\"]),\n     deps = [\n         \":address_computation_fusion_rewriter\",\n         \":gpu_device_info_for_tests\",\n@@ -3364,9 +3365,11 @@ xla_cc_test(\n         \"//xla/ffi:ffi_api\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_value\",\n+        \"//xla/service:custom_call_target_registry\",\n         \"//xla/service:executable\",\n         \"//xla/service:hlo_memory_scheduler\",\n         \"//xla/service:hlo_module_config\",\n+        \"//xla/stream_executor/gpu:gpu_types_header\",\n         \"//xla/tests:hlo_test_base\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/status\",\n"
        },
        {
            "name": "address_computation_fusion_rewriter.cc",
            "path": "third_party/xla/xla/service/gpu/address_computation_fusion_rewriter.cc",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 6,
                    "new_start": 18,
                    "new_length": 7,
                    "hunk": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <cstdint>\n #include <functional>\n #include <optional>\n+#include <string>\n #include <utility>\n #include <vector>\n \n"
                },
                {
                    "old_start": 29,
                    "old_length": 15,
                    "new_start": 30,
                    "new_length": 19,
                    "hunk": "@@ -29,15 +30,19 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/ffi/api/c_api.h\"\n+#include \"xla/ffi/ffi_api.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n+#include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/hlo_traversal.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/shape.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/errors.h\"\n #include \"tsl/platform/statusor.h\"\n"
                },
                {
                    "old_start": 52,
                    "old_length": 12,
                    "new_start": 57,
                    "new_length": 35,
                    "hunk": "@@ -52,12 +57,35 @@ bool IsNoOp(const HloInstruction* hlo) {\n                           HloOpcode::kGetTupleElement>(hlo);\n }\n \n-bool IsCustomCall(const HloInstruction* hlo) {\n+bool IsCustomCall(const HloInstruction* hlo, absl::string_view platform_name) {\n   auto* custom_call = DynCast<HloCustomCallInstruction>(hlo);\n   if (custom_call == nullptr) return false;\n \n-  return custom_call->api_version() ==\n-         CustomCallApiVersion::API_VERSION_TYPED_FFI;\n+  // TODO(vuson): properly handle token by following\n+  // `LhloDialectEmitter::EmitCustomCallOp`'s `CreateOperands` logic for\n+  // `LhloDialectEmitter::EmitFusionOp`'s `RewriteFusionOperand`\n+  if (custom_call->shape().IsTuple() &&\n+      absl::c_any_of(\n+          custom_call->shape().tuple_shapes(),\n+          [&](const Shape& sub_shape) { return sub_shape.IsToken(); }))\n+    return false;\n+\n+  const std::string call_target_name = custom_call->custom_call_target();\n+\n+  bool is_ffi_custom_call =\n+      custom_call->api_version() == CustomCallApiVersion::API_VERSION_TYPED_FFI;\n+\n+  void* call_target = CustomCallTargetRegistry::Global()->Lookup(\n+      call_target_name, std::string(platform_name));\n+\n+  absl::StatusOr<XLA_FFI_Handler*> handler =\n+      ffi::FindHandler(call_target_name, platform_name);\n+\n+  // At least one implementation should be available at run time.\n+  bool found_custom_call = !is_ffi_custom_call && call_target != nullptr;\n+  bool found_ffi_handler = is_ffi_custom_call && handler.ok();\n+\n+  return found_custom_call || found_ffi_handler;\n }\n \n absl::InlinedVector<HloInstruction*, 8> GetSlicedOperandChains(\n"
                },
                {
                    "old_start": 253,
                    "old_length": 7,
                    "new_start": 281,
                    "new_length": 7,
                    "hunk": "@@ -253,7 +281,7 @@ absl::StatusOr<bool> AddressComputationFusionRewriter::Run(\n   for (HloComputation* computation : module->computations()) {\n     if (computation->IsFusionComputation()) continue;\n     for (HloInstruction* instr : computation->instructions()) {\n-      if (IsLegacyCublasMatmul(*instr) || IsCustomCall(instr)) {\n+      if (IsLegacyCublasMatmul(*instr) || IsCustomCall(instr, platform_name_)) {\n         auto sliced_operand_chains = GetSlicedOperandChains(instr);\n         if (!(sliced_operand_chains.size() == 1 &&\n               sliced_operand_chains.front() == instr)) {\n"
                }
            ],
            "whole_deleted": "-bool IsCustomCall(const HloInstruction* hlo) {\n-  return custom_call->api_version() ==\n-         CustomCallApiVersion::API_VERSION_TYPED_FFI;\n-      if (IsLegacyCublasMatmul(*instr) || IsCustomCall(instr)) {\n",
            "whole_added": "+#include <string>\n+#include \"xla/ffi/api/c_api.h\"\n+#include \"xla/ffi/ffi_api.h\"\n+#include \"xla/service/custom_call_target_registry.h\"\n+#include \"xla/shape.h\"\n+bool IsCustomCall(const HloInstruction* hlo, absl::string_view platform_name) {\n+  // TODO(vuson): properly handle token by following\n+  // `LhloDialectEmitter::EmitCustomCallOp`'s `CreateOperands` logic for\n+  // `LhloDialectEmitter::EmitFusionOp`'s `RewriteFusionOperand`\n+  if (custom_call->shape().IsTuple() &&\n+      absl::c_any_of(\n+          custom_call->shape().tuple_shapes(),\n+          [&](const Shape& sub_shape) { return sub_shape.IsToken(); }))\n+    return false;\n+\n+  const std::string call_target_name = custom_call->custom_call_target();\n+\n+  bool is_ffi_custom_call =\n+      custom_call->api_version() == CustomCallApiVersion::API_VERSION_TYPED_FFI;\n+\n+  void* call_target = CustomCallTargetRegistry::Global()->Lookup(\n+      call_target_name, std::string(platform_name));\n+\n+  absl::StatusOr<XLA_FFI_Handler*> handler =\n+      ffi::FindHandler(call_target_name, platform_name);\n+\n+  // At least one implementation should be available at run time.\n+  bool found_custom_call = !is_ffi_custom_call && call_target != nullptr;\n+  bool found_ffi_handler = is_ffi_custom_call && handler.ok();\n+\n+  return found_custom_call || found_ffi_handler;\n+      if (IsLegacyCublasMatmul(*instr) || IsCustomCall(instr, platform_name_)) {\n",
            "whole_hunk": "@@ -18,6 +18,7 @@ limitations under the License.\n #include <cstdint>\n #include <functional>\n #include <optional>\n+#include <string>\n #include <utility>\n #include <vector>\n \n@@ -29,15 +30,19 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/ffi/api/c_api.h\"\n+#include \"xla/ffi/ffi_api.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n+#include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/gpu/cublas_cudnn.h\"\n #include \"xla/service/gpu/hlo_traversal.h\"\n #include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/shape.h\"\n #include \"xla/util.h\"\n #include \"tsl/platform/errors.h\"\n #include \"tsl/platform/statusor.h\"\n@@ -52,12 +57,35 @@ bool IsNoOp(const HloInstruction* hlo) {\n                           HloOpcode::kGetTupleElement>(hlo);\n }\n \n-bool IsCustomCall(const HloInstruction* hlo) {\n+bool IsCustomCall(const HloInstruction* hlo, absl::string_view platform_name) {\n   auto* custom_call = DynCast<HloCustomCallInstruction>(hlo);\n   if (custom_call == nullptr) return false;\n \n-  return custom_call->api_version() ==\n-         CustomCallApiVersion::API_VERSION_TYPED_FFI;\n+  // TODO(vuson): properly handle token by following\n+  // `LhloDialectEmitter::EmitCustomCallOp`'s `CreateOperands` logic for\n+  // `LhloDialectEmitter::EmitFusionOp`'s `RewriteFusionOperand`\n+  if (custom_call->shape().IsTuple() &&\n+      absl::c_any_of(\n+          custom_call->shape().tuple_shapes(),\n+          [&](const Shape& sub_shape) { return sub_shape.IsToken(); }))\n+    return false;\n+\n+  const std::string call_target_name = custom_call->custom_call_target();\n+\n+  bool is_ffi_custom_call =\n+      custom_call->api_version() == CustomCallApiVersion::API_VERSION_TYPED_FFI;\n+\n+  void* call_target = CustomCallTargetRegistry::Global()->Lookup(\n+      call_target_name, std::string(platform_name));\n+\n+  absl::StatusOr<XLA_FFI_Handler*> handler =\n+      ffi::FindHandler(call_target_name, platform_name);\n+\n+  // At least one implementation should be available at run time.\n+  bool found_custom_call = !is_ffi_custom_call && call_target != nullptr;\n+  bool found_ffi_handler = is_ffi_custom_call && handler.ok();\n+\n+  return found_custom_call || found_ffi_handler;\n }\n \n absl::InlinedVector<HloInstruction*, 8> GetSlicedOperandChains(\n@@ -253,7 +281,7 @@ absl::StatusOr<bool> AddressComputationFusionRewriter::Run(\n   for (HloComputation* computation : module->computations()) {\n     if (computation->IsFusionComputation()) continue;\n     for (HloInstruction* instr : computation->instructions()) {\n-      if (IsLegacyCublasMatmul(*instr) || IsCustomCall(instr)) {\n+      if (IsLegacyCublasMatmul(*instr) || IsCustomCall(instr, platform_name_)) {\n         auto sliced_operand_chains = GetSlicedOperandChains(instr);\n         if (!(sliced_operand_chains.size() == 1 &&\n               sliced_operand_chains.front() == instr)) {\n"
        },
        {
            "name": "address_computation_fusion_rewriter.h",
            "path": "third_party/xla/xla/service/gpu/address_computation_fusion_rewriter.h",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 6,
                    "new_start": 16,
                    "new_length": 7,
                    "hunk": "@@ -16,6 +16,7 @@ limitations under the License.\n #define XLA_SERVICE_GPU_ADDRESS_COMPUTATION_FUSION_REWRITER_H_\n \n #include <string>\n+#include <utility>\n \n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <utility>\n",
            "whole_hunk": "@@ -16,6 +16,7 @@ limitations under the License.\n #define XLA_SERVICE_GPU_ADDRESS_COMPUTATION_FUSION_REWRITER_H_\n \n #include <string>\n+#include <utility>\n \n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/status/statusor.h\"\n"
        },
        {
            "name": "address_computation_fusion_rewriter_test.cc",
            "path": "third_party/xla/xla/service/gpu/address_computation_fusion_rewriter_test.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/service/gpu/address_computation_fusion_rewriter.h\"\n \n+#include <cstddef>\n #include <cstdint>\n #include <functional>\n #include <optional>\n"
                },
                {
                    "old_start": 29,
                    "old_length": 12,
                    "new_start": 30,
                    "new_length": 14,
                    "hunk": "@@ -29,12 +30,14 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/service/buffer_value.h\"\n+#include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/hlo_memory_scheduler.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/gpu/gpu_types.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"tsl/platform/status.h\"\n #include \"tsl/platform/statusor.h\"\n"
                },
                {
                    "old_start": 941,
                    "old_length": 4,
                    "new_start": 944,
                    "new_length": 61,
                    "hunk": "@@ -941,4 +944,61 @@ TEST_F(AddressComputationFusionRewriterTest, SimpleCustomCall) {\n                             });\n }\n \n+void Callback_Void(se::gpu::GpuStreamHandle stream, void** buffers,\n+                   const char* /*opaque*/, size_t /*opaque_len*/) {}\n+\n+XLA_REGISTER_CUSTOM_CALL_TARGET(Callback_Void, PLATFORM);\n+\n+TEST_F(AddressComputationFusionRewriterTest, SimpleCustomCallLegacy) {\n+  XlaBuilder b(TestName());\n+  CustomCall(&b, \"Callback_Void\",\n+             /*operands=*/\n+             {Slice(Broadcast(ConstantR0WithType(&b, F32, 42.0), {256}), {0},\n+                    {128}, {1})},\n+             ShapeUtil::MakeShape(F32, {128}), /*opaque=*/\"\");\n+  TF_ASSERT_OK_AND_ASSIGN(auto computation, b.Build());\n+  xla::HloModuleConfig hlo_config(\n+      xla::ProgramShape(computation.proto().host_program_shape()),\n+      /*ignore_layouts=*/false);\n+  DebugOptions debug_options = GetDebugOptionsForTest();\n+  debug_options.set_xla_gpu_enable_address_computation_fusion(false);\n+  hlo_config.set_debug_options(debug_options);\n+  TF_ASSERT_OK_AND_ASSIGN(auto hlo, xla::HloModule::CreateFromProto(\n+                                        computation.proto(), hlo_config));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      HloSchedule schedule,\n+      ScheduleModule(hlo.get(), [](const BufferValue& buffer) {\n+        return ShapeUtil::ByteSizeOf(buffer.shape(), /*pointer_size=*/8);\n+      }));\n+  TF_CHECK_OK(hlo->set_schedule(std::move(schedule)));\n+\n+  const char* expected = R\"(\n+    ; CHECK:     %address-computation {{.*}} {\n+    ; CHECK:       [[P0:%[^ ]+]] = f32[256]{0} parameter(0)\n+    ; CHECK:       [[S0:%[^ ]+]] = f32[128]{0} slice([[P0]]), slice={[0:128]}\n+    ; CHECK:       ROOT [[CC:%[^ ]+]] = f32[128]{0} custom-call([[S0]]),\n+    ; CHECK:              custom_call_target=\"Callback_Void\"\n+    ; CHECK:     }\n+\n+    ; CHECK:     ENTRY %{{.*}} {\n+    ; CHECK:       [[C0:%[^ ]+]] = f32[] constant(42)\n+    ; CHECK:       [[BC:%[^ ]+]] = f32[256]{0} broadcast([[C0]])\n+    ; CHECK:       ROOT [[FUSION:%[^ ]+]] = f32[128]{0} fusion([[BC]])\n+    ; CHECK:         kind=kCustom, calls=%address-computation,\n+    ; CHECK:         backend_config={\n+    ; CHECK:           \"kind\":\"__custom_fusion\",\n+    ; CHECK:           \"custom_fusion_config\":{\"name\":\"address_computation\"}\n+    ; CHECK:         }\n+    ; CHECK:     }\n+  )\";\n+\n+  auto device = TestGpuDeviceInfo::RTXA6000DeviceInfo();\n+  RunAndFilecheckHloRewrite(hlo->ToString(),\n+                            AddressComputationFusionRewriter(PLATFORM),\n+                            expected, [](HloModule* module) {\n+                              EXPECT_TRUE(module->has_schedule());\n+                              TF_CHECK_OK(module->schedule().Verify());\n+                            });\n+}\n+\n }  // namespace xla::gpu"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <cstddef>\n+#include \"xla/service/custom_call_target_registry.h\"\n+#include \"xla/stream_executor/gpu/gpu_types.h\"\n+void Callback_Void(se::gpu::GpuStreamHandle stream, void** buffers,\n+                   const char* /*opaque*/, size_t /*opaque_len*/) {}\n+\n+XLA_REGISTER_CUSTOM_CALL_TARGET(Callback_Void, PLATFORM);\n+\n+TEST_F(AddressComputationFusionRewriterTest, SimpleCustomCallLegacy) {\n+  XlaBuilder b(TestName());\n+  CustomCall(&b, \"Callback_Void\",\n+             /*operands=*/\n+             {Slice(Broadcast(ConstantR0WithType(&b, F32, 42.0), {256}), {0},\n+                    {128}, {1})},\n+             ShapeUtil::MakeShape(F32, {128}), /*opaque=*/\"\");\n+  TF_ASSERT_OK_AND_ASSIGN(auto computation, b.Build());\n+  xla::HloModuleConfig hlo_config(\n+      xla::ProgramShape(computation.proto().host_program_shape()),\n+      /*ignore_layouts=*/false);\n+  DebugOptions debug_options = GetDebugOptionsForTest();\n+  debug_options.set_xla_gpu_enable_address_computation_fusion(false);\n+  hlo_config.set_debug_options(debug_options);\n+  TF_ASSERT_OK_AND_ASSIGN(auto hlo, xla::HloModule::CreateFromProto(\n+                                        computation.proto(), hlo_config));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      HloSchedule schedule,\n+      ScheduleModule(hlo.get(), [](const BufferValue& buffer) {\n+        return ShapeUtil::ByteSizeOf(buffer.shape(), /*pointer_size=*/8);\n+      }));\n+  TF_CHECK_OK(hlo->set_schedule(std::move(schedule)));\n+\n+  const char* expected = R\"(\n+    ; CHECK:     %address-computation {{.*}} {\n+    ; CHECK:       [[P0:%[^ ]+]] = f32[256]{0} parameter(0)\n+    ; CHECK:       [[S0:%[^ ]+]] = f32[128]{0} slice([[P0]]), slice={[0:128]}\n+    ; CHECK:       ROOT [[CC:%[^ ]+]] = f32[128]{0} custom-call([[S0]]),\n+    ; CHECK:              custom_call_target=\"Callback_Void\"\n+    ; CHECK:     }\n+\n+    ; CHECK:     ENTRY %{{.*}} {\n+    ; CHECK:       [[C0:%[^ ]+]] = f32[] constant(42)\n+    ; CHECK:       [[BC:%[^ ]+]] = f32[256]{0} broadcast([[C0]])\n+    ; CHECK:       ROOT [[FUSION:%[^ ]+]] = f32[128]{0} fusion([[BC]])\n+    ; CHECK:         kind=kCustom, calls=%address-computation,\n+    ; CHECK:         backend_config={\n+    ; CHECK:           \"kind\":\"__custom_fusion\",\n+    ; CHECK:           \"custom_fusion_config\":{\"name\":\"address_computation\"}\n+    ; CHECK:         }\n+    ; CHECK:     }\n+  )\";\n+\n+  auto device = TestGpuDeviceInfo::RTXA6000DeviceInfo();\n+  RunAndFilecheckHloRewrite(hlo->ToString(),\n+                            AddressComputationFusionRewriter(PLATFORM),\n+                            expected, [](HloModule* module) {\n+                              EXPECT_TRUE(module->has_schedule());\n+                              TF_CHECK_OK(module->schedule().Verify());\n+                            });\n+}\n+\n",
            "whole_hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/service/gpu/address_computation_fusion_rewriter.h\"\n \n+#include <cstddef>\n #include <cstdint>\n #include <functional>\n #include <optional>\n@@ -29,12 +30,14 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_schedule.h\"\n #include \"xla/service/buffer_value.h\"\n+#include \"xla/service/custom_call_target_registry.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/hlo_memory_scheduler.h\"\n #include \"xla/service/hlo_module_config.h\"\n #include \"xla/service/service_executable_run_options.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/stream_executor/gpu/gpu_types.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"tsl/platform/status.h\"\n #include \"tsl/platform/statusor.h\"\n@@ -941,4 +944,61 @@ TEST_F(AddressComputationFusionRewriterTest, SimpleCustomCall) {\n                             });\n }\n \n+void Callback_Void(se::gpu::GpuStreamHandle stream, void** buffers,\n+                   const char* /*opaque*/, size_t /*opaque_len*/) {}\n+\n+XLA_REGISTER_CUSTOM_CALL_TARGET(Callback_Void, PLATFORM);\n+\n+TEST_F(AddressComputationFusionRewriterTest, SimpleCustomCallLegacy) {\n+  XlaBuilder b(TestName());\n+  CustomCall(&b, \"Callback_Void\",\n+             /*operands=*/\n+             {Slice(Broadcast(ConstantR0WithType(&b, F32, 42.0), {256}), {0},\n+                    {128}, {1})},\n+             ShapeUtil::MakeShape(F32, {128}), /*opaque=*/\"\");\n+  TF_ASSERT_OK_AND_ASSIGN(auto computation, b.Build());\n+  xla::HloModuleConfig hlo_config(\n+      xla::ProgramShape(computation.proto().host_program_shape()),\n+      /*ignore_layouts=*/false);\n+  DebugOptions debug_options = GetDebugOptionsForTest();\n+  debug_options.set_xla_gpu_enable_address_computation_fusion(false);\n+  hlo_config.set_debug_options(debug_options);\n+  TF_ASSERT_OK_AND_ASSIGN(auto hlo, xla::HloModule::CreateFromProto(\n+                                        computation.proto(), hlo_config));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      HloSchedule schedule,\n+      ScheduleModule(hlo.get(), [](const BufferValue& buffer) {\n+        return ShapeUtil::ByteSizeOf(buffer.shape(), /*pointer_size=*/8);\n+      }));\n+  TF_CHECK_OK(hlo->set_schedule(std::move(schedule)));\n+\n+  const char* expected = R\"(\n+    ; CHECK:     %address-computation {{.*}} {\n+    ; CHECK:       [[P0:%[^ ]+]] = f32[256]{0} parameter(0)\n+    ; CHECK:       [[S0:%[^ ]+]] = f32[128]{0} slice([[P0]]), slice={[0:128]}\n+    ; CHECK:       ROOT [[CC:%[^ ]+]] = f32[128]{0} custom-call([[S0]]),\n+    ; CHECK:              custom_call_target=\"Callback_Void\"\n+    ; CHECK:     }\n+\n+    ; CHECK:     ENTRY %{{.*}} {\n+    ; CHECK:       [[C0:%[^ ]+]] = f32[] constant(42)\n+    ; CHECK:       [[BC:%[^ ]+]] = f32[256]{0} broadcast([[C0]])\n+    ; CHECK:       ROOT [[FUSION:%[^ ]+]] = f32[128]{0} fusion([[BC]])\n+    ; CHECK:         kind=kCustom, calls=%address-computation,\n+    ; CHECK:         backend_config={\n+    ; CHECK:           \"kind\":\"__custom_fusion\",\n+    ; CHECK:           \"custom_fusion_config\":{\"name\":\"address_computation\"}\n+    ; CHECK:         }\n+    ; CHECK:     }\n+  )\";\n+\n+  auto device = TestGpuDeviceInfo::RTXA6000DeviceInfo();\n+  RunAndFilecheckHloRewrite(hlo->ToString(),\n+                            AddressComputationFusionRewriter(PLATFORM),\n+                            expected, [](HloModule* module) {\n+                              EXPECT_TRUE(module->has_schedule());\n+                              TF_CHECK_OK(module->schedule().Verify());\n+                            });\n+}\n+\n }  // namespace xla::gpu"
        }
    ]
},
{
    "Id": 30,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e0b4ce7751e929ae4de6a0e43bf6b8081c8e5d25",
    "date": "2024-06-04T05:39:39-07:00",
    "message": "PR #13360: [oneDNN][BugFix] Fix dtype utility function for F16 AVX2 DL extension\n\nImported from GitHub PR https://github.com/openxla/xla/pull/13360\n\nIntel's Efficiency cores do not support AVX512 instructions. However, some of them do support lower precisions (BF16/FP16) by converting to and from FP32. Due to the lack of Byte and Word instruction set support, the current condition check always returns False, even on supporting E-cores. This PR fixes the condition to correctly return True on all compatible hardware.\nCopybara import of the project:\n\n--\nd138f97cb5004b199e626d4f705e50269b79e160 by Akhil Goel <akhil.goel@intel.com>:\n\nFix AVX2 FP16\n\nMerging this change closes #13360\n\nPiperOrigin-RevId: 640097657",
    "label": "YES",
    "changes": [
        {
            "name": "onednn_util.h",
            "path": "third_party/xla/xla/service/cpu/onednn_util.h",
            "patches": [
                {
                    "old_start": 41,
                    "old_length": 10,
                    "new_start": 41,
                    "new_length": 10,
                    "hunk": "@@ -41,10 +41,10 @@ inline bool IsSupportedType(xla::PrimitiveType dtype) {\n              TestCPUFeature(CPUFeature::AVX_NE_CONVERT) ||\n              TestCPUFeature(CPUFeature::AMX_BF16);\n     case F16:\n-      return TestCPUFeature(CPUFeature::AVX512BW) &&\n-             (TestCPUFeature(CPUFeature::AVX512_FP16) ||\n-              TestCPUFeature(CPUFeature::AMX_FP16) ||\n-              TestCPUFeature(CPUFeature::AVX_NE_CONVERT));\n+      return (TestCPUFeature(CPUFeature::AVX512BW) &&\n+              (TestCPUFeature(CPUFeature::AVX512_FP16) ||\n+               TestCPUFeature(CPUFeature::AMX_FP16))) ||\n+             TestCPUFeature(CPUFeature::AVX_NE_CONVERT);\n     default:\n       return false;\n   }"
                }
            ],
            "whole_deleted": "-      return TestCPUFeature(CPUFeature::AVX512BW) &&\n-             (TestCPUFeature(CPUFeature::AVX512_FP16) ||\n-              TestCPUFeature(CPUFeature::AMX_FP16) ||\n-              TestCPUFeature(CPUFeature::AVX_NE_CONVERT));\n",
            "whole_added": "+      return (TestCPUFeature(CPUFeature::AVX512BW) &&\n+              (TestCPUFeature(CPUFeature::AVX512_FP16) ||\n+               TestCPUFeature(CPUFeature::AMX_FP16))) ||\n+             TestCPUFeature(CPUFeature::AVX_NE_CONVERT);\n",
            "whole_hunk": "@@ -41,10 +41,10 @@ inline bool IsSupportedType(xla::PrimitiveType dtype) {\n              TestCPUFeature(CPUFeature::AVX_NE_CONVERT) ||\n              TestCPUFeature(CPUFeature::AMX_BF16);\n     case F16:\n-      return TestCPUFeature(CPUFeature::AVX512BW) &&\n-             (TestCPUFeature(CPUFeature::AVX512_FP16) ||\n-              TestCPUFeature(CPUFeature::AMX_FP16) ||\n-              TestCPUFeature(CPUFeature::AVX_NE_CONVERT));\n+      return (TestCPUFeature(CPUFeature::AVX512BW) &&\n+              (TestCPUFeature(CPUFeature::AVX512_FP16) ||\n+               TestCPUFeature(CPUFeature::AMX_FP16))) ||\n+             TestCPUFeature(CPUFeature::AVX_NE_CONVERT);\n     default:\n       return false;\n   }"
        }
    ]
},
{
    "Id": 69,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/64740d5dfdc689ba04f2a95c2ff565ef2c0a9f53",
    "date": "2024-04-19T10:36:43-07:00",
    "message": "Avoid overflows in check whether there is enough available input space\n\nbytes_to_write can overflow int32 in a way that makes it look smaller than\nAvailableInputSpace(). Instead do the comparison using size_t.\n\nPiperOrigin-RevId: 626402022",
    "label": "YES",
    "changes": [
        {
            "name": "zlib_outputbuffer.cc",
            "path": "third_party/xla/third_party/tsl/tsl/lib/io/zlib_outputbuffer.cc",
            "patches": [
                {
                    "old_start": 155,
                    "old_length": 7,
                    "new_start": 155,
                    "new_length": 7,
                    "hunk": "@@ -155,7 +155,7 @@ absl::Status ZlibOutputBuffer::Append(StringPiece data) {\n \n   size_t bytes_to_write = data.size();\n \n-  if (static_cast<int32>(bytes_to_write) <= AvailableInputSpace()) {\n+  if (bytes_to_write <= static_cast<size_t>(AvailableInputSpace())) {\n     AddToInputBuffer(data);\n     return absl::OkStatus();\n   }\n"
                },
                {
                    "old_start": 163,
                    "old_length": 7,
                    "new_start": 163,
                    "new_length": 7,
                    "hunk": "@@ -163,7 +163,7 @@ absl::Status ZlibOutputBuffer::Append(StringPiece data) {\n   TF_RETURN_IF_ERROR(DeflateBuffered(zlib_options_.flush_mode));\n \n   // At this point input stream should be empty.\n-  if (static_cast<int32>(bytes_to_write) <= AvailableInputSpace()) {\n+  if (bytes_to_write <= static_cast<size_t>(AvailableInputSpace())) {\n     AddToInputBuffer(data);\n     return absl::OkStatus();\n   }"
                }
            ],
            "whole_deleted": "-  if (static_cast<int32>(bytes_to_write) <= AvailableInputSpace()) {\n-  if (static_cast<int32>(bytes_to_write) <= AvailableInputSpace()) {\n",
            "whole_added": "+  if (bytes_to_write <= static_cast<size_t>(AvailableInputSpace())) {\n+  if (bytes_to_write <= static_cast<size_t>(AvailableInputSpace())) {\n",
            "whole_hunk": "@@ -155,7 +155,7 @@ absl::Status ZlibOutputBuffer::Append(StringPiece data) {\n \n   size_t bytes_to_write = data.size();\n \n-  if (static_cast<int32>(bytes_to_write) <= AvailableInputSpace()) {\n+  if (bytes_to_write <= static_cast<size_t>(AvailableInputSpace())) {\n     AddToInputBuffer(data);\n     return absl::OkStatus();\n   }\n@@ -163,7 +163,7 @@ absl::Status ZlibOutputBuffer::Append(StringPiece data) {\n   TF_RETURN_IF_ERROR(DeflateBuffered(zlib_options_.flush_mode));\n \n   // At this point input stream should be empty.\n-  if (static_cast<int32>(bytes_to_write) <= AvailableInputSpace()) {\n+  if (bytes_to_write <= static_cast<size_t>(AvailableInputSpace())) {\n     AddToInputBuffer(data);\n     return absl::OkStatus();\n   }"
        }
    ]
},
{
    "Id": 174,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a2383e62036c7a4036a10755a8de54855e9ac9a5",
    "date": "2024-01-31T22:53:26-08:00",
    "message": "[XLA:GPU] Add proper checks for code duplication to priority_fusion.\n\nWe were using fusion_node_indexing_evaluation which just supports checks\nwhether a single producer can be fused into a fusion, but we called it also\nwith producer fusions which were then not properly evaluated. We already have a\ndifferent way to check this via cost_analysis which is also used in\nFusionMerger, so we should be using that instead.\n\nPiperOrigin-RevId: 603267214",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 2505,
                    "old_length": 7,
                    "new_start": 2505,
                    "new_length": 6,
                    "hunk": "@@ -2505,7 +2505,6 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:dump\",\n-        \"//xla/service:fusion_node_indexing_evaluation\",\n         \"//xla/service:fusion_queue\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:hlo_pass\",\n"
                }
            ],
            "whole_deleted": "-        \"//xla/service:fusion_node_indexing_evaluation\",\n",
            "whole_added": "",
            "whole_hunk": "@@ -2505,7 +2505,6 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:dump\",\n-        \"//xla/service:fusion_node_indexing_evaluation\",\n         \"//xla/service:fusion_queue\",\n         \"//xla/service:hlo_cost_analysis\",\n         \"//xla/service:hlo_pass\",\n"
        },
        {
            "name": "priority_fusion.cc",
            "path": "third_party/xla/xla/service/gpu/priority_fusion.cc",
            "patches": [
                {
                    "old_start": 38,
                    "old_length": 7,
                    "new_start": 38,
                    "new_length": 6,
                    "hunk": "@@ -38,7 +38,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/dump.h\"\n-#include \"xla/service/fusion_node_indexing_evaluation.h\"\n #include \"xla/service/fusion_queue.h\"\n #include \"xla/service/gpu/fusion_process_dump.pb.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n"
                },
                {
                    "old_start": 276,
                    "old_length": 11,
                    "new_start": 275,
                    "new_length": 6,
                    "hunk": "@@ -276,11 +275,6 @@ class GpuPriorityFusionQueue : public FusionQueue {\n \n     gpu_performance_model_cache_.Invalidate(*instruction);\n     fusion_analysis_cache_.Invalidate(*instruction);\n-\n-    for (auto* user : instruction->users()) {\n-      fusion_node_evaluations_.erase(user);\n-    }\n-    fusion_node_evaluations_.erase(instruction);\n   }\n \n   // Updates data for the new fusion instruction and its users and operands.\n"
                },
                {
                    "old_start": 471,
                    "old_length": 18,
                    "new_start": 465,
                    "new_length": 8,
                    "hunk": "@@ -471,18 +465,8 @@ class GpuPriorityFusionQueue : public FusionQueue {\n     // have exponential time/memory requirements for emitting certain fusion\n     // kernels, in which case we don't want to fuse.\n     // TODO(b/119692968): Remove this once we have fixed our fusion emitter.\n-    if (consumer->opcode() == HloOpcode::kFusion) {\n-      absl::MutexLock lock(&fusion_node_evaluations_mutex_);\n-      if (fusion_node_evaluations_.find(consumer) ==\n-          fusion_node_evaluations_.end()) {\n-        // We have no cached results for this fusion node yet. Compute it now.\n-        fusion_node_evaluations_.emplace(\n-            consumer, FusionNodeIndexingEvaluation(consumer));\n-      }\n-      if (fusion_node_evaluations_.at(consumer).CodeDuplicationTooHigh(\n-              producer)) {\n-        return \"the fusion would result in an overly large code duplication\";\n-      }\n+    if (cost_analysis_.ProducerConsumerMergedTooLarge(*producer, *consumer)) {\n+      return \"the fusion would result in an overly large code duplication\";\n     }\n \n     // Don't fuse across a root instruction. There are situation when a root\n"
                },
                {
                    "old_start": 588,
                    "old_length": 12,
                    "new_start": 572,
                    "new_length": 6,
                    "hunk": "@@ -588,12 +572,6 @@ class GpuPriorityFusionQueue : public FusionQueue {\n   absl::Mutex can_fuse_cache_mutex_;\n \n   GpuPerformanceModelCache gpu_performance_model_cache_;\n-\n-  // Keep track of the number of times each instruction inside a fusion node is\n-  // indexed with different index vectors.\n-  absl::Mutex fusion_node_evaluations_mutex_;\n-  absl::flat_hash_map<const HloInstruction*, FusionNodeIndexingEvaluation>\n-      fusion_node_evaluations_;\n };\n \n }  // namespace\n"
                }
            ],
            "whole_deleted": "-#include \"xla/service/fusion_node_indexing_evaluation.h\"\n-\n-    for (auto* user : instruction->users()) {\n-      fusion_node_evaluations_.erase(user);\n-    }\n-    fusion_node_evaluations_.erase(instruction);\n-    if (consumer->opcode() == HloOpcode::kFusion) {\n-      absl::MutexLock lock(&fusion_node_evaluations_mutex_);\n-      if (fusion_node_evaluations_.find(consumer) ==\n-          fusion_node_evaluations_.end()) {\n-        // We have no cached results for this fusion node yet. Compute it now.\n-        fusion_node_evaluations_.emplace(\n-            consumer, FusionNodeIndexingEvaluation(consumer));\n-      }\n-      if (fusion_node_evaluations_.at(consumer).CodeDuplicationTooHigh(\n-              producer)) {\n-        return \"the fusion would result in an overly large code duplication\";\n-      }\n-\n-  // Keep track of the number of times each instruction inside a fusion node is\n-  // indexed with different index vectors.\n-  absl::Mutex fusion_node_evaluations_mutex_;\n-  absl::flat_hash_map<const HloInstruction*, FusionNodeIndexingEvaluation>\n-      fusion_node_evaluations_;\n",
            "whole_added": "+    if (cost_analysis_.ProducerConsumerMergedTooLarge(*producer, *consumer)) {\n+      return \"the fusion would result in an overly large code duplication\";\n",
            "whole_hunk": "@@ -38,7 +38,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/service/dump.h\"\n-#include \"xla/service/fusion_node_indexing_evaluation.h\"\n #include \"xla/service/fusion_queue.h\"\n #include \"xla/service/gpu/fusion_process_dump.pb.h\"\n #include \"xla/service/gpu/gpu_fusible.h\"\n@@ -276,11 +275,6 @@ class GpuPriorityFusionQueue : public FusionQueue {\n \n     gpu_performance_model_cache_.Invalidate(*instruction);\n     fusion_analysis_cache_.Invalidate(*instruction);\n-\n-    for (auto* user : instruction->users()) {\n-      fusion_node_evaluations_.erase(user);\n-    }\n-    fusion_node_evaluations_.erase(instruction);\n   }\n \n   // Updates data for the new fusion instruction and its users and operands.\n@@ -471,18 +465,8 @@ class GpuPriorityFusionQueue : public FusionQueue {\n     // have exponential time/memory requirements for emitting certain fusion\n     // kernels, in which case we don't want to fuse.\n     // TODO(b/119692968): Remove this once we have fixed our fusion emitter.\n-    if (consumer->opcode() == HloOpcode::kFusion) {\n-      absl::MutexLock lock(&fusion_node_evaluations_mutex_);\n-      if (fusion_node_evaluations_.find(consumer) ==\n-          fusion_node_evaluations_.end()) {\n-        // We have no cached results for this fusion node yet. Compute it now.\n-        fusion_node_evaluations_.emplace(\n-            consumer, FusionNodeIndexingEvaluation(consumer));\n-      }\n-      if (fusion_node_evaluations_.at(consumer).CodeDuplicationTooHigh(\n-              producer)) {\n-        return \"the fusion would result in an overly large code duplication\";\n-      }\n+    if (cost_analysis_.ProducerConsumerMergedTooLarge(*producer, *consumer)) {\n+      return \"the fusion would result in an overly large code duplication\";\n     }\n \n     // Don't fuse across a root instruction. There are situation when a root\n@@ -588,12 +572,6 @@ class GpuPriorityFusionQueue : public FusionQueue {\n   absl::Mutex can_fuse_cache_mutex_;\n \n   GpuPerformanceModelCache gpu_performance_model_cache_;\n-\n-  // Keep track of the number of times each instruction inside a fusion node is\n-  // indexed with different index vectors.\n-  absl::Mutex fusion_node_evaluations_mutex_;\n-  absl::flat_hash_map<const HloInstruction*, FusionNodeIndexingEvaluation>\n-      fusion_node_evaluations_;\n };\n \n }  // namespace\n"
        },
        {
            "name": "priority_fusion.h",
            "path": "third_party/xla/xla/service/gpu/priority_fusion.h",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 7,
                    "new_start": 26,
                    "new_length": 6,
                    "hunk": "@@ -26,7 +26,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/service/fusion_node_indexing_evaluation.h\"\n #include \"xla/service/fusion_queue.h\"\n #include \"xla/service/gpu/fusion_process_dump.pb.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n"
                }
            ],
            "whole_deleted": "-#include \"xla/service/fusion_node_indexing_evaluation.h\"\n",
            "whole_added": "",
            "whole_hunk": "@@ -26,7 +26,6 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n-#include \"xla/service/fusion_node_indexing_evaluation.h\"\n #include \"xla/service/fusion_queue.h\"\n #include \"xla/service/gpu/fusion_process_dump.pb.h\"\n #include \"xla/service/gpu/model/fusion_analysis_cache.h\"\n"
        },
        {
            "name": "priority_fusion_test.cc",
            "path": "third_party/xla/xla/service/gpu/priority_fusion_test.cc",
            "patches": [
                {
                    "old_start": 805,
                    "old_length": 5,
                    "new_start": 805,
                    "new_length": 56,
                    "hunk": "@@ -805,5 +805,56 @@ TEST_F(PriorityFusionTest, FuseOnlySmallConstant) {\n                   m::Add(m::Parameter(), m::Broadcast(m::Constant())))));\n }\n \n+TEST_F(PriorityFusionTest, DoNotFuseProducerConsumerMergedTooLarge) {\n+  auto module = *ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    fused_computation.1 {\n+      iota.9.7 = s32[3,1,1]{2,1,0} iota(), iota_dimension=0\n+      param_3.29 = s32[] parameter(2)\n+      pad.2.7 = s32[3,1,2]{2,1,0} pad(iota.9.7, param_3.29), padding=0_0x0_0x0_1\n+      param_2.39 = s32[] parameter(1)\n+      broadcast.76.1 = s32[3,1,2]{2,1,0} broadcast(param_2.39), dimensions={}\n+      compare.9.1 = pred[3,1,2]{2,1,0} compare(pad.2.7, broadcast.76.1), direction=GE\n+      param_1.73 = s32[2]{0} parameter(0)\n+      broadcast.78.1 = s32[3,2]{1,0} broadcast(param_1.73), dimensions={1}\n+      bitcast.1 = s32[3,2]{1,0} bitcast(pad.2.7)\n+      compare.10.1 = pred[3,2]{1,0} compare(bitcast.1, broadcast.78.1), direction=LE\n+      bitcast.2 = pred[3,1,2]{2,1,0} bitcast(compare.10.1)\n+      ROOT and.3.1 = pred[3,1,2]{2,1,0} and(compare.9.1, bitcast.2)\n+    }\n+\n+    and {\n+      x = pred[] parameter(0)\n+      y = pred[] parameter(1)\n+      ROOT and = pred[] and(x, y)\n+    }\n+\n+    fused_computation.2 {\n+      param0 = pred[3,1,2]{2,1,0} parameter(0)\n+      slice = pred[1,1,2]{2,1,0} slice(param0), slice={[0:1], [0:1], [0:2]}\n+      bitcast = pred[2]{0} bitcast(slice)\n+      init = pred[] constant(true)\n+      reduce = pred[2]{0} reduce(param0, init), dimensions={0,1}, to_apply=and\n+      and = pred[2]{0} and(bitcast, reduce)\n+      pad = pred[3]{0} pad(and, init), padding=0_1\n+      broadcast = pred[3,2]{1,0} broadcast(pad), dimensions={0}\n+      bitcast2 = pred[6]{0} bitcast(broadcast)\n+      broadcast2 = pred[2,3]{1,0} broadcast(pad), dimensions={1}\n+      bitcast3 = pred[6]{0} bitcast(broadcast2)\n+      ROOT and2 = pred[6]{0} and(bitcast2, bitcast3)\n+    }\n+\n+    ENTRY main {\n+      p0 = s32[2]{0} parameter(0)\n+      p1 = s32[] parameter(1)\n+      p2 = s32[] parameter(2)\n+      fusion1 = pred[3,1,2]{2,1,0} fusion(p0, p1, p2), kind=kLoop, calls=fused_computation.1\n+      ROOT fusion2 = pred[6]{0} fusion(fusion1), kind=kInput, calls=fused_computation.2\n+    }\n+  )\");\n+  EXPECT_THAT(priority_fusion_.Run(module.get()), IsOkAndHolds(false));\n+}\n+\n }  // namespace gpu\n }  // namespace xla\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(PriorityFusionTest, DoNotFuseProducerConsumerMergedTooLarge) {\n+  auto module = *ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    fused_computation.1 {\n+      iota.9.7 = s32[3,1,1]{2,1,0} iota(), iota_dimension=0\n+      param_3.29 = s32[] parameter(2)\n+      pad.2.7 = s32[3,1,2]{2,1,0} pad(iota.9.7, param_3.29), padding=0_0x0_0x0_1\n+      param_2.39 = s32[] parameter(1)\n+      broadcast.76.1 = s32[3,1,2]{2,1,0} broadcast(param_2.39), dimensions={}\n+      compare.9.1 = pred[3,1,2]{2,1,0} compare(pad.2.7, broadcast.76.1), direction=GE\n+      param_1.73 = s32[2]{0} parameter(0)\n+      broadcast.78.1 = s32[3,2]{1,0} broadcast(param_1.73), dimensions={1}\n+      bitcast.1 = s32[3,2]{1,0} bitcast(pad.2.7)\n+      compare.10.1 = pred[3,2]{1,0} compare(bitcast.1, broadcast.78.1), direction=LE\n+      bitcast.2 = pred[3,1,2]{2,1,0} bitcast(compare.10.1)\n+      ROOT and.3.1 = pred[3,1,2]{2,1,0} and(compare.9.1, bitcast.2)\n+    }\n+\n+    and {\n+      x = pred[] parameter(0)\n+      y = pred[] parameter(1)\n+      ROOT and = pred[] and(x, y)\n+    }\n+\n+    fused_computation.2 {\n+      param0 = pred[3,1,2]{2,1,0} parameter(0)\n+      slice = pred[1,1,2]{2,1,0} slice(param0), slice={[0:1], [0:1], [0:2]}\n+      bitcast = pred[2]{0} bitcast(slice)\n+      init = pred[] constant(true)\n+      reduce = pred[2]{0} reduce(param0, init), dimensions={0,1}, to_apply=and\n+      and = pred[2]{0} and(bitcast, reduce)\n+      pad = pred[3]{0} pad(and, init), padding=0_1\n+      broadcast = pred[3,2]{1,0} broadcast(pad), dimensions={0}\n+      bitcast2 = pred[6]{0} bitcast(broadcast)\n+      broadcast2 = pred[2,3]{1,0} broadcast(pad), dimensions={1}\n+      bitcast3 = pred[6]{0} bitcast(broadcast2)\n+      ROOT and2 = pred[6]{0} and(bitcast2, bitcast3)\n+    }\n+\n+    ENTRY main {\n+      p0 = s32[2]{0} parameter(0)\n+      p1 = s32[] parameter(1)\n+      p2 = s32[] parameter(2)\n+      fusion1 = pred[3,1,2]{2,1,0} fusion(p0, p1, p2), kind=kLoop, calls=fused_computation.1\n+      ROOT fusion2 = pred[6]{0} fusion(fusion1), kind=kInput, calls=fused_computation.2\n+    }\n+  )\");\n+  EXPECT_THAT(priority_fusion_.Run(module.get()), IsOkAndHolds(false));\n+}\n+\n",
            "whole_hunk": "@@ -805,5 +805,56 @@ TEST_F(PriorityFusionTest, FuseOnlySmallConstant) {\n                   m::Add(m::Parameter(), m::Broadcast(m::Constant())))));\n }\n \n+TEST_F(PriorityFusionTest, DoNotFuseProducerConsumerMergedTooLarge) {\n+  auto module = *ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    fused_computation.1 {\n+      iota.9.7 = s32[3,1,1]{2,1,0} iota(), iota_dimension=0\n+      param_3.29 = s32[] parameter(2)\n+      pad.2.7 = s32[3,1,2]{2,1,0} pad(iota.9.7, param_3.29), padding=0_0x0_0x0_1\n+      param_2.39 = s32[] parameter(1)\n+      broadcast.76.1 = s32[3,1,2]{2,1,0} broadcast(param_2.39), dimensions={}\n+      compare.9.1 = pred[3,1,2]{2,1,0} compare(pad.2.7, broadcast.76.1), direction=GE\n+      param_1.73 = s32[2]{0} parameter(0)\n+      broadcast.78.1 = s32[3,2]{1,0} broadcast(param_1.73), dimensions={1}\n+      bitcast.1 = s32[3,2]{1,0} bitcast(pad.2.7)\n+      compare.10.1 = pred[3,2]{1,0} compare(bitcast.1, broadcast.78.1), direction=LE\n+      bitcast.2 = pred[3,1,2]{2,1,0} bitcast(compare.10.1)\n+      ROOT and.3.1 = pred[3,1,2]{2,1,0} and(compare.9.1, bitcast.2)\n+    }\n+\n+    and {\n+      x = pred[] parameter(0)\n+      y = pred[] parameter(1)\n+      ROOT and = pred[] and(x, y)\n+    }\n+\n+    fused_computation.2 {\n+      param0 = pred[3,1,2]{2,1,0} parameter(0)\n+      slice = pred[1,1,2]{2,1,0} slice(param0), slice={[0:1], [0:1], [0:2]}\n+      bitcast = pred[2]{0} bitcast(slice)\n+      init = pred[] constant(true)\n+      reduce = pred[2]{0} reduce(param0, init), dimensions={0,1}, to_apply=and\n+      and = pred[2]{0} and(bitcast, reduce)\n+      pad = pred[3]{0} pad(and, init), padding=0_1\n+      broadcast = pred[3,2]{1,0} broadcast(pad), dimensions={0}\n+      bitcast2 = pred[6]{0} bitcast(broadcast)\n+      broadcast2 = pred[2,3]{1,0} broadcast(pad), dimensions={1}\n+      bitcast3 = pred[6]{0} bitcast(broadcast2)\n+      ROOT and2 = pred[6]{0} and(bitcast2, bitcast3)\n+    }\n+\n+    ENTRY main {\n+      p0 = s32[2]{0} parameter(0)\n+      p1 = s32[] parameter(1)\n+      p2 = s32[] parameter(2)\n+      fusion1 = pred[3,1,2]{2,1,0} fusion(p0, p1, p2), kind=kLoop, calls=fused_computation.1\n+      ROOT fusion2 = pred[6]{0} fusion(fusion1), kind=kInput, calls=fused_computation.2\n+    }\n+  )\");\n+  EXPECT_THAT(priority_fusion_.Run(module.get()), IsOkAndHolds(false));\n+}\n+\n }  // namespace gpu\n }  // namespace xla\n"
        },
        {
            "name": "gpu_too_many_blocks_test.cc",
            "path": "third_party/xla/xla/service/gpu/tests/gpu_too_many_blocks_test.cc",
            "patches": [
                {
                    "old_start": 55,
                    "old_length": 7,
                    "new_start": 55,
                    "new_length": 7,
                    "hunk": "@@ -55,7 +55,7 @@ ENTRY primitive_computation_mul.8 {\n   EXPECT_FALSE(failed_executable.ok());\n   EXPECT_THAT(\n       failed_executable.status().ToString(),\n-      ::testing::ContainsRegex(\"Kernel 'fusion.*' launch needs more blocks\"));\n+      ::testing::ContainsRegex(\"Kernel '.*fusion.*' launch needs more blocks\"));\n }\n \n }  // namespace"
                }
            ],
            "whole_deleted": "-      ::testing::ContainsRegex(\"Kernel 'fusion.*' launch needs more blocks\"));\n",
            "whole_added": "+      ::testing::ContainsRegex(\"Kernel '.*fusion.*' launch needs more blocks\"));\n",
            "whole_hunk": "@@ -55,7 +55,7 @@ ENTRY primitive_computation_mul.8 {\n   EXPECT_FALSE(failed_executable.ok());\n   EXPECT_THAT(\n       failed_executable.status().ToString(),\n-      ::testing::ContainsRegex(\"Kernel 'fusion.*' launch needs more blocks\"));\n+      ::testing::ContainsRegex(\"Kernel '.*fusion.*' launch needs more blocks\"));\n }\n \n }  // namespace"
        }
    ]
},
{
    "Id": 544,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9910b18b4bc3066352ec02b227d450c4f7749c6b",
    "date": "2023-02-08T05:15:42+00:00",
    "message": "stop kernel execution of validation fails",
    "label": "YES",
    "changes": [
        {
            "name": "lstm_ops.cc",
            "path": "tensorflow/core/kernels/rnn/lstm_ops.cc",
            "patches": [
                {
                    "old_start": 475,
                    "old_length": 7,
                    "new_start": 475,
                    "new_length": 7,
                    "hunk": "@@ -475,7 +475,7 @@ class LSTMBlockCellOp : public OpKernel {\n                 errors::InvalidArgument(\"h_tensor must be rank 2 but is rank \",\n                                         h_tensor->dims(), \".\"));\n \n-    functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n+    OP_REQUIRES_OK(ctx, functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n         batch_size, input_size, cell_size)(\n         ctx, device, forget_bias_, cell_clip_, use_peephole_,\n         x_tensor->matrix<T>(), cs_prev_tensor->matrix<T>(),\n"
                },
                {
                    "old_start": 484,
                    "old_length": 7,
                    "new_start": 484,
                    "new_length": 7,
                    "hunk": "@@ -484,7 +484,7 @@ class LSTMBlockCellOp : public OpKernel {\n         xh_tensor.matrix<T>(), i_tensor->matrix<T>(), cs_tensor->matrix<T>(),\n         f_tensor->matrix<T>(), o_tensor->matrix<T>(), ci_tensor->matrix<T>(),\n         co_tensor->matrix<T>(), gates_tensor.matrix<T>(),\n-        h_tensor->matrix<T>());\n+        h_tensor->matrix<T>()));\n   }\n \n  private:\n"
                },
                {
                    "old_start": 1046,
                    "old_length": 7,
                    "new_start": 1046,
                    "new_length": 7,
                    "hunk": "@@ -1046,7 +1046,7 @@ class BlockLSTMOp : public OpKernel {\n       Tensor co_tensor = slicer.OutputSlice(co_out, t, \"co_out\");\n       Tensor h_tensor = slicer.OutputSlice(h_out, t, \"h_out\");\n \n-      functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n+      OP_REQUIRES_OK(ctx, functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n           batch_size, input_size, cell_size)(\n           ctx, device, forget_bias_, cell_clip_, use_peephole_,\n           x_tensor.matrix<T>(), cs_prev_tensor2.matrix<T>(),\n"
                },
                {
                    "old_start": 1055,
                    "old_length": 7,
                    "new_start": 1055,
                    "new_length": 7,
                    "hunk": "@@ -1055,7 +1055,7 @@ class BlockLSTMOp : public OpKernel {\n           b_tensor->vec<T>(), xh_tensor.matrix<T>(), i_tensor.matrix<T>(),\n           cs_tensor.matrix<T>(), f_tensor.matrix<T>(), o_tensor.matrix<T>(),\n           ci_tensor.matrix<T>(), co_tensor.matrix<T>(),\n-          gates_tensor.matrix<T>(), h_tensor.matrix<T>());\n+          gates_tensor.matrix<T>(), h_tensor.matrix<T>()));\n       slicer.FinishTimeStep();\n     }\n "
                }
            ],
            "whole_deleted": "-    functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n-        h_tensor->matrix<T>());\n-      functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n-          gates_tensor.matrix<T>(), h_tensor.matrix<T>());\n",
            "whole_added": "+    OP_REQUIRES_OK(ctx, functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n+        h_tensor->matrix<T>()));\n+      OP_REQUIRES_OK(ctx, functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n+          gates_tensor.matrix<T>(), h_tensor.matrix<T>()));\n",
            "whole_hunk": "@@ -475,7 +475,7 @@ class LSTMBlockCellOp : public OpKernel {\n                 errors::InvalidArgument(\"h_tensor must be rank 2 but is rank \",\n                                         h_tensor->dims(), \".\"));\n \n-    functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n+    OP_REQUIRES_OK(ctx, functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n         batch_size, input_size, cell_size)(\n         ctx, device, forget_bias_, cell_clip_, use_peephole_,\n         x_tensor->matrix<T>(), cs_prev_tensor->matrix<T>(),\n@@ -484,7 +484,7 @@ class LSTMBlockCellOp : public OpKernel {\n         xh_tensor.matrix<T>(), i_tensor->matrix<T>(), cs_tensor->matrix<T>(),\n         f_tensor->matrix<T>(), o_tensor->matrix<T>(), ci_tensor->matrix<T>(),\n         co_tensor->matrix<T>(), gates_tensor.matrix<T>(),\n-        h_tensor->matrix<T>());\n+        h_tensor->matrix<T>()));\n   }\n \n  private:\n@@ -1046,7 +1046,7 @@ class BlockLSTMOp : public OpKernel {\n       Tensor co_tensor = slicer.OutputSlice(co_out, t, \"co_out\");\n       Tensor h_tensor = slicer.OutputSlice(h_out, t, \"h_out\");\n \n-      functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n+      OP_REQUIRES_OK(ctx, functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n           batch_size, input_size, cell_size)(\n           ctx, device, forget_bias_, cell_clip_, use_peephole_,\n           x_tensor.matrix<T>(), cs_prev_tensor2.matrix<T>(),\n@@ -1055,7 +1055,7 @@ class BlockLSTMOp : public OpKernel {\n           b_tensor->vec<T>(), xh_tensor.matrix<T>(), i_tensor.matrix<T>(),\n           cs_tensor.matrix<T>(), f_tensor.matrix<T>(), o_tensor.matrix<T>(),\n           ci_tensor.matrix<T>(), co_tensor.matrix<T>(),\n-          gates_tensor.matrix<T>(), h_tensor.matrix<T>());\n+          gates_tensor.matrix<T>(), h_tensor.matrix<T>()));\n       slicer.FinishTimeStep();\n     }\n "
        }
    ]
},
{
    "Id": 44,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/07e48f1c97963b9918e9ae6aaba79143226162de",
    "date": "2024-05-17T00:18:09-07:00",
    "message": "Fix scatter bounds checks for unsigned indices.\n\nI accidentally used sle where we need ule, resulting in incorrect\nresults for values >= 2**31 / 2**63.\n\nPiperOrigin-RevId: 634651640",
    "label": "YES",
    "changes": [
        {
            "name": "scatter_mlir.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/scatter_mlir.cc",
            "patches": [
                {
                    "old_start": 237,
                    "old_length": 17,
                    "new_start": 237,
                    "new_length": 15,
                    "hunk": "@@ -237,17 +237,15 @@ absl::Status MlirScatterFusion::EmitEntryFunction(\n             index = b.create<ma::IndexCastUIOp>(b.getIndexType(), index);\n           } else {\n             index = b.create<ma::IndexCastOp>(b.getIndexType(), index);\n-            auto c0 = b.create<ma::ConstantIndexOp>(0);\n-            in_bounds = b.create<ma::AndIOp>(\n-                in_bounds,\n-                b.create<ma::CmpIOp>(ma::CmpIPredicate::sge, index, c0));\n           }\n           Value ub = b.create<ma::ConstantIndexOp>(\n               scatter_operand->shape().dimensions(i) -\n               scatter_update->shape().dimensions(i + 1));\n+          // One bounds check is enough even for signed indices: `sge 0` is\n+          // implied by `ule ub`, because `ub >= 0`.\n           in_bounds = b.create<ma::AndIOp>(\n               in_bounds,\n-              b.create<ma::CmpIOp>(ma::CmpIPredicate::sle, index, ub));\n+              b.create<ma::CmpIOp>(ma::CmpIPredicate::ule, index, ub));\n           indices[i] = b.create<ma::AddIOp>(index, indices[i]);\n         }\n         // Call scatter's computation if is_in_bounds.\n"
                }
            ],
            "whole_deleted": "-            auto c0 = b.create<ma::ConstantIndexOp>(0);\n-            in_bounds = b.create<ma::AndIOp>(\n-                in_bounds,\n-                b.create<ma::CmpIOp>(ma::CmpIPredicate::sge, index, c0));\n-              b.create<ma::CmpIOp>(ma::CmpIPredicate::sle, index, ub));\n",
            "whole_added": "+          // One bounds check is enough even for signed indices: `sge 0` is\n+          // implied by `ule ub`, because `ub >= 0`.\n+              b.create<ma::CmpIOp>(ma::CmpIPredicate::ule, index, ub));\n",
            "whole_hunk": "@@ -237,17 +237,15 @@ absl::Status MlirScatterFusion::EmitEntryFunction(\n             index = b.create<ma::IndexCastUIOp>(b.getIndexType(), index);\n           } else {\n             index = b.create<ma::IndexCastOp>(b.getIndexType(), index);\n-            auto c0 = b.create<ma::ConstantIndexOp>(0);\n-            in_bounds = b.create<ma::AndIOp>(\n-                in_bounds,\n-                b.create<ma::CmpIOp>(ma::CmpIPredicate::sge, index, c0));\n           }\n           Value ub = b.create<ma::ConstantIndexOp>(\n               scatter_operand->shape().dimensions(i) -\n               scatter_update->shape().dimensions(i + 1));\n+          // One bounds check is enough even for signed indices: `sge 0` is\n+          // implied by `ule ub`, because `ub >= 0`.\n           in_bounds = b.create<ma::AndIOp>(\n               in_bounds,\n-              b.create<ma::CmpIOp>(ma::CmpIPredicate::sle, index, ub));\n+              b.create<ma::CmpIOp>(ma::CmpIPredicate::ule, index, ub));\n           indices[i] = b.create<ma::AddIOp>(index, indices[i]);\n         }\n         // Call scatter's computation if is_in_bounds.\n"
        },
        {
            "name": "scatter_mlir_test.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/scatter_mlir_test.cc",
            "patches": [
                {
                    "old_start": 207,
                    "old_length": 7,
                    "new_start": 207,
                    "new_length": 7,
                    "hunk": "@@ -207,7 +207,7 @@ TEST_F(MlirScatterFusionTest, Scatter_UniqueIndices) {\n     // CHECK:      xla_gpu.pure_call @scatter_indices(%[[OPERAND]], %[[INDICES]]\n     // CHECK-SAME:  %[[UPDATES]], %[[SLICE_ID]], %[[C0]])\n \n-    // CHECK:      %[[IN_BOUNDS:.*]] = arith.andi\n+    // CHECK:      %[[IN_BOUNDS:.*]] = arith.cmpi ule\n     // CHECK:      scf.if %[[IN_BOUNDS]] -> (tensor<10x5xf32>) {\n     // CHECK:        %[[CURRENT:.*]] = xla_gpu.pure_call @scatter_operand(\n     // CHECK-SAME:  %[[OPERAND]], %[[INDICES]], %[[UPDATES]],\n"
                },
                {
                    "old_start": 255,
                    "old_length": 6,
                    "new_start": 255,
                    "new_length": 10,
                    "hunk": "@@ -255,6 +255,10 @@ TEST_F(MlirScatterFusionTest, Scatter_Unsigned) {\n   )\";\n   TF_ASSERT_OK(EmitAndCheckIR(kHloString, R\"(\n     // CHECK: func.func @fused_computation(\n+    // CHECK: %[[PARAM:.*]] = xla_gpu.pure_call @scatter_indices\n+    // CHECK: %[[CVT:.*]] = builtin.unrealized_conversion_cast %[[PARAM]]\n+    // CHECK: %[[INDEX:.*]] = arith.index_castui %[[CVT]]\n+    // CHECK: arith.cmpi ule, %[[INDEX]]\n   )\"));\n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloString, ErrorSpec{1e-3}));\n }\n"
                },
                {
                    "old_start": 302,
                    "old_length": 7,
                    "new_start": 306,
                    "new_length": 7,
                    "hunk": "@@ -302,7 +306,7 @@ TEST_F(MlirScatterFusionTest, Scatter_Add) {\n     // CHECK-SAME:    %[[OUT:[a-zA-Z0-9]*]]: tensor<10x5xf32>\n \n     // CHECK: %[[UPD_ELEM:.*]] = xla_gpu.pure_call @scatter_update\n-    // CHECK: %[[IN_BOUNDS:.*]] = arith.andi\n+    // CHECK: %[[IN_BOUNDS:.*]] = arith.cmpi ule\n     // CHECK: scf.if %[[IN_BOUNDS]] -> (tensor<10x5xf32>) {\n     // CHECK:   %[[RMW:.*]] = xla_gpu.atomic_rmw %[[OUT]]\n     // CHECK:   ^bb0(%[[CUR_VALUE:.*]]: f32):\n"
                },
                {
                    "old_start": 359,
                    "old_length": 7,
                    "new_start": 363,
                    "new_length": 7,
                    "hunk": "@@ -359,7 +363,7 @@ TEST_F(MlirScatterFusionTest, Scatter_Overwrite) {\n     // CHECK-SAME:    %[[OUT:[a-zA-Z0-9]*]]: tensor<10x5xf32>\n \n     // CHECK: %[[UPD_ELEM:.*]] = xla_gpu.pure_call @scatter_update\n-    // CHECK: %[[IN_BOUNDS:.*]] = arith.andi\n+    // CHECK: %[[IN_BOUNDS:.*]] = arith.cmpi ule\n     // CHECK: scf.if %[[IN_BOUNDS]] -> (tensor<10x5xf32>) {\n     // CHECK:   %[[RMW:.*]] = xla_gpu.atomic_rmw %[[OUT]]\n     // CHECK:   ^bb0(%[[CUR_VALUE:.*]]: f32):"
                }
            ],
            "whole_deleted": "-    // CHECK:      %[[IN_BOUNDS:.*]] = arith.andi\n-    // CHECK: %[[IN_BOUNDS:.*]] = arith.andi\n-    // CHECK: %[[IN_BOUNDS:.*]] = arith.andi\n",
            "whole_added": "+    // CHECK:      %[[IN_BOUNDS:.*]] = arith.cmpi ule\n+    // CHECK: %[[PARAM:.*]] = xla_gpu.pure_call @scatter_indices\n+    // CHECK: %[[CVT:.*]] = builtin.unrealized_conversion_cast %[[PARAM]]\n+    // CHECK: %[[INDEX:.*]] = arith.index_castui %[[CVT]]\n+    // CHECK: arith.cmpi ule, %[[INDEX]]\n+    // CHECK: %[[IN_BOUNDS:.*]] = arith.cmpi ule\n+    // CHECK: %[[IN_BOUNDS:.*]] = arith.cmpi ule\n",
            "whole_hunk": "@@ -207,7 +207,7 @@ TEST_F(MlirScatterFusionTest, Scatter_UniqueIndices) {\n     // CHECK:      xla_gpu.pure_call @scatter_indices(%[[OPERAND]], %[[INDICES]]\n     // CHECK-SAME:  %[[UPDATES]], %[[SLICE_ID]], %[[C0]])\n \n-    // CHECK:      %[[IN_BOUNDS:.*]] = arith.andi\n+    // CHECK:      %[[IN_BOUNDS:.*]] = arith.cmpi ule\n     // CHECK:      scf.if %[[IN_BOUNDS]] -> (tensor<10x5xf32>) {\n     // CHECK:        %[[CURRENT:.*]] = xla_gpu.pure_call @scatter_operand(\n     // CHECK-SAME:  %[[OPERAND]], %[[INDICES]], %[[UPDATES]],\n@@ -255,6 +255,10 @@ TEST_F(MlirScatterFusionTest, Scatter_Unsigned) {\n   )\";\n   TF_ASSERT_OK(EmitAndCheckIR(kHloString, R\"(\n     // CHECK: func.func @fused_computation(\n+    // CHECK: %[[PARAM:.*]] = xla_gpu.pure_call @scatter_indices\n+    // CHECK: %[[CVT:.*]] = builtin.unrealized_conversion_cast %[[PARAM]]\n+    // CHECK: %[[INDEX:.*]] = arith.index_castui %[[CVT]]\n+    // CHECK: arith.cmpi ule, %[[INDEX]]\n   )\"));\n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloString, ErrorSpec{1e-3}));\n }\n@@ -302,7 +306,7 @@ TEST_F(MlirScatterFusionTest, Scatter_Add) {\n     // CHECK-SAME:    %[[OUT:[a-zA-Z0-9]*]]: tensor<10x5xf32>\n \n     // CHECK: %[[UPD_ELEM:.*]] = xla_gpu.pure_call @scatter_update\n-    // CHECK: %[[IN_BOUNDS:.*]] = arith.andi\n+    // CHECK: %[[IN_BOUNDS:.*]] = arith.cmpi ule\n     // CHECK: scf.if %[[IN_BOUNDS]] -> (tensor<10x5xf32>) {\n     // CHECK:   %[[RMW:.*]] = xla_gpu.atomic_rmw %[[OUT]]\n     // CHECK:   ^bb0(%[[CUR_VALUE:.*]]: f32):\n@@ -359,7 +363,7 @@ TEST_F(MlirScatterFusionTest, Scatter_Overwrite) {\n     // CHECK-SAME:    %[[OUT:[a-zA-Z0-9]*]]: tensor<10x5xf32>\n \n     // CHECK: %[[UPD_ELEM:.*]] = xla_gpu.pure_call @scatter_update\n-    // CHECK: %[[IN_BOUNDS:.*]] = arith.andi\n+    // CHECK: %[[IN_BOUNDS:.*]] = arith.cmpi ule\n     // CHECK: scf.if %[[IN_BOUNDS]] -> (tensor<10x5xf32>) {\n     // CHECK:   %[[RMW:.*]] = xla_gpu.atomic_rmw %[[OUT]]\n     // CHECK:   ^bb0(%[[CUR_VALUE:.*]]: f32):"
        }
    ]
},
{
    "Id": 290,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/72df60c5451ccec269d071cbff30038abb4e7d0b",
    "date": "2023-09-14T14:13:18-07:00",
    "message": "Change 4/6 for making MSA repacking slice aware.\n\nSplit FindChunkCandidates() into 3 methods:\n- GetMaxColocationSize\n- CreateSlicedAllocationFinder\n- PostProcessFindChunkCandidatesResult\n\nWe do this so that slice-aware repacking can make use of the individual methods. In particular, when more than 1 sliced allocation is colocated, we need to create a sliced allocation finder for each so that we can check sliced fits individually.\n\nPiperOrigin-RevId: 565471831",
    "label": "NO",
    "changes": [
        {
            "name": "heap_simulator.cc",
            "path": "third_party/xla/xla/service/heap_simulator.cc",
            "patches": [
                {
                    "old_start": 1715,
                    "old_length": 44,
                    "new_start": 1715,
                    "new_length": 70,
                    "hunk": "@@ -1715,44 +1715,70 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::FindChunkCandidates(\n   VLOG(1) << \"Finding chunks for sliced buffer interval: \"\n           << sliced_buffer_interval.ToString();\n \n-  // Find the max size of interval across its colocations and use this value\n-  // to determine whether the buffer will fit in the heap.\n   int64_t max_colocation_size =\n-      sliced_buffer_interval.full_buffer_interval().size;\n-  for (const BufferType* colocation : GetTransitiveColocations(\n-           sliced_buffer_interval.full_buffer_interval())) {\n+      GetMaxColocationSize(sliced_buffer_interval.full_buffer_interval());\n+  auto chunks =\n+      CreateSlicedAllocationFinder(sliced_buffer_interval, max_colocation_size,\n+                                   preferred_offset)\n+          .Find();\n+  return PostProcessFindChunkCandidatesResult(sliced_buffer_interval,\n+                                              std::move(chunks));\n+}\n+\n+template <typename BufferType>\n+int64_t GlobalDecreasingSizeBestFitHeap<BufferType>::GetMaxColocationSize(\n+    const BufferInterval& buffer_interval) const {\n+  int64_t max_colocation_size = buffer_interval.size;\n+  for (const BufferType* colocation :\n+       GetTransitiveColocations(buffer_interval)) {\n     max_colocation_size =\n         std::max(max_colocation_size, buffer_intervals_.at(colocation).size);\n   }\n \n+  return max_colocation_size;\n+}\n+\n+template <typename BufferType>\n+typename GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder\n+GlobalDecreasingSizeBestFitHeap<BufferType>::CreateSlicedAllocationFinder(\n+    const SlicedBufferInterval& sliced_interval, int64_t max_colocation_size,\n+    int64_t preferred_offset,\n+    absl::AnyInvocable<bool(int64_t) const> is_offset_allowed) const {\n   // Build up a list of free chunks for each slice time.\n   std::vector<FreeChunks> free_chunks_per_slice_time;\n-  free_chunks_per_slice_time.reserve(sliced_buffer_interval.num_slices());\n-  for (int slice_time = 0; slice_time < sliced_buffer_interval.num_slices() - 1;\n+  free_chunks_per_slice_time.reserve(sliced_interval.num_slices());\n+  for (int slice_time = 0; slice_time < sliced_interval.num_slices() - 1;\n        ++slice_time) {\n     // We don't need to account for colocation until the last slice time, in\n     // which we've allocated all the slices. So we set max_colocation_size to\n     // -1.\n-    free_chunks_per_slice_time.push_back(MakeFreeChunks(\n-        sliced_buffer_interval.IntervalForMakeFreeChunks(slice_time),\n-        /*max_colocation_size=*/-1));\n+    free_chunks_per_slice_time.push_back(\n+        MakeFreeChunks(sliced_interval.IntervalForMakeFreeChunks(slice_time),\n+                       /*max_colocation_size=*/-1));\n   }\n   // We account for colocation size in the last slice time, where we've\n   // allocated all the slices.\n-  free_chunks_per_slice_time.push_back(\n-      MakeFreeChunks(sliced_buffer_interval.IntervalForMakeFreeChunks(\n-                         sliced_buffer_interval.num_slices() - 1),\n-                     max_colocation_size));\n+  free_chunks_per_slice_time.push_back(MakeFreeChunks(\n+      sliced_interval.IntervalForMakeFreeChunks(sliced_interval.num_slices() -\n+                                                1),\n+      max_colocation_size));\n \n-  auto chunks =\n-      SlicedAllocationFinder(free_chunks_per_slice_time,\n-                             sliced_buffer_interval.SliceSizesSortedByOffset(),\n-                             max_colocation_size, preferred_offset, alignment_)\n-          .Find();\n+  return SlicedAllocationFinder(free_chunks_per_slice_time,\n+                                sliced_interval.SliceSizesSortedByOffset(),\n+                                max_colocation_size, preferred_offset,\n+                                alignment_, std::move(is_offset_allowed));\n+}\n+\n+template <typename BufferType>\n+std::vector<typename GlobalDecreasingSizeBestFitHeap<BufferType>::Chunk>\n+GlobalDecreasingSizeBestFitHeap<BufferType>::\n+    PostProcessFindChunkCandidatesResult(\n+        const SlicedBufferInterval& sliced_interval,\n+        std::vector<Chunk> chunks) const {\n   if (chunks.empty()) {\n     return {};\n   }\n-  CHECK_EQ(chunks.size(), sliced_buffer_interval.num_slices() + 1);\n+  CHECK_EQ(chunks.size(), sliced_interval.num_slices() + 1);\n   // The extra chunk is to ensure that colocations of larger sizes can fit.\n   // However, we don't need that extra space for the buffer for which we found\n   // chunks.\n"
                }
            ],
            "whole_deleted": "-  // Find the max size of interval across its colocations and use this value\n-  // to determine whether the buffer will fit in the heap.\n-      sliced_buffer_interval.full_buffer_interval().size;\n-  for (const BufferType* colocation : GetTransitiveColocations(\n-           sliced_buffer_interval.full_buffer_interval())) {\n-  free_chunks_per_slice_time.reserve(sliced_buffer_interval.num_slices());\n-  for (int slice_time = 0; slice_time < sliced_buffer_interval.num_slices() - 1;\n-    free_chunks_per_slice_time.push_back(MakeFreeChunks(\n-        sliced_buffer_interval.IntervalForMakeFreeChunks(slice_time),\n-        /*max_colocation_size=*/-1));\n-  free_chunks_per_slice_time.push_back(\n-      MakeFreeChunks(sliced_buffer_interval.IntervalForMakeFreeChunks(\n-                         sliced_buffer_interval.num_slices() - 1),\n-                     max_colocation_size));\n-  auto chunks =\n-      SlicedAllocationFinder(free_chunks_per_slice_time,\n-                             sliced_buffer_interval.SliceSizesSortedByOffset(),\n-                             max_colocation_size, preferred_offset, alignment_)\n-          .Find();\n-  CHECK_EQ(chunks.size(), sliced_buffer_interval.num_slices() + 1);\n",
            "whole_added": "+      GetMaxColocationSize(sliced_buffer_interval.full_buffer_interval());\n+  auto chunks =\n+      CreateSlicedAllocationFinder(sliced_buffer_interval, max_colocation_size,\n+                                   preferred_offset)\n+          .Find();\n+  return PostProcessFindChunkCandidatesResult(sliced_buffer_interval,\n+                                              std::move(chunks));\n+}\n+\n+template <typename BufferType>\n+int64_t GlobalDecreasingSizeBestFitHeap<BufferType>::GetMaxColocationSize(\n+    const BufferInterval& buffer_interval) const {\n+  int64_t max_colocation_size = buffer_interval.size;\n+  for (const BufferType* colocation :\n+       GetTransitiveColocations(buffer_interval)) {\n+  return max_colocation_size;\n+}\n+\n+template <typename BufferType>\n+typename GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder\n+GlobalDecreasingSizeBestFitHeap<BufferType>::CreateSlicedAllocationFinder(\n+    const SlicedBufferInterval& sliced_interval, int64_t max_colocation_size,\n+    int64_t preferred_offset,\n+    absl::AnyInvocable<bool(int64_t) const> is_offset_allowed) const {\n+  free_chunks_per_slice_time.reserve(sliced_interval.num_slices());\n+  for (int slice_time = 0; slice_time < sliced_interval.num_slices() - 1;\n+    free_chunks_per_slice_time.push_back(\n+        MakeFreeChunks(sliced_interval.IntervalForMakeFreeChunks(slice_time),\n+                       /*max_colocation_size=*/-1));\n+  free_chunks_per_slice_time.push_back(MakeFreeChunks(\n+      sliced_interval.IntervalForMakeFreeChunks(sliced_interval.num_slices() -\n+                                                1),\n+      max_colocation_size));\n+  return SlicedAllocationFinder(free_chunks_per_slice_time,\n+                                sliced_interval.SliceSizesSortedByOffset(),\n+                                max_colocation_size, preferred_offset,\n+                                alignment_, std::move(is_offset_allowed));\n+}\n+\n+template <typename BufferType>\n+std::vector<typename GlobalDecreasingSizeBestFitHeap<BufferType>::Chunk>\n+GlobalDecreasingSizeBestFitHeap<BufferType>::\n+    PostProcessFindChunkCandidatesResult(\n+        const SlicedBufferInterval& sliced_interval,\n+        std::vector<Chunk> chunks) const {\n+  CHECK_EQ(chunks.size(), sliced_interval.num_slices() + 1);\n",
            "whole_hunk": "@@ -1715,44 +1715,70 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::FindChunkCandidates(\n   VLOG(1) << \"Finding chunks for sliced buffer interval: \"\n           << sliced_buffer_interval.ToString();\n \n-  // Find the max size of interval across its colocations and use this value\n-  // to determine whether the buffer will fit in the heap.\n   int64_t max_colocation_size =\n-      sliced_buffer_interval.full_buffer_interval().size;\n-  for (const BufferType* colocation : GetTransitiveColocations(\n-           sliced_buffer_interval.full_buffer_interval())) {\n+      GetMaxColocationSize(sliced_buffer_interval.full_buffer_interval());\n+  auto chunks =\n+      CreateSlicedAllocationFinder(sliced_buffer_interval, max_colocation_size,\n+                                   preferred_offset)\n+          .Find();\n+  return PostProcessFindChunkCandidatesResult(sliced_buffer_interval,\n+                                              std::move(chunks));\n+}\n+\n+template <typename BufferType>\n+int64_t GlobalDecreasingSizeBestFitHeap<BufferType>::GetMaxColocationSize(\n+    const BufferInterval& buffer_interval) const {\n+  int64_t max_colocation_size = buffer_interval.size;\n+  for (const BufferType* colocation :\n+       GetTransitiveColocations(buffer_interval)) {\n     max_colocation_size =\n         std::max(max_colocation_size, buffer_intervals_.at(colocation).size);\n   }\n \n+  return max_colocation_size;\n+}\n+\n+template <typename BufferType>\n+typename GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder\n+GlobalDecreasingSizeBestFitHeap<BufferType>::CreateSlicedAllocationFinder(\n+    const SlicedBufferInterval& sliced_interval, int64_t max_colocation_size,\n+    int64_t preferred_offset,\n+    absl::AnyInvocable<bool(int64_t) const> is_offset_allowed) const {\n   // Build up a list of free chunks for each slice time.\n   std::vector<FreeChunks> free_chunks_per_slice_time;\n-  free_chunks_per_slice_time.reserve(sliced_buffer_interval.num_slices());\n-  for (int slice_time = 0; slice_time < sliced_buffer_interval.num_slices() - 1;\n+  free_chunks_per_slice_time.reserve(sliced_interval.num_slices());\n+  for (int slice_time = 0; slice_time < sliced_interval.num_slices() - 1;\n        ++slice_time) {\n     // We don't need to account for colocation until the last slice time, in\n     // which we've allocated all the slices. So we set max_colocation_size to\n     // -1.\n-    free_chunks_per_slice_time.push_back(MakeFreeChunks(\n-        sliced_buffer_interval.IntervalForMakeFreeChunks(slice_time),\n-        /*max_colocation_size=*/-1));\n+    free_chunks_per_slice_time.push_back(\n+        MakeFreeChunks(sliced_interval.IntervalForMakeFreeChunks(slice_time),\n+                       /*max_colocation_size=*/-1));\n   }\n   // We account for colocation size in the last slice time, where we've\n   // allocated all the slices.\n-  free_chunks_per_slice_time.push_back(\n-      MakeFreeChunks(sliced_buffer_interval.IntervalForMakeFreeChunks(\n-                         sliced_buffer_interval.num_slices() - 1),\n-                     max_colocation_size));\n+  free_chunks_per_slice_time.push_back(MakeFreeChunks(\n+      sliced_interval.IntervalForMakeFreeChunks(sliced_interval.num_slices() -\n+                                                1),\n+      max_colocation_size));\n \n-  auto chunks =\n-      SlicedAllocationFinder(free_chunks_per_slice_time,\n-                             sliced_buffer_interval.SliceSizesSortedByOffset(),\n-                             max_colocation_size, preferred_offset, alignment_)\n-          .Find();\n+  return SlicedAllocationFinder(free_chunks_per_slice_time,\n+                                sliced_interval.SliceSizesSortedByOffset(),\n+                                max_colocation_size, preferred_offset,\n+                                alignment_, std::move(is_offset_allowed));\n+}\n+\n+template <typename BufferType>\n+std::vector<typename GlobalDecreasingSizeBestFitHeap<BufferType>::Chunk>\n+GlobalDecreasingSizeBestFitHeap<BufferType>::\n+    PostProcessFindChunkCandidatesResult(\n+        const SlicedBufferInterval& sliced_interval,\n+        std::vector<Chunk> chunks) const {\n   if (chunks.empty()) {\n     return {};\n   }\n-  CHECK_EQ(chunks.size(), sliced_buffer_interval.num_slices() + 1);\n+  CHECK_EQ(chunks.size(), sliced_interval.num_slices() + 1);\n   // The extra chunk is to ensure that colocations of larger sizes can fit.\n   // However, we don't need that extra space for the buffer for which we found\n   // chunks.\n"
        },
        {
            "name": "heap_simulator.h",
            "path": "third_party/xla/xla/service/heap_simulator.h",
            "patches": [
                {
                    "old_start": 781,
                    "old_length": 6,
                    "new_start": 781,
                    "new_length": 17,
                    "hunk": "@@ -781,6 +781,17 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n   std::vector<Chunk> FindChunkCandidates(\n       const SlicedBufferInterval& sliced_buffer_interval,\n       int64_t preferred_offset = -1) const;\n+  // The following 3 methods are used to implement FindChunkCandidates.\n+  int64_t GetMaxColocationSize(const BufferInterval& buffer_interval) const;\n+  SlicedAllocationFinder CreateSlicedAllocationFinder(\n+      const SlicedBufferInterval& sliced_interval, int64_t max_colocation_size,\n+      int64_t preferred_offset,\n+      absl::AnyInvocable<bool(int64_t) const> is_offset_allowed =\n+          &SlicedAllocationFinder::AllOffsetsAllowed) const;\n+  std::vector<Chunk> PostProcessFindChunkCandidatesResult(\n+      const SlicedBufferInterval& sliced_interval,\n+      std::vector<Chunk> chunks) const;\n+\n   void CommitChunk(const BufferInterval& buffer_interval, Chunk chunk);\n \n   // Adds the buffer and the chunk to the result chunk map."
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // The following 3 methods are used to implement FindChunkCandidates.\n+  int64_t GetMaxColocationSize(const BufferInterval& buffer_interval) const;\n+  SlicedAllocationFinder CreateSlicedAllocationFinder(\n+      const SlicedBufferInterval& sliced_interval, int64_t max_colocation_size,\n+      int64_t preferred_offset,\n+      absl::AnyInvocable<bool(int64_t) const> is_offset_allowed =\n+          &SlicedAllocationFinder::AllOffsetsAllowed) const;\n+  std::vector<Chunk> PostProcessFindChunkCandidatesResult(\n+      const SlicedBufferInterval& sliced_interval,\n+      std::vector<Chunk> chunks) const;\n+\n",
            "whole_hunk": "@@ -781,6 +781,17 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n   std::vector<Chunk> FindChunkCandidates(\n       const SlicedBufferInterval& sliced_buffer_interval,\n       int64_t preferred_offset = -1) const;\n+  // The following 3 methods are used to implement FindChunkCandidates.\n+  int64_t GetMaxColocationSize(const BufferInterval& buffer_interval) const;\n+  SlicedAllocationFinder CreateSlicedAllocationFinder(\n+      const SlicedBufferInterval& sliced_interval, int64_t max_colocation_size,\n+      int64_t preferred_offset,\n+      absl::AnyInvocable<bool(int64_t) const> is_offset_allowed =\n+          &SlicedAllocationFinder::AllOffsetsAllowed) const;\n+  std::vector<Chunk> PostProcessFindChunkCandidatesResult(\n+      const SlicedBufferInterval& sliced_interval,\n+      std::vector<Chunk> chunks) const;\n+\n   void CommitChunk(const BufferInterval& buffer_interval, Chunk chunk);\n \n   // Adds the buffer and the chunk to the result chunk map."
        }
    ]
},
{
    "Id": 70,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/55cce91af005a2d2ab3a0f9c7546a86be3ee3187",
    "date": "2024-04-19T02:38:33-07:00",
    "message": "[XLA:GPU] Fix a bug in ProvideParameters\n\nIt did not handle the case when the subgraph has more than one root.\n\nPiperOrigin-RevId: 626298839",
    "label": "NO",
    "changes": [
        {
            "name": "concatenate_mlir.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/concatenate_mlir.cc",
            "patches": [
                {
                    "old_start": 120,
                    "old_length": 9,
                    "new_start": 120,
                    "new_length": 10,
                    "hunk": "@@ -120,9 +120,10 @@ absl::Status MlirConcatenateFusion::EmitEntryFunction(\n           mlir_converter::ApplyAffineMap(thread_id_to_input_map.GetAffineMap(),\n                                          dim_values, symbol_values, builder);\n \n-      auto result_scalars = mlir_converter::ProvideParameter(\n-          root_computation.FindSubgraph(concat), concat, operand_index,\n-          input_indices, call_targets, entry_function, builder);\n+      auto result_scalar = mlir_converter::ProvideParameter(\n+          root_computation, concat, operand_index, input_indices, call_targets,\n+          entry_function, builder);\n+      llvm::SmallVector<Value> result_scalars{result_scalar};\n       auto output_indices =\n           mlir_converter::ApplyAffineMap(thread_id_to_output_map.GetAffineMap(),\n                                          dim_values, symbol_values, builder);\n"
                }
            ],
            "whole_deleted": "-      auto result_scalars = mlir_converter::ProvideParameter(\n-          root_computation.FindSubgraph(concat), concat, operand_index,\n-          input_indices, call_targets, entry_function, builder);\n",
            "whole_added": "+      auto result_scalar = mlir_converter::ProvideParameter(\n+          root_computation, concat, operand_index, input_indices, call_targets,\n+          entry_function, builder);\n+      llvm::SmallVector<Value> result_scalars{result_scalar};\n",
            "whole_hunk": "@@ -120,9 +120,10 @@ absl::Status MlirConcatenateFusion::EmitEntryFunction(\n           mlir_converter::ApplyAffineMap(thread_id_to_input_map.GetAffineMap(),\n                                          dim_values, symbol_values, builder);\n \n-      auto result_scalars = mlir_converter::ProvideParameter(\n-          root_computation.FindSubgraph(concat), concat, operand_index,\n-          input_indices, call_targets, entry_function, builder);\n+      auto result_scalar = mlir_converter::ProvideParameter(\n+          root_computation, concat, operand_index, input_indices, call_targets,\n+          entry_function, builder);\n+      llvm::SmallVector<Value> result_scalars{result_scalar};\n       auto output_indices =\n           mlir_converter::ApplyAffineMap(thread_id_to_output_map.GetAffineMap(),\n                                          dim_values, symbol_values, builder);\n"
        },
        {
            "name": "in_place_dynamic_update_slice_mlir.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/in_place_dynamic_update_slice_mlir.cc",
            "patches": [
                {
                    "old_start": 124,
                    "old_length": 8,
                    "new_start": 124,
                    "new_length": 6,
                    "hunk": "@@ -124,8 +124,6 @@ absl::Status MlirInPlaceDynamicUpdateSliceFusion::EmitEntryFunction(\n \n   const auto& root_computation = computations.FindPartitionedComputation(\n       fusion.fused_instructions_computation());\n-  const auto& dus_subgraph = root_computation.FindSubgraph(dus_ops_.front());\n-\n   const auto* dus_instr =\n       Cast<HloDynamicUpdateSliceInstruction>(dus_ops_.front());\n   const auto& update_shape = dus_instr->update()->shape();\n"
                },
                {
                    "old_start": 136,
                    "old_length": 14,
                    "new_start": 134,
                    "new_length": 14,
                    "hunk": "@@ -136,14 +134,14 @@ absl::Status MlirInPlaceDynamicUpdateSliceFusion::EmitEntryFunction(\n         auto input_indices = ApplyAffineMap(indexing.GetAffineMap(), dim_values,\n                                             symbol_values, b);\n         SmallVector<Value> update_indices;\n+        auto start_indices = ProvideParameterRange(\n+            root_computation, dus_instr,\n+            dus_instr->first_index_operand_number(), update_shape.rank(), {},\n+            call_targets, entry_function, b);\n         for (int i = 0; i < update_shape.rank(); ++i) {\n           int64_t update_size = update_shape.dimensions(i);\n-          auto start_index =\n-              ProvideParameter(dus_subgraph, dus_instr,\n-                               i + dus_instr->first_index_operand_number(), {},\n-                               call_targets, entry_function, b)[0];\n-          start_index = ClampIndex(\n-              start_index,\n+          auto start_index = ClampIndex(\n+              start_indices[i],\n               primitive_util::IsUnsignedIntegralType(\n                   dus_instr\n                       ->operand(i + dus_instr->first_index_operand_number())\n"
                },
                {
                    "old_start": 156,
                    "old_length": 8,
                    "new_start": 154,
                    "new_length": 8,
                    "hunk": "@@ -156,8 +154,8 @@ absl::Status MlirInPlaceDynamicUpdateSliceFusion::EmitEntryFunction(\n         }\n \n         auto updated_value =\n-            ProvideParameter(dus_subgraph, dus_instr, kDUSUpdateIndex,\n-                             input_indices, call_targets, entry_function, b)[0];\n+            ProvideParameter(root_computation, dus_instr, kDUSUpdateIndex,\n+                             input_indices, call_targets, entry_function, b);\n         // Handle bitcasts under the DUS.\n         if (dus_instr->shape() != fusion.shape()) {\n           update_indices = ApplyAffineMap(\n"
                }
            ],
            "whole_deleted": "-  const auto& dus_subgraph = root_computation.FindSubgraph(dus_ops_.front());\n-\n-          auto start_index =\n-              ProvideParameter(dus_subgraph, dus_instr,\n-                               i + dus_instr->first_index_operand_number(), {},\n-                               call_targets, entry_function, b)[0];\n-          start_index = ClampIndex(\n-              start_index,\n-            ProvideParameter(dus_subgraph, dus_instr, kDUSUpdateIndex,\n-                             input_indices, call_targets, entry_function, b)[0];\n",
            "whole_added": "+        auto start_indices = ProvideParameterRange(\n+            root_computation, dus_instr,\n+            dus_instr->first_index_operand_number(), update_shape.rank(), {},\n+            call_targets, entry_function, b);\n+          auto start_index = ClampIndex(\n+              start_indices[i],\n+            ProvideParameter(root_computation, dus_instr, kDUSUpdateIndex,\n+                             input_indices, call_targets, entry_function, b);\n",
            "whole_hunk": "@@ -124,8 +124,6 @@ absl::Status MlirInPlaceDynamicUpdateSliceFusion::EmitEntryFunction(\n \n   const auto& root_computation = computations.FindPartitionedComputation(\n       fusion.fused_instructions_computation());\n-  const auto& dus_subgraph = root_computation.FindSubgraph(dus_ops_.front());\n-\n   const auto* dus_instr =\n       Cast<HloDynamicUpdateSliceInstruction>(dus_ops_.front());\n   const auto& update_shape = dus_instr->update()->shape();\n@@ -136,14 +134,14 @@ absl::Status MlirInPlaceDynamicUpdateSliceFusion::EmitEntryFunction(\n         auto input_indices = ApplyAffineMap(indexing.GetAffineMap(), dim_values,\n                                             symbol_values, b);\n         SmallVector<Value> update_indices;\n+        auto start_indices = ProvideParameterRange(\n+            root_computation, dus_instr,\n+            dus_instr->first_index_operand_number(), update_shape.rank(), {},\n+            call_targets, entry_function, b);\n         for (int i = 0; i < update_shape.rank(); ++i) {\n           int64_t update_size = update_shape.dimensions(i);\n-          auto start_index =\n-              ProvideParameter(dus_subgraph, dus_instr,\n-                               i + dus_instr->first_index_operand_number(), {},\n-                               call_targets, entry_function, b)[0];\n-          start_index = ClampIndex(\n-              start_index,\n+          auto start_index = ClampIndex(\n+              start_indices[i],\n               primitive_util::IsUnsignedIntegralType(\n                   dus_instr\n                       ->operand(i + dus_instr->first_index_operand_number())\n@@ -156,8 +154,8 @@ absl::Status MlirInPlaceDynamicUpdateSliceFusion::EmitEntryFunction(\n         }\n \n         auto updated_value =\n-            ProvideParameter(dus_subgraph, dus_instr, kDUSUpdateIndex,\n-                             input_indices, call_targets, entry_function, b)[0];\n+            ProvideParameter(root_computation, dus_instr, kDUSUpdateIndex,\n+                             input_indices, call_targets, entry_function, b);\n         // Handle bitcasts under the DUS.\n         if (dus_instr->shape() != fusion.shape()) {\n           update_indices = ApplyAffineMap(\n"
        },
        {
            "name": "in_place_dynamic_update_slice_mlir_test.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/in_place_dynamic_update_slice_mlir_test.cc",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 6,
                    "new_start": 14,
                    "new_length": 8,
                    "hunk": "@@ -14,6 +14,8 @@ limitations under the License.\n ==============================================================================*/\n #include \"xla/service/gpu/fusions/in_place_dynamic_update_slice_mlir.h\"\n \n+#include <optional>\n+\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"xla/error_spec.h\"\n"
                },
                {
                    "old_start": 113,
                    "old_length": 11,
                    "new_start": 115,
                    "new_length": 11,
                    "hunk": "@@ -113,11 +115,11 @@ TEST_F(MlirInPlaceDynamicUpdateSliceFusionTest, SimpleDUS) {\n     // CHECK:       %[[INPUT_INDEX_0:.*]] = affine.apply #[[MAP_1]]()[%[[THREAD_ID]]]\n     // CHECK:       %[[INPUT_INDEX_1:.*]] = affine.apply #[[MAP_2]]()[%[[THREAD_ID]]]\n     // CHECK:       %[[I0:.*]] = xla_gpu.pure_call @fused_computation_i0\n+    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n     // CHECK:       %[[IDX0:.*]] = arith.index_cast %[[I0]]\n     // CHECK:       %[[MIN0:.*]] = arith.minsi %[[IDX0]], %[[C_15]]\n     // CHECK:       %[[MAX0:.*]] = arith.maxsi %[[MIN0]], %[[C_0]]\n     // CHECK:       %[[ADD0:.*]] = arith.addi %[[INPUT_INDEX_0]], %[[MAX0]]\n-    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n     // CHECK:       %[[IDX1:.*]] = arith.index_cast %[[I1]]\n     // CHECK:       %[[MIN1:.*]] = arith.minsi %[[IDX1]], %[[C_24]]\n     // CHECK:       %[[MAX1:.*]] = arith.maxsi %[[MIN1]], %[[C_0]]\n"
                },
                {
                    "old_start": 163,
                    "old_length": 11,
                    "new_start": 165,
                    "new_length": 11,
                    "hunk": "@@ -163,11 +165,11 @@ TEST_F(MlirInPlaceDynamicUpdateSliceFusionTest, OutOfBoundDUS) {\n     // CHECK:       %[[INPUT_INDEX_0:.*]] = affine.apply #[[MAP_1]]()[%[[THREAD_ID]]]\n     // CHECK:       %[[INPUT_INDEX_1:.*]] = affine.apply #[[MAP_2]]()[%[[THREAD_ID]]]\n     // CHECK:       %[[I0:.*]] = xla_gpu.pure_call @fused_computation_i0\n+    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n     // CHECK:       %[[IDX0:.*]] = arith.index_cast %[[I0]]\n     // CHECK:       %[[MIN0:.*]] = arith.minsi %[[IDX0]], %[[C_5]]\n     // CHECK:       %[[MAX0:.*]] = arith.maxsi %[[MIN0]], %[[C_0]]\n     // CHECK:       %[[ADD0:.*]] = arith.addi %[[INPUT_INDEX_0]], %[[MAX0]]\n-    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n     // CHECK:       %[[IDX1:.*]] = arith.index_cast %[[I1]]\n     // CHECK:       %[[MIN1:.*]] = arith.minsi %[[IDX1]], %[[C_5]]\n     // CHECK:       %[[MAX1:.*]] = arith.maxsi %[[MIN1]], %[[C_0]]\n"
                },
                {
                    "old_start": 202,
                    "old_length": 6,
                    "new_start": 204,
                    "new_length": 57,
                    "hunk": "@@ -202,6 +204,57 @@ TEST_F(MlirInPlaceDynamicUpdateSliceFusionTest, BitcastDus) {\n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloString, ErrorSpec{1e-3}));\n }\n \n+TEST_F(MlirInPlaceDynamicUpdateSliceFusionTest, OperandSubgraphWithTwoRoots) {\n+  auto kHloString = R\"(\n+    HloModule in_place_dus\n+\n+    dus_fusion {\n+      param_0.8 = f32[512,512]{1,0} parameter(0)\n+      param_1.10 = f32[128,128]{1,0} parameter(1)\n+      param_3.32 = s32[] parameter(3)\n+      two = s32[] constant(2)\n+      param_3_mod_2 = s32[] remainder(param_3.32, two)\n+      one = s32[] constant(1)\n+      param_3_plus_one = s32[] add(param_3_mod_2, one)\n+      param_2.32 = s32[] parameter(2)\n+      param_2_plus_one = s32[] add(param_2.32, one)\n+      ROOT dynamic-update-slice.5.1 = f32[512,512]{1,0} dynamic-update-slice(param_0.8, param_1.10, param_2_plus_one, param_3_plus_one)\n+    }\n+    ENTRY entry {\n+      p0 = f32[512,512]{1,0} parameter(0)\n+      p1 = f32[128,128]{1,0} parameter(1)\n+      p2 = s32[] parameter(2)\n+      p3 = s32[] parameter(3)\n+      ROOT dus = f32[512,512]{1,0} fusion(p0, p1, p2, p3), kind=kLoop, calls=dus_fusion\n+    }\n+  )\";\n+  TF_ASSERT_OK(EmitAndCheckIR(kHloString, R\"(\n+    // CHECK:     func.func @fused_computation(\n+    // CHECK-SAME:  %[[ARG0:[^:]+]]: tensor<512x512xf32>\n+    // CHECK-SAME:  , %[[ARG1:[^:]+]]: tensor<128x128xf32>\n+    // CHECK-SAME:  , %[[ARG2:[^:]+]]: tensor<i32>\n+    // CHECK-SAME:  , %[[ARG3:[^:]+]]: tensor<i32>\n+    // CHECK-SAME:  , %[[ARG4:[^:]+]]: tensor<512x512xf32>\n+    // CHECK-DAG:   %[[C_384:.*]] = arith.constant 384\n+    // CHECK-DAG:   %[[C_0:.*]] = arith.constant 0\n+    // CHECK:       %[[THREAD_ID:.*]] = gpu.thread_id  x\n+    // CHECK:       %[[BLOCK_ID:.*]] = gpu.block_id  x\n+    // CHECK:       %[[I0:.*]], %[[I1:.*]] = xla_gpu.pure_call @dus_fusion_param_2_plus_one_param_3_plus_one\n+    // CHECK:       %[[IDX0:.*]] = arith.index_cast %[[I0]]\n+    // CHECK:       %[[MIN0:.*]] = arith.minsi %[[IDX0]], %[[C_384]]\n+    // CHECK:       %[[MAX0:.*]] = arith.maxsi %[[MIN0]], %[[C_0]]\n+    // CHECK:       %[[ADD0:.*]] = arith.addi %[[BLOCK_ID]], %[[MAX0]]\n+    // CHECK:       %[[IDX1:.*]] = arith.index_cast %[[I1]]\n+    // CHECK:       %[[MIN1:.*]] = arith.minsi %[[IDX1]], %[[C_384]]\n+    // CHECK:       %[[MAX1:.*]] = arith.maxsi %[[MIN1]], %[[C_0]]\n+    // CHECK:       %[[ADD1:.*]] = arith.addi %[[THREAD_ID]], %[[MAX1]]\n+    // CHECK:       %[[UPDATE:.*]] = xla_gpu.pure_call @dus_fusion_param_1_10\n+    // CHECK:       %[[INSERT:.*]] = tensor.insert %[[UPDATE:.*]] into %[[ARG4]][%[[ADD0]], %[[ADD1]]]\n+    // CHECK:       return %[[INSERT]]\n+  )\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(kHloString, ErrorSpec{1e-6}));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla\n"
                }
            ],
            "whole_deleted": "-    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n-    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n",
            "whole_added": "+#include <optional>\n+\n+    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n+    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n+TEST_F(MlirInPlaceDynamicUpdateSliceFusionTest, OperandSubgraphWithTwoRoots) {\n+  auto kHloString = R\"(\n+    HloModule in_place_dus\n+\n+    dus_fusion {\n+      param_0.8 = f32[512,512]{1,0} parameter(0)\n+      param_1.10 = f32[128,128]{1,0} parameter(1)\n+      param_3.32 = s32[] parameter(3)\n+      two = s32[] constant(2)\n+      param_3_mod_2 = s32[] remainder(param_3.32, two)\n+      one = s32[] constant(1)\n+      param_3_plus_one = s32[] add(param_3_mod_2, one)\n+      param_2.32 = s32[] parameter(2)\n+      param_2_plus_one = s32[] add(param_2.32, one)\n+      ROOT dynamic-update-slice.5.1 = f32[512,512]{1,0} dynamic-update-slice(param_0.8, param_1.10, param_2_plus_one, param_3_plus_one)\n+    }\n+    ENTRY entry {\n+      p0 = f32[512,512]{1,0} parameter(0)\n+      p1 = f32[128,128]{1,0} parameter(1)\n+      p2 = s32[] parameter(2)\n+      p3 = s32[] parameter(3)\n+      ROOT dus = f32[512,512]{1,0} fusion(p0, p1, p2, p3), kind=kLoop, calls=dus_fusion\n+    }\n+  )\";\n+  TF_ASSERT_OK(EmitAndCheckIR(kHloString, R\"(\n+    // CHECK:     func.func @fused_computation(\n+    // CHECK-SAME:  %[[ARG0:[^:]+]]: tensor<512x512xf32>\n+    // CHECK-SAME:  , %[[ARG1:[^:]+]]: tensor<128x128xf32>\n+    // CHECK-SAME:  , %[[ARG2:[^:]+]]: tensor<i32>\n+    // CHECK-SAME:  , %[[ARG3:[^:]+]]: tensor<i32>\n+    // CHECK-SAME:  , %[[ARG4:[^:]+]]: tensor<512x512xf32>\n+    // CHECK-DAG:   %[[C_384:.*]] = arith.constant 384\n+    // CHECK-DAG:   %[[C_0:.*]] = arith.constant 0\n+    // CHECK:       %[[THREAD_ID:.*]] = gpu.thread_id  x\n+    // CHECK:       %[[BLOCK_ID:.*]] = gpu.block_id  x\n+    // CHECK:       %[[I0:.*]], %[[I1:.*]] = xla_gpu.pure_call @dus_fusion_param_2_plus_one_param_3_plus_one\n+    // CHECK:       %[[IDX0:.*]] = arith.index_cast %[[I0]]\n+    // CHECK:       %[[MIN0:.*]] = arith.minsi %[[IDX0]], %[[C_384]]\n+    // CHECK:       %[[MAX0:.*]] = arith.maxsi %[[MIN0]], %[[C_0]]\n+    // CHECK:       %[[ADD0:.*]] = arith.addi %[[BLOCK_ID]], %[[MAX0]]\n+    // CHECK:       %[[IDX1:.*]] = arith.index_cast %[[I1]]\n+    // CHECK:       %[[MIN1:.*]] = arith.minsi %[[IDX1]], %[[C_384]]\n+    // CHECK:       %[[MAX1:.*]] = arith.maxsi %[[MIN1]], %[[C_0]]\n+    // CHECK:       %[[ADD1:.*]] = arith.addi %[[THREAD_ID]], %[[MAX1]]\n+    // CHECK:       %[[UPDATE:.*]] = xla_gpu.pure_call @dus_fusion_param_1_10\n+    // CHECK:       %[[INSERT:.*]] = tensor.insert %[[UPDATE:.*]] into %[[ARG4]][%[[ADD0]], %[[ADD1]]]\n+    // CHECK:       return %[[INSERT]]\n+  )\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(kHloString, ErrorSpec{1e-6}));\n+}\n+\n",
            "whole_hunk": "@@ -14,6 +14,8 @@ limitations under the License.\n ==============================================================================*/\n #include \"xla/service/gpu/fusions/in_place_dynamic_update_slice_mlir.h\"\n \n+#include <optional>\n+\n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n #include \"xla/error_spec.h\"\n@@ -113,11 +115,11 @@ TEST_F(MlirInPlaceDynamicUpdateSliceFusionTest, SimpleDUS) {\n     // CHECK:       %[[INPUT_INDEX_0:.*]] = affine.apply #[[MAP_1]]()[%[[THREAD_ID]]]\n     // CHECK:       %[[INPUT_INDEX_1:.*]] = affine.apply #[[MAP_2]]()[%[[THREAD_ID]]]\n     // CHECK:       %[[I0:.*]] = xla_gpu.pure_call @fused_computation_i0\n+    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n     // CHECK:       %[[IDX0:.*]] = arith.index_cast %[[I0]]\n     // CHECK:       %[[MIN0:.*]] = arith.minsi %[[IDX0]], %[[C_15]]\n     // CHECK:       %[[MAX0:.*]] = arith.maxsi %[[MIN0]], %[[C_0]]\n     // CHECK:       %[[ADD0:.*]] = arith.addi %[[INPUT_INDEX_0]], %[[MAX0]]\n-    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n     // CHECK:       %[[IDX1:.*]] = arith.index_cast %[[I1]]\n     // CHECK:       %[[MIN1:.*]] = arith.minsi %[[IDX1]], %[[C_24]]\n     // CHECK:       %[[MAX1:.*]] = arith.maxsi %[[MIN1]], %[[C_0]]\n@@ -163,11 +165,11 @@ TEST_F(MlirInPlaceDynamicUpdateSliceFusionTest, OutOfBoundDUS) {\n     // CHECK:       %[[INPUT_INDEX_0:.*]] = affine.apply #[[MAP_1]]()[%[[THREAD_ID]]]\n     // CHECK:       %[[INPUT_INDEX_1:.*]] = affine.apply #[[MAP_2]]()[%[[THREAD_ID]]]\n     // CHECK:       %[[I0:.*]] = xla_gpu.pure_call @fused_computation_i0\n+    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n     // CHECK:       %[[IDX0:.*]] = arith.index_cast %[[I0]]\n     // CHECK:       %[[MIN0:.*]] = arith.minsi %[[IDX0]], %[[C_5]]\n     // CHECK:       %[[MAX0:.*]] = arith.maxsi %[[MIN0]], %[[C_0]]\n     // CHECK:       %[[ADD0:.*]] = arith.addi %[[INPUT_INDEX_0]], %[[MAX0]]\n-    // CHECK:       %[[I1:.*]] = xla_gpu.pure_call @fused_computation_i1\n     // CHECK:       %[[IDX1:.*]] = arith.index_cast %[[I1]]\n     // CHECK:       %[[MIN1:.*]] = arith.minsi %[[IDX1]], %[[C_5]]\n     // CHECK:       %[[MAX1:.*]] = arith.maxsi %[[MIN1]], %[[C_0]]\n@@ -202,6 +204,57 @@ TEST_F(MlirInPlaceDynamicUpdateSliceFusionTest, BitcastDus) {\n   EXPECT_TRUE(RunAndCompareNoHloPasses(kHloString, ErrorSpec{1e-3}));\n }\n \n+TEST_F(MlirInPlaceDynamicUpdateSliceFusionTest, OperandSubgraphWithTwoRoots) {\n+  auto kHloString = R\"(\n+    HloModule in_place_dus\n+\n+    dus_fusion {\n+      param_0.8 = f32[512,512]{1,0} parameter(0)\n+      param_1.10 = f32[128,128]{1,0} parameter(1)\n+      param_3.32 = s32[] parameter(3)\n+      two = s32[] constant(2)\n+      param_3_mod_2 = s32[] remainder(param_3.32, two)\n+      one = s32[] constant(1)\n+      param_3_plus_one = s32[] add(param_3_mod_2, one)\n+      param_2.32 = s32[] parameter(2)\n+      param_2_plus_one = s32[] add(param_2.32, one)\n+      ROOT dynamic-update-slice.5.1 = f32[512,512]{1,0} dynamic-update-slice(param_0.8, param_1.10, param_2_plus_one, param_3_plus_one)\n+    }\n+    ENTRY entry {\n+      p0 = f32[512,512]{1,0} parameter(0)\n+      p1 = f32[128,128]{1,0} parameter(1)\n+      p2 = s32[] parameter(2)\n+      p3 = s32[] parameter(3)\n+      ROOT dus = f32[512,512]{1,0} fusion(p0, p1, p2, p3), kind=kLoop, calls=dus_fusion\n+    }\n+  )\";\n+  TF_ASSERT_OK(EmitAndCheckIR(kHloString, R\"(\n+    // CHECK:     func.func @fused_computation(\n+    // CHECK-SAME:  %[[ARG0:[^:]+]]: tensor<512x512xf32>\n+    // CHECK-SAME:  , %[[ARG1:[^:]+]]: tensor<128x128xf32>\n+    // CHECK-SAME:  , %[[ARG2:[^:]+]]: tensor<i32>\n+    // CHECK-SAME:  , %[[ARG3:[^:]+]]: tensor<i32>\n+    // CHECK-SAME:  , %[[ARG4:[^:]+]]: tensor<512x512xf32>\n+    // CHECK-DAG:   %[[C_384:.*]] = arith.constant 384\n+    // CHECK-DAG:   %[[C_0:.*]] = arith.constant 0\n+    // CHECK:       %[[THREAD_ID:.*]] = gpu.thread_id  x\n+    // CHECK:       %[[BLOCK_ID:.*]] = gpu.block_id  x\n+    // CHECK:       %[[I0:.*]], %[[I1:.*]] = xla_gpu.pure_call @dus_fusion_param_2_plus_one_param_3_plus_one\n+    // CHECK:       %[[IDX0:.*]] = arith.index_cast %[[I0]]\n+    // CHECK:       %[[MIN0:.*]] = arith.minsi %[[IDX0]], %[[C_384]]\n+    // CHECK:       %[[MAX0:.*]] = arith.maxsi %[[MIN0]], %[[C_0]]\n+    // CHECK:       %[[ADD0:.*]] = arith.addi %[[BLOCK_ID]], %[[MAX0]]\n+    // CHECK:       %[[IDX1:.*]] = arith.index_cast %[[I1]]\n+    // CHECK:       %[[MIN1:.*]] = arith.minsi %[[IDX1]], %[[C_384]]\n+    // CHECK:       %[[MAX1:.*]] = arith.maxsi %[[MIN1]], %[[C_0]]\n+    // CHECK:       %[[ADD1:.*]] = arith.addi %[[THREAD_ID]], %[[MAX1]]\n+    // CHECK:       %[[UPDATE:.*]] = xla_gpu.pure_call @dus_fusion_param_1_10\n+    // CHECK:       %[[INSERT:.*]] = tensor.insert %[[UPDATE:.*]] into %[[ARG4]][%[[ADD0]], %[[ADD1]]]\n+    // CHECK:       return %[[INSERT]]\n+  )\"));\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(kHloString, ErrorSpec{1e-6}));\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla\n"
        },
        {
            "name": "elemental_hlo_to_mlir.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/mlir/elemental_hlo_to_mlir.cc",
            "patches": [
                {
                    "old_start": 19,
                    "old_length": 6,
                    "new_start": 19,
                    "new_length": 7,
                    "hunk": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <functional>\n #include <iterator>\n #include <utility>\n+#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n"
                },
                {
                    "old_start": 35,
                    "old_length": 13,
                    "new_start": 36,
                    "new_length": 11,
                    "hunk": "@@ -35,13 +36,11 @@ limitations under the License.\n #include \"llvm/ADT/APInt.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n-#include \"llvm/Support/MathExtras.h\"\n #include \"mlir/Dialect/Affine/IR/AffineOps.h\"  // from @llvm-project\n #include \"mlir/Dialect/Affine/LoopUtils.h\"  // from @llvm-project\n #include \"mlir/Dialect/Arith/IR/Arith.h\"  // from @llvm-project\n #include \"mlir/Dialect/Complex/IR/Complex.h\"  // from @llvm-project\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // from @llvm-project\n #include \"mlir/Dialect/SCF/IR/SCF.h\"  // from @llvm-project\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"  // from @llvm-project\n #include \"mlir/IR/AffineExpr.h\"  // from @llvm-project\n"
                },
                {
                    "old_start": 100,
                    "old_length": 14,
                    "new_start": 99,
                    "new_length": 10,
                    "hunk": "@@ -100,14 +99,10 @@ using mlir::OpBuilder;\n using mlir::Value;\n using mlir::ValueRange;\n using mlir::arith::AndIOp;\n-using mlir::arith::CmpFOp;\n-using mlir::arith::CmpFPredicate;\n using mlir::arith::CmpIOp;\n using mlir::arith::CmpIPredicate;\n using mlir::arith::ConstantIndexOp;\n using mlir::arith::ConstantOp;\n-using mlir::arith::SelectOp;\n-using mlir::scf::ForOp;\n using mlir::scf::IfOp;\n using mlir::scf::YieldOp;\n \n"
                },
                {
                    "old_start": 1122,
                    "old_length": 17,
                    "new_start": 1117,
                    "new_length": 22,
                    "hunk": "@@ -1122,17 +1117,22 @@ bool IsHloConversionSupported(const HloFusionAdaptor& fusion,\n       });\n }\n \n-SmallVector<Value> ProvideParameter(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n-    int operand_index, ValueRange indices,\n-    const CallTargetProvider& call_target_provider, mlir::func::FuncOp this_fn,\n-    ImplicitLocOpBuilder& builder) {\n+Value ProvideParameter(const PartitionedComputation& computation,\n+                       const HloInstruction* instr, int operand_index,\n+                       ValueRange indices,\n+                       const CallTargetProvider& call_target_provider,\n+                       mlir::func::FuncOp this_fn,\n+                       ImplicitLocOpBuilder& builder,\n+                       const PartitionedComputation::Subgraph* caller) {\n   auto* operand = instr->operand(operand_index);\n \n-  const auto& injected_values = caller.injected_values;\n+  if (!caller) {\n+    caller = &computation.FindSubgraph(instr);\n+  }\n+  const auto& injected_values = caller->injected_values;\n   if (auto it = injected_values.find(operand); it != injected_values.end()) {\n     auto injected_param_values =\n-        this_fn.getArguments().take_back(caller.injected_values.size());\n+        this_fn.getArguments().take_back(caller->injected_values.size());\n     return {{injected_param_values[it->second]}};\n   }\n \n"
                },
                {
                    "old_start": 1140,
                    "old_length": 20,
                    "new_start": 1140,
                    "new_length": 26,
                    "hunk": "@@ -1140,20 +1140,26 @@ SmallVector<Value> ProvideParameter(\n   SmallVector<Value> operands(\n       this_fn.getArguments().take_front(instr->parent()->num_parameters()));\n   absl::c_copy(indices, std::back_inserter(operands));\n-  return builder.create<PureCallOp>(callee, operands).getResults();\n+  auto results = builder.create<PureCallOp>(callee, operands).getResults();\n+  if (results.size() == 1) {\n+    return results[0];\n+  }\n+  auto callee_subgraph = computation.FindSubgraph(operand);\n+  auto it = absl::c_find(callee_subgraph.roots, operand);\n+  CHECK(it != callee_subgraph.roots.end());\n+  return results[std::distance(callee_subgraph.roots.begin(), it)];\n }\n \n SmallVector<Value> ProvideParameterRange(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n+    const PartitionedComputation& computation, const HloInstruction* instr,\n     int start, int num, ValueRange indices,\n     const CallTargetProvider& call_target_provider, mlir::func::FuncOp this_fn,\n     ImplicitLocOpBuilder& builder) {\n   SmallVector<Value> scalars;\n+  scalars.reserve(num);\n   for (int i = 0; i < num; ++i) {\n-    auto scalar = ProvideParameter(caller, instr, i + start, indices,\n-                                   call_target_provider, this_fn, builder);\n-    CHECK_EQ(scalar.size(), 1);\n-    scalars.push_back(scalar.front());\n+    scalars.push_back(ProvideParameter(computation, instr, i + start, indices,\n+                                       call_target_provider, this_fn, builder));\n   }\n   return scalars;\n }\n"
                },
                {
                    "old_start": 1161,
                    "old_length": 6,
                    "new_start": 1167,
                    "new_length": 7,
                    "hunk": "@@ -1161,6 +1167,7 @@ SmallVector<Value> ProvideParameterRange(\n namespace {\n \n absl::StatusOr<SmallVector<Value>> SubgraphToMlir(\n+    const PartitionedComputation& computation,\n     const PartitionedComputation::Subgraph& subgraph,\n     mlir::func::FuncOp this_fn, const CallTargetProvider& call_target_provider,\n     ValueRange parameters, ValueRange indices, ImplicitLocOpBuilder& builder) {\n"
                },
                {
                    "old_start": 1181,
                    "old_length": 8,
                    "new_start": 1188,
                    "new_length": 8,
                    "hunk": "@@ -1181,8 +1188,8 @@ absl::StatusOr<SmallVector<Value>> SubgraphToMlir(\n       return emit_instr(operand, operand_indices);\n     }\n     return ConvertToSignless(\n-        ProvideParameter(subgraph, instr, index, operand_indices,\n-                         call_target_provider, this_fn, builder),\n+        {ProvideParameter(computation, instr, index, operand_indices,\n+                          call_target_provider, this_fn, builder, &subgraph)},\n         builder);\n   };\n \n"
                },
                {
                    "old_start": 1271,
                    "old_length": 9,
                    "new_start": 1278,
                    "new_length": 10,
                    "hunk": "@@ -1271,9 +1278,10 @@ absl::Status SubgraphToMlirFunction(\n       computation.computation().num_parameters());\n   int num_injected_values = subgraph.injected_values.size();\n   auto indices = indices_and_injected_values.drop_back(num_injected_values);\n-  TF_ASSIGN_OR_RETURN(auto results,\n-                      SubgraphToMlir(subgraph, func, call_target_provider,\n-                                     parameters, indices, builder));\n+  TF_ASSIGN_OR_RETURN(\n+      auto results,\n+      SubgraphToMlir(computation, subgraph, func, call_target_provider,\n+                     parameters, indices, builder));\n \n   // We have been converting signed types to signless types. To match the\n   // function signature, we have to convert back to signed types.\n"
                }
            ],
            "whole_deleted": "-#include \"llvm/Support/MathExtras.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // from @llvm-project\n-using mlir::arith::CmpFOp;\n-using mlir::arith::CmpFPredicate;\n-using mlir::arith::SelectOp;\n-using mlir::scf::ForOp;\n-SmallVector<Value> ProvideParameter(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n-    int operand_index, ValueRange indices,\n-    const CallTargetProvider& call_target_provider, mlir::func::FuncOp this_fn,\n-    ImplicitLocOpBuilder& builder) {\n-  const auto& injected_values = caller.injected_values;\n-        this_fn.getArguments().take_back(caller.injected_values.size());\n-  return builder.create<PureCallOp>(callee, operands).getResults();\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n-    auto scalar = ProvideParameter(caller, instr, i + start, indices,\n-                                   call_target_provider, this_fn, builder);\n-    CHECK_EQ(scalar.size(), 1);\n-    scalars.push_back(scalar.front());\n-        ProvideParameter(subgraph, instr, index, operand_indices,\n-                         call_target_provider, this_fn, builder),\n-  TF_ASSIGN_OR_RETURN(auto results,\n-                      SubgraphToMlir(subgraph, func, call_target_provider,\n-                                     parameters, indices, builder));\n",
            "whole_added": "+#include <variant>\n+Value ProvideParameter(const PartitionedComputation& computation,\n+                       const HloInstruction* instr, int operand_index,\n+                       ValueRange indices,\n+                       const CallTargetProvider& call_target_provider,\n+                       mlir::func::FuncOp this_fn,\n+                       ImplicitLocOpBuilder& builder,\n+                       const PartitionedComputation::Subgraph* caller) {\n+  if (!caller) {\n+    caller = &computation.FindSubgraph(instr);\n+  }\n+  const auto& injected_values = caller->injected_values;\n+        this_fn.getArguments().take_back(caller->injected_values.size());\n+  auto results = builder.create<PureCallOp>(callee, operands).getResults();\n+  if (results.size() == 1) {\n+    return results[0];\n+  }\n+  auto callee_subgraph = computation.FindSubgraph(operand);\n+  auto it = absl::c_find(callee_subgraph.roots, operand);\n+  CHECK(it != callee_subgraph.roots.end());\n+  return results[std::distance(callee_subgraph.roots.begin(), it)];\n+    const PartitionedComputation& computation, const HloInstruction* instr,\n+  scalars.reserve(num);\n+    scalars.push_back(ProvideParameter(computation, instr, i + start, indices,\n+                                       call_target_provider, this_fn, builder));\n+    const PartitionedComputation& computation,\n+        {ProvideParameter(computation, instr, index, operand_indices,\n+                          call_target_provider, this_fn, builder, &subgraph)},\n+  TF_ASSIGN_OR_RETURN(\n+      auto results,\n+      SubgraphToMlir(computation, subgraph, func, call_target_provider,\n+                     parameters, indices, builder));\n",
            "whole_hunk": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <functional>\n #include <iterator>\n #include <utility>\n+#include <variant>\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n@@ -35,13 +36,11 @@ limitations under the License.\n #include \"llvm/ADT/APInt.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n-#include \"llvm/Support/MathExtras.h\"\n #include \"mlir/Dialect/Affine/IR/AffineOps.h\"  // from @llvm-project\n #include \"mlir/Dialect/Affine/LoopUtils.h\"  // from @llvm-project\n #include \"mlir/Dialect/Arith/IR/Arith.h\"  // from @llvm-project\n #include \"mlir/Dialect/Complex/IR/Complex.h\"  // from @llvm-project\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"  // from @llvm-project\n #include \"mlir/Dialect/SCF/IR/SCF.h\"  // from @llvm-project\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"  // from @llvm-project\n #include \"mlir/IR/AffineExpr.h\"  // from @llvm-project\n@@ -100,14 +99,10 @@ using mlir::OpBuilder;\n using mlir::Value;\n using mlir::ValueRange;\n using mlir::arith::AndIOp;\n-using mlir::arith::CmpFOp;\n-using mlir::arith::CmpFPredicate;\n using mlir::arith::CmpIOp;\n using mlir::arith::CmpIPredicate;\n using mlir::arith::ConstantIndexOp;\n using mlir::arith::ConstantOp;\n-using mlir::arith::SelectOp;\n-using mlir::scf::ForOp;\n using mlir::scf::IfOp;\n using mlir::scf::YieldOp;\n \n@@ -1122,17 +1117,22 @@ bool IsHloConversionSupported(const HloFusionAdaptor& fusion,\n       });\n }\n \n-SmallVector<Value> ProvideParameter(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n-    int operand_index, ValueRange indices,\n-    const CallTargetProvider& call_target_provider, mlir::func::FuncOp this_fn,\n-    ImplicitLocOpBuilder& builder) {\n+Value ProvideParameter(const PartitionedComputation& computation,\n+                       const HloInstruction* instr, int operand_index,\n+                       ValueRange indices,\n+                       const CallTargetProvider& call_target_provider,\n+                       mlir::func::FuncOp this_fn,\n+                       ImplicitLocOpBuilder& builder,\n+                       const PartitionedComputation::Subgraph* caller) {\n   auto* operand = instr->operand(operand_index);\n \n-  const auto& injected_values = caller.injected_values;\n+  if (!caller) {\n+    caller = &computation.FindSubgraph(instr);\n+  }\n+  const auto& injected_values = caller->injected_values;\n   if (auto it = injected_values.find(operand); it != injected_values.end()) {\n     auto injected_param_values =\n-        this_fn.getArguments().take_back(caller.injected_values.size());\n+        this_fn.getArguments().take_back(caller->injected_values.size());\n     return {{injected_param_values[it->second]}};\n   }\n \n@@ -1140,20 +1140,26 @@ SmallVector<Value> ProvideParameter(\n   SmallVector<Value> operands(\n       this_fn.getArguments().take_front(instr->parent()->num_parameters()));\n   absl::c_copy(indices, std::back_inserter(operands));\n-  return builder.create<PureCallOp>(callee, operands).getResults();\n+  auto results = builder.create<PureCallOp>(callee, operands).getResults();\n+  if (results.size() == 1) {\n+    return results[0];\n+  }\n+  auto callee_subgraph = computation.FindSubgraph(operand);\n+  auto it = absl::c_find(callee_subgraph.roots, operand);\n+  CHECK(it != callee_subgraph.roots.end());\n+  return results[std::distance(callee_subgraph.roots.begin(), it)];\n }\n \n SmallVector<Value> ProvideParameterRange(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n+    const PartitionedComputation& computation, const HloInstruction* instr,\n     int start, int num, ValueRange indices,\n     const CallTargetProvider& call_target_provider, mlir::func::FuncOp this_fn,\n     ImplicitLocOpBuilder& builder) {\n   SmallVector<Value> scalars;\n+  scalars.reserve(num);\n   for (int i = 0; i < num; ++i) {\n-    auto scalar = ProvideParameter(caller, instr, i + start, indices,\n-                                   call_target_provider, this_fn, builder);\n-    CHECK_EQ(scalar.size(), 1);\n-    scalars.push_back(scalar.front());\n+    scalars.push_back(ProvideParameter(computation, instr, i + start, indices,\n+                                       call_target_provider, this_fn, builder));\n   }\n   return scalars;\n }\n@@ -1161,6 +1167,7 @@ SmallVector<Value> ProvideParameterRange(\n namespace {\n \n absl::StatusOr<SmallVector<Value>> SubgraphToMlir(\n+    const PartitionedComputation& computation,\n     const PartitionedComputation::Subgraph& subgraph,\n     mlir::func::FuncOp this_fn, const CallTargetProvider& call_target_provider,\n     ValueRange parameters, ValueRange indices, ImplicitLocOpBuilder& builder) {\n@@ -1181,8 +1188,8 @@ absl::StatusOr<SmallVector<Value>> SubgraphToMlir(\n       return emit_instr(operand, operand_indices);\n     }\n     return ConvertToSignless(\n-        ProvideParameter(subgraph, instr, index, operand_indices,\n-                         call_target_provider, this_fn, builder),\n+        {ProvideParameter(computation, instr, index, operand_indices,\n+                          call_target_provider, this_fn, builder, &subgraph)},\n         builder);\n   };\n \n@@ -1271,9 +1278,10 @@ absl::Status SubgraphToMlirFunction(\n       computation.computation().num_parameters());\n   int num_injected_values = subgraph.injected_values.size();\n   auto indices = indices_and_injected_values.drop_back(num_injected_values);\n-  TF_ASSIGN_OR_RETURN(auto results,\n-                      SubgraphToMlir(subgraph, func, call_target_provider,\n-                                     parameters, indices, builder));\n+  TF_ASSIGN_OR_RETURN(\n+      auto results,\n+      SubgraphToMlir(computation, subgraph, func, call_target_provider,\n+                     parameters, indices, builder));\n \n   // We have been converting signed types to signless types. To match the\n   // function signature, we have to convert back to signed types.\n"
        },
        {
            "name": "elemental_hlo_to_mlir.h",
            "path": "third_party/xla/xla/service/gpu/fusions/mlir/elemental_hlo_to_mlir.h",
            "patches": [
                {
                    "old_start": 41,
                    "old_length": 18,
                    "new_start": 41,
                    "new_length": 22,
                    "hunk": "@@ -41,18 +41,22 @@ using OperandProvider =\n     std::function<absl::StatusOr<llvm::SmallVector<mlir::Value>>(\n         const HloInstruction* instr, int index, mlir::ValueRange indices)>;\n \n-// Emits MLIR to produce the value(s) of a parameter. The parameter must be\n-// located outside the subgraph.\n-llvm::SmallVector<mlir::Value> ProvideParameter(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n+// Emits MLIR to produce the value of a parameter. The parameter must be located\n+// outside the subgraph. By default, the caller subgraph will be determined by\n+// searching in 'computation' for the subgraph that constains 'instr'. If\n+// 'instr' does not belong to 'computation', the caller subgraph can be passed\n+// directly.\n+mlir::Value ProvideParameter(\n+    const PartitionedComputation& computation, const HloInstruction* instr,\n     int operand_index, mlir::ValueRange indices,\n     const CallTargetProvider& call_target_provider, mlir::func::FuncOp this_fn,\n-    mlir::ImplicitLocOpBuilder& builder);\n+    mlir::ImplicitLocOpBuilder& builder,\n+    const PartitionedComputation::Subgraph* caller = nullptr);\n \n // Emits MLIR to produce the values of a range of parameters. The parameters\n // must all be scalars. The parameters are all evaluated at the same indices.\n llvm::SmallVector<mlir::Value> ProvideParameterRange(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n+    const PartitionedComputation& computation, const HloInstruction* instr,\n     int start, int num, mlir::ValueRange indices,\n     const CallTargetProvider& call_target_provider, mlir::func::FuncOp this_fn,\n     mlir::ImplicitLocOpBuilder& builder);\n"
                }
            ],
            "whole_deleted": "-// Emits MLIR to produce the value(s) of a parameter. The parameter must be\n-// located outside the subgraph.\n-llvm::SmallVector<mlir::Value> ProvideParameter(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n-    mlir::ImplicitLocOpBuilder& builder);\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n",
            "whole_added": "+// Emits MLIR to produce the value of a parameter. The parameter must be located\n+// outside the subgraph. By default, the caller subgraph will be determined by\n+// searching in 'computation' for the subgraph that constains 'instr'. If\n+// 'instr' does not belong to 'computation', the caller subgraph can be passed\n+// directly.\n+mlir::Value ProvideParameter(\n+    const PartitionedComputation& computation, const HloInstruction* instr,\n+    mlir::ImplicitLocOpBuilder& builder,\n+    const PartitionedComputation::Subgraph* caller = nullptr);\n+    const PartitionedComputation& computation, const HloInstruction* instr,\n",
            "whole_hunk": "@@ -41,18 +41,22 @@ using OperandProvider =\n     std::function<absl::StatusOr<llvm::SmallVector<mlir::Value>>(\n         const HloInstruction* instr, int index, mlir::ValueRange indices)>;\n \n-// Emits MLIR to produce the value(s) of a parameter. The parameter must be\n-// located outside the subgraph.\n-llvm::SmallVector<mlir::Value> ProvideParameter(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n+// Emits MLIR to produce the value of a parameter. The parameter must be located\n+// outside the subgraph. By default, the caller subgraph will be determined by\n+// searching in 'computation' for the subgraph that constains 'instr'. If\n+// 'instr' does not belong to 'computation', the caller subgraph can be passed\n+// directly.\n+mlir::Value ProvideParameter(\n+    const PartitionedComputation& computation, const HloInstruction* instr,\n     int operand_index, mlir::ValueRange indices,\n     const CallTargetProvider& call_target_provider, mlir::func::FuncOp this_fn,\n-    mlir::ImplicitLocOpBuilder& builder);\n+    mlir::ImplicitLocOpBuilder& builder,\n+    const PartitionedComputation::Subgraph* caller = nullptr);\n \n // Emits MLIR to produce the values of a range of parameters. The parameters\n // must all be scalars. The parameters are all evaluated at the same indices.\n llvm::SmallVector<mlir::Value> ProvideParameterRange(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n+    const PartitionedComputation& computation, const HloInstruction* instr,\n     int start, int num, mlir::ValueRange indices,\n     const CallTargetProvider& call_target_provider, mlir::func::FuncOp this_fn,\n     mlir::ImplicitLocOpBuilder& builder);\n"
        },
        {
            "name": "reduction_mlir.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/reduction_mlir.cc",
            "patches": [
                {
                    "old_start": 230,
                    "old_length": 9,
                    "new_start": 230,
                    "new_length": 9,
                    "hunk": "@@ -230,9 +230,9 @@ absl::Status MlirReductionFusion::EmitReduction(EmitterState& state) const {\n     int num_inputs = hero->operand_count() / 2;\n     const auto& computation =\n         state.computations.FindPartitionedComputation(hero->parent());\n-    inits[hero] = ProvideParameterRange(\n-        computation.FindSubgraph(hero), hero, num_inputs, num_inputs, {},\n-        state.call_target, state.entry_function, builder);\n+    inits[hero] =\n+        ProvideParameterRange(computation, hero, num_inputs, num_inputs, {},\n+                              state.call_target, state.entry_function, builder);\n   }\n \n   auto evaluate_epilogue =\n"
                },
                {
                    "old_start": 365,
                    "old_length": 9,
                    "new_start": 365,
                    "new_length": 10,
                    "hunk": "@@ -365,9 +365,10 @@ MlirReductionFusion::EmitterState::EmitPerThreadReducedElements(\n         input_indexing.GetAffineMap(), dim_values, symbol_values, builder);\n     auto operands = FusionParams();\n     absl::c_copy(indices, std::back_inserter(operands));\n-    auto values = ProvideParameterRange(computations.FindSubgraph(hero), hero,\n-                                        0, hero->operand_count() / 2, indices,\n-                                        call_target, entry_function, builder);\n+    auto values = ProvideParameterRange(\n+        computations.FindPartitionedComputation(hero->parent()), hero, 0,\n+        hero->operand_count() / 2, indices, call_target, entry_function,\n+        builder);\n \n     SmallVector<Value> reduce_args = outputs;\n     reduce_args.append(values.begin(), values.end());\n"
                }
            ],
            "whole_deleted": "-    inits[hero] = ProvideParameterRange(\n-        computation.FindSubgraph(hero), hero, num_inputs, num_inputs, {},\n-        state.call_target, state.entry_function, builder);\n-    auto values = ProvideParameterRange(computations.FindSubgraph(hero), hero,\n-                                        0, hero->operand_count() / 2, indices,\n-                                        call_target, entry_function, builder);\n",
            "whole_added": "+    inits[hero] =\n+        ProvideParameterRange(computation, hero, num_inputs, num_inputs, {},\n+                              state.call_target, state.entry_function, builder);\n+    auto values = ProvideParameterRange(\n+        computations.FindPartitionedComputation(hero->parent()), hero, 0,\n+        hero->operand_count() / 2, indices, call_target, entry_function,\n+        builder);\n",
            "whole_hunk": "@@ -230,9 +230,9 @@ absl::Status MlirReductionFusion::EmitReduction(EmitterState& state) const {\n     int num_inputs = hero->operand_count() / 2;\n     const auto& computation =\n         state.computations.FindPartitionedComputation(hero->parent());\n-    inits[hero] = ProvideParameterRange(\n-        computation.FindSubgraph(hero), hero, num_inputs, num_inputs, {},\n-        state.call_target, state.entry_function, builder);\n+    inits[hero] =\n+        ProvideParameterRange(computation, hero, num_inputs, num_inputs, {},\n+                              state.call_target, state.entry_function, builder);\n   }\n \n   auto evaluate_epilogue =\n@@ -365,9 +365,10 @@ MlirReductionFusion::EmitterState::EmitPerThreadReducedElements(\n         input_indexing.GetAffineMap(), dim_values, symbol_values, builder);\n     auto operands = FusionParams();\n     absl::c_copy(indices, std::back_inserter(operands));\n-    auto values = ProvideParameterRange(computations.FindSubgraph(hero), hero,\n-                                        0, hero->operand_count() / 2, indices,\n-                                        call_target, entry_function, builder);\n+    auto values = ProvideParameterRange(\n+        computations.FindPartitionedComputation(hero->parent()), hero, 0,\n+        hero->operand_count() / 2, indices, call_target, entry_function,\n+        builder);\n \n     SmallVector<Value> reduce_args = outputs;\n     reduce_args.append(values.begin(), values.end());\n"
        },
        {
            "name": "scatter_mlir.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/scatter_mlir.cc",
            "patches": [
                {
                    "old_start": 160,
                    "old_length": 9,
                    "new_start": 160,
                    "new_length": 9,
                    "hunk": "@@ -160,9 +160,9 @@ mlir::Value EmitScatterComputation(\n   auto reducer =\n       call_targets(scatter->called_computations()[0]->root_instruction());\n   if (scatter->unique_indices()) {\n-    auto operand_elem = ProvideParameter(root_computation.FindSubgraph(scatter),\n-                                         scatter, kScatterOperandIndex, indices,\n-                                         call_targets, entry_function, b)[0];\n+    auto operand_elem =\n+        ProvideParameter(root_computation, scatter, kScatterOperandIndex,\n+                         indices, call_targets, entry_function, b);\n     auto reduced_val = mlir_converter::InlineBlock(\n         b, reducer.getBody().front(), {operand_elem, update_elem})[0];\n \n"
                },
                {
                    "old_start": 203,
                    "old_length": 7,
                    "new_start": 203,
                    "new_length": 6,
                    "hunk": "@@ -203,7 +203,6 @@ absl::Status MlirScatterFusion::EmitEntryFunction(\n \n   const auto& root_computation = computations.FindPartitionedComputation(\n       fusion.fused_instructions_computation());\n-  const auto& scatter_subgraph = root_computation.FindSubgraph(scatter);\n   mlir::ImplicitLocOpBuilder b(entry_function.getLoc(), entry_function);\n   b.setInsertionPointToStart(entry_function.addEntryBlock());\n \n"
                },
                {
                    "old_start": 218,
                    "old_length": 11,
                    "new_start": 217,
                    "new_length": 9,
                    "hunk": "@@ -218,11 +217,9 @@ absl::Status MlirScatterFusion::EmitEntryFunction(\n         auto update_tensor_indices =\n             ApplyAffineMap(thread_id_to_update_map.GetAffineMap(), dim_values,\n                            symbol_values, b);\n-        auto update_elem =\n-            ProvideParameter(scatter_subgraph, scatter, kScatterUpdateIndex,\n-                             update_tensor_indices, call_targets,\n-                             entry_function, b)\n-                .front();\n+        auto update_elem = ProvideParameter(\n+            root_computation, scatter, kScatterUpdateIndex,\n+            update_tensor_indices, call_targets, entry_function, b);\n \n         // Extract slice offsets from scatter_indices operand, compute if the\n         // whole slice of scatter_update operand will fit into the output.\n"
                },
                {
                    "old_start": 236,
                    "old_length": 8,
                    "new_start": 233,
                    "new_length": 8,
                    "hunk": "@@ -236,8 +233,8 @@ absl::Status MlirScatterFusion::EmitEntryFunction(\n             SmallVector<Value, 4> indices_tensor_indices = {\n                 update_tensor_indices.front(), b.create<ConstantIndexOp>(i)};\n             extracted_index = ProvideParameter(\n-                scatter_subgraph, scatter, kScatterIndicesIndex,\n-                indices_tensor_indices, call_targets, entry_function, b)[0];\n+                root_computation, scatter, kScatterIndicesIndex,\n+                indices_tensor_indices, call_targets, entry_function, b);\n             if (extracted_index.getType() != b.getIndexType()) {\n               extracted_index = b.create<mlir::arith::IndexCastOp>(\n                   b.getIndexType(), extracted_index);\n"
                }
            ],
            "whole_deleted": "-    auto operand_elem = ProvideParameter(root_computation.FindSubgraph(scatter),\n-                                         scatter, kScatterOperandIndex, indices,\n-                                         call_targets, entry_function, b)[0];\n-  const auto& scatter_subgraph = root_computation.FindSubgraph(scatter);\n-        auto update_elem =\n-            ProvideParameter(scatter_subgraph, scatter, kScatterUpdateIndex,\n-                             update_tensor_indices, call_targets,\n-                             entry_function, b)\n-                .front();\n-                scatter_subgraph, scatter, kScatterIndicesIndex,\n-                indices_tensor_indices, call_targets, entry_function, b)[0];\n",
            "whole_added": "+    auto operand_elem =\n+        ProvideParameter(root_computation, scatter, kScatterOperandIndex,\n+                         indices, call_targets, entry_function, b);\n+        auto update_elem = ProvideParameter(\n+            root_computation, scatter, kScatterUpdateIndex,\n+            update_tensor_indices, call_targets, entry_function, b);\n+                root_computation, scatter, kScatterIndicesIndex,\n+                indices_tensor_indices, call_targets, entry_function, b);\n",
            "whole_hunk": "@@ -160,9 +160,9 @@ mlir::Value EmitScatterComputation(\n   auto reducer =\n       call_targets(scatter->called_computations()[0]->root_instruction());\n   if (scatter->unique_indices()) {\n-    auto operand_elem = ProvideParameter(root_computation.FindSubgraph(scatter),\n-                                         scatter, kScatterOperandIndex, indices,\n-                                         call_targets, entry_function, b)[0];\n+    auto operand_elem =\n+        ProvideParameter(root_computation, scatter, kScatterOperandIndex,\n+                         indices, call_targets, entry_function, b);\n     auto reduced_val = mlir_converter::InlineBlock(\n         b, reducer.getBody().front(), {operand_elem, update_elem})[0];\n \n@@ -203,7 +203,6 @@ absl::Status MlirScatterFusion::EmitEntryFunction(\n \n   const auto& root_computation = computations.FindPartitionedComputation(\n       fusion.fused_instructions_computation());\n-  const auto& scatter_subgraph = root_computation.FindSubgraph(scatter);\n   mlir::ImplicitLocOpBuilder b(entry_function.getLoc(), entry_function);\n   b.setInsertionPointToStart(entry_function.addEntryBlock());\n \n@@ -218,11 +217,9 @@ absl::Status MlirScatterFusion::EmitEntryFunction(\n         auto update_tensor_indices =\n             ApplyAffineMap(thread_id_to_update_map.GetAffineMap(), dim_values,\n                            symbol_values, b);\n-        auto update_elem =\n-            ProvideParameter(scatter_subgraph, scatter, kScatterUpdateIndex,\n-                             update_tensor_indices, call_targets,\n-                             entry_function, b)\n-                .front();\n+        auto update_elem = ProvideParameter(\n+            root_computation, scatter, kScatterUpdateIndex,\n+            update_tensor_indices, call_targets, entry_function, b);\n \n         // Extract slice offsets from scatter_indices operand, compute if the\n         // whole slice of scatter_update operand will fit into the output.\n@@ -236,8 +233,8 @@ absl::Status MlirScatterFusion::EmitEntryFunction(\n             SmallVector<Value, 4> indices_tensor_indices = {\n                 update_tensor_indices.front(), b.create<ConstantIndexOp>(i)};\n             extracted_index = ProvideParameter(\n-                scatter_subgraph, scatter, kScatterIndicesIndex,\n-                indices_tensor_indices, call_targets, entry_function, b)[0];\n+                root_computation, scatter, kScatterIndicesIndex,\n+                indices_tensor_indices, call_targets, entry_function, b);\n             if (extracted_index.getType() != b.getIndexType()) {\n               extracted_index = b.create<mlir::arith::IndexCastOp>(\n                   b.getIndexType(), extracted_index);\n"
        },
        {
            "name": "transpose_mlir.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/transpose_mlir.cc",
            "patches": [
                {
                    "old_start": 251,
                    "old_length": 17,
                    "new_start": 251,
                    "new_length": 16,
                    "hunk": "@@ -251,17 +251,16 @@ absl::StatusOr<SmallVector<Value, 4>> MlirTransposeFusion::EmitWriteToShMemMlir(\n               ApplyAffineMap(shmem_input_indexing.GetAffineMap(), dim_values,\n                              symbol_values, builder);\n \n-          auto result_scalars = mlir_converter::ProvideParameter(\n-              root_computation.FindSubgraph(transpose), transpose,\n+          auto result_scalar = mlir_converter::ProvideParameter(\n+              root_computation, transpose,\n               /*operand_index=*/0, input_indices, call_target_provider,\n               entry_function, builder);\n \n           SmallVector<Value> result_tensors;\n           result_tensors.reserve(num_outputs);\n-          for (auto [tensor, value] :\n-               llvm::zip(output_tensors, result_scalars)) {\n+          for (auto tensor : output_tensors) {\n             result_tensors.push_back(\n-                builder.create<InsertOp>(value, tensor, shmem_indices));\n+                builder.create<InsertOp>(result_scalar, tensor, shmem_indices));\n           }\n           return result_tensors;\n         });"
                }
            ],
            "whole_deleted": "-          auto result_scalars = mlir_converter::ProvideParameter(\n-              root_computation.FindSubgraph(transpose), transpose,\n-          for (auto [tensor, value] :\n-               llvm::zip(output_tensors, result_scalars)) {\n-                builder.create<InsertOp>(value, tensor, shmem_indices));\n",
            "whole_added": "+          auto result_scalar = mlir_converter::ProvideParameter(\n+              root_computation, transpose,\n+          for (auto tensor : output_tensors) {\n+                builder.create<InsertOp>(result_scalar, tensor, shmem_indices));\n",
            "whole_hunk": "@@ -251,17 +251,16 @@ absl::StatusOr<SmallVector<Value, 4>> MlirTransposeFusion::EmitWriteToShMemMlir(\n               ApplyAffineMap(shmem_input_indexing.GetAffineMap(), dim_values,\n                              symbol_values, builder);\n \n-          auto result_scalars = mlir_converter::ProvideParameter(\n-              root_computation.FindSubgraph(transpose), transpose,\n+          auto result_scalar = mlir_converter::ProvideParameter(\n+              root_computation, transpose,\n               /*operand_index=*/0, input_indices, call_target_provider,\n               entry_function, builder);\n \n           SmallVector<Value> result_tensors;\n           result_tensors.reserve(num_outputs);\n-          for (auto [tensor, value] :\n-               llvm::zip(output_tensors, result_scalars)) {\n+          for (auto tensor : output_tensors) {\n             result_tensors.push_back(\n-                builder.create<InsertOp>(value, tensor, shmem_indices));\n+                builder.create<InsertOp>(result_scalar, tensor, shmem_indices));\n           }\n           return result_tensors;\n         });"
        }
    ]
},
{
    "Id": 277,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/0b86e66216e547c8dd5c825457ecbd54b40c8ffe",
    "date": "2023-09-27T01:28:30-07:00",
    "message": "PR #5914: [ROCm] fixing build errors for ROCm nightly tests\n\nImported from GitHub PR https://github.com/openxla/xla/pull/5914\n\nThis commit fixes current build errors for ROCm platform.\n\n@akuegel @ddunl @ezhulenev : can someone of you check this commit please ?\n\nHere I have addressed the issues mentioned by Eugene from https://github.com/openxla/xla/pull/5867\nabout stream_executor targets.\n\nCopybara import of the project:\n\n--\n307781ca50e80fc870c9b2593458a25ea7c2321a by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nfixing build errors for ROCm nightly tests\n\nMerging this change closes #5914\n\nPiperOrigin-RevId: 568775978",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/stream_executor/rocm/BUILD",
            "patches": [
                {
                    "old_start": 110,
                    "old_length": 7,
                    "new_start": 110,
                    "new_length": 6,
                    "hunk": "@@ -110,7 +110,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"//xla/stream_executor\",\n         \"//xla/stream_executor:plugin_registry\",\n-        \"//xla/stream_executor:stream_executor_internal\",\n         \"//xla/stream_executor/gpu:gpu_activation_header\",\n         \"//xla/stream_executor/gpu:gpu_event\",\n         \"//xla/stream_executor/gpu:gpu_kernel_header\",\n"
                },
                {
                    "old_start": 158,
                    "old_length": 8,
                    "new_start": 157,
                    "new_length": 6,
                    "hunk": "@@ -158,8 +157,6 @@ cc_library(\n         \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/memory\",\n         \"//xla/stream_executor\",  # buildcleaner: keep\n-        \"//xla/stream_executor:multi_platform_manager\",\n-        \"//xla/stream_executor:stream_executor_pimpl_header\",\n         \"//xla/stream_executor/platform\",\n     ]),\n     alwayslink = True,  # Registers itself with the MultiPlatformManager.\n"
                },
                {
                    "old_start": 312,
                    "old_length": 12,
                    "new_start": 309,
                    "new_length": 10,
                    "hunk": "@@ -312,12 +309,10 @@ cc_library(\n         \":rocm_gpu_executor\",\n         \":rocm_platform_id\",\n         \"@eigen_archive//:eigen3\",\n+        \"//xla/stream_executor\",\n         \"//xla/stream_executor:dnn\",\n-        \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:plugin_registry\",\n         \"//xla/stream_executor:scratch_allocator\",\n-        \"//xla/stream_executor:stream_executor_pimpl\",\n-        \"//xla/stream_executor:temporary_device_memory\",\n         \"//xla/stream_executor/gpu:gpu_activation_header\",\n         \"//xla/stream_executor/gpu:gpu_stream_header\",\n         \"//xla/stream_executor/gpu:gpu_timer_header\",\n"
                }
            ],
            "whole_deleted": "-        \"//xla/stream_executor:stream_executor_internal\",\n-        \"//xla/stream_executor:multi_platform_manager\",\n-        \"//xla/stream_executor:stream_executor_pimpl_header\",\n-        \"//xla/stream_executor:event\",\n-        \"//xla/stream_executor:stream_executor_pimpl\",\n-        \"//xla/stream_executor:temporary_device_memory\",\n",
            "whole_added": "+        \"//xla/stream_executor\",\n",
            "whole_hunk": "@@ -110,7 +110,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"//xla/stream_executor\",\n         \"//xla/stream_executor:plugin_registry\",\n-        \"//xla/stream_executor:stream_executor_internal\",\n         \"//xla/stream_executor/gpu:gpu_activation_header\",\n         \"//xla/stream_executor/gpu:gpu_event\",\n         \"//xla/stream_executor/gpu:gpu_kernel_header\",\n@@ -158,8 +157,6 @@ cc_library(\n         \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/memory\",\n         \"//xla/stream_executor\",  # buildcleaner: keep\n-        \"//xla/stream_executor:multi_platform_manager\",\n-        \"//xla/stream_executor:stream_executor_pimpl_header\",\n         \"//xla/stream_executor/platform\",\n     ]),\n     alwayslink = True,  # Registers itself with the MultiPlatformManager.\n@@ -312,12 +309,10 @@ cc_library(\n         \":rocm_gpu_executor\",\n         \":rocm_platform_id\",\n         \"@eigen_archive//:eigen3\",\n+        \"//xla/stream_executor\",\n         \"//xla/stream_executor:dnn\",\n-        \"//xla/stream_executor:event\",\n         \"//xla/stream_executor:plugin_registry\",\n         \"//xla/stream_executor:scratch_allocator\",\n-        \"//xla/stream_executor:stream_executor_pimpl\",\n-        \"//xla/stream_executor:temporary_device_memory\",\n         \"//xla/stream_executor/gpu:gpu_activation_header\",\n         \"//xla/stream_executor/gpu:gpu_stream_header\",\n         \"//xla/stream_executor/gpu:gpu_timer_header\",\n"
        },
        {
            "name": "rocm_gpu_executor.cc",
            "path": "third_party/xla/xla/stream_executor/rocm/rocm_gpu_executor.cc",
            "patches": [
                {
                    "old_start": 727,
                    "old_length": 8,
                    "new_start": 727,
                    "new_length": 10,
                    "hunk": "@@ -727,8 +727,10 @@ GpuExecutor::GetStreamImplementation() {\n \n tsl::StatusOr<std::unique_ptr<internal::CommandBufferInterface>>\n GpuExecutor::GetCommandBufferImplementation() {\n-  return std::unique_ptr<internal::CommandBufferInterface>(\n-      new GpuCommandBuffer());\n+  VLOG(2) << \"Create ROCm command buffer (ROCm graph)\";\n+  GpuGraphHandle graph = nullptr;\n+  TF_RETURN_IF_ERROR(GpuDriver::CreateGraph(&graph));\n+  return std::make_unique<GpuCommandBuffer>(this, graph);\n }\n \n void* GpuExecutor::GpuContextHack() { return context_; }"
                }
            ],
            "whole_deleted": "-  return std::unique_ptr<internal::CommandBufferInterface>(\n-      new GpuCommandBuffer());\n",
            "whole_added": "+  VLOG(2) << \"Create ROCm command buffer (ROCm graph)\";\n+  GpuGraphHandle graph = nullptr;\n+  TF_RETURN_IF_ERROR(GpuDriver::CreateGraph(&graph));\n+  return std::make_unique<GpuCommandBuffer>(this, graph);\n",
            "whole_hunk": "@@ -727,8 +727,10 @@ GpuExecutor::GetStreamImplementation() {\n \n tsl::StatusOr<std::unique_ptr<internal::CommandBufferInterface>>\n GpuExecutor::GetCommandBufferImplementation() {\n-  return std::unique_ptr<internal::CommandBufferInterface>(\n-      new GpuCommandBuffer());\n+  VLOG(2) << \"Create ROCm command buffer (ROCm graph)\";\n+  GpuGraphHandle graph = nullptr;\n+  TF_RETURN_IF_ERROR(GpuDriver::CreateGraph(&graph));\n+  return std::make_unique<GpuCommandBuffer>(this, graph);\n }\n \n void* GpuExecutor::GpuContextHack() { return context_; }"
        }
    ]
},
{
    "Id": 294,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/4e9db184297de547884b4b3c1d1422658ec37a6e",
    "date": "2023-09-11T20:52:35-07:00",
    "message": "#tf-data Output of a model node can be deleted by a thread while some of its input nodes are in the process of being deleted. If such a case happens, the `output_` of these input nodes becomes a dangling pointer. This CL adds a check for such a scenario.\n\nPiperOrigin-RevId: 564583894",
    "label": "YES",
    "changes": [
        {
            "name": "model.cc",
            "path": "tensorflow/core/framework/model.cc",
            "patches": [
                {
                    "old_start": 2338,
                    "old_length": 7,
                    "new_start": 2338,
                    "new_length": 7,
                    "hunk": "@@ -2338,7 +2338,7 @@ void Model::Optimize(AutotuneAlgorithm algorithm, int64_t cpu_budget,\n void Model::RemoveNode(std::shared_ptr<Node> node) {\n   mutex_lock l(mu_);\n   if (node) {\n-    if (node->output()) {\n+    if (node->output() && !node->output_deleted()) {\n       node->output()->remove_input(node);\n     }\n     VLOG(3) << \"Removing \" << node->long_name();\n"
                }
            ],
            "whole_deleted": "-    if (node->output()) {\n",
            "whole_added": "+    if (node->output() && !node->output_deleted()) {\n",
            "whole_hunk": "@@ -2338,7 +2338,7 @@ void Model::Optimize(AutotuneAlgorithm algorithm, int64_t cpu_budget,\n void Model::RemoveNode(std::shared_ptr<Node> node) {\n   mutex_lock l(mu_);\n   if (node) {\n-    if (node->output()) {\n+    if (node->output() && !node->output_deleted()) {\n       node->output()->remove_input(node);\n     }\n     VLOG(3) << \"Removing \" << node->long_name();\n"
        },
        {
            "name": "model.h",
            "path": "tensorflow/core/framework/model.h",
            "patches": [
                {
                    "old_start": 266,
                    "old_length": 7,
                    "new_start": 266,
                    "new_length": 8,
                    "hunk": "@@ -266,7 +266,8 @@ class Node {\n         processing_time_(0),\n         record_metrics_(true),\n         metrics_(name_),\n-        output_(args.output.get()) {}\n+        output_(args.output.get()),\n+        output_weak_ptr_(args.output) {}\n \n   virtual ~Node() {\n     // Clear the sub-nodes instead of relying on implicit shared pointer\n"
                },
                {
                    "old_start": 376,
                    "old_length": 6,
                    "new_start": 377,
                    "new_length": 7,
                    "hunk": "@@ -376,6 +377,7 @@ class Node {\n \n   // Returns the node output.\n   Node* output() const { return output_; }\n+  bool output_deleted() { return output_weak_ptr_.expired(); }\n \n   // Returns the parameter value.\n   double parameter_value(const string& name) const TF_LOCKS_EXCLUDED(mu_) {\n"
                },
                {
                    "old_start": 801,
                    "old_length": 6,
                    "new_start": 803,
                    "new_length": 7,
                    "hunk": "@@ -801,6 +803,7 @@ class Node {\n   // The reference to the output node is not owned so that deletion of a\n   // node results in recursive deletion of the subtree rooted in the node.\n   Node* const output_;\n+  std::weak_ptr<Node> output_weak_ptr_;\n };\n \n // InterleaveMany is used to model datasets whose inputs are used to create"
                }
            ],
            "whole_deleted": "-        output_(args.output.get()) {}\n",
            "whole_added": "+        output_(args.output.get()),\n+        output_weak_ptr_(args.output) {}\n+  bool output_deleted() { return output_weak_ptr_.expired(); }\n+  std::weak_ptr<Node> output_weak_ptr_;\n",
            "whole_hunk": "@@ -266,7 +266,8 @@ class Node {\n         processing_time_(0),\n         record_metrics_(true),\n         metrics_(name_),\n-        output_(args.output.get()) {}\n+        output_(args.output.get()),\n+        output_weak_ptr_(args.output) {}\n \n   virtual ~Node() {\n     // Clear the sub-nodes instead of relying on implicit shared pointer\n@@ -376,6 +377,7 @@ class Node {\n \n   // Returns the node output.\n   Node* output() const { return output_; }\n+  bool output_deleted() { return output_weak_ptr_.expired(); }\n \n   // Returns the parameter value.\n   double parameter_value(const string& name) const TF_LOCKS_EXCLUDED(mu_) {\n@@ -801,6 +803,7 @@ class Node {\n   // The reference to the output node is not owned so that deletion of a\n   // node results in recursive deletion of the subtree rooted in the node.\n   Node* const output_;\n+  std::weak_ptr<Node> output_weak_ptr_;\n };\n \n // InterleaveMany is used to model datasets whose inputs are used to create"
        }
    ]
},
{
    "Id": 210,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/adb0d424d4c2312f39def8360db82e01c0c1e073",
    "date": "2024-01-08T05:57:42-08:00",
    "message": "Fix NCCL definition check.\n\nPiperOrigin-RevId: 596567472",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "patches": [
                {
                    "old_start": 11,
                    "old_length": 7,
                    "new_start": 11,
                    "new_length": 7,
                    "hunk": "@@ -11,7 +11,7 @@ load(\n     \"tf_additional_cudnn_plugin_copts\",\n     \"tf_additional_gpu_compilation_copts\",\n )\n-load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"if_nccl\", \"set_external_visibility\", \"tsl_copts\")\n+load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"set_external_visibility\", \"tsl_copts\")\n load(\n     \"@local_tsl//tsl/platform:build_config_root.bzl\",\n     \"if_static\",\n"
                },
                {
                    "old_start": 130,
                    "old_length": 7,
                    "new_start": 130,
                    "new_length": 7,
                    "hunk": "@@ -130,7 +130,7 @@ cc_library(\n     name = \"cuda_driver\",\n     srcs = if_cuda_is_configured([\"cuda_driver.cc\"]),\n     hdrs = if_cuda_is_configured([\"cuda_driver.h\"]),\n-    defines = if_nccl([\"NCCL_ENABLED=1\"]),\n+    defines = if_cuda([\"XLA_ENABLE_XCCL\"]),\n     visibility = [\"//visibility:public\"],\n     deps = if_cuda_is_configured([\n         \":cuda_diagnostics\",\n"
                },
                {
                    "old_start": 164,
                    "old_length": 7,
                    "new_start": 164,
                    "new_length": 7,
                    "hunk": "@@ -164,7 +164,7 @@ cc_library(\n         \"@local_tsl//tsl/platform:stacktrace\",\n         \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n-    ]) + if_nccl([\"@local_config_nccl//:nccl\"]),\n+    ]) + if_cuda([\"@local_config_nccl//:nccl\"]),\n )\n \n cc_library(\n"
                }
            ],
            "whole_deleted": "-load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"if_nccl\", \"set_external_visibility\", \"tsl_copts\")\n-    defines = if_nccl([\"NCCL_ENABLED=1\"]),\n-    ]) + if_nccl([\"@local_config_nccl//:nccl\"]),\n",
            "whole_added": "+load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"set_external_visibility\", \"tsl_copts\")\n+    defines = if_cuda([\"XLA_ENABLE_XCCL\"]),\n+    ]) + if_cuda([\"@local_config_nccl//:nccl\"]),\n",
            "whole_hunk": "@@ -11,7 +11,7 @@ load(\n     \"tf_additional_cudnn_plugin_copts\",\n     \"tf_additional_gpu_compilation_copts\",\n )\n-load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"if_nccl\", \"set_external_visibility\", \"tsl_copts\")\n+load(\"@local_tsl//tsl:tsl.bzl\", \"if_google\", \"set_external_visibility\", \"tsl_copts\")\n load(\n     \"@local_tsl//tsl/platform:build_config_root.bzl\",\n     \"if_static\",\n@@ -130,7 +130,7 @@ cc_library(\n     name = \"cuda_driver\",\n     srcs = if_cuda_is_configured([\"cuda_driver.cc\"]),\n     hdrs = if_cuda_is_configured([\"cuda_driver.h\"]),\n-    defines = if_nccl([\"NCCL_ENABLED=1\"]),\n+    defines = if_cuda([\"XLA_ENABLE_XCCL\"]),\n     visibility = [\"//visibility:public\"],\n     deps = if_cuda_is_configured([\n         \":cuda_diagnostics\",\n@@ -164,7 +164,7 @@ cc_library(\n         \"@local_tsl//tsl/platform:stacktrace\",\n         \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n-    ]) + if_nccl([\"@local_config_nccl//:nccl\"]),\n+    ]) + if_cuda([\"@local_config_nccl//:nccl\"]),\n )\n \n cc_library(\n"
        },
        {
            "name": "cuda_driver.cc",
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_driver.cc",
            "patches": [
                {
                    "old_start": 57,
                    "old_length": 9,
                    "new_start": 57,
                    "new_length": 9,
                    "hunk": "@@ -57,9 +57,9 @@ limitations under the License.\n #include \"tsl/platform/statusor.h\"\n #include \"tsl/platform/threadpool.h\"\n \n-#ifdef NCCL_ENABLED\n+#ifdef XLA_ENABLE_XCCL\n #include \"third_party/nccl/nccl.h\"\n-#endif  // NCCL_ENABLED\n+#endif  // XLA_ENABLE_XCCL\n \n static constexpr bool FLAGS_gpuexec_cuda_driver_inject_init_error = false;\n static constexpr bool FLAGS_gpuexec_cuda_sync_around_driver_calls = false;\n"
                },
                {
                    "old_start": 1636,
                    "old_length": 7,
                    "new_start": 1636,
                    "new_length": 7,
                    "hunk": "@@ -1636,7 +1636,7 @@ struct BitPatternToValue {\n   ScopedActivateContext activated{context};\n   void* ptr = nullptr;\n \n-#ifdef NCCL_ENABLED\n+#ifdef XLA_ENABLE_XCCL\n   ncclResult_t res = ncclMemAlloc(&ptr, bytes);\n   if (res != ncclSuccess) {\n     return absl::InternalError(absl::StrFormat(\n"
                },
                {
                    "old_start": 1659,
                    "old_length": 7,
                    "new_start": 1659,
                    "new_length": 7,
                    "hunk": "@@ -1659,7 +1659,7 @@ struct BitPatternToValue {\n     GpuContext* context, void* location) {\n   ScopedActivateContext activation(context);\n \n-#ifdef NCCL_ENABLED\n+#ifdef XLA_ENABLE_XCCL\n   ncclResult_t res = ncclMemFree(location);\n   if (res != ncclSuccess) {\n     return absl::InternalError(absl::StrFormat("
                }
            ],
            "whole_deleted": "-#ifdef NCCL_ENABLED\n-#endif  // NCCL_ENABLED\n-#ifdef NCCL_ENABLED\n-#ifdef NCCL_ENABLED\n",
            "whole_added": "+#ifdef XLA_ENABLE_XCCL\n+#endif  // XLA_ENABLE_XCCL\n+#ifdef XLA_ENABLE_XCCL\n+#ifdef XLA_ENABLE_XCCL\n",
            "whole_hunk": "@@ -57,9 +57,9 @@ limitations under the License.\n #include \"tsl/platform/statusor.h\"\n #include \"tsl/platform/threadpool.h\"\n \n-#ifdef NCCL_ENABLED\n+#ifdef XLA_ENABLE_XCCL\n #include \"third_party/nccl/nccl.h\"\n-#endif  // NCCL_ENABLED\n+#endif  // XLA_ENABLE_XCCL\n \n static constexpr bool FLAGS_gpuexec_cuda_driver_inject_init_error = false;\n static constexpr bool FLAGS_gpuexec_cuda_sync_around_driver_calls = false;\n@@ -1636,7 +1636,7 @@ struct BitPatternToValue {\n   ScopedActivateContext activated{context};\n   void* ptr = nullptr;\n \n-#ifdef NCCL_ENABLED\n+#ifdef XLA_ENABLE_XCCL\n   ncclResult_t res = ncclMemAlloc(&ptr, bytes);\n   if (res != ncclSuccess) {\n     return absl::InternalError(absl::StrFormat(\n@@ -1659,7 +1659,7 @@ struct BitPatternToValue {\n     GpuContext* context, void* location) {\n   ScopedActivateContext activation(context);\n \n-#ifdef NCCL_ENABLED\n+#ifdef XLA_ENABLE_XCCL\n   ncclResult_t res = ncclMemFree(location);\n   if (res != ncclSuccess) {\n     return absl::InternalError(absl::StrFormat("
        }
    ]
},
{
    "Id": 619,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/0e74b6ecb39527636d894e3530789899e97172c4",
    "date": "2022-12-05T08:17:57-08:00",
    "message": "[XLA:GPU] Simplify EmitF32ToBF16 in GpuElementalIrEmitter for Cuda capability >= 8.0\n\nWe are using the more complex default operation for Cuda capability < 8.0,\nand a faster hardware solution for Cuda capability >= 8.0.\n\nWe are now running the elemental_ir_emitter_test on gpu_sm70only and gpu_sm80only in addition to cpu and gpu.\n\nWe also pass down the IrEmitterContext to GpuElementalIrEmitter to be able to check the Cuda capability.\n\nPiperOrigin-RevId: 493008856",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/service/BUILD",
            "patches": [
                {
                    "old_start": 5252,
                    "old_length": 6,
                    "new_start": 5252,
                    "new_length": 11,
                    "hunk": "@@ -5252,6 +5252,11 @@ cc_library(\n xla_test(\n     name = \"elemental_ir_emitter_test\",\n     srcs = [\"elemental_ir_emitter_test.cc\"],\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-nvidia\",\n+        \"requires-gpu-sm70-only\",\n+        \"requires-gpu-sm80-only\",\n+    ]},\n     backends = [\n         \"cpu\",\n         \"gpu\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-nvidia\",\n+        \"requires-gpu-sm70-only\",\n+        \"requires-gpu-sm80-only\",\n+    ]},\n",
            "whole_hunk": "@@ -5252,6 +5252,11 @@ cc_library(\n xla_test(\n     name = \"elemental_ir_emitter_test\",\n     srcs = [\"elemental_ir_emitter_test.cc\"],\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-nvidia\",\n+        \"requires-gpu-sm70-only\",\n+        \"requires-gpu-sm80-only\",\n+    ]},\n     backends = [\n         \"cpu\",\n         \"gpu\",\n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 245,
                    "old_length": 17,
                    "new_start": 245,
                    "new_length": 34,
                    "hunk": "@@ -245,17 +245,34 @@ tf_cc_test(\n     ],\n )\n \n+cc_library(\n+    name = \"ir_emitter_context\",\n+    srcs = [\n+        \"ir_emitter_context.cc\",\n+    ],\n+    hdrs = [\n+        \"ir_emitter_context.h\",\n+    ],\n+    deps = [\n+        \":gpu_constants\",\n+        \":gpu_device_info\",\n+        \":gpu_executable\",\n+        \"//tensorflow/compiler/xla/service:buffer_assignment\",\n+        \"//tensorflow/compiler/xla/service:name_uniquer\",\n+        \"@llvm-project//llvm:ir_headers\",\n+        \"@llvm-project//mlir:IR\",\n+    ],\n+)\n+\n cc_library(\n     name = \"ir_emitter\",\n     srcs = [\n         \"ir_emitter.cc\",\n-        \"ir_emitter_context.cc\",\n         \"ir_emitter_nested.cc\",\n         \"ir_emitter_unnested.cc\",\n     ],\n     hdrs = [\n         \"ir_emitter.h\",\n-        \"ir_emitter_context.h\",\n         \"ir_emitter_nested.h\",\n         \"ir_emitter_unnested.h\",\n         \"kernel_mapping_scheme.h\",\n"
                },
                {
                    "old_start": 265,
                    "old_length": 6,
                    "new_start": 282,
                    "new_length": 7,
                    "hunk": "@@ -265,6 +282,7 @@ cc_library(\n         \":backend_configs_cc\",\n         \":buffer_allocations\",\n         \":elemental_ir_emitter\",\n+        \":ir_emitter_context\",\n         \":fft_thunk\",\n         \":gpu_asm_opts_util\",\n         \":gpu_constants\",\n"
                },
                {
                    "old_start": 301,
                    "old_length": 7,
                    "new_start": 319,
                    "new_length": 7,
                    "hunk": "@@ -301,7 +319,7 @@ cc_library(\n         \"@llvm-project//mlir:ROCDLToLLVMIRTranslation\",\n         \"@llvm-project//mlir:Support\",\n         \"@llvm-project//mlir:NVVMToLLVMIRTranslation\",\n-        \"//tensorflow/compiler/xla/translate/mhlo_to_lhlo_with_xla:mhlo_to_lhlo_with_xla\",\n+        \"//tensorflow/compiler/xla/translate/mhlo_to_lhlo_with_xla\",\n         \"//tensorflow/compiler/xla/mlir_hlo\",\n         \"//tensorflow/compiler/xla/mlir_hlo:lhlo\",\n         \"//tensorflow/compiler/xla/mlir_hlo:lhlo_gpu\",\n"
                },
                {
                    "old_start": 384,
                    "old_length": 6,
                    "new_start": 402,
                    "new_length": 7,
                    "hunk": "@@ -384,6 +402,7 @@ cc_library(\n     hdrs = [\"elemental_ir_emitter.h\"],\n     deps = [\n         \":backend_configs_cc\",\n+        \":ir_emitter_context\",\n         \":target_util\",\n         \"//tensorflow/compiler/xla:literal\",\n         \"//tensorflow/compiler/xla:shape_util\",\n"
                },
                {
                    "old_start": 1754,
                    "old_length": 6,
                    "new_start": 1773,
                    "new_length": 7,
                    "hunk": "@@ -1754,6 +1773,7 @@ cc_library(\n         \":instruction_fusion\",\n         \":ir_emission_utils\",\n         \":ir_emitter\",\n+        \":ir_emitter_context\",\n         \":jitrt_custom_calls\",\n         \":matmul_utils\",\n         \":metrics\",\n"
                }
            ],
            "whole_deleted": "-        \"ir_emitter_context.cc\",\n-        \"ir_emitter_context.h\",\n-        \"//tensorflow/compiler/xla/translate/mhlo_to_lhlo_with_xla:mhlo_to_lhlo_with_xla\",\n",
            "whole_added": "+cc_library(\n+    name = \"ir_emitter_context\",\n+    srcs = [\n+        \"ir_emitter_context.cc\",\n+    ],\n+    hdrs = [\n+        \"ir_emitter_context.h\",\n+    ],\n+    deps = [\n+        \":gpu_constants\",\n+        \":gpu_device_info\",\n+        \":gpu_executable\",\n+        \"//tensorflow/compiler/xla/service:buffer_assignment\",\n+        \"//tensorflow/compiler/xla/service:name_uniquer\",\n+        \"@llvm-project//llvm:ir_headers\",\n+        \"@llvm-project//mlir:IR\",\n+    ],\n+)\n+\n+        \":ir_emitter_context\",\n+        \"//tensorflow/compiler/xla/translate/mhlo_to_lhlo_with_xla\",\n+        \":ir_emitter_context\",\n+        \":ir_emitter_context\",\n",
            "whole_hunk": "@@ -245,17 +245,34 @@ tf_cc_test(\n     ],\n )\n \n+cc_library(\n+    name = \"ir_emitter_context\",\n+    srcs = [\n+        \"ir_emitter_context.cc\",\n+    ],\n+    hdrs = [\n+        \"ir_emitter_context.h\",\n+    ],\n+    deps = [\n+        \":gpu_constants\",\n+        \":gpu_device_info\",\n+        \":gpu_executable\",\n+        \"//tensorflow/compiler/xla/service:buffer_assignment\",\n+        \"//tensorflow/compiler/xla/service:name_uniquer\",\n+        \"@llvm-project//llvm:ir_headers\",\n+        \"@llvm-project//mlir:IR\",\n+    ],\n+)\n+\n cc_library(\n     name = \"ir_emitter\",\n     srcs = [\n         \"ir_emitter.cc\",\n-        \"ir_emitter_context.cc\",\n         \"ir_emitter_nested.cc\",\n         \"ir_emitter_unnested.cc\",\n     ],\n     hdrs = [\n         \"ir_emitter.h\",\n-        \"ir_emitter_context.h\",\n         \"ir_emitter_nested.h\",\n         \"ir_emitter_unnested.h\",\n         \"kernel_mapping_scheme.h\",\n@@ -265,6 +282,7 @@ cc_library(\n         \":backend_configs_cc\",\n         \":buffer_allocations\",\n         \":elemental_ir_emitter\",\n+        \":ir_emitter_context\",\n         \":fft_thunk\",\n         \":gpu_asm_opts_util\",\n         \":gpu_constants\",\n@@ -301,7 +319,7 @@ cc_library(\n         \"@llvm-project//mlir:ROCDLToLLVMIRTranslation\",\n         \"@llvm-project//mlir:Support\",\n         \"@llvm-project//mlir:NVVMToLLVMIRTranslation\",\n-        \"//tensorflow/compiler/xla/translate/mhlo_to_lhlo_with_xla:mhlo_to_lhlo_with_xla\",\n+        \"//tensorflow/compiler/xla/translate/mhlo_to_lhlo_with_xla\",\n         \"//tensorflow/compiler/xla/mlir_hlo\",\n         \"//tensorflow/compiler/xla/mlir_hlo:lhlo\",\n         \"//tensorflow/compiler/xla/mlir_hlo:lhlo_gpu\",\n@@ -384,6 +402,7 @@ cc_library(\n     hdrs = [\"elemental_ir_emitter.h\"],\n     deps = [\n         \":backend_configs_cc\",\n+        \":ir_emitter_context\",\n         \":target_util\",\n         \"//tensorflow/compiler/xla:literal\",\n         \"//tensorflow/compiler/xla:shape_util\",\n@@ -1754,6 +1773,7 @@ cc_library(\n         \":instruction_fusion\",\n         \":ir_emission_utils\",\n         \":ir_emitter\",\n+        \":ir_emitter_context\",\n         \":jitrt_custom_calls\",\n         \":matmul_utils\",\n         \":metrics\",\n"
        },
        {
            "name": "elemental_ir_emitter.cc",
            "path": "tensorflow/compiler/xla/service/gpu/elemental_ir_emitter.cc",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 8,
                    "new_start": 17,
                    "new_length": 10,
                    "hunk": "@@ -17,8 +17,10 @@ limitations under the License.\n \n #include <stddef.h>\n \n+#include <utility>\n #include <vector>\n \n+#include \"llvm/IR/IntrinsicsNVPTX.h\"\n #include \"tensorflow/tsl/platform/logging.h\"\n // IWYU pragma: no_include \"llvm/IR/Attributes.gen.inc\"\n // IWYU pragma: no_include \"llvm/IR/Intrinsics.gen.inc\"\n"
                },
                {
                    "old_start": 71,
                    "old_length": 10,
                    "new_start": 73,
                    "new_length": 12,
                    "hunk": "@@ -71,10 +73,12 @@ bool IsFPLiteralWithValue(const HloInstruction* operand, float value) {\n \n GpuElementalIrEmitter::GpuElementalIrEmitter(\n     const HloModuleConfig& hlo_module_config, llvm::Module* module,\n-    llvm::IRBuilder<>* b, NestedComputer compute_nested)\n+    llvm::IRBuilder<>* b, NestedComputer compute_nested,\n+    IrEmitterContext* ir_emitter_context)\n     : ElementalIrEmitter(module, b),\n       hlo_module_config_(hlo_module_config),\n-      compute_nested_(std::move(compute_nested)) {}\n+      compute_nested_(std::move(compute_nested)),\n+      ir_emitter_context_(ir_emitter_context) {}\n \n StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitDeviceMathCall(\n     TargetDeviceFunctionID funcid, absl::Span<llvm::Value* const> operands,\n"
                },
                {
                    "old_start": 335,
                    "old_length": 5,
                    "new_start": 339,
                    "new_length": 16,
                    "hunk": "@@ -335,5 +339,16 @@ llvm::Value* GpuElementalIrEmitter::EmitThreadId() {\n   return NSWAdd(NSWMul(block_id, threads_per_block), thread_id_in_block);\n }\n \n+StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitF32ToBF16(\n+    llvm::Value* f32_value) {\n+  if (ir_emitter_context_->cuda_compute_capability().IsAtLeast(8)) {\n+    return llvm_ir::EmitCallToIntrinsic(llvm::Intrinsic::nvvm_f2bf16_rn,\n+                                        {f32_value}, {}, b());\n+  } else {\n+    // More complex fallback solution.\n+    return ElementalIrEmitter::EmitF32ToBF16(f32_value);\n+  }\n+}\n+\n }  // namespace gpu\n }  // namespace xla\n"
                }
            ],
            "whole_deleted": "-    llvm::IRBuilder<>* b, NestedComputer compute_nested)\n-      compute_nested_(std::move(compute_nested)) {}\n",
            "whole_added": "+#include <utility>\n+#include \"llvm/IR/IntrinsicsNVPTX.h\"\n+    llvm::IRBuilder<>* b, NestedComputer compute_nested,\n+    IrEmitterContext* ir_emitter_context)\n+      compute_nested_(std::move(compute_nested)),\n+      ir_emitter_context_(ir_emitter_context) {}\n+StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitF32ToBF16(\n+    llvm::Value* f32_value) {\n+  if (ir_emitter_context_->cuda_compute_capability().IsAtLeast(8)) {\n+    return llvm_ir::EmitCallToIntrinsic(llvm::Intrinsic::nvvm_f2bf16_rn,\n+                                        {f32_value}, {}, b());\n+  } else {\n+    // More complex fallback solution.\n+    return ElementalIrEmitter::EmitF32ToBF16(f32_value);\n+  }\n+}\n+\n",
            "whole_hunk": "@@ -17,8 +17,10 @@ limitations under the License.\n \n #include <stddef.h>\n \n+#include <utility>\n #include <vector>\n \n+#include \"llvm/IR/IntrinsicsNVPTX.h\"\n #include \"tensorflow/tsl/platform/logging.h\"\n // IWYU pragma: no_include \"llvm/IR/Attributes.gen.inc\"\n // IWYU pragma: no_include \"llvm/IR/Intrinsics.gen.inc\"\n@@ -71,10 +73,12 @@ bool IsFPLiteralWithValue(const HloInstruction* operand, float value) {\n \n GpuElementalIrEmitter::GpuElementalIrEmitter(\n     const HloModuleConfig& hlo_module_config, llvm::Module* module,\n-    llvm::IRBuilder<>* b, NestedComputer compute_nested)\n+    llvm::IRBuilder<>* b, NestedComputer compute_nested,\n+    IrEmitterContext* ir_emitter_context)\n     : ElementalIrEmitter(module, b),\n       hlo_module_config_(hlo_module_config),\n-      compute_nested_(std::move(compute_nested)) {}\n+      compute_nested_(std::move(compute_nested)),\n+      ir_emitter_context_(ir_emitter_context) {}\n \n StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitDeviceMathCall(\n     TargetDeviceFunctionID funcid, absl::Span<llvm::Value* const> operands,\n@@ -335,5 +339,16 @@ llvm::Value* GpuElementalIrEmitter::EmitThreadId() {\n   return NSWAdd(NSWMul(block_id, threads_per_block), thread_id_in_block);\n }\n \n+StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitF32ToBF16(\n+    llvm::Value* f32_value) {\n+  if (ir_emitter_context_->cuda_compute_capability().IsAtLeast(8)) {\n+    return llvm_ir::EmitCallToIntrinsic(llvm::Intrinsic::nvvm_f2bf16_rn,\n+                                        {f32_value}, {}, b());\n+  } else {\n+    // More complex fallback solution.\n+    return ElementalIrEmitter::EmitF32ToBF16(f32_value);\n+  }\n+}\n+\n }  // namespace gpu\n }  // namespace xla\n"
        },
        {
            "name": "elemental_ir_emitter.h",
            "path": "tensorflow/compiler/xla/service/gpu/elemental_ir_emitter.h",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 6,
                    "new_start": 26,
                    "new_length": 7,
                    "hunk": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_computation.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/service/elemental_ir_emitter.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/ir_emitter_context.h\"\n #include \"tensorflow/compiler/xla/service/gpu/target_util.h\"\n #include \"tensorflow/compiler/xla/service/hlo_module_config.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/loop_emitter.h\"\n"
                },
                {
                    "old_start": 43,
                    "old_length": 9,
                    "new_start": 44,
                    "new_length": 14,
                    "hunk": "@@ -43,9 +44,14 @@ class GpuElementalIrEmitter : public ElementalIrEmitter {\n   using NestedComputer = std::function<StatusOr<std::vector<llvm::Value*>>(\n       const HloComputation&, absl::Span<llvm::Value* const>)>;\n \n+  // Constructs a GpuElementalIrEmitter.\n+  //\n+  // ir_emitter_context is owned by the caller and should outlive the\n+  // GpuElementalIrEmitter object.\n   GpuElementalIrEmitter(const HloModuleConfig& hlo_module_config,\n                         llvm::Module* module, llvm::IRBuilder<>* b,\n-                        NestedComputer compute_nested);\n+                        NestedComputer compute_nested,\n+                        IrEmitterContext* ir_emitter_context);\n \n  protected:\n   llvm_ir::IrArray::Index GetSourceIndexOfBitcast(\n"
                },
                {
                    "old_start": 101,
                    "old_length": 6,
                    "new_start": 107,
                    "new_length": 8,
                    "hunk": "@@ -101,6 +107,8 @@ class GpuElementalIrEmitter : public ElementalIrEmitter {\n \n   llvm::Value* EmitThreadId() override;\n \n+  StatusOr<llvm::Value*> EmitF32ToBF16(llvm::Value* f32_value) override;\n+\n   bool fast_min_max() override {\n     return hlo_module_config_.debug_options().xla_gpu_enable_fast_min_max();\n   }\n"
                },
                {
                    "old_start": 136,
                    "old_length": 6,
                    "new_start": 144,
                    "new_length": 8,
                    "hunk": "@@ -136,6 +144,8 @@ class GpuElementalIrEmitter : public ElementalIrEmitter {\n   const HloModuleConfig& hlo_module_config_;\n \n   NestedComputer compute_nested_;\n+\n+  IrEmitterContext* ir_emitter_context_;\n };\n \n }  // namespace gpu\n"
                }
            ],
            "whole_deleted": "-                        NestedComputer compute_nested);\n",
            "whole_added": "+#include \"tensorflow/compiler/xla/service/gpu/ir_emitter_context.h\"\n+  // Constructs a GpuElementalIrEmitter.\n+  //\n+  // ir_emitter_context is owned by the caller and should outlive the\n+  // GpuElementalIrEmitter object.\n+                        NestedComputer compute_nested,\n+                        IrEmitterContext* ir_emitter_context);\n+  StatusOr<llvm::Value*> EmitF32ToBF16(llvm::Value* f32_value) override;\n+\n+\n+  IrEmitterContext* ir_emitter_context_;\n",
            "whole_hunk": "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_computation.h\"\n #include \"tensorflow/compiler/xla/hlo/ir/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/service/elemental_ir_emitter.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/ir_emitter_context.h\"\n #include \"tensorflow/compiler/xla/service/gpu/target_util.h\"\n #include \"tensorflow/compiler/xla/service/hlo_module_config.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/loop_emitter.h\"\n@@ -43,9 +44,14 @@ class GpuElementalIrEmitter : public ElementalIrEmitter {\n   using NestedComputer = std::function<StatusOr<std::vector<llvm::Value*>>(\n       const HloComputation&, absl::Span<llvm::Value* const>)>;\n \n+  // Constructs a GpuElementalIrEmitter.\n+  //\n+  // ir_emitter_context is owned by the caller and should outlive the\n+  // GpuElementalIrEmitter object.\n   GpuElementalIrEmitter(const HloModuleConfig& hlo_module_config,\n                         llvm::Module* module, llvm::IRBuilder<>* b,\n-                        NestedComputer compute_nested);\n+                        NestedComputer compute_nested,\n+                        IrEmitterContext* ir_emitter_context);\n \n  protected:\n   llvm_ir::IrArray::Index GetSourceIndexOfBitcast(\n@@ -101,6 +107,8 @@ class GpuElementalIrEmitter : public ElementalIrEmitter {\n \n   llvm::Value* EmitThreadId() override;\n \n+  StatusOr<llvm::Value*> EmitF32ToBF16(llvm::Value* f32_value) override;\n+\n   bool fast_min_max() override {\n     return hlo_module_config_.debug_options().xla_gpu_enable_fast_min_max();\n   }\n@@ -136,6 +144,8 @@ class GpuElementalIrEmitter : public ElementalIrEmitter {\n   const HloModuleConfig& hlo_module_config_;\n \n   NestedComputer compute_nested_;\n+\n+  IrEmitterContext* ir_emitter_context_;\n };\n \n }  // namespace gpu\n"
        },
        {
            "name": "ir_emitter.cc",
            "path": "tensorflow/compiler/xla/service/gpu/ir_emitter.cc",
            "patches": [
                {
                    "old_start": 87,
                    "old_length": 7,
                    "new_start": 87,
                    "new_length": 7,
                    "hunk": "@@ -87,7 +87,7 @@ Status IrEmitter::DefaultAction(HloInstruction* hlo) {\n   }\n   return EmitTargetElementLoop(\n       *hlo, GpuElementalIrEmitter(hlo_module_config_, module_, &b_,\n-                                  GetNestedComputer())\n+                                  GetNestedComputer(), ir_emitter_context_)\n                 .MakeElementGenerator(hlo, operand_to_generator));\n }\n \n"
                },
                {
                    "old_start": 560,
                    "old_length": 7,
                    "new_start": 560,
                    "new_length": 8,
                    "hunk": "@@ -560,7 +560,8 @@ Status IrEmitter::HandleFusion(HloInstruction* fusion) {\n   // IrEmitterUnnested::HandleFusion.\n   CHECK_EQ(HloInstruction::FusionKind::kLoop, fusion->fusion_kind());\n   GpuElementalIrEmitter elemental_emitter(hlo_module_config_, module_, &b_,\n-                                          GetNestedComputer());\n+                                          GetNestedComputer(),\n+                                          ir_emitter_context_);\n   FusedIrEmitter fused_emitter(elemental_emitter);\n   BindFusionArguments(fusion, &fused_emitter);\n   TF_ASSIGN_OR_RETURN(auto generator, fused_emitter.GetGenerator(\n"
                }
            ],
            "whole_deleted": "-                                  GetNestedComputer())\n-                                          GetNestedComputer());\n",
            "whole_added": "+                                  GetNestedComputer(), ir_emitter_context_)\n+                                          GetNestedComputer(),\n+                                          ir_emitter_context_);\n",
            "whole_hunk": "@@ -87,7 +87,7 @@ Status IrEmitter::DefaultAction(HloInstruction* hlo) {\n   }\n   return EmitTargetElementLoop(\n       *hlo, GpuElementalIrEmitter(hlo_module_config_, module_, &b_,\n-                                  GetNestedComputer())\n+                                  GetNestedComputer(), ir_emitter_context_)\n                 .MakeElementGenerator(hlo, operand_to_generator));\n }\n \n@@ -560,7 +560,8 @@ Status IrEmitter::HandleFusion(HloInstruction* fusion) {\n   // IrEmitterUnnested::HandleFusion.\n   CHECK_EQ(HloInstruction::FusionKind::kLoop, fusion->fusion_kind());\n   GpuElementalIrEmitter elemental_emitter(hlo_module_config_, module_, &b_,\n-                                          GetNestedComputer());\n+                                          GetNestedComputer(),\n+                                          ir_emitter_context_);\n   FusedIrEmitter fused_emitter(elemental_emitter);\n   BindFusionArguments(fusion, &fused_emitter);\n   TF_ASSIGN_OR_RETURN(auto generator, fused_emitter.GetGenerator(\n"
        },
        {
            "name": "ir_emitter_unnested.cc",
            "path": "tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc",
            "patches": [
                {
                    "old_start": 456,
                    "old_length": 8,
                    "new_start": 456,
                    "new_length": 8,
                    "hunk": "@@ -456,8 +456,8 @@ int RowReductionGetRowsPerWarp(int reduced_dimension_size) {\n IrEmitterUnnested::IrEmitterUnnested(const HloModuleConfig& hlo_module_config,\n                                      IrEmitterContext* ir_emitter_context)\n     : IrEmitter(hlo_module_config, ir_emitter_context, /*is_nested=*/false),\n-      elemental_emitter_(hlo_module_config_, module_, &b_,\n-                         GetNestedComputer()) {}\n+      elemental_emitter_(hlo_module_config_, module_, &b_, GetNestedComputer(),\n+                         ir_emitter_context) {}\n \n StatusOr<std::unique_ptr<IrEmitterUnnested>> IrEmitterUnnested::Create(\n     const HloModuleConfig& hlo_module_config,\n"
                }
            ],
            "whole_deleted": "-      elemental_emitter_(hlo_module_config_, module_, &b_,\n-                         GetNestedComputer()) {}\n",
            "whole_added": "+      elemental_emitter_(hlo_module_config_, module_, &b_, GetNestedComputer(),\n+                         ir_emitter_context) {}\n",
            "whole_hunk": "@@ -456,8 +456,8 @@ int RowReductionGetRowsPerWarp(int reduced_dimension_size) {\n IrEmitterUnnested::IrEmitterUnnested(const HloModuleConfig& hlo_module_config,\n                                      IrEmitterContext* ir_emitter_context)\n     : IrEmitter(hlo_module_config, ir_emitter_context, /*is_nested=*/false),\n-      elemental_emitter_(hlo_module_config_, module_, &b_,\n-                         GetNestedComputer()) {}\n+      elemental_emitter_(hlo_module_config_, module_, &b_, GetNestedComputer(),\n+                         ir_emitter_context) {}\n \n StatusOr<std::unique_ptr<IrEmitterUnnested>> IrEmitterUnnested::Create(\n     const HloModuleConfig& hlo_module_config,\n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/service/gpu/tests/BUILD",
            "patches": [
                {
                    "old_start": 711,
                    "old_length": 6,
                    "new_start": 711,
                    "new_length": 25,
                    "hunk": "@@ -711,6 +711,25 @@ tf_cc_test(\n     ],\n )\n \n+xla_test(\n+    name = \"elemental_ir_emitter_test\",\n+    srcs = [\"elemental_ir_emitter_test.cc\"],\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-nvidia\",\n+        \"requires-gpu-sm70-only\",\n+        \"requires-gpu-sm80-only\",\n+    ]},\n+    backends = [\n+        \"gpu\",\n+    ],\n+    deps = [\n+        \":gpu_codegen_test\",\n+        \"//tensorflow/compiler/xla/tests:filecheck\",\n+        \"//tensorflow/tsl/platform:test\",\n+        \"//tensorflow/tsl/platform:test_main\",\n+    ],\n+)\n+\n tf_cc_test(\n     name = \"gpu_input_fusible_slice_test\",\n     srcs = [\"gpu_input_fusible_slice_test.cc\"],\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+xla_test(\n+    name = \"elemental_ir_emitter_test\",\n+    srcs = [\"elemental_ir_emitter_test.cc\"],\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-nvidia\",\n+        \"requires-gpu-sm70-only\",\n+        \"requires-gpu-sm80-only\",\n+    ]},\n+    backends = [\n+        \"gpu\",\n+    ],\n+    deps = [\n+        \":gpu_codegen_test\",\n+        \"//tensorflow/compiler/xla/tests:filecheck\",\n+        \"//tensorflow/tsl/platform:test\",\n+        \"//tensorflow/tsl/platform:test_main\",\n+    ],\n+)\n+\n",
            "whole_hunk": "@@ -711,6 +711,25 @@ tf_cc_test(\n     ],\n )\n \n+xla_test(\n+    name = \"elemental_ir_emitter_test\",\n+    srcs = [\"elemental_ir_emitter_test.cc\"],\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-nvidia\",\n+        \"requires-gpu-sm70-only\",\n+        \"requires-gpu-sm80-only\",\n+    ]},\n+    backends = [\n+        \"gpu\",\n+    ],\n+    deps = [\n+        \":gpu_codegen_test\",\n+        \"//tensorflow/compiler/xla/tests:filecheck\",\n+        \"//tensorflow/tsl/platform:test\",\n+        \"//tensorflow/tsl/platform:test_main\",\n+    ],\n+)\n+\n tf_cc_test(\n     name = \"gpu_input_fusible_slice_test\",\n     srcs = [\"gpu_input_fusible_slice_test.cc\"],\n"
        },
        {
            "name": "elemental_ir_emitter_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/tests/elemental_ir_emitter_test.cc",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 65,
                    "hunk": "@@ -0,0 +1,65 @@\n+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"tensorflow/compiler/xla/service/gpu/tests/gpu_codegen_test.h\"\n+#include \"tensorflow/compiler/xla/tests/filecheck.h\"\n+#include \"tensorflow/tsl/platform/test.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class ElementalIrEmitterTest : public GpuCodegenTest {\n+ protected:\n+  se::CudaComputeCapability GetCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n+};\n+\n+TEST_F(ElementalIrEmitterTest, TestConvertF32ToBF16) {\n+  const char* hlo_string = R\"(\n+    HloModule convertF32ToBF16\n+\n+    ENTRY main {\n+      f32_ = f32[] parameter(0)\n+      ROOT bf16_ = bf16[] convert(f32[] f32_)\n+    }\n+  )\";\n+\n+  if (GetCudaComputeCapability().IsAtLeast(8)) {\n+    CompileAndVerifyIr(hlo_string, R\"(\n+CHECK: call i16 @llvm.nvvm.f2bf16.rn(float %{{.*}})\n+)\");\n+  } else {\n+    CompileAndVerifyIr(hlo_string, R\"(\n+CHECK-NOT: nvvm.f2bf16\n+)\");\n+\n+    CompileAndVerifyIr(hlo_string, R\"(\n+CHECK: bitcast float %{{.*}} to i32\n+CHECK: trunc i32 %{{.*}} to i16\n+)\");\n+  }\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"tensorflow/compiler/xla/service/gpu/tests/gpu_codegen_test.h\"\n+#include \"tensorflow/compiler/xla/tests/filecheck.h\"\n+#include \"tensorflow/tsl/platform/test.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class ElementalIrEmitterTest : public GpuCodegenTest {\n+ protected:\n+  se::CudaComputeCapability GetCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n+};\n+\n+TEST_F(ElementalIrEmitterTest, TestConvertF32ToBF16) {\n+  const char* hlo_string = R\"(\n+    HloModule convertF32ToBF16\n+\n+    ENTRY main {\n+      f32_ = f32[] parameter(0)\n+      ROOT bf16_ = bf16[] convert(f32[] f32_)\n+    }\n+  )\";\n+\n+  if (GetCudaComputeCapability().IsAtLeast(8)) {\n+    CompileAndVerifyIr(hlo_string, R\"(\n+CHECK: call i16 @llvm.nvvm.f2bf16.rn(float %{{.*}})\n+)\");\n+  } else {\n+    CompileAndVerifyIr(hlo_string, R\"(\n+CHECK-NOT: nvvm.f2bf16\n+)\");\n+\n+    CompileAndVerifyIr(hlo_string, R\"(\n+CHECK: bitcast float %{{.*}} to i32\n+CHECK: trunc i32 %{{.*}} to i16\n+)\");\n+  }\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla\n",
            "whole_hunk": "@@ -0,0 +1,65 @@\n+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"tensorflow/compiler/xla/service/gpu/tests/gpu_codegen_test.h\"\n+#include \"tensorflow/compiler/xla/tests/filecheck.h\"\n+#include \"tensorflow/tsl/platform/test.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class ElementalIrEmitterTest : public GpuCodegenTest {\n+ protected:\n+  se::CudaComputeCapability GetCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n+};\n+\n+TEST_F(ElementalIrEmitterTest, TestConvertF32ToBF16) {\n+  const char* hlo_string = R\"(\n+    HloModule convertF32ToBF16\n+\n+    ENTRY main {\n+      f32_ = f32[] parameter(0)\n+      ROOT bf16_ = bf16[] convert(f32[] f32_)\n+    }\n+  )\";\n+\n+  if (GetCudaComputeCapability().IsAtLeast(8)) {\n+    CompileAndVerifyIr(hlo_string, R\"(\n+CHECK: call i16 @llvm.nvvm.f2bf16.rn(float %{{.*}})\n+)\");\n+  } else {\n+    CompileAndVerifyIr(hlo_string, R\"(\n+CHECK-NOT: nvvm.f2bf16\n+)\");\n+\n+    CompileAndVerifyIr(hlo_string, R\"(\n+CHECK: bitcast float %{{.*}} to i32\n+CHECK: trunc i32 %{{.*}} to i16\n+)\");\n+  }\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        }
    ]
},
{
    "Id": 507,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9bef202848856f4c08ac29a1d874241813a3f6a0",
    "date": "2023-03-03T02:30:14-08:00",
    "message": "Do not allow to multi-output fuse two transposes with different permutations.\n\nThe fast transpose emitter relies on all transpose heroes in the multi-output\nfusion to be of the same kind. The previous check for equality of hero shapes\nand operand shapes was not good enough.\n\nPiperOrigin-RevId: 513771585",
    "label": "YES",
    "changes": [
        {
            "name": "gpu_fusible.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible.cc",
            "patches": [
                {
                    "old_start": 187,
                    "old_length": 12,
                    "new_start": 187,
                    "new_length": 14,
                    "hunk": "@@ -187,12 +187,14 @@ FusionDecision ShapesCompatibleForMultiOutputFusion(\n   const HloInstruction* hero1 = GetRealHeroForMultiOutputFusion(instr1);\n   const HloInstruction* hero2 = GetRealHeroForMultiOutputFusion(instr2);\n \n-  bool hero1_is_unnested_reduce =\n+  auto hero1_is_unnested_reduce =\n       IsReductionFromOrToContiguousDimensions(*hero1);\n-  bool hero1_is_unnested_transpose = FindAnyTiledTranspose(*hero1).has_value();\n+  auto tiled_transpose_hero1 = FindAnyTiledTranspose(*hero1);\n+  bool hero1_is_unnested_transpose = tiled_transpose_hero1.has_value();\n   bool hero2_is_unnested_reduce =\n       IsReductionFromOrToContiguousDimensions(*hero2);\n-  bool hero2_is_unnested_transpose = FindAnyTiledTranspose(*hero2).has_value();\n+  auto tiled_transpose_hero2 = FindAnyTiledTranspose(*hero2);\n+  bool hero2_is_unnested_transpose = tiled_transpose_hero2.has_value();\n \n   if (hero1_is_unnested_reduce && hero2_is_unnested_reduce &&\n       !IsFusedReductionOutputConsistent(hero2, hero1)) {\n"
                },
                {
                    "old_start": 200,
                    "old_length": 8,
                    "new_start": 202,
                    "new_length": 7,
                    "hunk": "@@ -200,8 +202,7 @@ FusionDecision ShapesCompatibleForMultiOutputFusion(\n   } else if (hero1_is_unnested_transpose && hero2_is_unnested_transpose &&\n              (!ShapeUtil::EqualIgnoringElementType(hero1->shape(),\n                                                    hero2->shape()) ||\n-              !ShapeUtil::EqualIgnoringElementType(\n-                  hero1->operand(0)->shape(), hero2->operand(0)->shape()))) {\n+              tiled_transpose_hero1->second != tiled_transpose_hero2->second)) {\n     return \"tiled transposes with different shapes\";\n   } else if ((hero1_is_unnested_transpose && hero2_is_unnested_reduce) ||\n              (hero1_is_unnested_reduce && hero2_is_unnested_transpose)) {\n"
                }
            ],
            "whole_deleted": "-  bool hero1_is_unnested_reduce =\n-  bool hero1_is_unnested_transpose = FindAnyTiledTranspose(*hero1).has_value();\n-  bool hero2_is_unnested_transpose = FindAnyTiledTranspose(*hero2).has_value();\n-              !ShapeUtil::EqualIgnoringElementType(\n-                  hero1->operand(0)->shape(), hero2->operand(0)->shape()))) {\n",
            "whole_added": "+  auto hero1_is_unnested_reduce =\n+  auto tiled_transpose_hero1 = FindAnyTiledTranspose(*hero1);\n+  bool hero1_is_unnested_transpose = tiled_transpose_hero1.has_value();\n+  auto tiled_transpose_hero2 = FindAnyTiledTranspose(*hero2);\n+  bool hero2_is_unnested_transpose = tiled_transpose_hero2.has_value();\n+              tiled_transpose_hero1->second != tiled_transpose_hero2->second)) {\n",
            "whole_hunk": "@@ -187,12 +187,14 @@ FusionDecision ShapesCompatibleForMultiOutputFusion(\n   const HloInstruction* hero1 = GetRealHeroForMultiOutputFusion(instr1);\n   const HloInstruction* hero2 = GetRealHeroForMultiOutputFusion(instr2);\n \n-  bool hero1_is_unnested_reduce =\n+  auto hero1_is_unnested_reduce =\n       IsReductionFromOrToContiguousDimensions(*hero1);\n-  bool hero1_is_unnested_transpose = FindAnyTiledTranspose(*hero1).has_value();\n+  auto tiled_transpose_hero1 = FindAnyTiledTranspose(*hero1);\n+  bool hero1_is_unnested_transpose = tiled_transpose_hero1.has_value();\n   bool hero2_is_unnested_reduce =\n       IsReductionFromOrToContiguousDimensions(*hero2);\n-  bool hero2_is_unnested_transpose = FindAnyTiledTranspose(*hero2).has_value();\n+  auto tiled_transpose_hero2 = FindAnyTiledTranspose(*hero2);\n+  bool hero2_is_unnested_transpose = tiled_transpose_hero2.has_value();\n \n   if (hero1_is_unnested_reduce && hero2_is_unnested_reduce &&\n       !IsFusedReductionOutputConsistent(hero2, hero1)) {\n@@ -200,8 +202,7 @@ FusionDecision ShapesCompatibleForMultiOutputFusion(\n   } else if (hero1_is_unnested_transpose && hero2_is_unnested_transpose &&\n              (!ShapeUtil::EqualIgnoringElementType(hero1->shape(),\n                                                    hero2->shape()) ||\n-              !ShapeUtil::EqualIgnoringElementType(\n-                  hero1->operand(0)->shape(), hero2->operand(0)->shape()))) {\n+              tiled_transpose_hero1->second != tiled_transpose_hero2->second)) {\n     return \"tiled transposes with different shapes\";\n   } else if ((hero1_is_unnested_transpose && hero2_is_unnested_reduce) ||\n              (hero1_is_unnested_reduce && hero2_is_unnested_transpose)) {\n"
        },
        {
            "name": "gpu_fusible_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible_test.cc",
            "patches": [
                {
                    "old_start": 578,
                    "old_length": 24,
                    "new_start": 578,
                    "new_length": 22,
                    "hunk": "@@ -578,24 +578,22 @@ TEST_F(\n     ShapesCompatibleForMultiOutputFusion_SiblingTransposeFusionsNotCompatible) {\n   auto module = ParseAndReturnVerifiedModule(absl::StrCat(kModulePrefix, R\"(\n     fused_021_transpose {\n-      param_0 = f32[1024,1024,1024]{2,1,0} parameter(0)\n-      slice = f32[1024,256,512]{2,1,0} slice(param_0), slice={[0:1024:1], [0:256:1], [0:512:1]}\n-      transpose = f32[1024,512,256]{2,1,0} transpose(slice), dimensions={0,2,1}\n-      ROOT bitcast = f32[134217728]{0} bitcast(transpose)\n+      param_0 = f32[20,20,20]{2,1,0} parameter(0)\n+      transpose = f32[20,20,20]{2,1,0} transpose(param_0), dimensions={0,2,1}\n+      ROOT bitcast = f32[8000]{0} bitcast(transpose)\n     }\n \n-    fused_210_transpose {\n-      param_0 = f32[1024,1024,1024]{2,1,0} parameter(0)\n-      slice = f32[256,512,1024]{2,1,0} slice(param_0), slice={[0:256:1], [0:512:1], [0:1024:1]}\n-      transpose = f32[1024,512,256]{2,1,0} transpose(slice), dimensions={2,1,0}\n-      ROOT bitcast = f32[134217728]{0} bitcast(transpose)\n+    fused_220_transpose {\n+      param_0 = f32[20,20,20]{2,1,0} parameter(0)\n+      transpose = f32[20,20,20]{2,1,0} transpose(param_0), dimensions={2,1,0}\n+      ROOT bitcast = f32[8000]{0} bitcast(transpose)\n     }\n \n     ENTRY reduce {\n-      p0 = f32[1024,1024,1024]{2,1,0} parameter(0)\n-      fusion = f32[134217728]{0} fusion(p0), kind=kInput, calls=fused_021_transpose\n-      fusion.1 = f32[134217728]{0} fusion(p0), kind=kInput, calls=fused_210_transpose\n-      ROOT root = (f32[134217728]{0}, f32[134217728]{0}) tuple(fusion, fusion.1)\n+      p0 = f32[20,20,20]{2,1,0} parameter(0)\n+      fusion = f32[8000]{0} fusion(p0), kind=kInput, calls=fused_021_transpose\n+      fusion.1 = f32[8000]{0} fusion(p0), kind=kInput, calls=fused_220_transpose\n+      ROOT root = (f32[8000]{0}, f32[8000]{0}) tuple(fusion, fusion.1)\n     })\"))\n                     .value();\n   const HloInstruction* fusion_1 =\n"
                }
            ],
            "whole_deleted": "-      param_0 = f32[1024,1024,1024]{2,1,0} parameter(0)\n-      slice = f32[1024,256,512]{2,1,0} slice(param_0), slice={[0:1024:1], [0:256:1], [0:512:1]}\n-      transpose = f32[1024,512,256]{2,1,0} transpose(slice), dimensions={0,2,1}\n-      ROOT bitcast = f32[134217728]{0} bitcast(transpose)\n-    fused_210_transpose {\n-      param_0 = f32[1024,1024,1024]{2,1,0} parameter(0)\n-      slice = f32[256,512,1024]{2,1,0} slice(param_0), slice={[0:256:1], [0:512:1], [0:1024:1]}\n-      transpose = f32[1024,512,256]{2,1,0} transpose(slice), dimensions={2,1,0}\n-      ROOT bitcast = f32[134217728]{0} bitcast(transpose)\n-      p0 = f32[1024,1024,1024]{2,1,0} parameter(0)\n-      fusion = f32[134217728]{0} fusion(p0), kind=kInput, calls=fused_021_transpose\n-      fusion.1 = f32[134217728]{0} fusion(p0), kind=kInput, calls=fused_210_transpose\n-      ROOT root = (f32[134217728]{0}, f32[134217728]{0}) tuple(fusion, fusion.1)\n",
            "whole_added": "+      param_0 = f32[20,20,20]{2,1,0} parameter(0)\n+      transpose = f32[20,20,20]{2,1,0} transpose(param_0), dimensions={0,2,1}\n+      ROOT bitcast = f32[8000]{0} bitcast(transpose)\n+    fused_220_transpose {\n+      param_0 = f32[20,20,20]{2,1,0} parameter(0)\n+      transpose = f32[20,20,20]{2,1,0} transpose(param_0), dimensions={2,1,0}\n+      ROOT bitcast = f32[8000]{0} bitcast(transpose)\n+      p0 = f32[20,20,20]{2,1,0} parameter(0)\n+      fusion = f32[8000]{0} fusion(p0), kind=kInput, calls=fused_021_transpose\n+      fusion.1 = f32[8000]{0} fusion(p0), kind=kInput, calls=fused_220_transpose\n+      ROOT root = (f32[8000]{0}, f32[8000]{0}) tuple(fusion, fusion.1)\n",
            "whole_hunk": "@@ -578,24 +578,22 @@ TEST_F(\n     ShapesCompatibleForMultiOutputFusion_SiblingTransposeFusionsNotCompatible) {\n   auto module = ParseAndReturnVerifiedModule(absl::StrCat(kModulePrefix, R\"(\n     fused_021_transpose {\n-      param_0 = f32[1024,1024,1024]{2,1,0} parameter(0)\n-      slice = f32[1024,256,512]{2,1,0} slice(param_0), slice={[0:1024:1], [0:256:1], [0:512:1]}\n-      transpose = f32[1024,512,256]{2,1,0} transpose(slice), dimensions={0,2,1}\n-      ROOT bitcast = f32[134217728]{0} bitcast(transpose)\n+      param_0 = f32[20,20,20]{2,1,0} parameter(0)\n+      transpose = f32[20,20,20]{2,1,0} transpose(param_0), dimensions={0,2,1}\n+      ROOT bitcast = f32[8000]{0} bitcast(transpose)\n     }\n \n-    fused_210_transpose {\n-      param_0 = f32[1024,1024,1024]{2,1,0} parameter(0)\n-      slice = f32[256,512,1024]{2,1,0} slice(param_0), slice={[0:256:1], [0:512:1], [0:1024:1]}\n-      transpose = f32[1024,512,256]{2,1,0} transpose(slice), dimensions={2,1,0}\n-      ROOT bitcast = f32[134217728]{0} bitcast(transpose)\n+    fused_220_transpose {\n+      param_0 = f32[20,20,20]{2,1,0} parameter(0)\n+      transpose = f32[20,20,20]{2,1,0} transpose(param_0), dimensions={2,1,0}\n+      ROOT bitcast = f32[8000]{0} bitcast(transpose)\n     }\n \n     ENTRY reduce {\n-      p0 = f32[1024,1024,1024]{2,1,0} parameter(0)\n-      fusion = f32[134217728]{0} fusion(p0), kind=kInput, calls=fused_021_transpose\n-      fusion.1 = f32[134217728]{0} fusion(p0), kind=kInput, calls=fused_210_transpose\n-      ROOT root = (f32[134217728]{0}, f32[134217728]{0}) tuple(fusion, fusion.1)\n+      p0 = f32[20,20,20]{2,1,0} parameter(0)\n+      fusion = f32[8000]{0} fusion(p0), kind=kInput, calls=fused_021_transpose\n+      fusion.1 = f32[8000]{0} fusion(p0), kind=kInput, calls=fused_220_transpose\n+      ROOT root = (f32[8000]{0}, f32[8000]{0}) tuple(fusion, fusion.1)\n     })\"))\n                     .value();\n   const HloInstruction* fusion_1 =\n"
        },
        {
            "name": "ir_emitter_unnested.cc",
            "path": "tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc",
            "patches": [
                {
                    "old_start": 4362,
                    "old_length": 17,
                    "new_start": 4362,
                    "new_length": 21,
                    "hunk": "@@ -4362,17 +4362,21 @@ Status IrEmitterUnnested::EmitTransposeTile(\n \n   const Shape& out_shape = first_transpose->shape();\n   const Shape& transpose_in_shape = first_transpose->operand(0)->shape();\n+  Vector3 first_tiled_transpose_permutation =\n+      FindAnyTiledTranspose(*first_transpose)->second;\n \n   // We need the following invariant:\n   // For every tuple element:\n   //  -> EITHER it's a kCopy: S{L} -> S{L'}\n   //  -> OR it's an elementwise op of shape S{L}\n   for (HloInstruction* root : hlo_roots) {\n-    if (FindAnyTiledTranspose(*root)) {\n+    auto tiled_transpose = FindAnyTiledTranspose(*root);\n+    if (tiled_transpose) {\n       const HloInstruction& hero = FindNonTrivialHero(*root);\n       CHECK(ShapeUtil::EqualIgnoringElementType(transpose_in_shape,\n                                                 hero.operand(0)->shape()));\n       CHECK(ShapeUtil::EqualIgnoringElementType(out_shape, hero.shape()));\n+      CHECK(tiled_transpose->second == first_tiled_transpose_permutation);\n     } else {\n       CHECK(ShapeUtil::IsReshapeOrTransposeBitcast(\n           root->shape(), transpose_in_shape,"
                }
            ],
            "whole_deleted": "-    if (FindAnyTiledTranspose(*root)) {\n",
            "whole_added": "+  Vector3 first_tiled_transpose_permutation =\n+      FindAnyTiledTranspose(*first_transpose)->second;\n+    auto tiled_transpose = FindAnyTiledTranspose(*root);\n+    if (tiled_transpose) {\n+      CHECK(tiled_transpose->second == first_tiled_transpose_permutation);\n",
            "whole_hunk": "@@ -4362,17 +4362,21 @@ Status IrEmitterUnnested::EmitTransposeTile(\n \n   const Shape& out_shape = first_transpose->shape();\n   const Shape& transpose_in_shape = first_transpose->operand(0)->shape();\n+  Vector3 first_tiled_transpose_permutation =\n+      FindAnyTiledTranspose(*first_transpose)->second;\n \n   // We need the following invariant:\n   // For every tuple element:\n   //  -> EITHER it's a kCopy: S{L} -> S{L'}\n   //  -> OR it's an elementwise op of shape S{L}\n   for (HloInstruction* root : hlo_roots) {\n-    if (FindAnyTiledTranspose(*root)) {\n+    auto tiled_transpose = FindAnyTiledTranspose(*root);\n+    if (tiled_transpose) {\n       const HloInstruction& hero = FindNonTrivialHero(*root);\n       CHECK(ShapeUtil::EqualIgnoringElementType(transpose_in_shape,\n                                                 hero.operand(0)->shape()));\n       CHECK(ShapeUtil::EqualIgnoringElementType(out_shape, hero.shape()));\n+      CHECK(tiled_transpose->second == first_tiled_transpose_permutation);\n     } else {\n       CHECK(ShapeUtil::IsReshapeOrTransposeBitcast(\n           root->shape(), transpose_in_shape,"
        }
    ]
},
{
    "Id": 323,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/85d5a350394b8b26baed04fbba0c019e1a5693b0",
    "date": "2023-08-10T18:27:12-07:00",
    "message": "Added unit tests, aot check for loadsavedmodel, deserialization process for aot_packages if found, a new aot model and according unit testing.\n\nPiperOrigin-RevId: 555736121",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/mlir/tfrt/BUILD",
            "patches": [
                {
                    "old_start": 367,
                    "old_length": 6,
                    "new_start": 367,
                    "new_length": 8,
                    "hunk": "@@ -367,6 +367,8 @@ cc_library(\n         \"//tensorflow/core/tfrt/fallback:fallback_state\",\n         \"//tensorflow/core/tfrt/runtime\",\n         \"//tensorflow/tsl/platform:errors\",\n+        \"//tensorflow/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n         \"@llvm-project//mlir:FuncExtensions\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Pass\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n",
            "whole_hunk": "@@ -367,6 +367,8 @@ cc_library(\n         \"//tensorflow/core/tfrt/fallback:fallback_state\",\n         \"//tensorflow/core/tfrt/runtime\",\n         \"//tensorflow/tsl/platform:errors\",\n+        \"//tensorflow/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status\",\n         \"@llvm-project//mlir:FuncExtensions\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Pass\",\n"
        },
        {
            "name": "import_model.cc",
            "path": "tensorflow/compiler/mlir/tfrt/translate/import_model.cc",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 6,
                    "new_start": 21,
                    "new_length": 7,
                    "hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/status/status.h\"\n #include \"mlir/Dialect/Func/Extensions/AllExtensions.h\"  // from @llvm-project\n #include \"mlir/IR/Builders.h\"  // from @llvm-project\n #include \"mlir/IR/DialectRegistry.h\"  // from @llvm-project\n"
                },
                {
                    "old_start": 41,
                    "old_length": 7,
                    "new_start": 42,
                    "new_length": 10,
                    "hunk": "@@ -41,7 +42,10 @@ limitations under the License.\n #include \"tensorflow/compiler/mlir/tfrt/translate/tfrt_compile_options.h\"\n #include \"tensorflow/core/common_runtime/function_body.h\"\n #include \"tensorflow/core/common_runtime/function_def_utils.h\"\n+#include \"tensorflow/core/platform/status.h\"\n+#include \"tensorflow/core/tfrt/fallback/fallback_state.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n+#include \"tensorflow/tsl/platform/statusor.h\"\n #include \"tfrt/bef_converter/mlir_to_bef.h\"  // from @tf_runtime\n \n namespace tensorflow {\n"
                },
                {
                    "old_start": 319,
                    "old_length": 4,
                    "new_start": 323,
                    "new_length": 26,
                    "hunk": "@@ -319,4 +323,26 @@ std::unique_ptr<tensorflow::TfrtPipelineOptions> GetTfrtPipelineOptions(\n   return pipeline_options;\n }\n \n+tensorflow::Status RunTFXLABridgeAndAddXlaFunctions(\n+    const TfrtCompileOptions& options, tfrt_stub::FallbackState* fallback_state,\n+    mlir::ModuleOp mlir_module) {\n+  if (options.device_target == TfrtDeviceInfraTarget::kGpu) {\n+    // Update fallback_state\n+\n+    Status status = mlir::TF::RunTFXLABridge(mlir_module);\n+\n+    if (fallback_state != nullptr) {\n+      TF_ASSIGN_OR_RETURN(const std::vector<FunctionDef> xla_func_defs,\n+                          ExportXlaFunctions(mlir_module));\n+      for (const auto& func_def : xla_func_defs) {\n+        TF_RETURN_IF_ERROR(fallback_state->AddFunctionDef(func_def));\n+      }\n+    }\n+    return status;\n+\n+  } else {\n+    return absl::UnimplementedError(\"Non-GPU device_target is not supported.\");\n+  }\n+}\n+\n }  // namespace tensorflow\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"absl/status/status.h\"\n+#include \"tensorflow/core/platform/status.h\"\n+#include \"tensorflow/core/tfrt/fallback/fallback_state.h\"\n+#include \"tensorflow/tsl/platform/statusor.h\"\n+tensorflow::Status RunTFXLABridgeAndAddXlaFunctions(\n+    const TfrtCompileOptions& options, tfrt_stub::FallbackState* fallback_state,\n+    mlir::ModuleOp mlir_module) {\n+  if (options.device_target == TfrtDeviceInfraTarget::kGpu) {\n+    // Update fallback_state\n+\n+    Status status = mlir::TF::RunTFXLABridge(mlir_module);\n+\n+    if (fallback_state != nullptr) {\n+      TF_ASSIGN_OR_RETURN(const std::vector<FunctionDef> xla_func_defs,\n+                          ExportXlaFunctions(mlir_module));\n+      for (const auto& func_def : xla_func_defs) {\n+        TF_RETURN_IF_ERROR(fallback_state->AddFunctionDef(func_def));\n+      }\n+    }\n+    return status;\n+\n+  } else {\n+    return absl::UnimplementedError(\"Non-GPU device_target is not supported.\");\n+  }\n+}\n+\n",
            "whole_hunk": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/status/status.h\"\n #include \"mlir/Dialect/Func/Extensions/AllExtensions.h\"  // from @llvm-project\n #include \"mlir/IR/Builders.h\"  // from @llvm-project\n #include \"mlir/IR/DialectRegistry.h\"  // from @llvm-project\n@@ -41,7 +42,10 @@ limitations under the License.\n #include \"tensorflow/compiler/mlir/tfrt/translate/tfrt_compile_options.h\"\n #include \"tensorflow/core/common_runtime/function_body.h\"\n #include \"tensorflow/core/common_runtime/function_def_utils.h\"\n+#include \"tensorflow/core/platform/status.h\"\n+#include \"tensorflow/core/tfrt/fallback/fallback_state.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n+#include \"tensorflow/tsl/platform/statusor.h\"\n #include \"tfrt/bef_converter/mlir_to_bef.h\"  // from @tf_runtime\n \n namespace tensorflow {\n@@ -319,4 +323,26 @@ std::unique_ptr<tensorflow::TfrtPipelineOptions> GetTfrtPipelineOptions(\n   return pipeline_options;\n }\n \n+tensorflow::Status RunTFXLABridgeAndAddXlaFunctions(\n+    const TfrtCompileOptions& options, tfrt_stub::FallbackState* fallback_state,\n+    mlir::ModuleOp mlir_module) {\n+  if (options.device_target == TfrtDeviceInfraTarget::kGpu) {\n+    // Update fallback_state\n+\n+    Status status = mlir::TF::RunTFXLABridge(mlir_module);\n+\n+    if (fallback_state != nullptr) {\n+      TF_ASSIGN_OR_RETURN(const std::vector<FunctionDef> xla_func_defs,\n+                          ExportXlaFunctions(mlir_module));\n+      for (const auto& func_def : xla_func_defs) {\n+        TF_RETURN_IF_ERROR(fallback_state->AddFunctionDef(func_def));\n+      }\n+    }\n+    return status;\n+\n+  } else {\n+    return absl::UnimplementedError(\"Non-GPU device_target is not supported.\");\n+  }\n+}\n+\n }  // namespace tensorflow\n"
        },
        {
            "name": "import_model.h",
            "path": "tensorflow/compiler/mlir/tfrt/translate/import_model.h",
            "patches": [
                {
                    "old_start": 67,
                    "old_length": 6,
                    "new_start": 67,
                    "new_length": 12,
                    "hunk": "@@ -67,6 +67,12 @@ Status ConvertTfMlirToRuntimeExecutable(\n std::unique_ptr<tensorflow::TfrtPipelineOptions> GetTfrtPipelineOptions(\n     const TfrtCompileOptions& options);\n \n+// TODO(b/295241000): Remove bridge run After MLIR can be deserialized.\n+// AddXLAFunctions will still be needed.\n+tensorflow::Status RunTFXLABridgeAndAddXlaFunctions(\n+    const TfrtCompileOptions& options, tfrt_stub::FallbackState* fallback_state,\n+    mlir::ModuleOp mlir_module);\n+\n }  // namespace tensorflow\n \n #endif  // TENSORFLOW_COMPILER_MLIR_TFRT_TRANSLATE_IMPORT_MODEL_H_\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// TODO(b/295241000): Remove bridge run After MLIR can be deserialized.\n+// AddXLAFunctions will still be needed.\n+tensorflow::Status RunTFXLABridgeAndAddXlaFunctions(\n+    const TfrtCompileOptions& options, tfrt_stub::FallbackState* fallback_state,\n+    mlir::ModuleOp mlir_module);\n+\n",
            "whole_hunk": "@@ -67,6 +67,12 @@ Status ConvertTfMlirToRuntimeExecutable(\n std::unique_ptr<tensorflow::TfrtPipelineOptions> GetTfrtPipelineOptions(\n     const TfrtCompileOptions& options);\n \n+// TODO(b/295241000): Remove bridge run After MLIR can be deserialized.\n+// AddXLAFunctions will still be needed.\n+tensorflow::Status RunTFXLABridgeAndAddXlaFunctions(\n+    const TfrtCompileOptions& options, tfrt_stub::FallbackState* fallback_state,\n+    mlir::ModuleOp mlir_module);\n+\n }  // namespace tensorflow\n \n #endif  // TENSORFLOW_COMPILER_MLIR_TFRT_TRANSLATE_IMPORT_MODEL_H_\n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/core/tfrt/saved_model/BUILD",
            "patches": [
                {
                    "old_start": 77,
                    "old_length": 6,
                    "new_start": 77,
                    "new_length": 7,
                    "hunk": "@@ -77,6 +77,7 @@ cc_library(\n         \"//tensorflow/cc/saved_model:reader\",\n         \"//tensorflow/compiler/mlir/tensorflow\",\n         \"//tensorflow/compiler/mlir/tensorflow:import_model\",\n+        \"//tensorflow/compiler/mlir/tensorflow:tensorflow_passes\",\n         \"//tensorflow/compiler/mlir/tensorflow:translate_lib\",\n         \"//tensorflow/compiler/mlir/tensorflow:upgrade_graph\",\n         \"//tensorflow/compiler/mlir/tfrt:import_model\",\n"
                },
                {
                    "old_start": 88,
                    "old_length": 6,
                    "new_start": 89,
                    "new_length": 7,
                    "hunk": "@@ -88,6 +89,7 @@ cc_library(\n         \"//tensorflow/core:core_cpu_base\",\n         \"//tensorflow/core:framework\",\n         \"//tensorflow/core:lib\",\n+        \"//tensorflow/core/framework:function_proto_cc\",\n         \"//tensorflow/core/framework:graph_proto_cc\",\n         \"//tensorflow/core/framework:tensor_proto_cc\",\n         \"//tensorflow/core/ops\",\n"
                },
                {
                    "old_start": 99,
                    "old_length": 10,
                    "new_start": 101,
                    "new_length": 13,
                    "hunk": "@@ -99,10 +101,13 @@ cc_library(\n         \"//tensorflow/core/runtime_fallback/kernel:kernel_fallback_compat_request_state\",\n         \"//tensorflow/core/runtime_fallback/kernel:kernel_fallback_execute_compat\",\n         \"//tensorflow/core/tfrt/fallback:fallback_state\",\n+        \"//tensorflow/core/tfrt/fallback:op_kernel_runner\",\n         \"//tensorflow/core/tfrt/graph_executor\",\n         \"//tensorflow/core/tfrt/graph_executor:export_mlir\",\n         \"//tensorflow/core/tfrt/graph_executor:graph_execution_options\",\n         \"//tensorflow/core/tfrt/mlrt/bytecode\",\n+        \"//tensorflow/core/tfrt/mlrt/bytecode:executable\",\n+        \"//tensorflow/core/tfrt/mlrt/interpreter:context\",\n         \"//tensorflow/core/tfrt/mlrt/kernel\",\n         \"//tensorflow/core/tfrt/mlrt/kernel:batch_kernel\",\n         \"//tensorflow/core/tfrt/runtime\",\n"
                },
                {
                    "old_start": 117,
                    "old_length": 6,
                    "new_start": 122,
                    "new_length": 7,
                    "hunk": "@@ -117,6 +122,7 @@ cc_library(\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/time\",\n"
                },
                {
                    "old_start": 227,
                    "old_length": 17,
                    "new_start": 233,
                    "new_length": 20,
                    "hunk": "@@ -227,17 +233,20 @@ cc_library(\n         \"//tensorflow/cc/saved_model:reader\",\n         \"//tensorflow/compiler/mlir/tensorflow\",\n         \"//tensorflow/compiler/mlir/tensorflow:import_model\",\n+        \"//tensorflow/compiler/mlir/tfrt:import_model\",\n         \"//tensorflow/compiler/mlir/tfrt:saved_model\",\n         \"//tensorflow/core:framework_types_hdr\",\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core/framework:graph_proto_cc\",\n         \"//tensorflow/core/framework:tensor_proto_cc\",\n+        \"//tensorflow/core/platform:path\",\n         \"//tensorflow/core/platform:thread_annotations\",\n         \"//tensorflow/core/protobuf:for_core_protos_cc\",\n         \"//tensorflow/core/tfrt/fallback:fallback_state\",\n         \"//tensorflow/core/tfrt/graph_executor\",\n         \"//tensorflow/core/tfrt/graph_executor:graph_execution_options\",\n         \"//tensorflow/core/tfrt/runtime\",\n+        \"//tensorflow/core/tfrt/saved_model/utils:serialize_bef_utils\",\n         \"//tensorflow/tsl/platform:protobuf\",\n         \"//tensorflow/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/compiler/mlir/tensorflow:tensorflow_passes\",\n+        \"//tensorflow/core/framework:function_proto_cc\",\n+        \"//tensorflow/core/tfrt/fallback:op_kernel_runner\",\n+        \"//tensorflow/core/tfrt/mlrt/bytecode:executable\",\n+        \"//tensorflow/core/tfrt/mlrt/interpreter:context\",\n+        \"@com_google_absl//absl/status\",\n+        \"//tensorflow/compiler/mlir/tfrt:import_model\",\n+        \"//tensorflow/core/platform:path\",\n+        \"//tensorflow/core/tfrt/saved_model/utils:serialize_bef_utils\",\n",
            "whole_hunk": "@@ -77,6 +77,7 @@ cc_library(\n         \"//tensorflow/cc/saved_model:reader\",\n         \"//tensorflow/compiler/mlir/tensorflow\",\n         \"//tensorflow/compiler/mlir/tensorflow:import_model\",\n+        \"//tensorflow/compiler/mlir/tensorflow:tensorflow_passes\",\n         \"//tensorflow/compiler/mlir/tensorflow:translate_lib\",\n         \"//tensorflow/compiler/mlir/tensorflow:upgrade_graph\",\n         \"//tensorflow/compiler/mlir/tfrt:import_model\",\n@@ -88,6 +89,7 @@ cc_library(\n         \"//tensorflow/core:core_cpu_base\",\n         \"//tensorflow/core:framework\",\n         \"//tensorflow/core:lib\",\n+        \"//tensorflow/core/framework:function_proto_cc\",\n         \"//tensorflow/core/framework:graph_proto_cc\",\n         \"//tensorflow/core/framework:tensor_proto_cc\",\n         \"//tensorflow/core/ops\",\n@@ -99,10 +101,13 @@ cc_library(\n         \"//tensorflow/core/runtime_fallback/kernel:kernel_fallback_compat_request_state\",\n         \"//tensorflow/core/runtime_fallback/kernel:kernel_fallback_execute_compat\",\n         \"//tensorflow/core/tfrt/fallback:fallback_state\",\n+        \"//tensorflow/core/tfrt/fallback:op_kernel_runner\",\n         \"//tensorflow/core/tfrt/graph_executor\",\n         \"//tensorflow/core/tfrt/graph_executor:export_mlir\",\n         \"//tensorflow/core/tfrt/graph_executor:graph_execution_options\",\n         \"//tensorflow/core/tfrt/mlrt/bytecode\",\n+        \"//tensorflow/core/tfrt/mlrt/bytecode:executable\",\n+        \"//tensorflow/core/tfrt/mlrt/interpreter:context\",\n         \"//tensorflow/core/tfrt/mlrt/kernel\",\n         \"//tensorflow/core/tfrt/mlrt/kernel:batch_kernel\",\n         \"//tensorflow/core/tfrt/runtime\",\n@@ -117,6 +122,7 @@ cc_library(\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/time\",\n@@ -227,17 +233,20 @@ cc_library(\n         \"//tensorflow/cc/saved_model:reader\",\n         \"//tensorflow/compiler/mlir/tensorflow\",\n         \"//tensorflow/compiler/mlir/tensorflow:import_model\",\n+        \"//tensorflow/compiler/mlir/tfrt:import_model\",\n         \"//tensorflow/compiler/mlir/tfrt:saved_model\",\n         \"//tensorflow/core:framework_types_hdr\",\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core/framework:graph_proto_cc\",\n         \"//tensorflow/core/framework:tensor_proto_cc\",\n+        \"//tensorflow/core/platform:path\",\n         \"//tensorflow/core/platform:thread_annotations\",\n         \"//tensorflow/core/protobuf:for_core_protos_cc\",\n         \"//tensorflow/core/tfrt/fallback:fallback_state\",\n         \"//tensorflow/core/tfrt/graph_executor\",\n         \"//tensorflow/core/tfrt/graph_executor:graph_execution_options\",\n         \"//tensorflow/core/tfrt/runtime\",\n+        \"//tensorflow/core/tfrt/saved_model/utils:serialize_bef_utils\",\n         \"//tensorflow/tsl/platform:protobuf\",\n         \"//tensorflow/tsl/platform:statusor\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n"
        },
        {
            "name": "saved_model.cc",
            "path": "tensorflow/core/tfrt/saved_model/saved_model.cc",
            "patches": [
                {
                    "old_start": 28,
                    "old_length": 6,
                    "new_start": 28,
                    "new_length": 7,
                    "hunk": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/cleanup/cleanup.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n"
                },
                {
                    "old_start": 36,
                    "old_length": 6,
                    "new_start": 37,
                    "new_length": 7,
                    "hunk": "@@ -36,6 +37,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"mlir/Dialect/Func/Extensions/AllExtensions.h\"  // from @llvm-project\n #include \"mlir/IR/DialectRegistry.h\"  // from @llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n #include \"tensorflow/cc/saved_model/reader.h\"\n #include \"tensorflow/compiler/mlir/tensorflow/ir/tf_saved_model.h\"\n #include \"tensorflow/compiler/mlir/tensorflow/translate/import_model.h\"\n"
                },
                {
                    "old_start": 45,
                    "old_length": 10,
                    "new_start": 47,
                    "new_length": 12,
                    "hunk": "@@ -45,10 +47,12 @@ limitations under the License.\n #include \"tensorflow/compiler/mlir/tfrt/translate/import_model.h\"\n #include \"tensorflow/compiler/mlir/tfrt/translate/tfrt_compile_options.h\"\n #include \"tensorflow/compiler/xla/status_macros.h\"\n+#include \"tensorflow/core/framework/function.pb.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor.pb.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/lib/monitoring/gauge.h\"\n+#include \"tensorflow/core/platform/env.h\"\n #include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/logging.h\"\n #include \"tensorflow/core/platform/mutex.h\"\n"
                },
                {
                    "old_start": 64,
                    "old_length": 14,
                    "new_start": 68,
                    "new_length": 18,
                    "hunk": "@@ -64,14 +68,18 @@ limitations under the License.\n #include \"tensorflow/core/tfrt/graph_executor/graph_execution_options.h\"\n #include \"tensorflow/core/tfrt/graph_executor/graph_executor.h\"\n #include \"tensorflow/core/tfrt/mlrt/bytecode/bytecode.h\"\n+#include \"tensorflow/core/tfrt/mlrt/bytecode/executable.h\"\n+#include \"tensorflow/core/tfrt/mlrt/interpreter/context.h\"\n #include \"tensorflow/core/tfrt/mlrt/kernel/batch_kernel.h\"\n #include \"tensorflow/core/tfrt/mlrt/kernel/kernel.h\"\n+#include \"tensorflow/core/tfrt/runtime/runtime.h\"\n #include \"tensorflow/core/tfrt/runtime/work_queue_interface.h\"\n #include \"tensorflow/core/tfrt/saved_model/saved_model_util.h\"\n #include \"tensorflow/core/tfrt/saved_model/utils/serialize_bef_utils.h\"\n #include \"tensorflow/core/tfrt/utils/error_util.h\"\n #include \"tensorflow/core/tfrt/utils/fallback_tensor.h\"\n #include \"tensorflow/core/tfrt/utils/utils.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/statusor.h\"\n #include \"tfrt/bef/bef_buffer.h\"  // from @tf_runtime\n #include \"tfrt/bef_executor/bef_file.h\"  // from @tf_runtime\n"
                },
                {
                    "old_start": 81,
                    "old_length": 7,
                    "new_start": 89,
                    "new_length": 9,
                    "hunk": "@@ -81,7 +89,9 @@ limitations under the License.\n #include \"tfrt/host_context/execution_context.h\"  // from @tf_runtime\n #include \"tfrt/host_context/function.h\"  // from @tf_runtime\n #include \"tfrt/host_context/host_context.h\"  // from @tf_runtime\n+#include \"tfrt/host_context/kernel_registry.h\"  // from @tf_runtime\n #include \"tfrt/host_context/request_deadline_tracker.h\"  // from @tf_runtime\n+#include \"tfrt/host_context/resource_context.h\"  // from @tf_runtime\n #include \"tfrt/metrics/common_metrics.h\"  // from @tf_runtime\n #include \"tfrt/support/ref_count.h\"  // from @tf_runtime\n \n"
                },
                {
                    "old_start": 326,
                    "old_length": 6,
                    "new_start": 336,
                    "new_length": 12,
                    "hunk": "@@ -326,6 +336,12 @@ tensorflow::Status PreprocessSignature(\n   return OkStatus();\n }\n \n+bool AotPackageExists(absl::string_view saved_model_dir) {\n+  Env* env = Env::Default();\n+  const std::string aot_package_directory = GetAotPackagePath(saved_model_dir);\n+  return env->FileExists(aot_package_directory).ok();\n+}\n+\n }  // namespace\n \n SavedModel::~SavedModel() = default;  // Out-of-line C++ key function.\n"
                },
                {
                    "old_start": 423,
                    "old_length": 6,
                    "new_start": 439,
                    "new_length": 7,
                    "hunk": "@@ -423,6 +439,7 @@ SavedModelImpl::LoadSavedModel(Options options,\n   options.graph_execution_options.compile_options.saved_model_dir =\n       saved_model_dir;\n \n+  // Register TFRT dialects\n   mlir::DialectRegistry registry;\n   RegisterMlirDialect(registry);\n   mlir::MLIRContext context(registry);\n"
                },
                {
                    "old_start": 468,
                    "old_length": 8,
                    "new_start": 486,
                    "new_length": 9,
                    "hunk": "@@ -468,8 +486,9 @@ SavedModelImpl::LoadSavedModel(Options options,\n   SymbolUids symbol_uids;\n   symbol_uids.tf_symbol_uid = MaybeUploadMlirToXsymbol(mlir_module.get());\n \n+  const std::string saved_model_dir_string = std::string(saved_model_dir);\n   const auto import_duration = absl::Now() - import_start_time;\n-  saved_model_import_time_seconds->GetCell(std::string(saved_model_dir))\n+  saved_model_import_time_seconds->GetCell(saved_model_dir_string)\n       ->Set(absl::ToInt64Seconds(import_duration));\n   LOG(INFO) << \"TFRT finished importing savedmodel. Took \"\n             << absl::ToInt64Milliseconds(import_duration) << \" ms.\";\n"
                },
                {
                    "old_start": 494,
                    "old_length": 9,
                    "new_start": 514,
                    "new_length": 6,
                    "hunk": "@@ -494,9 +514,6 @@ SavedModelImpl::LoadSavedModel(Options options,\n   auto resource_array = std::make_unique<tfd::FallbackResourceArray>();\n \n   auto kernel_registry = std::make_unique<mlrt::KernelRegistry>();\n-  // Register infra and standard math kernels\n-  tensorflow::tf_mlrt::RegisterTfMlrtKernels(*kernel_registry);\n-  tensorflow::tf_mlrt::RegisterTfMlrtBatchKernels(*kernel_registry);\n \n   // Creates a ResourceContext and populate it with per model resource from\n   // Runtime.\n"
                },
                {
                    "old_start": 514,
                    "old_length": 6,
                    "new_start": 531,
                    "new_length": 31,
                    "hunk": "@@ -514,6 +531,31 @@ SavedModelImpl::LoadSavedModel(Options options,\n     model_context.set_meta_graph_def(nullptr);\n   }\n \n+  mlrt::bc::Buffer bytecode;\n+  tfrt::BefBuffer bef;\n+  if (AotPackageExists(saved_model_dir)) {\n+    LOG(INFO) << \"Found AoT package\";\n+\n+    ASSIGN_OR_RETURN_IN_COMPILE(\n+        bef, LoadAotPackages(options.graph_execution_options.compile_options,\n+                             mlir_module.get(), saved_model_dir_string, bef,\n+                             fallback_state.get()));\n+  } else {\n+    tensorflow::tf_mlrt::RegisterTfMlrtKernels(*kernel_registry);\n+    tensorflow::tf_mlrt::RegisterTfMlrtBatchKernels(*kernel_registry);\n+\n+    if (options.graph_execution_options.enable_mlrt) {\n+      ASSIGN_OR_RETURN_IN_COMPILE(\n+          bytecode, tensorflow::mlrt_compiler::ConvertTfMlirToBytecode(\n+                        options.graph_execution_options.compile_options,\n+                        *fallback_state, mlir_module.get(), model_context));\n+    } else {\n+      RETURN_IF_ERROR_IN_COMPILE(tensorflow::ConvertTfMlirToBef(\n+          options.graph_execution_options.compile_options, mlir_module.get(),\n+          &bef, model_context, fallback_state.get()));\n+    }\n+  }\n+\n   ASSIGN_OR_RETURN_WITH_STAGE_INFO(\n       \"graph_executor creation\", auto graph_executor,\n       GraphExecutor::Create(options.graph_execution_options, *fallback_state,\n"
                },
                {
                    "old_start": 521,
                    "old_length": 21,
                    "new_start": 563,
                    "new_length": 9,
                    "hunk": "@@ -521,21 +563,9 @@ SavedModelImpl::LoadSavedModel(Options options,\n                             std::move(*meta_graph_def.mutable_graph_def()),\n                             std::move(kernel_registry)));\n \n-  mlrt::bc::Buffer bytecode;\n-  tfrt::BefBuffer bef;\n-  if (options.graph_execution_options.enable_mlrt) {\n-    ASSIGN_OR_RETURN_IN_COMPILE(\n-        bytecode, tensorflow::mlrt_compiler::ConvertTfMlirToBytecode(\n-                      options.graph_execution_options.compile_options,\n-                      *fallback_state, mlir_module.get(), model_context));\n-  } else {\n-    RETURN_IF_ERROR_IN_COMPILE(tensorflow::ConvertTfMlirToBef(\n-        options.graph_execution_options.compile_options, mlir_module.get(),\n-        &bef, model_context, fallback_state.get()));\n-  }\n   symbol_uids.tfrt_symbol_uid = MaybeUploadMlirToXsymbol(mlir_module.get());\n   const auto compile_duration = absl::Now() - compile_start_time;\n-  saved_model_compile_time_seconds->GetCell(std::string(saved_model_dir))\n+  saved_model_compile_time_seconds->GetCell(saved_model_dir_string)\n       ->Set(absl::ToInt64Seconds(compile_duration));\n   LOG(INFO) << \"TFRT finished compiling savedmodel. Took \"\n             << absl::ToInt64Milliseconds(compile_duration) << \" ms.\";\n"
                },
                {
                    "old_start": 550,
                    "old_length": 13,
                    "new_start": 580,
                    "new_length": 6,
                    "hunk": "@@ -550,13 +580,6 @@ SavedModelImpl::LoadSavedModel(Options options,\n                               graph_executor->kernel_registry());\n   } else {\n     DCHECK(!bef.empty());\n-    // TODO(cesarmagana)\n-    // Call code if bef exists, make into its own util\n-    // Deserialization is only called if BEF is found\n-\n-    // and if bef file exists this will be called\n-    // Create another function where we first detect if bef_file exists in\n-    // saved_model dir then we run code below if not we call original code.\n     ASSIGN_OR_RETURN_IN_INIT(\n         bef_file, tfrt::CreateBefFileFromBefBuffer(\n                       *options.graph_execution_options.runtime, bef));\n"
                },
                {
                    "old_start": 576,
                    "old_length": 7,
                    "new_start": 598,
                    "new_length": 7,
                    "hunk": "@@ -576,7 +598,7 @@ SavedModelImpl::LoadSavedModel(Options options,\n   }\n \n   const auto init_duration = absl::Now() - init_start_time;\n-  saved_model_init_time_seconds->GetCell(std::string(saved_model_dir))\n+  saved_model_init_time_seconds->GetCell(saved_model_dir_string)\n       ->Set(absl::ToInt64Seconds(init_duration));\n   LOG(INFO) << \"TFRT finished initializing savedmodel. Took \"\n             << absl::ToInt64Milliseconds(init_duration) << \" ms.\";\n"
                }
            ],
            "whole_deleted": "-  saved_model_import_time_seconds->GetCell(std::string(saved_model_dir))\n-  // Register infra and standard math kernels\n-  tensorflow::tf_mlrt::RegisterTfMlrtKernels(*kernel_registry);\n-  tensorflow::tf_mlrt::RegisterTfMlrtBatchKernels(*kernel_registry);\n-  mlrt::bc::Buffer bytecode;\n-  tfrt::BefBuffer bef;\n-  if (options.graph_execution_options.enable_mlrt) {\n-    ASSIGN_OR_RETURN_IN_COMPILE(\n-        bytecode, tensorflow::mlrt_compiler::ConvertTfMlirToBytecode(\n-                      options.graph_execution_options.compile_options,\n-                      *fallback_state, mlir_module.get(), model_context));\n-  } else {\n-    RETURN_IF_ERROR_IN_COMPILE(tensorflow::ConvertTfMlirToBef(\n-        options.graph_execution_options.compile_options, mlir_module.get(),\n-        &bef, model_context, fallback_state.get()));\n-  }\n-  saved_model_compile_time_seconds->GetCell(std::string(saved_model_dir))\n-    // TODO(cesarmagana)\n-    // Call code if bef exists, make into its own util\n-    // Deserialization is only called if BEF is found\n-\n-    // and if bef file exists this will be called\n-    // Create another function where we first detect if bef_file exists in\n-    // saved_model dir then we run code below if not we call original code.\n-  saved_model_init_time_seconds->GetCell(std::string(saved_model_dir))\n",
            "whole_added": "+#include \"absl/status/status.h\"\n+#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n+#include \"tensorflow/core/framework/function.pb.h\"\n+#include \"tensorflow/core/platform/env.h\"\n+#include \"tensorflow/core/tfrt/mlrt/bytecode/executable.h\"\n+#include \"tensorflow/core/tfrt/mlrt/interpreter/context.h\"\n+#include \"tensorflow/core/tfrt/runtime/runtime.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n+#include \"tfrt/host_context/kernel_registry.h\"  // from @tf_runtime\n+#include \"tfrt/host_context/resource_context.h\"  // from @tf_runtime\n+bool AotPackageExists(absl::string_view saved_model_dir) {\n+  Env* env = Env::Default();\n+  const std::string aot_package_directory = GetAotPackagePath(saved_model_dir);\n+  return env->FileExists(aot_package_directory).ok();\n+}\n+\n+  // Register TFRT dialects\n+  const std::string saved_model_dir_string = std::string(saved_model_dir);\n+  saved_model_import_time_seconds->GetCell(saved_model_dir_string)\n+  mlrt::bc::Buffer bytecode;\n+  tfrt::BefBuffer bef;\n+  if (AotPackageExists(saved_model_dir)) {\n+    LOG(INFO) << \"Found AoT package\";\n+\n+    ASSIGN_OR_RETURN_IN_COMPILE(\n+        bef, LoadAotPackages(options.graph_execution_options.compile_options,\n+                             mlir_module.get(), saved_model_dir_string, bef,\n+                             fallback_state.get()));\n+  } else {\n+    tensorflow::tf_mlrt::RegisterTfMlrtKernels(*kernel_registry);\n+    tensorflow::tf_mlrt::RegisterTfMlrtBatchKernels(*kernel_registry);\n+\n+    if (options.graph_execution_options.enable_mlrt) {\n+      ASSIGN_OR_RETURN_IN_COMPILE(\n+          bytecode, tensorflow::mlrt_compiler::ConvertTfMlirToBytecode(\n+                        options.graph_execution_options.compile_options,\n+                        *fallback_state, mlir_module.get(), model_context));\n+    } else {\n+      RETURN_IF_ERROR_IN_COMPILE(tensorflow::ConvertTfMlirToBef(\n+          options.graph_execution_options.compile_options, mlir_module.get(),\n+          &bef, model_context, fallback_state.get()));\n+    }\n+  }\n+\n+  saved_model_compile_time_seconds->GetCell(saved_model_dir_string)\n+  saved_model_init_time_seconds->GetCell(saved_model_dir_string)\n",
            "whole_hunk": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"absl/cleanup/cleanup.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n@@ -36,6 +37,7 @@ limitations under the License.\n #include \"absl/types/span.h\"\n #include \"mlir/Dialect/Func/Extensions/AllExtensions.h\"  // from @llvm-project\n #include \"mlir/IR/DialectRegistry.h\"  // from @llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n #include \"tensorflow/cc/saved_model/reader.h\"\n #include \"tensorflow/compiler/mlir/tensorflow/ir/tf_saved_model.h\"\n #include \"tensorflow/compiler/mlir/tensorflow/translate/import_model.h\"\n@@ -45,10 +47,12 @@ limitations under the License.\n #include \"tensorflow/compiler/mlir/tfrt/translate/import_model.h\"\n #include \"tensorflow/compiler/mlir/tfrt/translate/tfrt_compile_options.h\"\n #include \"tensorflow/compiler/xla/status_macros.h\"\n+#include \"tensorflow/core/framework/function.pb.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor.pb.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/lib/monitoring/gauge.h\"\n+#include \"tensorflow/core/platform/env.h\"\n #include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/logging.h\"\n #include \"tensorflow/core/platform/mutex.h\"\n@@ -64,14 +68,18 @@ limitations under the License.\n #include \"tensorflow/core/tfrt/graph_executor/graph_execution_options.h\"\n #include \"tensorflow/core/tfrt/graph_executor/graph_executor.h\"\n #include \"tensorflow/core/tfrt/mlrt/bytecode/bytecode.h\"\n+#include \"tensorflow/core/tfrt/mlrt/bytecode/executable.h\"\n+#include \"tensorflow/core/tfrt/mlrt/interpreter/context.h\"\n #include \"tensorflow/core/tfrt/mlrt/kernel/batch_kernel.h\"\n #include \"tensorflow/core/tfrt/mlrt/kernel/kernel.h\"\n+#include \"tensorflow/core/tfrt/runtime/runtime.h\"\n #include \"tensorflow/core/tfrt/runtime/work_queue_interface.h\"\n #include \"tensorflow/core/tfrt/saved_model/saved_model_util.h\"\n #include \"tensorflow/core/tfrt/saved_model/utils/serialize_bef_utils.h\"\n #include \"tensorflow/core/tfrt/utils/error_util.h\"\n #include \"tensorflow/core/tfrt/utils/fallback_tensor.h\"\n #include \"tensorflow/core/tfrt/utils/utils.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/statusor.h\"\n #include \"tfrt/bef/bef_buffer.h\"  // from @tf_runtime\n #include \"tfrt/bef_executor/bef_file.h\"  // from @tf_runtime\n@@ -81,7 +89,9 @@ limitations under the License.\n #include \"tfrt/host_context/execution_context.h\"  // from @tf_runtime\n #include \"tfrt/host_context/function.h\"  // from @tf_runtime\n #include \"tfrt/host_context/host_context.h\"  // from @tf_runtime\n+#include \"tfrt/host_context/kernel_registry.h\"  // from @tf_runtime\n #include \"tfrt/host_context/request_deadline_tracker.h\"  // from @tf_runtime\n+#include \"tfrt/host_context/resource_context.h\"  // from @tf_runtime\n #include \"tfrt/metrics/common_metrics.h\"  // from @tf_runtime\n #include \"tfrt/support/ref_count.h\"  // from @tf_runtime\n \n@@ -326,6 +336,12 @@ tensorflow::Status PreprocessSignature(\n   return OkStatus();\n }\n \n+bool AotPackageExists(absl::string_view saved_model_dir) {\n+  Env* env = Env::Default();\n+  const std::string aot_package_directory = GetAotPackagePath(saved_model_dir);\n+  return env->FileExists(aot_package_directory).ok();\n+}\n+\n }  // namespace\n \n SavedModel::~SavedModel() = default;  // Out-of-line C++ key function.\n@@ -423,6 +439,7 @@ SavedModelImpl::LoadSavedModel(Options options,\n   options.graph_execution_options.compile_options.saved_model_dir =\n       saved_model_dir;\n \n+  // Register TFRT dialects\n   mlir::DialectRegistry registry;\n   RegisterMlirDialect(registry);\n   mlir::MLIRContext context(registry);\n@@ -468,8 +486,9 @@ SavedModelImpl::LoadSavedModel(Options options,\n   SymbolUids symbol_uids;\n   symbol_uids.tf_symbol_uid = MaybeUploadMlirToXsymbol(mlir_module.get());\n \n+  const std::string saved_model_dir_string = std::string(saved_model_dir);\n   const auto import_duration = absl::Now() - import_start_time;\n-  saved_model_import_time_seconds->GetCell(std::string(saved_model_dir))\n+  saved_model_import_time_seconds->GetCell(saved_model_dir_string)\n       ->Set(absl::ToInt64Seconds(import_duration));\n   LOG(INFO) << \"TFRT finished importing savedmodel. Took \"\n             << absl::ToInt64Milliseconds(import_duration) << \" ms.\";\n@@ -494,9 +514,6 @@ SavedModelImpl::LoadSavedModel(Options options,\n   auto resource_array = std::make_unique<tfd::FallbackResourceArray>();\n \n   auto kernel_registry = std::make_unique<mlrt::KernelRegistry>();\n-  // Register infra and standard math kernels\n-  tensorflow::tf_mlrt::RegisterTfMlrtKernels(*kernel_registry);\n-  tensorflow::tf_mlrt::RegisterTfMlrtBatchKernels(*kernel_registry);\n \n   // Creates a ResourceContext and populate it with per model resource from\n   // Runtime.\n@@ -514,6 +531,31 @@ SavedModelImpl::LoadSavedModel(Options options,\n     model_context.set_meta_graph_def(nullptr);\n   }\n \n+  mlrt::bc::Buffer bytecode;\n+  tfrt::BefBuffer bef;\n+  if (AotPackageExists(saved_model_dir)) {\n+    LOG(INFO) << \"Found AoT package\";\n+\n+    ASSIGN_OR_RETURN_IN_COMPILE(\n+        bef, LoadAotPackages(options.graph_execution_options.compile_options,\n+                             mlir_module.get(), saved_model_dir_string, bef,\n+                             fallback_state.get()));\n+  } else {\n+    tensorflow::tf_mlrt::RegisterTfMlrtKernels(*kernel_registry);\n+    tensorflow::tf_mlrt::RegisterTfMlrtBatchKernels(*kernel_registry);\n+\n+    if (options.graph_execution_options.enable_mlrt) {\n+      ASSIGN_OR_RETURN_IN_COMPILE(\n+          bytecode, tensorflow::mlrt_compiler::ConvertTfMlirToBytecode(\n+                        options.graph_execution_options.compile_options,\n+                        *fallback_state, mlir_module.get(), model_context));\n+    } else {\n+      RETURN_IF_ERROR_IN_COMPILE(tensorflow::ConvertTfMlirToBef(\n+          options.graph_execution_options.compile_options, mlir_module.get(),\n+          &bef, model_context, fallback_state.get()));\n+    }\n+  }\n+\n   ASSIGN_OR_RETURN_WITH_STAGE_INFO(\n       \"graph_executor creation\", auto graph_executor,\n       GraphExecutor::Create(options.graph_execution_options, *fallback_state,\n@@ -521,21 +563,9 @@ SavedModelImpl::LoadSavedModel(Options options,\n                             std::move(*meta_graph_def.mutable_graph_def()),\n                             std::move(kernel_registry)));\n \n-  mlrt::bc::Buffer bytecode;\n-  tfrt::BefBuffer bef;\n-  if (options.graph_execution_options.enable_mlrt) {\n-    ASSIGN_OR_RETURN_IN_COMPILE(\n-        bytecode, tensorflow::mlrt_compiler::ConvertTfMlirToBytecode(\n-                      options.graph_execution_options.compile_options,\n-                      *fallback_state, mlir_module.get(), model_context));\n-  } else {\n-    RETURN_IF_ERROR_IN_COMPILE(tensorflow::ConvertTfMlirToBef(\n-        options.graph_execution_options.compile_options, mlir_module.get(),\n-        &bef, model_context, fallback_state.get()));\n-  }\n   symbol_uids.tfrt_symbol_uid = MaybeUploadMlirToXsymbol(mlir_module.get());\n   const auto compile_duration = absl::Now() - compile_start_time;\n-  saved_model_compile_time_seconds->GetCell(std::string(saved_model_dir))\n+  saved_model_compile_time_seconds->GetCell(saved_model_dir_string)\n       ->Set(absl::ToInt64Seconds(compile_duration));\n   LOG(INFO) << \"TFRT finished compiling savedmodel. Took \"\n             << absl::ToInt64Milliseconds(compile_duration) << \" ms.\";\n@@ -550,13 +580,6 @@ SavedModelImpl::LoadSavedModel(Options options,\n                               graph_executor->kernel_registry());\n   } else {\n     DCHECK(!bef.empty());\n-    // TODO(cesarmagana)\n-    // Call code if bef exists, make into its own util\n-    // Deserialization is only called if BEF is found\n-\n-    // and if bef file exists this will be called\n-    // Create another function where we first detect if bef_file exists in\n-    // saved_model dir then we run code below if not we call original code.\n     ASSIGN_OR_RETURN_IN_INIT(\n         bef_file, tfrt::CreateBefFileFromBefBuffer(\n                       *options.graph_execution_options.runtime, bef));\n@@ -576,7 +598,7 @@ SavedModelImpl::LoadSavedModel(Options options,\n   }\n \n   const auto init_duration = absl::Now() - init_start_time;\n-  saved_model_init_time_seconds->GetCell(std::string(saved_model_dir))\n+  saved_model_init_time_seconds->GetCell(saved_model_dir_string)\n       ->Set(absl::ToInt64Seconds(init_duration));\n   LOG(INFO) << \"TFRT finished initializing savedmodel. Took \"\n             << absl::ToInt64Milliseconds(init_duration) << \" ms.\";\n"
        },
        {
            "name": "saved_model_aot_compile.cc",
            "path": "tensorflow/core/tfrt/saved_model/saved_model_aot_compile.cc",
            "patches": [
                {
                    "old_start": 175,
                    "old_length": 35,
                    "new_start": 175,
                    "new_length": 33,
                    "hunk": "@@ -175,35 +175,33 @@ Status AotCompileSavedModel(absl::string_view input_model_dir,\n     TF_RETURN_IF_ERROR(env->RecursivelyCreateDir(output_dir, {}));\n   }\n   const std::string aot_directory =\n-      io::JoinPath(std::string(output_model_dir), \"aot_packages\");\n+      io::JoinPath(output_dir, kAoTPackagesDirectory);\n   TF_RETURN_IF_ERROR(env->RecursivelyCreateDir(aot_directory));\n \n   // Serialize MLIR to a file under aot_packages\n   const std::string mlir_module_file =\n-      io::JoinPath(std::string(aot_directory), \"serialized_mlir.mlir\");\n+      io::JoinPath(aot_directory, kMLIRModuleFilename);\n   std::string mlir_module_string = SerializeMlirModule(mlir_module.get());\n   TF_RETURN_IF_ERROR(\n       WriteStringToFile(env, mlir_module_file, mlir_module_string));\n \n   // Serialize BEF buffer to a file under aot_packages\n   const std::string serialized_bef_path =\n-      io::JoinPath(aot_directory, \"serialized_bef.mlir.bef\");\n+      io::JoinPath(aot_directory, kBefBufferFilenameMLIRBEF);\n   TF_RETURN_IF_ERROR(SerializeBEF(bef, serialized_bef_path));\n \n   if (pb_found) {\n     const std::string output_file_directory =\n-        io::JoinPath(std::string(output_model_dir),\n-                     absl::StrCat(\"aot_\", kSavedModelFilenamePb));\n+        io::JoinPath(std::string(output_model_dir), kSavedModelFilenamePb);\n     return env->CopyFile(saved_model_pb_path, output_file_directory);\n   } else {\n     const std::string output_file_directory =\n-        io::JoinPath(std::string(output_model_dir),\n-                     absl::StrCat(\"aot_\", kSavedModelFilenamePbTxt));\n+        io::JoinPath(std::string(output_model_dir), kSavedModelFilenamePbTxt);\n     return env->CopyFile(saved_model_pbtxt_path, output_file_directory);\n   }\n }\n \n-// TODO: b/294095043 - Create a a function (ex Status\n+// TODO(b/294095043): Create a function (ex Status\n // SerializeAotResult(AotResult)) to avoid using temp directories.\n \n }  // namespace tensorflow::tfrt_stub\n"
                }
            ],
            "whole_deleted": "-      io::JoinPath(std::string(output_model_dir), \"aot_packages\");\n-      io::JoinPath(std::string(aot_directory), \"serialized_mlir.mlir\");\n-      io::JoinPath(aot_directory, \"serialized_bef.mlir.bef\");\n-        io::JoinPath(std::string(output_model_dir),\n-                     absl::StrCat(\"aot_\", kSavedModelFilenamePb));\n-        io::JoinPath(std::string(output_model_dir),\n-                     absl::StrCat(\"aot_\", kSavedModelFilenamePbTxt));\n-// TODO: b/294095043 - Create a a function (ex Status\n",
            "whole_added": "+      io::JoinPath(output_dir, kAoTPackagesDirectory);\n+      io::JoinPath(aot_directory, kMLIRModuleFilename);\n+      io::JoinPath(aot_directory, kBefBufferFilenameMLIRBEF);\n+        io::JoinPath(std::string(output_model_dir), kSavedModelFilenamePb);\n+        io::JoinPath(std::string(output_model_dir), kSavedModelFilenamePbTxt);\n+// TODO(b/294095043): Create a function (ex Status\n",
            "whole_hunk": "@@ -175,35 +175,33 @@ Status AotCompileSavedModel(absl::string_view input_model_dir,\n     TF_RETURN_IF_ERROR(env->RecursivelyCreateDir(output_dir, {}));\n   }\n   const std::string aot_directory =\n-      io::JoinPath(std::string(output_model_dir), \"aot_packages\");\n+      io::JoinPath(output_dir, kAoTPackagesDirectory);\n   TF_RETURN_IF_ERROR(env->RecursivelyCreateDir(aot_directory));\n \n   // Serialize MLIR to a file under aot_packages\n   const std::string mlir_module_file =\n-      io::JoinPath(std::string(aot_directory), \"serialized_mlir.mlir\");\n+      io::JoinPath(aot_directory, kMLIRModuleFilename);\n   std::string mlir_module_string = SerializeMlirModule(mlir_module.get());\n   TF_RETURN_IF_ERROR(\n       WriteStringToFile(env, mlir_module_file, mlir_module_string));\n \n   // Serialize BEF buffer to a file under aot_packages\n   const std::string serialized_bef_path =\n-      io::JoinPath(aot_directory, \"serialized_bef.mlir.bef\");\n+      io::JoinPath(aot_directory, kBefBufferFilenameMLIRBEF);\n   TF_RETURN_IF_ERROR(SerializeBEF(bef, serialized_bef_path));\n \n   if (pb_found) {\n     const std::string output_file_directory =\n-        io::JoinPath(std::string(output_model_dir),\n-                     absl::StrCat(\"aot_\", kSavedModelFilenamePb));\n+        io::JoinPath(std::string(output_model_dir), kSavedModelFilenamePb);\n     return env->CopyFile(saved_model_pb_path, output_file_directory);\n   } else {\n     const std::string output_file_directory =\n-        io::JoinPath(std::string(output_model_dir),\n-                     absl::StrCat(\"aot_\", kSavedModelFilenamePbTxt));\n+        io::JoinPath(std::string(output_model_dir), kSavedModelFilenamePbTxt);\n     return env->CopyFile(saved_model_pbtxt_path, output_file_directory);\n   }\n }\n \n-// TODO: b/294095043 - Create a a function (ex Status\n+// TODO(b/294095043): Create a function (ex Status\n // SerializeAotResult(AotResult)) to avoid using temp directories.\n \n }  // namespace tensorflow::tfrt_stub\n"
        },
        {
            "name": "saved_model_util.cc",
            "path": "tensorflow/core/tfrt/saved_model/saved_model_util.cc",
            "patches": [
                {
                    "old_start": 39,
                    "old_length": 13,
                    "new_start": 39,
                    "new_length": 16,
                    "hunk": "@@ -39,13 +39,16 @@ limitations under the License.\n #include \"tensorflow/compiler/mlir/tensorflow/ir/tf_saved_model.h\"\n #include \"tensorflow/compiler/mlir/tensorflow/translate/import_model.h\"\n #include \"tensorflow/compiler/mlir/tfrt/saved_model/saved_model.h\"\n+#include \"tensorflow/compiler/mlir/tfrt/translate/import_model.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/lib/monitoring/gauge.h\"\n #include \"tensorflow/core/protobuf/meta_graph.pb.h\"\n #include \"tensorflow/core/protobuf/rewriter_config.pb.h\"\n #include \"tensorflow/core/tfrt/fallback/fallback_state.h\"\n #include \"tensorflow/core/tfrt/saved_model/saved_model_import_input.h\"\n+#include \"tensorflow/core/tfrt/saved_model/utils/serialize_bef_utils.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n+#include \"tensorflow/tsl/platform/path.h\"\n #include \"tensorflow/tsl/platform/statusor.h\"\n \n namespace tensorflow {\n"
                },
                {
                    "old_start": 213,
                    "old_length": 5,
                    "new_start": 216,
                    "new_length": 36,
                    "hunk": "@@ -213,5 +216,36 @@ StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> ImportSavedModel(\n   return module;\n }\n \n+std::string GetAotPackagePath(absl::string_view saved_model_dir) {\n+  return tsl::io::JoinPath(std::string(saved_model_dir), kAoTPackagesDirectory);\n+}\n+\n+std::string GetBEFFilePath(std::string aot_package_directory) {\n+  return tsl::io::JoinPath(aot_package_directory,\n+                           std::string(kBefBufferFilenameMLIRBEF));\n+}\n+\n+// TODO(b/295241000): Implement MLIR deserialization to skip it AoT and remove\n+// redundant steps\n+absl::StatusOr<tfrt::BefBuffer> LoadAotPackages(\n+    const TfrtCompileOptions& options, mlir::ModuleOp mlir_module,\n+    const std::string& saved_model_dir, tfrt::BefBuffer bef,\n+    tfrt_stub::FallbackState* fallback_state) {\n+  const std::string aot_package_directory = GetAotPackagePath(saved_model_dir);\n+  // Deserialize BEF buffer\n+  const std::string bef_file_path =\n+      tfrt_stub::GetBEFFilePath(aot_package_directory);\n+  TF_ASSIGN_OR_RETURN(bef, DeserializeBEFBuffer(bef_file_path));\n+\n+  if (bef.empty()) {\n+    return absl::InternalError(\"BefBuffer is empty.\");\n+  }\n+  // TODO (b/295241000): Currently AoT for TFRT only supports GPU so we only\n+  // check for GPU. Remove after MLIR deserialization.\n+  TF_RETURN_IF_ERROR(\n+      RunTFXLABridgeAndAddXlaFunctions(options, fallback_state, mlir_module));\n+  return bef;\n+}\n+\n }  // namespace tfrt_stub\n }  // namespace tensorflow\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"tensorflow/compiler/mlir/tfrt/translate/import_model.h\"\n+#include \"tensorflow/core/tfrt/saved_model/utils/serialize_bef_utils.h\"\n+#include \"tensorflow/tsl/platform/path.h\"\n+std::string GetAotPackagePath(absl::string_view saved_model_dir) {\n+  return tsl::io::JoinPath(std::string(saved_model_dir), kAoTPackagesDirectory);\n+}\n+\n+std::string GetBEFFilePath(std::string aot_package_directory) {\n+  return tsl::io::JoinPath(aot_package_directory,\n+                           std::string(kBefBufferFilenameMLIRBEF));\n+}\n+\n+// TODO(b/295241000): Implement MLIR deserialization to skip it AoT and remove\n+// redundant steps\n+absl::StatusOr<tfrt::BefBuffer> LoadAotPackages(\n+    const TfrtCompileOptions& options, mlir::ModuleOp mlir_module,\n+    const std::string& saved_model_dir, tfrt::BefBuffer bef,\n+    tfrt_stub::FallbackState* fallback_state) {\n+  const std::string aot_package_directory = GetAotPackagePath(saved_model_dir);\n+  // Deserialize BEF buffer\n+  const std::string bef_file_path =\n+      tfrt_stub::GetBEFFilePath(aot_package_directory);\n+  TF_ASSIGN_OR_RETURN(bef, DeserializeBEFBuffer(bef_file_path));\n+\n+  if (bef.empty()) {\n+    return absl::InternalError(\"BefBuffer is empty.\");\n+  }\n+  // TODO (b/295241000): Currently AoT for TFRT only supports GPU so we only\n+  // check for GPU. Remove after MLIR deserialization.\n+  TF_RETURN_IF_ERROR(\n+      RunTFXLABridgeAndAddXlaFunctions(options, fallback_state, mlir_module));\n+  return bef;\n+}\n+\n",
            "whole_hunk": "@@ -39,13 +39,16 @@ limitations under the License.\n #include \"tensorflow/compiler/mlir/tensorflow/ir/tf_saved_model.h\"\n #include \"tensorflow/compiler/mlir/tensorflow/translate/import_model.h\"\n #include \"tensorflow/compiler/mlir/tfrt/saved_model/saved_model.h\"\n+#include \"tensorflow/compiler/mlir/tfrt/translate/import_model.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/lib/monitoring/gauge.h\"\n #include \"tensorflow/core/protobuf/meta_graph.pb.h\"\n #include \"tensorflow/core/protobuf/rewriter_config.pb.h\"\n #include \"tensorflow/core/tfrt/fallback/fallback_state.h\"\n #include \"tensorflow/core/tfrt/saved_model/saved_model_import_input.h\"\n+#include \"tensorflow/core/tfrt/saved_model/utils/serialize_bef_utils.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n+#include \"tensorflow/tsl/platform/path.h\"\n #include \"tensorflow/tsl/platform/statusor.h\"\n \n namespace tensorflow {\n@@ -213,5 +216,36 @@ StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> ImportSavedModel(\n   return module;\n }\n \n+std::string GetAotPackagePath(absl::string_view saved_model_dir) {\n+  return tsl::io::JoinPath(std::string(saved_model_dir), kAoTPackagesDirectory);\n+}\n+\n+std::string GetBEFFilePath(std::string aot_package_directory) {\n+  return tsl::io::JoinPath(aot_package_directory,\n+                           std::string(kBefBufferFilenameMLIRBEF));\n+}\n+\n+// TODO(b/295241000): Implement MLIR deserialization to skip it AoT and remove\n+// redundant steps\n+absl::StatusOr<tfrt::BefBuffer> LoadAotPackages(\n+    const TfrtCompileOptions& options, mlir::ModuleOp mlir_module,\n+    const std::string& saved_model_dir, tfrt::BefBuffer bef,\n+    tfrt_stub::FallbackState* fallback_state) {\n+  const std::string aot_package_directory = GetAotPackagePath(saved_model_dir);\n+  // Deserialize BEF buffer\n+  const std::string bef_file_path =\n+      tfrt_stub::GetBEFFilePath(aot_package_directory);\n+  TF_ASSIGN_OR_RETURN(bef, DeserializeBEFBuffer(bef_file_path));\n+\n+  if (bef.empty()) {\n+    return absl::InternalError(\"BefBuffer is empty.\");\n+  }\n+  // TODO (b/295241000): Currently AoT for TFRT only supports GPU so we only\n+  // check for GPU. Remove after MLIR deserialization.\n+  TF_RETURN_IF_ERROR(\n+      RunTFXLABridgeAndAddXlaFunctions(options, fallback_state, mlir_module));\n+  return bef;\n+}\n+\n }  // namespace tfrt_stub\n }  // namespace tensorflow\n"
        },
        {
            "name": "saved_model_util.h",
            "path": "tensorflow/core/tfrt/saved_model/saved_model_util.h",
            "patches": [
                {
                    "old_start": 46,
                    "old_length": 6,
                    "new_start": 46,
                    "new_length": 15,
                    "hunk": "@@ -46,6 +46,15 @@ limitations under the License.\n namespace tensorflow {\n namespace tfrt_stub {\n \n+// Filename for serialized BEF Buffer.\n+inline constexpr char kBefBufferFilenameMLIRBEF[] = \"serialized_bef.mlir.bef\";\n+\n+// Filename for serialized MLIR_MODULE.\n+inline constexpr char kMLIRModuleFilename[] = \"serialized_mlir.mlir\";\n+\n+// Subdirectory where AoT Packages are saved\n+inline constexpr char kAoTPackagesDirectory[] = \"aot_packages\";\n+\n // TODO(tfrt-dev): Replace tfrt::TensorSpec with tensorflow::TensorSpec once the\n // latter is checked in.\n struct TensorSpec {\n"
                },
                {
                    "old_start": 103,
                    "old_length": 6,
                    "new_start": 112,
                    "new_length": 17,
                    "hunk": "@@ -103,6 +112,17 @@ struct InitializersAndSignatures {\n StatusOr<InitializersAndSignatures> GetInitializersAndSignatures(\n     mlir::ModuleOp module);\n \n+std::string GetAotPackagePath(absl::string_view saved_model_dir);\n+\n+std::string GetBEFFilePath(std::string aot_package_directory);\n+\n+// TODO(b/295241000): Implement MLIR deserialization to skip it AoT and remove\n+// redundant steps\n+absl::StatusOr<tfrt::BefBuffer> LoadAotPackages(\n+    const TfrtCompileOptions& options, mlir::ModuleOp mlir_module,\n+    const std::string& saved_model_dir, tfrt::BefBuffer bef,\n+    tfrt_stub::FallbackState* fallback_state);\n+\n }  // namespace tfrt_stub\n }  // namespace tensorflow\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// Filename for serialized BEF Buffer.\n+inline constexpr char kBefBufferFilenameMLIRBEF[] = \"serialized_bef.mlir.bef\";\n+\n+// Filename for serialized MLIR_MODULE.\n+inline constexpr char kMLIRModuleFilename[] = \"serialized_mlir.mlir\";\n+\n+// Subdirectory where AoT Packages are saved\n+inline constexpr char kAoTPackagesDirectory[] = \"aot_packages\";\n+\n+std::string GetAotPackagePath(absl::string_view saved_model_dir);\n+\n+std::string GetBEFFilePath(std::string aot_package_directory);\n+\n+// TODO(b/295241000): Implement MLIR deserialization to skip it AoT and remove\n+// redundant steps\n+absl::StatusOr<tfrt::BefBuffer> LoadAotPackages(\n+    const TfrtCompileOptions& options, mlir::ModuleOp mlir_module,\n+    const std::string& saved_model_dir, tfrt::BefBuffer bef,\n+    tfrt_stub::FallbackState* fallback_state);\n+\n",
            "whole_hunk": "@@ -46,6 +46,15 @@ limitations under the License.\n namespace tensorflow {\n namespace tfrt_stub {\n \n+// Filename for serialized BEF Buffer.\n+inline constexpr char kBefBufferFilenameMLIRBEF[] = \"serialized_bef.mlir.bef\";\n+\n+// Filename for serialized MLIR_MODULE.\n+inline constexpr char kMLIRModuleFilename[] = \"serialized_mlir.mlir\";\n+\n+// Subdirectory where AoT Packages are saved\n+inline constexpr char kAoTPackagesDirectory[] = \"aot_packages\";\n+\n // TODO(tfrt-dev): Replace tfrt::TensorSpec with tensorflow::TensorSpec once the\n // latter is checked in.\n struct TensorSpec {\n@@ -103,6 +112,17 @@ struct InitializersAndSignatures {\n StatusOr<InitializersAndSignatures> GetInitializersAndSignatures(\n     mlir::ModuleOp module);\n \n+std::string GetAotPackagePath(absl::string_view saved_model_dir);\n+\n+std::string GetBEFFilePath(std::string aot_package_directory);\n+\n+// TODO(b/295241000): Implement MLIR deserialization to skip it AoT and remove\n+// redundant steps\n+absl::StatusOr<tfrt::BefBuffer> LoadAotPackages(\n+    const TfrtCompileOptions& options, mlir::ModuleOp mlir_module,\n+    const std::string& saved_model_dir, tfrt::BefBuffer bef,\n+    tfrt_stub::FallbackState* fallback_state);\n+\n }  // namespace tfrt_stub\n }  // namespace tensorflow\n \n"
        },
        {
            "name": "BUILD",
            "path": "tensorflow/core/tfrt/saved_model/utils/BUILD",
            "patches": [
                {
                    "old_start": 12,
                    "old_length": 6,
                    "new_start": 12,
                    "new_length": 7,
                    "hunk": "@@ -12,6 +12,7 @@ package_group(\n         # Authorized users go here.\n         \"//tensorflow/core/tfrt/saved_model/...\",\n         \"//learning/brain/tfrt/cpp_tests/gpu_inference/...\",\n+        \"//tensorflow/compiler/mlir/tfrt/...\",\n     ],\n )\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/compiler/mlir/tfrt/...\",\n",
            "whole_hunk": "@@ -12,6 +12,7 @@ package_group(\n         # Authorized users go here.\n         \"//tensorflow/core/tfrt/saved_model/...\",\n         \"//learning/brain/tfrt/cpp_tests/gpu_inference/...\",\n+        \"//tensorflow/compiler/mlir/tfrt/...\",\n     ],\n )\n "
        }
    ]
},
{
    "Id": 204,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/71af215f4ed19707309bea5dcc34adc2e489b5b5",
    "date": "2024-01-10T13:48:21-08:00",
    "message": "This is change 2/2 for allowing the user to tell the HeapSimulator how to iterate over slice time permutations. (A permutation of slice times tells the compiler when to start each slice allocation.) In the future, we want MSA to be able to trim the iteration space for the HeapSimulator based on higher-level knowledge.\n\nIn this change we make the SlicedAllocationFinder explicitly take a SliceTimePermutationIterator, so the user can specify which permutations to try. In addition, we move the code to check for legal permutations from the best fit repacker to the HeapSimulator, so all iterators can use it.\n\nThis is mostly a refactoring.\n\nPiperOrigin-RevId: 597340523",
    "label": "NO",
    "changes": [
        {
            "name": "allocation_block.h",
            "path": "third_party/xla/xla/service/allocation_block.h",
            "patches": [
                {
                    "old_start": 52,
                    "old_length": 6,
                    "new_start": 52,
                    "new_length": 8,
                    "hunk": "@@ -52,6 +52,8 @@ struct SlicedAllocationData {\n \n   std::vector<int64_t> SortedInclusiveStartTimes() const;\n \n+  int64_t num_slices() const { return slices_sorted_by_offset.size(); }\n+\n   std::string ToString() const;\n \n   bool operator==(const SlicedAllocationData& rhs) const;\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  int64_t num_slices() const { return slices_sorted_by_offset.size(); }\n+\n",
            "whole_hunk": "@@ -52,6 +52,8 @@ struct SlicedAllocationData {\n \n   std::vector<int64_t> SortedInclusiveStartTimes() const;\n \n+  int64_t num_slices() const { return slices_sorted_by_offset.size(); }\n+\n   std::string ToString() const;\n \n   bool operator==(const SlicedAllocationData& rhs) const;\n"
        },
        {
            "name": "heap_simulator.cc",
            "path": "third_party/xla/xla/service/heap_simulator.cc",
            "patches": [
                {
                    "old_start": 1011,
                    "old_length": 6,
                    "new_start": 1011,
                    "new_length": 150,
                    "hunk": "@@ -1011,6 +1011,150 @@ std::string GlobalDecreasingSizeBestFitHeap<\n       absl::StrJoin(slice_sizes_sorted_by_offset_, \", \"), \" } }\");\n }\n \n+namespace {\n+\n+// A class that indicates if a permutation of starting slice times is valid. See\n+// SliceTimePermutationIterator for the meaning of slice time permutations.\n+//\n+// In non-repacking scenarios, all slices are valid. In repacking scenarios,\n+// a permutation is invalid if it does not maintain the mapping between slice\n+// times and slice sizes of the original placement.\n+class SliceTimePermutationValidator {\n+ public:\n+  explicit SliceTimePermutationValidator(\n+      const SlicedAllocationData* original_slices)\n+      : original_num_slices_(original_slices ? original_slices->num_slices()\n+                                             : 0) {\n+    if (original_num_slices_ <= 0) {\n+      return;\n+    }\n+    slice_time_to_inclusive_schedule_time_ =\n+        original_slices->SortedInclusiveStartTimes();\n+    absl::c_sort(slice_time_to_inclusive_schedule_time_);\n+\n+    original_slice_sizes_and_start_times_pairwise_sorted_.reserve(\n+        original_num_slices_);\n+    for (const AllocatedSlice& slice :\n+         original_slices->slices_sorted_by_offset) {\n+      original_slice_sizes_and_start_times_pairwise_sorted_.push_back(\n+          std::make_pair(slice.size, slice.inclusive_start_time));\n+    }\n+    absl::c_sort(original_slice_sizes_and_start_times_pairwise_sorted_);\n+\n+    sizes_sorted_by_offset_ = original_slices->SizesSortedByOffset();\n+  }\n+\n+  bool IsValid(absl::Span<const int64_t> permutation) {\n+    if (original_num_slices_ <= 0) {\n+      return true;\n+    }\n+\n+    // Compute the slice size to slice start time mapping proposed by the\n+    // permutation.\n+    std::vector<std::pair<int64_t, int64_t>>\n+        proposed_slice_sizes_and_start_times_pairwise_sorted;\n+    proposed_slice_sizes_and_start_times_pairwise_sorted.reserve(\n+        original_num_slices_);\n+    CHECK_EQ(sizes_sorted_by_offset_.size(), original_num_slices_);\n+    CHECK_EQ(permutation.size(), original_num_slices_);\n+    for (int i = 0; i < original_num_slices_; ++i) {\n+      proposed_slice_sizes_and_start_times_pairwise_sorted.push_back(\n+          std::make_pair(\n+              sizes_sorted_by_offset_[i],\n+              slice_time_to_inclusive_schedule_time_[permutation[i]]));\n+    }\n+    absl::c_sort(proposed_slice_sizes_and_start_times_pairwise_sorted);\n+\n+    bool allowed = (original_slice_sizes_and_start_times_pairwise_sorted_ ==\n+                    proposed_slice_sizes_and_start_times_pairwise_sorted);\n+    VLOG(3) << [&]() {\n+      auto export_pair = [](std::string* out,\n+                            const std::pair<int64_t, int64_t>& p) {\n+        absl::StrAppend(out, \"<\", p.first, \", \", p.second, \">\");\n+      };\n+      return absl::StrCat(\n+          \"Slice permutation \", (allowed ? \"allowed\" : \"disallowed\"),\n+          \". Original slice <size, start_time> mapping: \",\n+          absl::StrJoin(original_slice_sizes_and_start_times_pairwise_sorted_,\n+                        \", \", export_pair),\n+          \". Proposed mapping: \",\n+          absl::StrJoin(proposed_slice_sizes_and_start_times_pairwise_sorted,\n+                        \", \", export_pair),\n+          \".\");\n+    }();\n+\n+    return allowed;\n+  }\n+\n+ private:\n+  int64_t original_num_slices_;\n+\n+  // The original allocation mapping from slice times to schedule times.\n+  std::vector<int64_t> slice_time_to_inclusive_schedule_time_;\n+\n+  std::vector<std::pair<int64_t, int64_t>>\n+      original_slice_sizes_and_start_times_pairwise_sorted_;\n+\n+  std::vector<int64_t> sizes_sorted_by_offset_;\n+};\n+\n+// A SliceTimePermutationIterator that iterates over all valid (see\n+// SliceTimePermutationValidator for more details) permutations of slice times.\n+class SliceTimeAllPermutationIterator : public SliceTimePermutationIterator {\n+ public:\n+  SliceTimeAllPermutationIterator(\n+      int64_t num_slices,\n+      const SlicedAllocationData* original_sliced_allocation)\n+      : validator_(original_sliced_allocation),\n+        num_slices_(num_slices),\n+        permutation_(num_slices, 0) {}\n+\n+  ~SliceTimeAllPermutationIterator() override = default;\n+\n+  void Begin() override {\n+    done_ = (num_slices_ <= 0);\n+\n+    for (int64_t i = 0; i < num_slices_; ++i) {\n+      permutation_[i] = i;\n+    }\n+\n+    if (!Done() && !validator_.IsValid(Get())) {\n+      Next();\n+    }\n+  }\n+\n+  bool Done() const override { return done_; }\n+\n+  void Next() override {\n+    if (Done()) {\n+      return;\n+    }\n+    do {\n+      done_ = !absl::c_next_permutation(permutation_);\n+    } while (!Done() && !validator_.IsValid(Get()));\n+  }\n+\n+  absl::Span<const int64_t> Get() const override { return permutation_; }\n+\n+ private:\n+  SliceTimeAllPermutationIterator() = default;\n+\n+  SliceTimePermutationValidator validator_;\n+  int64_t num_slices_;\n+  bool done_ = true;\n+  std::vector<int64_t> permutation_;\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<SliceTimePermutationIterator>\n+SliceTimePermutationIterator::Create(\n+    int64_t num_slices,\n+    const SlicedAllocationData* original_sliced_allocation) {\n+  return std::make_unique<SliceTimeAllPermutationIterator>(\n+      num_slices, original_sliced_allocation);\n+}\n+\n template <typename BufferType>\n std::string GlobalDecreasingSizeBestFitHeap<\n     BufferType>::SlicedAllocationFinder::FreeChunkPiece::ToString() const {\n"
                },
                {
                    "old_start": 1157,
                    "old_length": 9,
                    "new_start": 1301,
                    "new_length": 9,
                    "hunk": "@@ -1157,9 +1301,9 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n         absl::Span<const FreeChunks> free_chunks_per_slice_time,\n         std::vector<int64_t> sorted_slice_sizes, int64_t max_colocation_size,\n         int64_t preferred_offset, int64_t alignment,\n-        absl::AnyInvocable<bool(int64_t) const> is_offset_allowed,\n-        absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-            is_slice_time_permutation_allowed)\n+        std::unique_ptr<SliceTimePermutationIterator>\n+            slice_time_permutation_iterator,\n+        absl::AnyInvocable<bool(int64_t) const> is_offset_allowed)\n     : sorted_slice_sizes_(std::move(sorted_slice_sizes)),\n       slice_size_sum_(std::accumulate(sorted_slice_sizes_.begin(),\n                                       sorted_slice_sizes_.end(),\n"
                },
                {
                    "old_start": 1167,
                    "old_length": 9,
                    "new_start": 1311,
                    "new_length": 9,
                    "hunk": "@@ -1167,9 +1311,9 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n       max_colocation_size_(max_colocation_size),\n       preferred_offset_(preferred_offset),\n       alignment_(alignment),\n-      is_offset_allowed_(std::move(is_offset_allowed)),\n-      is_slice_time_permutation_allowed_(\n-          std::move(is_slice_time_permutation_allowed)) {\n+      slice_time_permutation_iterator_(\n+          std::move(slice_time_permutation_iterator)),\n+      is_offset_allowed_(std::move(is_offset_allowed)) {\n   CHECK_EQ(sorted_slice_sizes_.size(), free_chunks_per_slice_time.size())\n       << \"We expect a data structure explaining the free chunks at each slice \"\n          \"time.\";\n"
                },
                {
                    "old_start": 1410,
                    "old_length": 7,
                    "new_start": 1554,
                    "new_length": 7,
                    "hunk": "@@ -1410,7 +1554,7 @@ GlobalDecreasingSizeBestFitHeap<\n \n template <typename BufferType>\n Status GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n-    DoesPermutationFit(const std::vector<int64_t>& permutation_of_slice_times,\n+    DoesPermutationFit(absl::Span<const int64_t> permutation_of_slice_times,\n                        const FreeChunkRoot& root, int64_t offset) const {\n   Status result =\n       DoesPermutationFitImpl(permutation_of_slice_times, root, offset);\n"
                },
                {
                    "old_start": 1425,
                    "old_length": 9,
                    "new_start": 1569,
                    "new_length": 8,
                    "hunk": "@@ -1425,9 +1569,8 @@ Status GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n \n template <typename BufferType>\n Status GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n-    DoesPermutationFitImpl(\n-        const std::vector<int64_t>& permutation_of_slice_times,\n-        const FreeChunkRoot& root, int64_t offset) const {\n+    DoesPermutationFitImpl(absl::Span<const int64_t> permutation_of_slice_times,\n+                           const FreeChunkRoot& root, int64_t offset) const {\n   if (permutation_of_slice_times.size() != sorted_slice_sizes_.size()) {\n     return InvalidArgumentStrCat(\n         sorted_slice_sizes_.size(), \" slices times expected in permutation. \",\n"
                },
                {
                    "old_start": 1512,
                    "old_length": 49,
                    "new_start": 1655,
                    "new_length": 6,
                    "hunk": "@@ -1512,49 +1655,6 @@ Status GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n   return OkStatus();\n }\n \n-namespace {\n-\n-// An iterator for iterating through permutations of slice times.\n-class SliceTimePermutationIterator {\n- public:\n-  SliceTimePermutationIterator(\n-      int64_t latest_slice_time,\n-      const absl::AnyInvocable<bool(const std::vector<int64_t>&) const>&\n-          is_slice_time_permutation_allowed)\n-      : is_slice_time_permutation_allowed_(is_slice_time_permutation_allowed),\n-        done_(latest_slice_time < 0) {\n-    permutation_.reserve(latest_slice_time + 1);\n-    for (int64_t i = 0; i <= latest_slice_time; ++i) {\n-      permutation_.push_back(i);\n-    }\n-\n-    if (!Done() && !is_slice_time_permutation_allowed(permutation_)) {\n-      Next();\n-    }\n-  }\n-\n-  bool Done() const { return done_; }\n-\n-  void Next() {\n-    if (Done()) {\n-      return;\n-    }\n-    do {\n-      done_ = !absl::c_next_permutation(permutation_);\n-    } while (!Done() && !is_slice_time_permutation_allowed_(permutation_));\n-  }\n-\n-  const std::vector<int64_t>& Get() const { return permutation_; }\n-\n- private:\n-  const absl::AnyInvocable<bool(const std::vector<int64_t>&) const>&\n-      is_slice_time_permutation_allowed_;\n-  bool done_ = false;\n-  std::vector<int64_t> permutation_;\n-};\n-\n-}  // namespace\n-\n // Future opportunities:\n // 1) Potential optimization: We don't have to try every offset in\n //    [root.chunk.offset, root.chunk.chunk_end()). If a permutation doesn't fit\n"
                },
                {
                    "old_start": 1586,
                    "old_length": 11,
                    "new_start": 1686,
                    "new_length": 14,
                    "hunk": "@@ -1586,11 +1686,14 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::FindInRoot(\n   CHECK_EQ(first_offset % alignment_, 0);\n   for (int64_t offset = first_offset; offset + max_colocation_size_ <= last_end;\n        offset += alignment_) {\n-    for (SliceTimePermutationIterator permutation_it(\n-             LatestSliceTime(), is_slice_time_permutation_allowed_);\n-         !permutation_it.Done(); permutation_it.Next()) {\n-      if (DoesPermutationFit(permutation_it.Get(), root, offset).ok()) {\n-        return PermutationToChunks(permutation_it.Get(), offset);\n+    for (slice_time_permutation_iterator_->Begin();\n+         !slice_time_permutation_iterator_->Done();\n+         slice_time_permutation_iterator_->Next()) {\n+      if (DoesPermutationFit(slice_time_permutation_iterator_->Get(), root,\n+                             offset)\n+              .ok()) {\n+        return PermutationToChunks(slice_time_permutation_iterator_->Get(),\n+                                   offset);\n       }\n     }\n \n"
                },
                {
                    "old_start": 1609,
                    "old_length": 7,
                    "new_start": 1712,
                    "new_length": 7,
                    "hunk": "@@ -1609,7 +1712,7 @@ template <typename BufferType>\n typename GlobalDecreasingSizeBestFitHeap<\n     BufferType>::SlicedAllocationFinder::ChunksSortedBySliceTime\n GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n-    PermutationToChunks(const std::vector<int64_t>& permutation_of_slice_times,\n+    PermutationToChunks(absl::Span<const int64_t> permutation_of_slice_times,\n                         int64_t offset) const {\n   ChunksSortedBySliceTime chunks(permutation_of_slice_times.size() + 1,\n                                  Chunk::FromOffsetSize(-1, 1));\n"
                },
                {
                    "old_start": 1753,
                    "old_length": 7,
                    "new_start": 1856,
                    "new_length": 9,
                    "hunk": "@@ -1753,7 +1856,9 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::FindChunkCandidates(\n       GetMaxColocationSize(sliced_buffer_interval.full_buffer_interval());\n   auto chunks =\n       CreateSlicedAllocationFinder(sliced_buffer_interval, max_colocation_size,\n-                                   preferred_offset)\n+                                   preferred_offset,\n+                                   SliceTimePermutationIterator::Create(\n+                                       sliced_buffer_interval.num_slices()))\n           .Find();\n   return PostProcessFindChunkCandidatesResult(sliced_buffer_interval,\n                                               std::move(chunks));\n"
                },
                {
                    "old_start": 1777,
                    "old_length": 9,
                    "new_start": 1882,
                    "new_length": 9,
                    "hunk": "@@ -1777,9 +1882,9 @@ typename GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder\n GlobalDecreasingSizeBestFitHeap<BufferType>::CreateSlicedAllocationFinder(\n     const SlicedBufferInterval& sliced_interval, int64_t max_colocation_size,\n     int64_t preferred_offset,\n-    absl::AnyInvocable<bool(int64_t) const> is_offset_allowed,\n-    absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-        is_slice_time_permutation_allowed) const {\n+    std::unique_ptr<SliceTimePermutationIterator>\n+        slice_time_permutation_iterator,\n+    absl::AnyInvocable<bool(int64_t) const> is_offset_allowed) const {\n   // Build up a list of free chunks for each slice time.\n   std::vector<FreeChunks> free_chunks_per_slice_time;\n   free_chunks_per_slice_time.reserve(sliced_interval.num_slices());\n"
                },
                {
                    "old_start": 1799,
                    "old_length": 11,
                    "new_start": 1904,
                    "new_length": 10,
                    "hunk": "@@ -1799,11 +1904,10 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::CreateSlicedAllocationFinder(\n                                                 1),\n       max_colocation_size));\n \n-  return SlicedAllocationFinder(free_chunks_per_slice_time,\n-                                sliced_interval.SliceSizesSortedByOffset(),\n-                                max_colocation_size, preferred_offset,\n-                                alignment_, std::move(is_offset_allowed),\n-                                std::move(is_slice_time_permutation_allowed));\n+  return SlicedAllocationFinder(\n+      free_chunks_per_slice_time, sliced_interval.SliceSizesSortedByOffset(),\n+      max_colocation_size, preferred_offset, alignment_,\n+      std::move(slice_time_permutation_iterator), std::move(is_offset_allowed));\n }\n \n template <typename BufferType>\n"
                }
            ],
            "whole_deleted": "-        absl::AnyInvocable<bool(int64_t) const> is_offset_allowed,\n-        absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-            is_slice_time_permutation_allowed)\n-      is_offset_allowed_(std::move(is_offset_allowed)),\n-      is_slice_time_permutation_allowed_(\n-          std::move(is_slice_time_permutation_allowed)) {\n-    DoesPermutationFit(const std::vector<int64_t>& permutation_of_slice_times,\n-    DoesPermutationFitImpl(\n-        const std::vector<int64_t>& permutation_of_slice_times,\n-        const FreeChunkRoot& root, int64_t offset) const {\n-namespace {\n-\n-// An iterator for iterating through permutations of slice times.\n-class SliceTimePermutationIterator {\n- public:\n-  SliceTimePermutationIterator(\n-      int64_t latest_slice_time,\n-      const absl::AnyInvocable<bool(const std::vector<int64_t>&) const>&\n-          is_slice_time_permutation_allowed)\n-      : is_slice_time_permutation_allowed_(is_slice_time_permutation_allowed),\n-        done_(latest_slice_time < 0) {\n-    permutation_.reserve(latest_slice_time + 1);\n-    for (int64_t i = 0; i <= latest_slice_time; ++i) {\n-      permutation_.push_back(i);\n-    }\n-\n-    if (!Done() && !is_slice_time_permutation_allowed(permutation_)) {\n-      Next();\n-    }\n-  }\n-\n-  bool Done() const { return done_; }\n-\n-  void Next() {\n-    if (Done()) {\n-      return;\n-    }\n-    do {\n-      done_ = !absl::c_next_permutation(permutation_);\n-    } while (!Done() && !is_slice_time_permutation_allowed_(permutation_));\n-  }\n-\n-  const std::vector<int64_t>& Get() const { return permutation_; }\n-\n- private:\n-  const absl::AnyInvocable<bool(const std::vector<int64_t>&) const>&\n-      is_slice_time_permutation_allowed_;\n-  bool done_ = false;\n-  std::vector<int64_t> permutation_;\n-};\n-\n-}  // namespace\n-\n-    for (SliceTimePermutationIterator permutation_it(\n-             LatestSliceTime(), is_slice_time_permutation_allowed_);\n-         !permutation_it.Done(); permutation_it.Next()) {\n-      if (DoesPermutationFit(permutation_it.Get(), root, offset).ok()) {\n-        return PermutationToChunks(permutation_it.Get(), offset);\n-    PermutationToChunks(const std::vector<int64_t>& permutation_of_slice_times,\n-                                   preferred_offset)\n-    absl::AnyInvocable<bool(int64_t) const> is_offset_allowed,\n-    absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-        is_slice_time_permutation_allowed) const {\n-  return SlicedAllocationFinder(free_chunks_per_slice_time,\n-                                sliced_interval.SliceSizesSortedByOffset(),\n-                                max_colocation_size, preferred_offset,\n-                                alignment_, std::move(is_offset_allowed),\n-                                std::move(is_slice_time_permutation_allowed));\n",
            "whole_added": "+namespace {\n+\n+// A class that indicates if a permutation of starting slice times is valid. See\n+// SliceTimePermutationIterator for the meaning of slice time permutations.\n+//\n+// In non-repacking scenarios, all slices are valid. In repacking scenarios,\n+// a permutation is invalid if it does not maintain the mapping between slice\n+// times and slice sizes of the original placement.\n+class SliceTimePermutationValidator {\n+ public:\n+  explicit SliceTimePermutationValidator(\n+      const SlicedAllocationData* original_slices)\n+      : original_num_slices_(original_slices ? original_slices->num_slices()\n+                                             : 0) {\n+    if (original_num_slices_ <= 0) {\n+      return;\n+    }\n+    slice_time_to_inclusive_schedule_time_ =\n+        original_slices->SortedInclusiveStartTimes();\n+    absl::c_sort(slice_time_to_inclusive_schedule_time_);\n+\n+    original_slice_sizes_and_start_times_pairwise_sorted_.reserve(\n+        original_num_slices_);\n+    for (const AllocatedSlice& slice :\n+         original_slices->slices_sorted_by_offset) {\n+      original_slice_sizes_and_start_times_pairwise_sorted_.push_back(\n+          std::make_pair(slice.size, slice.inclusive_start_time));\n+    }\n+    absl::c_sort(original_slice_sizes_and_start_times_pairwise_sorted_);\n+\n+    sizes_sorted_by_offset_ = original_slices->SizesSortedByOffset();\n+  }\n+\n+  bool IsValid(absl::Span<const int64_t> permutation) {\n+    if (original_num_slices_ <= 0) {\n+      return true;\n+    }\n+\n+    // Compute the slice size to slice start time mapping proposed by the\n+    // permutation.\n+    std::vector<std::pair<int64_t, int64_t>>\n+        proposed_slice_sizes_and_start_times_pairwise_sorted;\n+    proposed_slice_sizes_and_start_times_pairwise_sorted.reserve(\n+        original_num_slices_);\n+    CHECK_EQ(sizes_sorted_by_offset_.size(), original_num_slices_);\n+    CHECK_EQ(permutation.size(), original_num_slices_);\n+    for (int i = 0; i < original_num_slices_; ++i) {\n+      proposed_slice_sizes_and_start_times_pairwise_sorted.push_back(\n+          std::make_pair(\n+              sizes_sorted_by_offset_[i],\n+              slice_time_to_inclusive_schedule_time_[permutation[i]]));\n+    }\n+    absl::c_sort(proposed_slice_sizes_and_start_times_pairwise_sorted);\n+\n+    bool allowed = (original_slice_sizes_and_start_times_pairwise_sorted_ ==\n+                    proposed_slice_sizes_and_start_times_pairwise_sorted);\n+    VLOG(3) << [&]() {\n+      auto export_pair = [](std::string* out,\n+                            const std::pair<int64_t, int64_t>& p) {\n+        absl::StrAppend(out, \"<\", p.first, \", \", p.second, \">\");\n+      };\n+      return absl::StrCat(\n+          \"Slice permutation \", (allowed ? \"allowed\" : \"disallowed\"),\n+          \". Original slice <size, start_time> mapping: \",\n+          absl::StrJoin(original_slice_sizes_and_start_times_pairwise_sorted_,\n+                        \", \", export_pair),\n+          \". Proposed mapping: \",\n+          absl::StrJoin(proposed_slice_sizes_and_start_times_pairwise_sorted,\n+                        \", \", export_pair),\n+          \".\");\n+    }();\n+\n+    return allowed;\n+  }\n+\n+ private:\n+  int64_t original_num_slices_;\n+\n+  // The original allocation mapping from slice times to schedule times.\n+  std::vector<int64_t> slice_time_to_inclusive_schedule_time_;\n+\n+  std::vector<std::pair<int64_t, int64_t>>\n+      original_slice_sizes_and_start_times_pairwise_sorted_;\n+\n+  std::vector<int64_t> sizes_sorted_by_offset_;\n+};\n+\n+// A SliceTimePermutationIterator that iterates over all valid (see\n+// SliceTimePermutationValidator for more details) permutations of slice times.\n+class SliceTimeAllPermutationIterator : public SliceTimePermutationIterator {\n+ public:\n+  SliceTimeAllPermutationIterator(\n+      int64_t num_slices,\n+      const SlicedAllocationData* original_sliced_allocation)\n+      : validator_(original_sliced_allocation),\n+        num_slices_(num_slices),\n+        permutation_(num_slices, 0) {}\n+\n+  ~SliceTimeAllPermutationIterator() override = default;\n+\n+  void Begin() override {\n+    done_ = (num_slices_ <= 0);\n+\n+    for (int64_t i = 0; i < num_slices_; ++i) {\n+      permutation_[i] = i;\n+    }\n+\n+    if (!Done() && !validator_.IsValid(Get())) {\n+      Next();\n+    }\n+  }\n+\n+  bool Done() const override { return done_; }\n+\n+  void Next() override {\n+    if (Done()) {\n+      return;\n+    }\n+    do {\n+      done_ = !absl::c_next_permutation(permutation_);\n+    } while (!Done() && !validator_.IsValid(Get()));\n+  }\n+\n+  absl::Span<const int64_t> Get() const override { return permutation_; }\n+\n+ private:\n+  SliceTimeAllPermutationIterator() = default;\n+\n+  SliceTimePermutationValidator validator_;\n+  int64_t num_slices_;\n+  bool done_ = true;\n+  std::vector<int64_t> permutation_;\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<SliceTimePermutationIterator>\n+SliceTimePermutationIterator::Create(\n+    int64_t num_slices,\n+    const SlicedAllocationData* original_sliced_allocation) {\n+  return std::make_unique<SliceTimeAllPermutationIterator>(\n+      num_slices, original_sliced_allocation);\n+}\n+\n+        std::unique_ptr<SliceTimePermutationIterator>\n+            slice_time_permutation_iterator,\n+        absl::AnyInvocable<bool(int64_t) const> is_offset_allowed)\n+      slice_time_permutation_iterator_(\n+          std::move(slice_time_permutation_iterator)),\n+      is_offset_allowed_(std::move(is_offset_allowed)) {\n+    DoesPermutationFit(absl::Span<const int64_t> permutation_of_slice_times,\n+    DoesPermutationFitImpl(absl::Span<const int64_t> permutation_of_slice_times,\n+                           const FreeChunkRoot& root, int64_t offset) const {\n+    for (slice_time_permutation_iterator_->Begin();\n+         !slice_time_permutation_iterator_->Done();\n+         slice_time_permutation_iterator_->Next()) {\n+      if (DoesPermutationFit(slice_time_permutation_iterator_->Get(), root,\n+                             offset)\n+              .ok()) {\n+        return PermutationToChunks(slice_time_permutation_iterator_->Get(),\n+                                   offset);\n+    PermutationToChunks(absl::Span<const int64_t> permutation_of_slice_times,\n+                                   preferred_offset,\n+                                   SliceTimePermutationIterator::Create(\n+                                       sliced_buffer_interval.num_slices()))\n+    std::unique_ptr<SliceTimePermutationIterator>\n+        slice_time_permutation_iterator,\n+    absl::AnyInvocable<bool(int64_t) const> is_offset_allowed) const {\n+  return SlicedAllocationFinder(\n+      free_chunks_per_slice_time, sliced_interval.SliceSizesSortedByOffset(),\n+      max_colocation_size, preferred_offset, alignment_,\n+      std::move(slice_time_permutation_iterator), std::move(is_offset_allowed));\n",
            "whole_hunk": "@@ -1011,6 +1011,150 @@ std::string GlobalDecreasingSizeBestFitHeap<\n       absl::StrJoin(slice_sizes_sorted_by_offset_, \", \"), \" } }\");\n }\n \n+namespace {\n+\n+// A class that indicates if a permutation of starting slice times is valid. See\n+// SliceTimePermutationIterator for the meaning of slice time permutations.\n+//\n+// In non-repacking scenarios, all slices are valid. In repacking scenarios,\n+// a permutation is invalid if it does not maintain the mapping between slice\n+// times and slice sizes of the original placement.\n+class SliceTimePermutationValidator {\n+ public:\n+  explicit SliceTimePermutationValidator(\n+      const SlicedAllocationData* original_slices)\n+      : original_num_slices_(original_slices ? original_slices->num_slices()\n+                                             : 0) {\n+    if (original_num_slices_ <= 0) {\n+      return;\n+    }\n+    slice_time_to_inclusive_schedule_time_ =\n+        original_slices->SortedInclusiveStartTimes();\n+    absl::c_sort(slice_time_to_inclusive_schedule_time_);\n+\n+    original_slice_sizes_and_start_times_pairwise_sorted_.reserve(\n+        original_num_slices_);\n+    for (const AllocatedSlice& slice :\n+         original_slices->slices_sorted_by_offset) {\n+      original_slice_sizes_and_start_times_pairwise_sorted_.push_back(\n+          std::make_pair(slice.size, slice.inclusive_start_time));\n+    }\n+    absl::c_sort(original_slice_sizes_and_start_times_pairwise_sorted_);\n+\n+    sizes_sorted_by_offset_ = original_slices->SizesSortedByOffset();\n+  }\n+\n+  bool IsValid(absl::Span<const int64_t> permutation) {\n+    if (original_num_slices_ <= 0) {\n+      return true;\n+    }\n+\n+    // Compute the slice size to slice start time mapping proposed by the\n+    // permutation.\n+    std::vector<std::pair<int64_t, int64_t>>\n+        proposed_slice_sizes_and_start_times_pairwise_sorted;\n+    proposed_slice_sizes_and_start_times_pairwise_sorted.reserve(\n+        original_num_slices_);\n+    CHECK_EQ(sizes_sorted_by_offset_.size(), original_num_slices_);\n+    CHECK_EQ(permutation.size(), original_num_slices_);\n+    for (int i = 0; i < original_num_slices_; ++i) {\n+      proposed_slice_sizes_and_start_times_pairwise_sorted.push_back(\n+          std::make_pair(\n+              sizes_sorted_by_offset_[i],\n+              slice_time_to_inclusive_schedule_time_[permutation[i]]));\n+    }\n+    absl::c_sort(proposed_slice_sizes_and_start_times_pairwise_sorted);\n+\n+    bool allowed = (original_slice_sizes_and_start_times_pairwise_sorted_ ==\n+                    proposed_slice_sizes_and_start_times_pairwise_sorted);\n+    VLOG(3) << [&]() {\n+      auto export_pair = [](std::string* out,\n+                            const std::pair<int64_t, int64_t>& p) {\n+        absl::StrAppend(out, \"<\", p.first, \", \", p.second, \">\");\n+      };\n+      return absl::StrCat(\n+          \"Slice permutation \", (allowed ? \"allowed\" : \"disallowed\"),\n+          \". Original slice <size, start_time> mapping: \",\n+          absl::StrJoin(original_slice_sizes_and_start_times_pairwise_sorted_,\n+                        \", \", export_pair),\n+          \". Proposed mapping: \",\n+          absl::StrJoin(proposed_slice_sizes_and_start_times_pairwise_sorted,\n+                        \", \", export_pair),\n+          \".\");\n+    }();\n+\n+    return allowed;\n+  }\n+\n+ private:\n+  int64_t original_num_slices_;\n+\n+  // The original allocation mapping from slice times to schedule times.\n+  std::vector<int64_t> slice_time_to_inclusive_schedule_time_;\n+\n+  std::vector<std::pair<int64_t, int64_t>>\n+      original_slice_sizes_and_start_times_pairwise_sorted_;\n+\n+  std::vector<int64_t> sizes_sorted_by_offset_;\n+};\n+\n+// A SliceTimePermutationIterator that iterates over all valid (see\n+// SliceTimePermutationValidator for more details) permutations of slice times.\n+class SliceTimeAllPermutationIterator : public SliceTimePermutationIterator {\n+ public:\n+  SliceTimeAllPermutationIterator(\n+      int64_t num_slices,\n+      const SlicedAllocationData* original_sliced_allocation)\n+      : validator_(original_sliced_allocation),\n+        num_slices_(num_slices),\n+        permutation_(num_slices, 0) {}\n+\n+  ~SliceTimeAllPermutationIterator() override = default;\n+\n+  void Begin() override {\n+    done_ = (num_slices_ <= 0);\n+\n+    for (int64_t i = 0; i < num_slices_; ++i) {\n+      permutation_[i] = i;\n+    }\n+\n+    if (!Done() && !validator_.IsValid(Get())) {\n+      Next();\n+    }\n+  }\n+\n+  bool Done() const override { return done_; }\n+\n+  void Next() override {\n+    if (Done()) {\n+      return;\n+    }\n+    do {\n+      done_ = !absl::c_next_permutation(permutation_);\n+    } while (!Done() && !validator_.IsValid(Get()));\n+  }\n+\n+  absl::Span<const int64_t> Get() const override { return permutation_; }\n+\n+ private:\n+  SliceTimeAllPermutationIterator() = default;\n+\n+  SliceTimePermutationValidator validator_;\n+  int64_t num_slices_;\n+  bool done_ = true;\n+  std::vector<int64_t> permutation_;\n+};\n+\n+}  // namespace\n+\n+std::unique_ptr<SliceTimePermutationIterator>\n+SliceTimePermutationIterator::Create(\n+    int64_t num_slices,\n+    const SlicedAllocationData* original_sliced_allocation) {\n+  return std::make_unique<SliceTimeAllPermutationIterator>(\n+      num_slices, original_sliced_allocation);\n+}\n+\n template <typename BufferType>\n std::string GlobalDecreasingSizeBestFitHeap<\n     BufferType>::SlicedAllocationFinder::FreeChunkPiece::ToString() const {\n@@ -1157,9 +1301,9 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n         absl::Span<const FreeChunks> free_chunks_per_slice_time,\n         std::vector<int64_t> sorted_slice_sizes, int64_t max_colocation_size,\n         int64_t preferred_offset, int64_t alignment,\n-        absl::AnyInvocable<bool(int64_t) const> is_offset_allowed,\n-        absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-            is_slice_time_permutation_allowed)\n+        std::unique_ptr<SliceTimePermutationIterator>\n+            slice_time_permutation_iterator,\n+        absl::AnyInvocable<bool(int64_t) const> is_offset_allowed)\n     : sorted_slice_sizes_(std::move(sorted_slice_sizes)),\n       slice_size_sum_(std::accumulate(sorted_slice_sizes_.begin(),\n                                       sorted_slice_sizes_.end(),\n@@ -1167,9 +1311,9 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n       max_colocation_size_(max_colocation_size),\n       preferred_offset_(preferred_offset),\n       alignment_(alignment),\n-      is_offset_allowed_(std::move(is_offset_allowed)),\n-      is_slice_time_permutation_allowed_(\n-          std::move(is_slice_time_permutation_allowed)) {\n+      slice_time_permutation_iterator_(\n+          std::move(slice_time_permutation_iterator)),\n+      is_offset_allowed_(std::move(is_offset_allowed)) {\n   CHECK_EQ(sorted_slice_sizes_.size(), free_chunks_per_slice_time.size())\n       << \"We expect a data structure explaining the free chunks at each slice \"\n          \"time.\";\n@@ -1410,7 +1554,7 @@ GlobalDecreasingSizeBestFitHeap<\n \n template <typename BufferType>\n Status GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n-    DoesPermutationFit(const std::vector<int64_t>& permutation_of_slice_times,\n+    DoesPermutationFit(absl::Span<const int64_t> permutation_of_slice_times,\n                        const FreeChunkRoot& root, int64_t offset) const {\n   Status result =\n       DoesPermutationFitImpl(permutation_of_slice_times, root, offset);\n@@ -1425,9 +1569,8 @@ Status GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n \n template <typename BufferType>\n Status GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n-    DoesPermutationFitImpl(\n-        const std::vector<int64_t>& permutation_of_slice_times,\n-        const FreeChunkRoot& root, int64_t offset) const {\n+    DoesPermutationFitImpl(absl::Span<const int64_t> permutation_of_slice_times,\n+                           const FreeChunkRoot& root, int64_t offset) const {\n   if (permutation_of_slice_times.size() != sorted_slice_sizes_.size()) {\n     return InvalidArgumentStrCat(\n         sorted_slice_sizes_.size(), \" slices times expected in permutation. \",\n@@ -1512,49 +1655,6 @@ Status GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n   return OkStatus();\n }\n \n-namespace {\n-\n-// An iterator for iterating through permutations of slice times.\n-class SliceTimePermutationIterator {\n- public:\n-  SliceTimePermutationIterator(\n-      int64_t latest_slice_time,\n-      const absl::AnyInvocable<bool(const std::vector<int64_t>&) const>&\n-          is_slice_time_permutation_allowed)\n-      : is_slice_time_permutation_allowed_(is_slice_time_permutation_allowed),\n-        done_(latest_slice_time < 0) {\n-    permutation_.reserve(latest_slice_time + 1);\n-    for (int64_t i = 0; i <= latest_slice_time; ++i) {\n-      permutation_.push_back(i);\n-    }\n-\n-    if (!Done() && !is_slice_time_permutation_allowed(permutation_)) {\n-      Next();\n-    }\n-  }\n-\n-  bool Done() const { return done_; }\n-\n-  void Next() {\n-    if (Done()) {\n-      return;\n-    }\n-    do {\n-      done_ = !absl::c_next_permutation(permutation_);\n-    } while (!Done() && !is_slice_time_permutation_allowed_(permutation_));\n-  }\n-\n-  const std::vector<int64_t>& Get() const { return permutation_; }\n-\n- private:\n-  const absl::AnyInvocable<bool(const std::vector<int64_t>&) const>&\n-      is_slice_time_permutation_allowed_;\n-  bool done_ = false;\n-  std::vector<int64_t> permutation_;\n-};\n-\n-}  // namespace\n-\n // Future opportunities:\n // 1) Potential optimization: We don't have to try every offset in\n //    [root.chunk.offset, root.chunk.chunk_end()). If a permutation doesn't fit\n@@ -1586,11 +1686,14 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::FindInRoot(\n   CHECK_EQ(first_offset % alignment_, 0);\n   for (int64_t offset = first_offset; offset + max_colocation_size_ <= last_end;\n        offset += alignment_) {\n-    for (SliceTimePermutationIterator permutation_it(\n-             LatestSliceTime(), is_slice_time_permutation_allowed_);\n-         !permutation_it.Done(); permutation_it.Next()) {\n-      if (DoesPermutationFit(permutation_it.Get(), root, offset).ok()) {\n-        return PermutationToChunks(permutation_it.Get(), offset);\n+    for (slice_time_permutation_iterator_->Begin();\n+         !slice_time_permutation_iterator_->Done();\n+         slice_time_permutation_iterator_->Next()) {\n+      if (DoesPermutationFit(slice_time_permutation_iterator_->Get(), root,\n+                             offset)\n+              .ok()) {\n+        return PermutationToChunks(slice_time_permutation_iterator_->Get(),\n+                                   offset);\n       }\n     }\n \n@@ -1609,7 +1712,7 @@ template <typename BufferType>\n typename GlobalDecreasingSizeBestFitHeap<\n     BufferType>::SlicedAllocationFinder::ChunksSortedBySliceTime\n GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::\n-    PermutationToChunks(const std::vector<int64_t>& permutation_of_slice_times,\n+    PermutationToChunks(absl::Span<const int64_t> permutation_of_slice_times,\n                         int64_t offset) const {\n   ChunksSortedBySliceTime chunks(permutation_of_slice_times.size() + 1,\n                                  Chunk::FromOffsetSize(-1, 1));\n@@ -1753,7 +1856,9 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::FindChunkCandidates(\n       GetMaxColocationSize(sliced_buffer_interval.full_buffer_interval());\n   auto chunks =\n       CreateSlicedAllocationFinder(sliced_buffer_interval, max_colocation_size,\n-                                   preferred_offset)\n+                                   preferred_offset,\n+                                   SliceTimePermutationIterator::Create(\n+                                       sliced_buffer_interval.num_slices()))\n           .Find();\n   return PostProcessFindChunkCandidatesResult(sliced_buffer_interval,\n                                               std::move(chunks));\n@@ -1777,9 +1882,9 @@ typename GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder\n GlobalDecreasingSizeBestFitHeap<BufferType>::CreateSlicedAllocationFinder(\n     const SlicedBufferInterval& sliced_interval, int64_t max_colocation_size,\n     int64_t preferred_offset,\n-    absl::AnyInvocable<bool(int64_t) const> is_offset_allowed,\n-    absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-        is_slice_time_permutation_allowed) const {\n+    std::unique_ptr<SliceTimePermutationIterator>\n+        slice_time_permutation_iterator,\n+    absl::AnyInvocable<bool(int64_t) const> is_offset_allowed) const {\n   // Build up a list of free chunks for each slice time.\n   std::vector<FreeChunks> free_chunks_per_slice_time;\n   free_chunks_per_slice_time.reserve(sliced_interval.num_slices());\n@@ -1799,11 +1904,10 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::CreateSlicedAllocationFinder(\n                                                 1),\n       max_colocation_size));\n \n-  return SlicedAllocationFinder(free_chunks_per_slice_time,\n-                                sliced_interval.SliceSizesSortedByOffset(),\n-                                max_colocation_size, preferred_offset,\n-                                alignment_, std::move(is_offset_allowed),\n-                                std::move(is_slice_time_permutation_allowed));\n+  return SlicedAllocationFinder(\n+      free_chunks_per_slice_time, sliced_interval.SliceSizesSortedByOffset(),\n+      max_colocation_size, preferred_offset, alignment_,\n+      std::move(slice_time_permutation_iterator), std::move(is_offset_allowed));\n }\n \n template <typename BufferType>\n"
        },
        {
            "name": "heap_simulator.h",
            "path": "third_party/xla/xla/service/heap_simulator.h",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 6,
                    "new_start": 20,
                    "new_length": 7,
                    "hunk": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <cstdint>\n #include <functional>\n #include <iostream>\n+#include <list>\n #include <memory>\n #include <optional>\n #include <set>\n"
                },
                {
                    "old_start": 387,
                    "old_length": 6,
                    "new_start": 388,
                    "new_length": 39,
                    "hunk": "@@ -387,6 +388,39 @@ class BufferIntervalTree {\n   std::list<BufferIntervalTreeNode> node_storage_;\n };\n \n+// An iterator that is passed to\n+// GlobalDecreasingSizeBestFitHeap::CreateSlicedAllocationFinder() when trying\n+// to place a buffer, telling the finder which permutations of starting slice\n+// times to try (and in which order to try them). Note, the set of slice times\n+// is the set {x : x \u2208 [0, num_slices - 1]}. If a buffer is not sliced, it will\n+// only have 1 permutation, containing slice time 0.\n+//\n+// Begin() must be called to initialize the iterator before it can be used.\n+class SliceTimePermutationIterator {\n+ public:\n+  // A new iterator is typically created for each buffer to be placed.\n+  // - num_slices: number of slices in the buffer. 1 if not sliced.\n+  // - original_sliced_allocation: For a repacking scenario, the original\n+  //   details of each slice in a sliced buffer. nullptr is used if this is not\n+  //   a repacking scenario or the buffer is not sliced.\n+  static std::unique_ptr<SliceTimePermutationIterator> Create(\n+      int64_t num_slices,\n+      const SlicedAllocationData* original_sliced_allocation = nullptr);\n+\n+  virtual ~SliceTimePermutationIterator() = default;\n+\n+  virtual void Begin() = 0;\n+  virtual bool Done() const = 0;\n+  virtual void Next() = 0;\n+\n+  // A permutation of starting slice times. The ith value is the slice time for\n+  // the slice at the ith smallest offset.\n+  virtual absl::Span<const int64_t> Get() const = 0;\n+\n+ protected:\n+  SliceTimePermutationIterator() = default;\n+};\n+\n // GlobalDecreasingSizeBestFitHeap collects the live intervals of all buffers,\n // then allocates them in decreasing spatial or temporal size regardless of the\n // alloc/free time. It internally tracks the allocated buffers and their live\n"
                },
                {
                    "old_start": 613,
                    "old_length": 14,
                    "new_start": 647,
                    "new_length": 6,
                    "hunk": "@@ -613,14 +647,6 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     // SlicedAllocationFinder() that permits placement at any offset.\n     static bool AllOffsetsAllowed(int64_t offset) { return true; }\n \n-    // A method that can be passed to SlicedAllocationFinder's\n-    // is_slice_time_permutation_allowed parameter that permits any\n-    // permutation.\n-    static bool AllSliceTimePermutationsAllowed(\n-        const std::vector<int64_t>& permutation_of_slice_times) {\n-      return true;\n-    }\n-\n     // Arguments:\n     // - free_chunks_per_slice_time[i]: Describes free chunks at slice time i.\n     // - sorted_slice_sizes: A sliced allocation request. In space, the i+1th\n"
                },
                {
                    "old_start": 629,
                    "old_length": 12,
                    "new_start": 655,
                    "new_length": 13,
                    "hunk": "@@ -629,12 +655,13 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     //   with the fully allocated sliced allocation.\n     // - preferred_offset: The preferred starting offset for the fully allocated\n     //   sliced allocation.\n+    // - slice_time_permutation_iterator: An iterator for iterating over the\n+    //   different slice time permutations for slices. Users may specify the\n+    //   order in which different permutations are tried by the HeapSimulator.\n+    //   Users are also responsbile for ensuring that returned permutations are\n+    //   legal.\n     // - is_offset_allowed: Indicates if a the entire sliced allocation is\n     //   allowed to be allocated at a given offset.\n-    // - is_slice_time_permutation_allowed: Indicates if the permutation of a\n-    //   vector of slice times is allowed. The vector V of slice times contains\n-    //   the values {0, ..., num_slices-1}. If V[i] = j, the slice at the ith\n-    //   smallest offset will start at the jth earliest slice start time.\n     //\n     // REQUIRES:\n     // - sorted_slice_sizes.size() == free_chunks_per_slice_time.size()\n"
                },
                {
                    "old_start": 649,
                    "old_length": 11,
                    "new_start": 676,
                    "new_length": 10,
                    "hunk": "@@ -649,11 +676,10 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n         absl::Span<const FreeChunks> free_chunks_per_slice_time,\n         std::vector<int64_t> sorted_slice_sizes, int64_t max_colocation_size,\n         int64_t preferred_offset, int64_t alignment,\n+        std::unique_ptr<SliceTimePermutationIterator>\n+            slice_time_permutation_iterator,\n         absl::AnyInvocable<bool(int64_t) const> is_offset_allowed =\n-            &AllOffsetsAllowed,\n-        absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-            is_slice_time_permutation_allowed =\n-                &AllSliceTimePermutationsAllowed);\n+            &AllOffsetsAllowed);\n \n     std::string FreeChunksToAsciiArt() const;\n     std::string ToString() const;\n"
                },
                {
                    "old_start": 690,
                    "old_length": 14,
                    "new_start": 716,
                    "new_length": 14,
                    "hunk": "@@ -690,14 +716,14 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     // sorted_slice_sizes_[i] and would be allocated at offset +\n     // sum(sorted_slice_sizes[j], for j in [0, i-1]).\n     Status DoesPermutationFit(\n-        const std::vector<int64_t>& permutation_of_slice_times,\n+        absl::Span<const int64_t> permutation_of_slice_times,\n         const FreeChunkRoot& root, int64_t offset) const;\n \n     // Only DoesSlicedPermutationFit() should call this method directly. Other\n     // callers should call DoesSlicedPermutationFit(), which contains some\n     // wrapper VLOGGING.\n     Status DoesPermutationFitImpl(\n-        const std::vector<int64_t>& permutation_of_slice_times,\n+        absl::Span<const int64_t> permutation_of_slice_times,\n         const FreeChunkRoot& root, int64_t offset) const;\n \n     // Same as Find() except only checks root, to see if it can hold the sliced\n"
                },
                {
                    "old_start": 717,
                    "old_length": 7,
                    "new_start": 743,
                    "new_length": 7,
                    "hunk": "@@ -717,7 +743,7 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     // end of the result to account for an additional colocation space that\n     // need to be allocated. This Chunk is added, even if it is of size 0.\n     ChunksSortedBySliceTime PermutationToChunks(\n-        const std::vector<int64_t>& permutation_of_slice_times,\n+        absl::Span<const int64_t> permutation_of_slice_times,\n         int64_t offset) const;\n \n     std::vector<int64_t> sorted_slice_sizes_;\n"
                },
                {
                    "old_start": 726,
                    "old_length": 9,
                    "new_start": 752,
                    "new_length": 9,
                    "hunk": "@@ -726,9 +752,9 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     int64_t preferred_offset_;\n     int64_t alignment_;\n     FreeChunkRoots free_chunks_;\n+    std::unique_ptr<SliceTimePermutationIterator>\n+        slice_time_permutation_iterator_;\n     absl::AnyInvocable<bool(int64_t) const> is_offset_allowed_;\n-    absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-        is_slice_time_permutation_allowed_;\n   };\n \n   explicit GlobalDecreasingSizeBestFitHeap(\n"
                },
                {
                    "old_start": 809,
                    "old_length": 11,
                    "new_start": 835,
                    "new_length": 10,
                    "hunk": "@@ -809,11 +835,10 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n   SlicedAllocationFinder CreateSlicedAllocationFinder(\n       const SlicedBufferInterval& sliced_interval, int64_t max_colocation_size,\n       int64_t preferred_offset,\n+      std::unique_ptr<SliceTimePermutationIterator>\n+          slice_time_permutation_iterator,\n       absl::AnyInvocable<bool(int64_t) const> is_offset_allowed =\n-          &SlicedAllocationFinder::AllOffsetsAllowed,\n-      absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-          is_slice_time_permutation_allowed =\n-              &SlicedAllocationFinder::AllSliceTimePermutationsAllowed) const;\n+          &SlicedAllocationFinder::AllOffsetsAllowed) const;\n   std::vector<Chunk> PostProcessFindChunkCandidatesResult(\n       const SlicedBufferInterval& sliced_interval,\n       std::vector<Chunk> chunks) const;\n"
                }
            ],
            "whole_deleted": "-    // A method that can be passed to SlicedAllocationFinder's\n-    // is_slice_time_permutation_allowed parameter that permits any\n-    // permutation.\n-    static bool AllSliceTimePermutationsAllowed(\n-        const std::vector<int64_t>& permutation_of_slice_times) {\n-      return true;\n-    }\n-\n-    // - is_slice_time_permutation_allowed: Indicates if the permutation of a\n-    //   vector of slice times is allowed. The vector V of slice times contains\n-    //   the values {0, ..., num_slices-1}. If V[i] = j, the slice at the ith\n-    //   smallest offset will start at the jth earliest slice start time.\n-            &AllOffsetsAllowed,\n-        absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-            is_slice_time_permutation_allowed =\n-                &AllSliceTimePermutationsAllowed);\n-        const std::vector<int64_t>& permutation_of_slice_times,\n-        const std::vector<int64_t>& permutation_of_slice_times,\n-        const std::vector<int64_t>& permutation_of_slice_times,\n-    absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-        is_slice_time_permutation_allowed_;\n-          &SlicedAllocationFinder::AllOffsetsAllowed,\n-      absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-          is_slice_time_permutation_allowed =\n-              &SlicedAllocationFinder::AllSliceTimePermutationsAllowed) const;\n",
            "whole_added": "+#include <list>\n+// An iterator that is passed to\n+// GlobalDecreasingSizeBestFitHeap::CreateSlicedAllocationFinder() when trying\n+// to place a buffer, telling the finder which permutations of starting slice\n+// times to try (and in which order to try them). Note, the set of slice times\n+// is the set {x : x \u2208 [0, num_slices - 1]}. If a buffer is not sliced, it will\n+// only have 1 permutation, containing slice time 0.\n+//\n+// Begin() must be called to initialize the iterator before it can be used.\n+class SliceTimePermutationIterator {\n+ public:\n+  // A new iterator is typically created for each buffer to be placed.\n+  // - num_slices: number of slices in the buffer. 1 if not sliced.\n+  // - original_sliced_allocation: For a repacking scenario, the original\n+  //   details of each slice in a sliced buffer. nullptr is used if this is not\n+  //   a repacking scenario or the buffer is not sliced.\n+  static std::unique_ptr<SliceTimePermutationIterator> Create(\n+      int64_t num_slices,\n+      const SlicedAllocationData* original_sliced_allocation = nullptr);\n+\n+  virtual ~SliceTimePermutationIterator() = default;\n+\n+  virtual void Begin() = 0;\n+  virtual bool Done() const = 0;\n+  virtual void Next() = 0;\n+\n+  // A permutation of starting slice times. The ith value is the slice time for\n+  // the slice at the ith smallest offset.\n+  virtual absl::Span<const int64_t> Get() const = 0;\n+\n+ protected:\n+  SliceTimePermutationIterator() = default;\n+};\n+\n+    // - slice_time_permutation_iterator: An iterator for iterating over the\n+    //   different slice time permutations for slices. Users may specify the\n+    //   order in which different permutations are tried by the HeapSimulator.\n+    //   Users are also responsbile for ensuring that returned permutations are\n+    //   legal.\n+        std::unique_ptr<SliceTimePermutationIterator>\n+            slice_time_permutation_iterator,\n+            &AllOffsetsAllowed);\n+        absl::Span<const int64_t> permutation_of_slice_times,\n+        absl::Span<const int64_t> permutation_of_slice_times,\n+        absl::Span<const int64_t> permutation_of_slice_times,\n+    std::unique_ptr<SliceTimePermutationIterator>\n+        slice_time_permutation_iterator_;\n+      std::unique_ptr<SliceTimePermutationIterator>\n+          slice_time_permutation_iterator,\n+          &SlicedAllocationFinder::AllOffsetsAllowed) const;\n",
            "whole_hunk": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <cstdint>\n #include <functional>\n #include <iostream>\n+#include <list>\n #include <memory>\n #include <optional>\n #include <set>\n@@ -387,6 +388,39 @@ class BufferIntervalTree {\n   std::list<BufferIntervalTreeNode> node_storage_;\n };\n \n+// An iterator that is passed to\n+// GlobalDecreasingSizeBestFitHeap::CreateSlicedAllocationFinder() when trying\n+// to place a buffer, telling the finder which permutations of starting slice\n+// times to try (and in which order to try them). Note, the set of slice times\n+// is the set {x : x \u2208 [0, num_slices - 1]}. If a buffer is not sliced, it will\n+// only have 1 permutation, containing slice time 0.\n+//\n+// Begin() must be called to initialize the iterator before it can be used.\n+class SliceTimePermutationIterator {\n+ public:\n+  // A new iterator is typically created for each buffer to be placed.\n+  // - num_slices: number of slices in the buffer. 1 if not sliced.\n+  // - original_sliced_allocation: For a repacking scenario, the original\n+  //   details of each slice in a sliced buffer. nullptr is used if this is not\n+  //   a repacking scenario or the buffer is not sliced.\n+  static std::unique_ptr<SliceTimePermutationIterator> Create(\n+      int64_t num_slices,\n+      const SlicedAllocationData* original_sliced_allocation = nullptr);\n+\n+  virtual ~SliceTimePermutationIterator() = default;\n+\n+  virtual void Begin() = 0;\n+  virtual bool Done() const = 0;\n+  virtual void Next() = 0;\n+\n+  // A permutation of starting slice times. The ith value is the slice time for\n+  // the slice at the ith smallest offset.\n+  virtual absl::Span<const int64_t> Get() const = 0;\n+\n+ protected:\n+  SliceTimePermutationIterator() = default;\n+};\n+\n // GlobalDecreasingSizeBestFitHeap collects the live intervals of all buffers,\n // then allocates them in decreasing spatial or temporal size regardless of the\n // alloc/free time. It internally tracks the allocated buffers and their live\n@@ -613,14 +647,6 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     // SlicedAllocationFinder() that permits placement at any offset.\n     static bool AllOffsetsAllowed(int64_t offset) { return true; }\n \n-    // A method that can be passed to SlicedAllocationFinder's\n-    // is_slice_time_permutation_allowed parameter that permits any\n-    // permutation.\n-    static bool AllSliceTimePermutationsAllowed(\n-        const std::vector<int64_t>& permutation_of_slice_times) {\n-      return true;\n-    }\n-\n     // Arguments:\n     // - free_chunks_per_slice_time[i]: Describes free chunks at slice time i.\n     // - sorted_slice_sizes: A sliced allocation request. In space, the i+1th\n@@ -629,12 +655,13 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     //   with the fully allocated sliced allocation.\n     // - preferred_offset: The preferred starting offset for the fully allocated\n     //   sliced allocation.\n+    // - slice_time_permutation_iterator: An iterator for iterating over the\n+    //   different slice time permutations for slices. Users may specify the\n+    //   order in which different permutations are tried by the HeapSimulator.\n+    //   Users are also responsbile for ensuring that returned permutations are\n+    //   legal.\n     // - is_offset_allowed: Indicates if a the entire sliced allocation is\n     //   allowed to be allocated at a given offset.\n-    // - is_slice_time_permutation_allowed: Indicates if the permutation of a\n-    //   vector of slice times is allowed. The vector V of slice times contains\n-    //   the values {0, ..., num_slices-1}. If V[i] = j, the slice at the ith\n-    //   smallest offset will start at the jth earliest slice start time.\n     //\n     // REQUIRES:\n     // - sorted_slice_sizes.size() == free_chunks_per_slice_time.size()\n@@ -649,11 +676,10 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n         absl::Span<const FreeChunks> free_chunks_per_slice_time,\n         std::vector<int64_t> sorted_slice_sizes, int64_t max_colocation_size,\n         int64_t preferred_offset, int64_t alignment,\n+        std::unique_ptr<SliceTimePermutationIterator>\n+            slice_time_permutation_iterator,\n         absl::AnyInvocable<bool(int64_t) const> is_offset_allowed =\n-            &AllOffsetsAllowed,\n-        absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-            is_slice_time_permutation_allowed =\n-                &AllSliceTimePermutationsAllowed);\n+            &AllOffsetsAllowed);\n \n     std::string FreeChunksToAsciiArt() const;\n     std::string ToString() const;\n@@ -690,14 +716,14 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     // sorted_slice_sizes_[i] and would be allocated at offset +\n     // sum(sorted_slice_sizes[j], for j in [0, i-1]).\n     Status DoesPermutationFit(\n-        const std::vector<int64_t>& permutation_of_slice_times,\n+        absl::Span<const int64_t> permutation_of_slice_times,\n         const FreeChunkRoot& root, int64_t offset) const;\n \n     // Only DoesSlicedPermutationFit() should call this method directly. Other\n     // callers should call DoesSlicedPermutationFit(), which contains some\n     // wrapper VLOGGING.\n     Status DoesPermutationFitImpl(\n-        const std::vector<int64_t>& permutation_of_slice_times,\n+        absl::Span<const int64_t> permutation_of_slice_times,\n         const FreeChunkRoot& root, int64_t offset) const;\n \n     // Same as Find() except only checks root, to see if it can hold the sliced\n@@ -717,7 +743,7 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     // end of the result to account for an additional colocation space that\n     // need to be allocated. This Chunk is added, even if it is of size 0.\n     ChunksSortedBySliceTime PermutationToChunks(\n-        const std::vector<int64_t>& permutation_of_slice_times,\n+        absl::Span<const int64_t> permutation_of_slice_times,\n         int64_t offset) const;\n \n     std::vector<int64_t> sorted_slice_sizes_;\n@@ -726,9 +752,9 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     int64_t preferred_offset_;\n     int64_t alignment_;\n     FreeChunkRoots free_chunks_;\n+    std::unique_ptr<SliceTimePermutationIterator>\n+        slice_time_permutation_iterator_;\n     absl::AnyInvocable<bool(int64_t) const> is_offset_allowed_;\n-    absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-        is_slice_time_permutation_allowed_;\n   };\n \n   explicit GlobalDecreasingSizeBestFitHeap(\n@@ -809,11 +835,10 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n   SlicedAllocationFinder CreateSlicedAllocationFinder(\n       const SlicedBufferInterval& sliced_interval, int64_t max_colocation_size,\n       int64_t preferred_offset,\n+      std::unique_ptr<SliceTimePermutationIterator>\n+          slice_time_permutation_iterator,\n       absl::AnyInvocable<bool(int64_t) const> is_offset_allowed =\n-          &SlicedAllocationFinder::AllOffsetsAllowed,\n-      absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-          is_slice_time_permutation_allowed =\n-              &SlicedAllocationFinder::AllSliceTimePermutationsAllowed) const;\n+          &SlicedAllocationFinder::AllOffsetsAllowed) const;\n   std::vector<Chunk> PostProcessFindChunkCandidatesResult(\n       const SlicedBufferInterval& sliced_interval,\n       std::vector<Chunk> chunks) const;\n"
        },
        {
            "name": "heap_simulator_test.cc",
            "path": "third_party/xla/xla/service/heap_simulator_test.cc",
            "patches": [
                {
                    "old_start": 2200,
                    "old_length": 8,
                    "new_start": 2200,
                    "new_length": 10,
                    "hunk": "@@ -2200,8 +2200,10 @@ The full buffer goes in the smallest chunk that fits.\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(Chunk::FromOffsetSize(45, 3),\n"
                },
                {
                    "old_start": 2234,
                    "old_length": 8,
                    "new_start": 2236,
                    "new_length": 10,
                    "hunk": "@@ -2234,8 +2236,10 @@ The max colocation size does not fit in the smallest free chunk.\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(Chunk::FromOffsetSize(60, 3),\n"
                },
                {
                    "old_start": 2269,
                    "old_length": 8,
                    "new_start": 2273,
                    "new_length": 10,
                    "hunk": "@@ -2269,8 +2273,10 @@ Multiple free chunks have size 3. We pick the one with the smallest offset.\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(Chunk::FromOffsetSize(10, 3),\n"
                },
                {
                    "old_start": 2317,
                    "old_length": 8,
                    "new_start": 2323,
                    "new_length": 10,
                    "hunk": "@@ -2317,8 +2323,10 @@ t0 |xxxxx  xxx                              xxxxx000xxxxxxxxxxxx          x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2366,
                    "old_length": 8,
                    "new_start": 2374,
                    "new_length": 10,
                    "hunk": "@@ -2366,8 +2374,10 @@ t0 |xxxxx  xxx                              xxxxxxxxxxx222xxxxxx          x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2415,
                    "old_length": 8,
                    "new_start": 2425,
                    "new_length": 10,
                    "hunk": "@@ -2415,8 +2425,10 @@ t0 |xxxxx  xxx                              xxxxxxxx111xxxxxxxxx          x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2469,
                    "old_length": 8,
                    "new_start": 2481,
                    "new_length": 10,
                    "hunk": "@@ -2469,8 +2481,10 @@ subsliced by MSA.)\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2515,
                    "old_length": 8,
                    "new_start": 2529,
                    "new_length": 10,
                    "hunk": "@@ -2515,8 +2529,10 @@ t0 |xxxxxx                                          111                 xxx\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2564,
                    "old_length": 8,
                    "new_start": 2580,
                    "new_length": 10,
                    "hunk": "@@ -2564,8 +2580,10 @@ t0 |xxxxx  xxx                              xxxxxx 111 xxxxxxxxx          x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2613,
                    "old_length": 8,
                    "new_start": 2631,
                    "new_length": 10,
                    "hunk": "@@ -2613,8 +2631,10 @@ t0 |xxxxx  xxx00000                         xxxxxx   xxxxxxxxxxx          x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2662,
                    "old_length": 8,
                    "new_start": 2682,
                    "new_length": 10,
                    "hunk": "@@ -2662,8 +2682,10 @@ t0 |xxxxxxxxxx                              xxxxxxxxxxxxxxxxxxxx000       x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2711,
                    "old_length": 8,
                    "new_start": 2733,
                    "new_length": 10,
                    "hunk": "@@ -2711,8 +2733,10 @@ t0 |xxxxx  xxx                              xxxxxxxx   xxxxxxxxx000       x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2760,
                    "old_length": 8,
                    "new_start": 2784,
                    "new_length": 10,
                    "hunk": "@@ -2760,8 +2784,10 @@ t0 |xxxxx  xxx          000                 xxxxxxxx   xxxxxxxxx          x\n   int64_t preferred_offset = 20;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2811,
                    "old_length": 8,
                    "new_start": 2837,
                    "new_length": 10,
                    "hunk": "@@ -2811,8 +2837,10 @@ The sliced allocation does not fit at the preferred offset.\n   int64_t preferred_offset = 35;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2863,
                    "old_length": 8,
                    "new_start": 2891,
                    "new_length": 10,
                    "hunk": "@@ -2863,8 +2891,10 @@ on spatial boundaries of 2.\n   int64_t preferred_offset = -1;\n   int64_t alignment = 2;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2915,
                    "old_length": 8,
                    "new_start": 2945,
                    "new_length": 10,
                    "hunk": "@@ -2915,8 +2945,10 @@ on spatial boundaries of 2.\n   int64_t preferred_offset = 21;\n   int64_t alignment = 2;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n"
                },
                {
                    "old_start": 2952,
                    "old_length": 8,
                    "new_start": 2984,
                    "new_length": 10,
                    "hunk": "@@ -2952,8 +2984,10 @@ t0 |xxxxx000   xxxx      xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(Chunk::FromOffsetSize(5, 3),\n"
                },
                {
                    "old_start": 2991,
                    "old_length": 8,
                    "new_start": 3025,
                    "new_length": 10,
                    "hunk": "@@ -2991,8 +3025,10 @@ t0 |xxxxx000        xxxx      xxxxxxxxxxxxxx   xxxxxxxxxxxxxxxxxxxxxxxxxxxx\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(Chunk::FromOffsetSize(5, 3),\n"
                },
                {
                    "old_start": 3043,
                    "old_length": 6,
                    "new_start": 3079,
                    "new_length": 7,
                    "hunk": "@@ -3043,6 +3079,7 @@ t0 |xxxxx  xxx                              xxxxx 000xxxxxxxxxxx          x\n   Finder finder(\n       free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n       preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n       /*is_offset_allowed=*/[](int64_t offset) { return offset != 45; });\n \n   EXPECT_THAT(finder.Find(),\n"
                },
                {
                    "old_start": 3094,
                    "old_length": 6,
                    "new_start": 3131,
                    "new_length": 7,
                    "hunk": "@@ -3094,6 +3131,7 @@ t0 |xxxxx  xxx                              xxxxx000xxxxxxxxxxxx          x\n   Finder finder(\n       free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n       preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n       // We're not allowed to start at offset 46, but we can include it.\n       /*is_offset_allowed=*/[](int64_t offset) { return offset != 46; });\n \n"
                },
                {
                    "old_start": 3146,
                    "old_length": 6,
                    "new_start": 3184,
                    "new_length": 7,
                    "hunk": "@@ -3146,6 +3184,7 @@ t0 |xxxxx  xxx                              xxxxxxxxxxx 222xxxxx          x\n   Finder finder(\n       free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n       preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n       /*is_offset_allowed=*/[](int64_t offset) { return offset != 45; });\n \n   EXPECT_THAT(finder.Find(),\n"
                },
                {
                    "old_start": 3197,
                    "old_length": 6,
                    "new_start": 3236,
                    "new_length": 7,
                    "hunk": "@@ -3197,6 +3236,7 @@ t0 |xxxxx  xxx                              xxxxxxxxxxx   xxxxxx000       x\n   Finder finder(\n       free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n       preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n       /*is_offset_allowed=*/[](int64_t offset) { return offset != 45; });\n \n   EXPECT_THAT(finder.Find(),\n"
                },
                {
                    "old_start": 3248,
                    "old_length": 6,
                    "new_start": 3288,
                    "new_length": 7,
                    "hunk": "@@ -3248,6 +3288,7 @@ t0 |xxxxx  xxx                              xxxxx    xxxxxxxxxxx          x\n   Finder finder(\n       free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n       preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n       /*is_offset_allowed=*/[](int64_t offset) { return offset != 45; });\n \n   EXPECT_THAT(finder.FindForOffset(10),\n"
                }
            ],
            "whole_deleted": "-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n",
            "whole_added": "+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n",
            "whole_hunk": "@@ -2200,8 +2200,10 @@ The full buffer goes in the smallest chunk that fits.\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(Chunk::FromOffsetSize(45, 3),\n@@ -2234,8 +2236,10 @@ The max colocation size does not fit in the smallest free chunk.\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(Chunk::FromOffsetSize(60, 3),\n@@ -2269,8 +2273,10 @@ Multiple free chunks have size 3. We pick the one with the smallest offset.\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(Chunk::FromOffsetSize(10, 3),\n@@ -2317,8 +2323,10 @@ t0 |xxxxx  xxx                              xxxxx000xxxxxxxxxxxx          x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2366,8 +2374,10 @@ t0 |xxxxx  xxx                              xxxxxxxxxxx222xxxxxx          x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2415,8 +2425,10 @@ t0 |xxxxx  xxx                              xxxxxxxx111xxxxxxxxx          x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2469,8 +2481,10 @@ subsliced by MSA.)\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2515,8 +2529,10 @@ t0 |xxxxxx                                          111                 xxx\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2564,8 +2580,10 @@ t0 |xxxxx  xxx                              xxxxxx 111 xxxxxxxxx          x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2613,8 +2631,10 @@ t0 |xxxxx  xxx00000                         xxxxxx   xxxxxxxxxxx          x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2662,8 +2682,10 @@ t0 |xxxxxxxxxx                              xxxxxxxxxxxxxxxxxxxx000       x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2711,8 +2733,10 @@ t0 |xxxxx  xxx                              xxxxxxxx   xxxxxxxxx000       x\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2760,8 +2784,10 @@ t0 |xxxxx  xxx          000                 xxxxxxxx   xxxxxxxxx          x\n   int64_t preferred_offset = 20;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2811,8 +2837,10 @@ The sliced allocation does not fit at the preferred offset.\n   int64_t preferred_offset = 35;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2863,8 +2891,10 @@ on spatial boundaries of 2.\n   int64_t preferred_offset = -1;\n   int64_t alignment = 2;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2915,8 +2945,10 @@ on spatial boundaries of 2.\n   int64_t preferred_offset = 21;\n   int64_t alignment = 2;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(\n@@ -2952,8 +2984,10 @@ t0 |xxxxx000   xxxx      xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(Chunk::FromOffsetSize(5, 3),\n@@ -2991,8 +3025,10 @@ t0 |xxxxx000        xxxx      xxxxxxxxxxxxxx   xxxxxxxxxxxxxxxxxxxxxxxxxxxx\n   int64_t preferred_offset = -1;\n   int64_t alignment = 1;\n \n-  Finder finder(free_chunks_per_slice_time, sorted_slice_sizes,\n-                max_colocation_size, preferred_offset, alignment);\n+  Finder finder(\n+      free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n+      preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()));\n \n   EXPECT_THAT(finder.Find(),\n               ::testing::ElementsAre(Chunk::FromOffsetSize(5, 3),\n@@ -3043,6 +3079,7 @@ t0 |xxxxx  xxx                              xxxxx 000xxxxxxxxxxx          x\n   Finder finder(\n       free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n       preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n       /*is_offset_allowed=*/[](int64_t offset) { return offset != 45; });\n \n   EXPECT_THAT(finder.Find(),\n@@ -3094,6 +3131,7 @@ t0 |xxxxx  xxx                              xxxxx000xxxxxxxxxxxx          x\n   Finder finder(\n       free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n       preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n       // We're not allowed to start at offset 46, but we can include it.\n       /*is_offset_allowed=*/[](int64_t offset) { return offset != 46; });\n \n@@ -3146,6 +3184,7 @@ t0 |xxxxx  xxx                              xxxxxxxxxxx 222xxxxx          x\n   Finder finder(\n       free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n       preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n       /*is_offset_allowed=*/[](int64_t offset) { return offset != 45; });\n \n   EXPECT_THAT(finder.Find(),\n@@ -3197,6 +3236,7 @@ t0 |xxxxx  xxx                              xxxxxxxxxxx   xxxxxx000       x\n   Finder finder(\n       free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n       preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n       /*is_offset_allowed=*/[](int64_t offset) { return offset != 45; });\n \n   EXPECT_THAT(finder.Find(),\n@@ -3248,6 +3288,7 @@ t0 |xxxxx  xxx                              xxxxx    xxxxxxxxxxx          x\n   Finder finder(\n       free_chunks_per_slice_time, sorted_slice_sizes, max_colocation_size,\n       preferred_offset, alignment,\n+      SliceTimePermutationIterator::Create(sorted_slice_sizes.size()),\n       /*is_offset_allowed=*/[](int64_t offset) { return offset != 45; });\n \n   EXPECT_THAT(finder.FindForOffset(10),\n"
        },
        {
            "name": "best_fit_repacker.cc",
            "path": "third_party/xla/xla/service/memory_space_assignment/best_fit_repacker.cc",
            "patches": [
                {
                    "old_start": 161,
                    "old_length": 6,
                    "new_start": 161,
                    "new_length": 14,
                    "hunk": "@@ -161,6 +161,14 @@ std::vector<const AllocationBlock*> SortAllocationBlocks(const T& container) {\n   return result;\n }\n \n+const SlicedAllocationData* GetSlicedAllocationDataPointer(\n+    const std::optional<SlicedAllocationData>& sliced_allocation_data) {\n+  if (!sliced_allocation_data.has_value()) {\n+    return nullptr;\n+  }\n+  return &(*sliced_allocation_data);\n+}\n+\n // A slice-aware best-fit repacker.\n class BestFitRepacker\n     : public GlobalDecreasingSizeBestFitHeap<AllocationBlock> {\n"
                },
                {
                    "old_start": 411,
                    "old_length": 8,
                    "new_start": 419,
                    "new_length": 11,
                    "hunk": "@@ -411,8 +419,11 @@ class BestFitRepacker\n             CreateSlicedAllocationFinder(\n                 colocation_sliced_buffer_interval, max_colocation_size,\n                 /*preferred_offset=*/-1,\n-                &SlicedAllocationFinder::AllOffsetsAllowed,\n-                CreateIsSliceTimePermutationAllowedFn(colocation));\n+                SliceTimePermutationIterator::Create(\n+                    colocation_sliced_buffer_interval.num_slices(),\n+                    GetSlicedAllocationDataPointer(\n+                        colocation->original_slice_data)),\n+                &SlicedAllocationFinder::AllOffsetsAllowed);\n         sliced_buffer_map.insert(std::make_pair(\n             colocation,\n             SlicedColocationData{&colocation_sliced_buffer_interval,\n"
                },
                {
                    "old_start": 446,
                    "old_length": 8,
                    "new_start": 457,
                    "new_length": 11,
                    "hunk": "@@ -446,8 +457,11 @@ class BestFitRepacker\n     // Find chunks for allocation_block and its colocations.\n     SlicedAllocationFinder finder = CreateSlicedAllocationFinder(\n         sliced_buffer_interval, max_colocation_size, /*preferred_offset=*/-1,\n-        is_offset_allowed,\n-        CreateIsSliceTimePermutationAllowedFn(allocation_block));\n+        SliceTimePermutationIterator::Create(\n+            sliced_buffer_interval.num_slices(),\n+            GetSlicedAllocationDataPointer(\n+                allocation_block->original_slice_data)),\n+        is_offset_allowed);\n     std::vector<Chunk> chunks = PostProcessFindChunkCandidatesResult(\n         sliced_buffer_interval, finder.Find());\n     int64_t min_offset =\n"
                },
                {
                    "old_start": 590,
                    "old_length": 78,
                    "new_start": 604,
                    "new_length": 6,
                    "hunk": "@@ -590,78 +604,6 @@ class BestFitRepacker\n   }\n \n  private:\n-  // A slice time permutation is allowed if the original mapping between slice\n-  // sizes and slice start times is preserved with the new permutation.\n-  static absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-  CreateIsSliceTimePermutationAllowedFn(\n-      const AllocationBlock* allocation_block) {\n-    if (!IsSliced(allocation_block)) {\n-      return &SlicedAllocationFinder::AllSliceTimePermutationsAllowed;\n-    }\n-\n-    int64_t num_slices =\n-        allocation_block->original_slice_data->slices_sorted_by_offset.size();\n-\n-    // Element slice_time_to_schedule_time_inclusive[i] is the (inclusive)\n-    // schedule time corresponding to slice_time i.\n-    std::vector<int64_t> slice_time_to_schedule_time_inclusive =\n-        allocation_block->original_slice_data->SortedInclusiveStartTimes();\n-    absl::c_sort(slice_time_to_schedule_time_inclusive);\n-\n-    // Compute the original slice size to slice start time mapping.\n-    std::vector<std::pair<int64_t, int64_t>>\n-        original_slice_sizes_and_start_times_pairwise_sorted;\n-    original_slice_sizes_and_start_times_pairwise_sorted.reserve(num_slices);\n-    for (const AllocatedSlice& slice :\n-         allocation_block->original_slice_data->slices_sorted_by_offset) {\n-      original_slice_sizes_and_start_times_pairwise_sorted.push_back(\n-          std::make_pair(slice.size, slice.inclusive_start_time));\n-    }\n-    absl::c_sort(original_slice_sizes_and_start_times_pairwise_sorted);\n-\n-    std::vector<int64_t> sizes_sorted_by_offset =\n-        allocation_block->original_slice_data->SizesSortedByOffset();\n-\n-    return [original_slice_sizes_and_start_times_pairwise_sorted,\n-            sizes_sorted_by_offset, slice_time_to_schedule_time_inclusive,\n-            num_slices](const std::vector<int64_t>& slice_time_permutation) {\n-      // Compute the slice size to slice start time mapping proposed by the\n-      // permutation.\n-      std::vector<std::pair<int64_t, int64_t>>\n-          proposed_slice_sizes_and_start_times_pairwise_sorted;\n-      proposed_slice_sizes_and_start_times_pairwise_sorted.reserve(num_slices);\n-      CHECK_EQ(sizes_sorted_by_offset.size(), num_slices);\n-      CHECK_EQ(slice_time_permutation.size(), num_slices);\n-      for (int i = 0; i < num_slices; ++i) {\n-        proposed_slice_sizes_and_start_times_pairwise_sorted.push_back(\n-            std::make_pair(sizes_sorted_by_offset[i],\n-                           slice_time_to_schedule_time_inclusive\n-                               [slice_time_permutation[i]]));\n-      }\n-      absl::c_sort(proposed_slice_sizes_and_start_times_pairwise_sorted);\n-\n-      bool allowed = (original_slice_sizes_and_start_times_pairwise_sorted ==\n-                      proposed_slice_sizes_and_start_times_pairwise_sorted);\n-      VLOG(3) << [&]() {\n-        auto export_pair = [](std::string* out,\n-                              const std::pair<int64_t, int64_t>& p) {\n-          absl::StrAppend(out, \"<\", p.first, \", \", p.second, \">\");\n-        };\n-        return absl::StrCat(\n-            \"Slice permutation \", (allowed ? \"allowed\" : \"disallowed\"),\n-            \". Original slice <size, start_time> mapping: \",\n-            absl::StrJoin(original_slice_sizes_and_start_times_pairwise_sorted,\n-                          \", \", export_pair),\n-            \". Proposed mapping: \",\n-            absl::StrJoin(proposed_slice_sizes_and_start_times_pairwise_sorted,\n-                          \", \", export_pair),\n-            \".\");\n-      }();\n-\n-      return allowed;\n-    };\n-  }\n-\n   // If true, we run a potentially expensive validation to make sure there are\n   // no overlaps in the repacked chunks. Note, there should never be an overlap.\n   bool validate_ = false;"
                }
            ],
            "whole_deleted": "-                &SlicedAllocationFinder::AllOffsetsAllowed,\n-                CreateIsSliceTimePermutationAllowedFn(colocation));\n-        is_offset_allowed,\n-        CreateIsSliceTimePermutationAllowedFn(allocation_block));\n-  // A slice time permutation is allowed if the original mapping between slice\n-  // sizes and slice start times is preserved with the new permutation.\n-  static absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-  CreateIsSliceTimePermutationAllowedFn(\n-      const AllocationBlock* allocation_block) {\n-    if (!IsSliced(allocation_block)) {\n-      return &SlicedAllocationFinder::AllSliceTimePermutationsAllowed;\n-    }\n-\n-    int64_t num_slices =\n-        allocation_block->original_slice_data->slices_sorted_by_offset.size();\n-\n-    // Element slice_time_to_schedule_time_inclusive[i] is the (inclusive)\n-    // schedule time corresponding to slice_time i.\n-    std::vector<int64_t> slice_time_to_schedule_time_inclusive =\n-        allocation_block->original_slice_data->SortedInclusiveStartTimes();\n-    absl::c_sort(slice_time_to_schedule_time_inclusive);\n-\n-    // Compute the original slice size to slice start time mapping.\n-    std::vector<std::pair<int64_t, int64_t>>\n-        original_slice_sizes_and_start_times_pairwise_sorted;\n-    original_slice_sizes_and_start_times_pairwise_sorted.reserve(num_slices);\n-    for (const AllocatedSlice& slice :\n-         allocation_block->original_slice_data->slices_sorted_by_offset) {\n-      original_slice_sizes_and_start_times_pairwise_sorted.push_back(\n-          std::make_pair(slice.size, slice.inclusive_start_time));\n-    }\n-    absl::c_sort(original_slice_sizes_and_start_times_pairwise_sorted);\n-\n-    std::vector<int64_t> sizes_sorted_by_offset =\n-        allocation_block->original_slice_data->SizesSortedByOffset();\n-\n-    return [original_slice_sizes_and_start_times_pairwise_sorted,\n-            sizes_sorted_by_offset, slice_time_to_schedule_time_inclusive,\n-            num_slices](const std::vector<int64_t>& slice_time_permutation) {\n-      // Compute the slice size to slice start time mapping proposed by the\n-      // permutation.\n-      std::vector<std::pair<int64_t, int64_t>>\n-          proposed_slice_sizes_and_start_times_pairwise_sorted;\n-      proposed_slice_sizes_and_start_times_pairwise_sorted.reserve(num_slices);\n-      CHECK_EQ(sizes_sorted_by_offset.size(), num_slices);\n-      CHECK_EQ(slice_time_permutation.size(), num_slices);\n-      for (int i = 0; i < num_slices; ++i) {\n-        proposed_slice_sizes_and_start_times_pairwise_sorted.push_back(\n-            std::make_pair(sizes_sorted_by_offset[i],\n-                           slice_time_to_schedule_time_inclusive\n-                               [slice_time_permutation[i]]));\n-      }\n-      absl::c_sort(proposed_slice_sizes_and_start_times_pairwise_sorted);\n-\n-      bool allowed = (original_slice_sizes_and_start_times_pairwise_sorted ==\n-                      proposed_slice_sizes_and_start_times_pairwise_sorted);\n-      VLOG(3) << [&]() {\n-        auto export_pair = [](std::string* out,\n-                              const std::pair<int64_t, int64_t>& p) {\n-          absl::StrAppend(out, \"<\", p.first, \", \", p.second, \">\");\n-        };\n-        return absl::StrCat(\n-            \"Slice permutation \", (allowed ? \"allowed\" : \"disallowed\"),\n-            \". Original slice <size, start_time> mapping: \",\n-            absl::StrJoin(original_slice_sizes_and_start_times_pairwise_sorted,\n-                          \", \", export_pair),\n-            \". Proposed mapping: \",\n-            absl::StrJoin(proposed_slice_sizes_and_start_times_pairwise_sorted,\n-                          \", \", export_pair),\n-            \".\");\n-      }();\n-\n-      return allowed;\n-    };\n-  }\n-\n",
            "whole_added": "+const SlicedAllocationData* GetSlicedAllocationDataPointer(\n+    const std::optional<SlicedAllocationData>& sliced_allocation_data) {\n+  if (!sliced_allocation_data.has_value()) {\n+    return nullptr;\n+  }\n+  return &(*sliced_allocation_data);\n+}\n+\n+                SliceTimePermutationIterator::Create(\n+                    colocation_sliced_buffer_interval.num_slices(),\n+                    GetSlicedAllocationDataPointer(\n+                        colocation->original_slice_data)),\n+                &SlicedAllocationFinder::AllOffsetsAllowed);\n+        SliceTimePermutationIterator::Create(\n+            sliced_buffer_interval.num_slices(),\n+            GetSlicedAllocationDataPointer(\n+                allocation_block->original_slice_data)),\n+        is_offset_allowed);\n",
            "whole_hunk": "@@ -161,6 +161,14 @@ std::vector<const AllocationBlock*> SortAllocationBlocks(const T& container) {\n   return result;\n }\n \n+const SlicedAllocationData* GetSlicedAllocationDataPointer(\n+    const std::optional<SlicedAllocationData>& sliced_allocation_data) {\n+  if (!sliced_allocation_data.has_value()) {\n+    return nullptr;\n+  }\n+  return &(*sliced_allocation_data);\n+}\n+\n // A slice-aware best-fit repacker.\n class BestFitRepacker\n     : public GlobalDecreasingSizeBestFitHeap<AllocationBlock> {\n@@ -411,8 +419,11 @@ class BestFitRepacker\n             CreateSlicedAllocationFinder(\n                 colocation_sliced_buffer_interval, max_colocation_size,\n                 /*preferred_offset=*/-1,\n-                &SlicedAllocationFinder::AllOffsetsAllowed,\n-                CreateIsSliceTimePermutationAllowedFn(colocation));\n+                SliceTimePermutationIterator::Create(\n+                    colocation_sliced_buffer_interval.num_slices(),\n+                    GetSlicedAllocationDataPointer(\n+                        colocation->original_slice_data)),\n+                &SlicedAllocationFinder::AllOffsetsAllowed);\n         sliced_buffer_map.insert(std::make_pair(\n             colocation,\n             SlicedColocationData{&colocation_sliced_buffer_interval,\n@@ -446,8 +457,11 @@ class BestFitRepacker\n     // Find chunks for allocation_block and its colocations.\n     SlicedAllocationFinder finder = CreateSlicedAllocationFinder(\n         sliced_buffer_interval, max_colocation_size, /*preferred_offset=*/-1,\n-        is_offset_allowed,\n-        CreateIsSliceTimePermutationAllowedFn(allocation_block));\n+        SliceTimePermutationIterator::Create(\n+            sliced_buffer_interval.num_slices(),\n+            GetSlicedAllocationDataPointer(\n+                allocation_block->original_slice_data)),\n+        is_offset_allowed);\n     std::vector<Chunk> chunks = PostProcessFindChunkCandidatesResult(\n         sliced_buffer_interval, finder.Find());\n     int64_t min_offset =\n@@ -590,78 +604,6 @@ class BestFitRepacker\n   }\n \n  private:\n-  // A slice time permutation is allowed if the original mapping between slice\n-  // sizes and slice start times is preserved with the new permutation.\n-  static absl::AnyInvocable<bool(const std::vector<int64_t>&) const>\n-  CreateIsSliceTimePermutationAllowedFn(\n-      const AllocationBlock* allocation_block) {\n-    if (!IsSliced(allocation_block)) {\n-      return &SlicedAllocationFinder::AllSliceTimePermutationsAllowed;\n-    }\n-\n-    int64_t num_slices =\n-        allocation_block->original_slice_data->slices_sorted_by_offset.size();\n-\n-    // Element slice_time_to_schedule_time_inclusive[i] is the (inclusive)\n-    // schedule time corresponding to slice_time i.\n-    std::vector<int64_t> slice_time_to_schedule_time_inclusive =\n-        allocation_block->original_slice_data->SortedInclusiveStartTimes();\n-    absl::c_sort(slice_time_to_schedule_time_inclusive);\n-\n-    // Compute the original slice size to slice start time mapping.\n-    std::vector<std::pair<int64_t, int64_t>>\n-        original_slice_sizes_and_start_times_pairwise_sorted;\n-    original_slice_sizes_and_start_times_pairwise_sorted.reserve(num_slices);\n-    for (const AllocatedSlice& slice :\n-         allocation_block->original_slice_data->slices_sorted_by_offset) {\n-      original_slice_sizes_and_start_times_pairwise_sorted.push_back(\n-          std::make_pair(slice.size, slice.inclusive_start_time));\n-    }\n-    absl::c_sort(original_slice_sizes_and_start_times_pairwise_sorted);\n-\n-    std::vector<int64_t> sizes_sorted_by_offset =\n-        allocation_block->original_slice_data->SizesSortedByOffset();\n-\n-    return [original_slice_sizes_and_start_times_pairwise_sorted,\n-            sizes_sorted_by_offset, slice_time_to_schedule_time_inclusive,\n-            num_slices](const std::vector<int64_t>& slice_time_permutation) {\n-      // Compute the slice size to slice start time mapping proposed by the\n-      // permutation.\n-      std::vector<std::pair<int64_t, int64_t>>\n-          proposed_slice_sizes_and_start_times_pairwise_sorted;\n-      proposed_slice_sizes_and_start_times_pairwise_sorted.reserve(num_slices);\n-      CHECK_EQ(sizes_sorted_by_offset.size(), num_slices);\n-      CHECK_EQ(slice_time_permutation.size(), num_slices);\n-      for (int i = 0; i < num_slices; ++i) {\n-        proposed_slice_sizes_and_start_times_pairwise_sorted.push_back(\n-            std::make_pair(sizes_sorted_by_offset[i],\n-                           slice_time_to_schedule_time_inclusive\n-                               [slice_time_permutation[i]]));\n-      }\n-      absl::c_sort(proposed_slice_sizes_and_start_times_pairwise_sorted);\n-\n-      bool allowed = (original_slice_sizes_and_start_times_pairwise_sorted ==\n-                      proposed_slice_sizes_and_start_times_pairwise_sorted);\n-      VLOG(3) << [&]() {\n-        auto export_pair = [](std::string* out,\n-                              const std::pair<int64_t, int64_t>& p) {\n-          absl::StrAppend(out, \"<\", p.first, \", \", p.second, \">\");\n-        };\n-        return absl::StrCat(\n-            \"Slice permutation \", (allowed ? \"allowed\" : \"disallowed\"),\n-            \". Original slice <size, start_time> mapping: \",\n-            absl::StrJoin(original_slice_sizes_and_start_times_pairwise_sorted,\n-                          \", \", export_pair),\n-            \". Proposed mapping: \",\n-            absl::StrJoin(proposed_slice_sizes_and_start_times_pairwise_sorted,\n-                          \", \", export_pair),\n-            \".\");\n-      }();\n-\n-      return allowed;\n-    };\n-  }\n-\n   // If true, we run a potentially expensive validation to make sure there are\n   // no overlaps in the repacked chunks. Note, there should never be an overlap.\n   bool validate_ = false;"
        }
    ]
},
{
    "Id": 390,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2dcf5bb323ab773ad13633cd0e4998d420eff0b4",
    "date": "2023-06-23T15:16:23-07:00",
    "message": "Update type annotations and checks for FunctionType\n\nPiperOrigin-RevId: 542969197",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/core/function/polymorphism/BUILD",
            "patches": [
                {
                    "old_start": 76,
                    "old_length": 6,
                    "new_start": 76,
                    "new_length": 7,
                    "hunk": "@@ -76,6 +76,7 @@ pytype_strict_library(\n         \"//tensorflow/core/function/polymorphism:function_type_proto_py\",\n         \"//tensorflow/core/function/trace_type\",\n         \"//tensorflow/core/function/trace_type:serialization\",\n+        \"//tensorflow/python/types:core\",\n         \"//tensorflow/python/types:trace\",\n         \"@absl_py//absl/logging\",\n     ],\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/python/types:core\",\n",
            "whole_hunk": "@@ -76,6 +76,7 @@ pytype_strict_library(\n         \"//tensorflow/core/function/polymorphism:function_type_proto_py\",\n         \"//tensorflow/core/function/trace_type\",\n         \"//tensorflow/core/function/trace_type:serialization\",\n+        \"//tensorflow/python/types:core\",\n         \"//tensorflow/python/types:trace\",\n         \"@absl_py//absl/logging\",\n     ],\n"
        },
        {
            "name": "function_type.py",
            "path": "tensorflow/core/function/polymorphism/function_type.py",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 15,
                    "new_start": 16,
                    "new_length": 17,
                    "hunk": "@@ -16,15 +16,17 @@\n \n import collections\n import inspect\n-from typing import Any, Callable, Dict, Mapping, Optional, Sequence, Tuple\n+from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Tuple\n \n from absl import logging\n \n from tensorflow.core.function import trace_type\n from tensorflow.core.function.polymorphism import function_type_pb2\n from tensorflow.core.function.trace_type import serialization\n+from tensorflow.python.types import core\n from tensorflow.python.types import trace\n \n+\n # Represents a defined parameter default value that is saved alongside the\n # function's captures.\n CAPTURED_DEFAULT_VALUE = object()\n"
                },
                {
                    "old_start": 202,
                    "old_length": 7,
                    "new_start": 204,
                    "new_length": 6,
                    "hunk": "@@ -202,7 +204,6 @@ class FunctionType(inspect.Signature):\n         else None\n     )\n \n-  # TODO(fmuham): Use this method instead of fullargspec and tf_inspect.\n   @classmethod\n   def from_callable(cls,\n                     obj: Callable[..., Any],\n"
                },
                {
                    "old_start": 351,
                    "old_length": 7,
                    "new_start": 352,
                    "new_length": 7,
                    "hunk": "@@ -351,7 +352,7 @@ class FunctionType(inspect.Signature):\n     return inspect.BoundArguments(self, arguments)\n \n   @property\n-  def flat_inputs(self):\n+  def flat_inputs(self) -> List[trace.TraceType]:\n     \"\"\"Flat tensor inputs accepted by this FunctionType.\"\"\"\n     if not hasattr(self, \"_cached_flat_inputs\"):\n       self._cached_flat_inputs = []\n"
                },
                {
                    "old_start": 360,
                    "old_length": 7,
                    "new_start": 361,
                    "new_length": 9,
                    "hunk": "@@ -360,7 +361,9 @@ class FunctionType(inspect.Signature):\n \n     return self._cached_flat_inputs\n \n-  def unpack_inputs(self, bound_parameters: inspect.BoundArguments):\n+  def unpack_inputs(\n+      self, bound_parameters: inspect.BoundArguments\n+  ) -> List[core.Tensor]:\n     \"\"\"Unpacks python arguments to flat tensor inputs accepted by this type.\"\"\"\n     # Sort keyword-only parameters by name.\n     sorted_parameters = []\n"
                },
                {
                    "old_start": 393,
                    "old_length": 7,
                    "new_start": 396,
                    "new_length": 7,
                    "hunk": "@@ -393,7 +396,7 @@ class FunctionType(inspect.Signature):\n     return dealiased_inputs\n \n   @property\n-  def flat_captures(self):\n+  def flat_captures(self) -> List[trace.TraceType]:\n     \"\"\"Flat tensor captures needed by this FunctionType.\"\"\"\n     if not hasattr(self, \"_cached_flat_captures\"):\n       self._cached_flat_captures = []\n"
                },
                {
                    "old_start": 402,
                    "old_length": 15,
                    "new_start": 405,
                    "new_length": 20,
                    "hunk": "@@ -402,15 +405,20 @@ class FunctionType(inspect.Signature):\n \n     return self._cached_flat_captures\n \n-  def unpack_captures(self, captures):\n+  def unpack_captures(self, captures) -> List[core.Tensor]:\n     \"\"\"Unpacks captures to flat tensors.\"\"\"\n     flat = []\n     for v, t in zip(captures, self.captures.values()):\n       flat.extend(t._to_tensors(v))  # pylint: disable=protected-access\n+    if len(flat) != len(self.flat_captures):\n+      raise TypeError(\n+          f\"Flattening captures {captures} with type {self!r} produced\"\n+          f\" {len(flat)} tensors instead of {len(self.flat_captures)}\"\n+      )\n     return flat\n \n   @property\n-  def flat_outputs(self):\n+  def flat_outputs(self) -> List[trace.TraceType]:\n     \"\"\"Flat tensor outputs returned by this FunctionType.\"\"\"\n     if not hasattr(self, \"_cached_flat_outputs\"):\n       if self.output is not None:\n"
                },
                {
                    "old_start": 418,
                    "old_length": 7,
                    "new_start": 426,
                    "new_length": 7,
                    "hunk": "@@ -418,7 +426,7 @@ class FunctionType(inspect.Signature):\n \n     return self._cached_flat_outputs\n \n-  def pack_output(self, flat_values):\n+  def pack_output(self, flat_values: Sequence[core.Tensor]) -> Any:\n     \"\"\"Packs flat tensors to generate a value of the output type.\"\"\"\n     if flat_values is None:\n       flat_values = []"
                }
            ],
            "whole_deleted": "-from typing import Any, Callable, Dict, Mapping, Optional, Sequence, Tuple\n-  # TODO(fmuham): Use this method instead of fullargspec and tf_inspect.\n-  def flat_inputs(self):\n-  def unpack_inputs(self, bound_parameters: inspect.BoundArguments):\n-  def flat_captures(self):\n-  def unpack_captures(self, captures):\n-  def flat_outputs(self):\n-  def pack_output(self, flat_values):\n",
            "whole_added": "+from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Tuple\n+from tensorflow.python.types import core\n+\n+  def flat_inputs(self) -> List[trace.TraceType]:\n+  def unpack_inputs(\n+      self, bound_parameters: inspect.BoundArguments\n+  ) -> List[core.Tensor]:\n+  def flat_captures(self) -> List[trace.TraceType]:\n+  def unpack_captures(self, captures) -> List[core.Tensor]:\n+    if len(flat) != len(self.flat_captures):\n+      raise TypeError(\n+          f\"Flattening captures {captures} with type {self!r} produced\"\n+          f\" {len(flat)} tensors instead of {len(self.flat_captures)}\"\n+      )\n+  def flat_outputs(self) -> List[trace.TraceType]:\n+  def pack_output(self, flat_values: Sequence[core.Tensor]) -> Any:\n",
            "whole_hunk": "@@ -16,15 +16,17 @@\n \n import collections\n import inspect\n-from typing import Any, Callable, Dict, Mapping, Optional, Sequence, Tuple\n+from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Tuple\n \n from absl import logging\n \n from tensorflow.core.function import trace_type\n from tensorflow.core.function.polymorphism import function_type_pb2\n from tensorflow.core.function.trace_type import serialization\n+from tensorflow.python.types import core\n from tensorflow.python.types import trace\n \n+\n # Represents a defined parameter default value that is saved alongside the\n # function's captures.\n CAPTURED_DEFAULT_VALUE = object()\n@@ -202,7 +204,6 @@ class FunctionType(inspect.Signature):\n         else None\n     )\n \n-  # TODO(fmuham): Use this method instead of fullargspec and tf_inspect.\n   @classmethod\n   def from_callable(cls,\n                     obj: Callable[..., Any],\n@@ -351,7 +352,7 @@ class FunctionType(inspect.Signature):\n     return inspect.BoundArguments(self, arguments)\n \n   @property\n-  def flat_inputs(self):\n+  def flat_inputs(self) -> List[trace.TraceType]:\n     \"\"\"Flat tensor inputs accepted by this FunctionType.\"\"\"\n     if not hasattr(self, \"_cached_flat_inputs\"):\n       self._cached_flat_inputs = []\n@@ -360,7 +361,9 @@ class FunctionType(inspect.Signature):\n \n     return self._cached_flat_inputs\n \n-  def unpack_inputs(self, bound_parameters: inspect.BoundArguments):\n+  def unpack_inputs(\n+      self, bound_parameters: inspect.BoundArguments\n+  ) -> List[core.Tensor]:\n     \"\"\"Unpacks python arguments to flat tensor inputs accepted by this type.\"\"\"\n     # Sort keyword-only parameters by name.\n     sorted_parameters = []\n@@ -393,7 +396,7 @@ class FunctionType(inspect.Signature):\n     return dealiased_inputs\n \n   @property\n-  def flat_captures(self):\n+  def flat_captures(self) -> List[trace.TraceType]:\n     \"\"\"Flat tensor captures needed by this FunctionType.\"\"\"\n     if not hasattr(self, \"_cached_flat_captures\"):\n       self._cached_flat_captures = []\n@@ -402,15 +405,20 @@ class FunctionType(inspect.Signature):\n \n     return self._cached_flat_captures\n \n-  def unpack_captures(self, captures):\n+  def unpack_captures(self, captures) -> List[core.Tensor]:\n     \"\"\"Unpacks captures to flat tensors.\"\"\"\n     flat = []\n     for v, t in zip(captures, self.captures.values()):\n       flat.extend(t._to_tensors(v))  # pylint: disable=protected-access\n+    if len(flat) != len(self.flat_captures):\n+      raise TypeError(\n+          f\"Flattening captures {captures} with type {self!r} produced\"\n+          f\" {len(flat)} tensors instead of {len(self.flat_captures)}\"\n+      )\n     return flat\n \n   @property\n-  def flat_outputs(self):\n+  def flat_outputs(self) -> List[trace.TraceType]:\n     \"\"\"Flat tensor outputs returned by this FunctionType.\"\"\"\n     if not hasattr(self, \"_cached_flat_outputs\"):\n       if self.output is not None:\n@@ -418,7 +426,7 @@ class FunctionType(inspect.Signature):\n \n     return self._cached_flat_outputs\n \n-  def pack_output(self, flat_values):\n+  def pack_output(self, flat_values: Sequence[core.Tensor]) -> Any:\n     \"\"\"Packs flat tensors to generate a value of the output type.\"\"\"\n     if flat_values is None:\n       flat_values = []"
        }
    ]
},
{
    "Id": 65,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1cd945adaf5fcee4bc4497c87529586765e52e10",
    "date": "2024-04-25T10:41:07-07:00",
    "message": "[pjrt] Add a run time check to guarantee that only one unique future created from a promise for move-only types\n\nPiperOrigin-RevId: 628114989",
    "label": "NO",
    "changes": [
        {
            "name": "pjrt_future.h",
            "path": "third_party/xla/xla/pjrt/pjrt_future.h",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 8,
                    "new_start": 17,
                    "new_length": 10,
                    "hunk": "@@ -17,8 +17,10 @@ limitations under the License.\n #define XLA_PJRT_PJRT_FUTURE_H_\n \n #include <algorithm>\n+#include <atomic>\n #include <cstdint>\n #include <functional>\n+#include <memory>\n #include <optional>\n #include <type_traits>\n #include <utility>\n"
                },
                {
                    "old_start": 185,
                    "old_length": 11,
                    "new_start": 187,
                    "new_length": 8,
                    "hunk": "@@ -185,11 +187,8 @@ class PjRtFutureBase : public PjRtFutureMoveControl<\n     Promise(Promise&& other) = default;\n     Promise& operator=(Promise&& other) = default;\n \n-    Promise(const Promise& other) : ref_(other.ref_.CopyRef()) {}\n-    Promise& operator=(const Promise& other) {\n-      ref_ = other.ref_.CopyRef();\n-      return *this;\n-    }\n+    Promise(const Promise& other) = default;\n+    Promise& operator=(const Promise& other) = default;\n \n     operator bool() const { return static_cast<bool>(ref_); }  // NOLINT\n \n"
                },
                {
                    "old_start": 215,
                    "old_length": 7,
                    "new_start": 214,
                    "new_length": 7,
                    "hunk": "@@ -215,7 +214,7 @@ class PjRtFutureBase : public PjRtFutureMoveControl<\n       ref_.template emplace<T>(std::forward<Args>(args)...);\n     }\n \n-    tsl::AsyncValueRef<T> ExtractRef() && { return std::move(ref_); }\n+    tsl::AsyncValueRef<T> release() { return std::move(ref_); }\n \n     tsl::RCReference<tsl::AsyncValue> CopyRCRef() const {\n       return ref_.CopyRCRef();\n"
                },
                {
                    "old_start": 223,
                    "old_length": 8,
                    "new_start": 222,
                    "new_length": 22,
                    "hunk": "@@ -223,8 +222,22 @@ class PjRtFutureBase : public PjRtFutureMoveControl<\n \n     tsl::AsyncValue* GetAsyncValue() const { return ref_.GetAsyncValue(); }\n \n+#ifndef NDEBUG\n+    int64_t AddFuture() { return num_futures_->fetch_add(1); }\n+#endif\n+\n    private:\n     tsl::AsyncValueRef<T> ref_;\n+\n+#ifndef NDEBUG\n+    // In debug builds we track the number of futures created from a promise to\n+    // detect when a promise for a move-only type can be accidentally shared by\n+    // multiple futures. We wrap the counter into shared pointer because promise\n+    // for a unique future is still copyable, but only one future can be created\n+    // from all the copies.\n+    std::shared_ptr<std::atomic<int64_t>> num_futures_ =\n+        std::make_shared<std::atomic<int64_t>>(0);\n+#endif\n   };\n \n   PjRtFutureBase() = default;\n"
                },
                {
                    "old_start": 348,
                    "old_length": 8,
                    "new_start": 361,
                    "new_length": 15,
                    "hunk": "@@ -348,8 +361,15 @@ class PjRtFuture : public internal::PjRtFutureBase<T> {\n       Promise promise,\n       PjRtFutureHelpers::OnBlockStartFn on_block_start = nullptr,\n       PjRtFutureHelpers::OnBlockEndFn on_block_end = nullptr)\n-      : Base(std::move(promise).ExtractRef(), std::move(on_block_start),\n-             std::move(on_block_end)) {}\n+      : Base(promise.release(), std::move(on_block_start),\n+             std::move(on_block_end)) {\n+#ifndef NDEBUG\n+    if constexpr (Base::is_unique()) {\n+      DCHECK_EQ(promise.AddFuture(), 0)\n+          << \"Unique PjRtFuture cannot share a promise object\";\n+    }\n+#endif\n+  }\n \n   // Blocks the calling thread until the future is ready, then returns the\n   // final value.\n"
                },
                {
                    "old_start": 467,
                    "old_length": 7,
                    "new_start": 487,
                    "new_length": 7,
                    "hunk": "@@ -467,7 +487,7 @@ class PjRtFuture<void> : public internal::PjRtFutureBase<std::nullopt_t> {\n       Promise promise,\n       PjRtFutureHelpers::OnBlockStartFn on_block_start = nullptr,\n       PjRtFutureHelpers::OnBlockEndFn on_block_end = nullptr)\n-      : Base(std::move(promise).ExtractRef(), std::move(on_block_start),\n+      : Base(promise.release(), std::move(on_block_start),\n              std::move(on_block_end)) {}\n \n   // Blocks the calling thread until the future is ready."
                }
            ],
            "whole_deleted": "-    Promise(const Promise& other) : ref_(other.ref_.CopyRef()) {}\n-    Promise& operator=(const Promise& other) {\n-      ref_ = other.ref_.CopyRef();\n-      return *this;\n-    }\n-    tsl::AsyncValueRef<T> ExtractRef() && { return std::move(ref_); }\n-      : Base(std::move(promise).ExtractRef(), std::move(on_block_start),\n-             std::move(on_block_end)) {}\n-      : Base(std::move(promise).ExtractRef(), std::move(on_block_start),\n",
            "whole_added": "+#include <atomic>\n+#include <memory>\n+    Promise(const Promise& other) = default;\n+    Promise& operator=(const Promise& other) = default;\n+    tsl::AsyncValueRef<T> release() { return std::move(ref_); }\n+#ifndef NDEBUG\n+    int64_t AddFuture() { return num_futures_->fetch_add(1); }\n+#endif\n+\n+\n+#ifndef NDEBUG\n+    // In debug builds we track the number of futures created from a promise to\n+    // detect when a promise for a move-only type can be accidentally shared by\n+    // multiple futures. We wrap the counter into shared pointer because promise\n+    // for a unique future is still copyable, but only one future can be created\n+    // from all the copies.\n+    std::shared_ptr<std::atomic<int64_t>> num_futures_ =\n+        std::make_shared<std::atomic<int64_t>>(0);\n+#endif\n+      : Base(promise.release(), std::move(on_block_start),\n+             std::move(on_block_end)) {\n+#ifndef NDEBUG\n+    if constexpr (Base::is_unique()) {\n+      DCHECK_EQ(promise.AddFuture(), 0)\n+          << \"Unique PjRtFuture cannot share a promise object\";\n+    }\n+#endif\n+  }\n+      : Base(promise.release(), std::move(on_block_start),\n",
            "whole_hunk": "@@ -17,8 +17,10 @@ limitations under the License.\n #define XLA_PJRT_PJRT_FUTURE_H_\n \n #include <algorithm>\n+#include <atomic>\n #include <cstdint>\n #include <functional>\n+#include <memory>\n #include <optional>\n #include <type_traits>\n #include <utility>\n@@ -185,11 +187,8 @@ class PjRtFutureBase : public PjRtFutureMoveControl<\n     Promise(Promise&& other) = default;\n     Promise& operator=(Promise&& other) = default;\n \n-    Promise(const Promise& other) : ref_(other.ref_.CopyRef()) {}\n-    Promise& operator=(const Promise& other) {\n-      ref_ = other.ref_.CopyRef();\n-      return *this;\n-    }\n+    Promise(const Promise& other) = default;\n+    Promise& operator=(const Promise& other) = default;\n \n     operator bool() const { return static_cast<bool>(ref_); }  // NOLINT\n \n@@ -215,7 +214,7 @@ class PjRtFutureBase : public PjRtFutureMoveControl<\n       ref_.template emplace<T>(std::forward<Args>(args)...);\n     }\n \n-    tsl::AsyncValueRef<T> ExtractRef() && { return std::move(ref_); }\n+    tsl::AsyncValueRef<T> release() { return std::move(ref_); }\n \n     tsl::RCReference<tsl::AsyncValue> CopyRCRef() const {\n       return ref_.CopyRCRef();\n@@ -223,8 +222,22 @@ class PjRtFutureBase : public PjRtFutureMoveControl<\n \n     tsl::AsyncValue* GetAsyncValue() const { return ref_.GetAsyncValue(); }\n \n+#ifndef NDEBUG\n+    int64_t AddFuture() { return num_futures_->fetch_add(1); }\n+#endif\n+\n    private:\n     tsl::AsyncValueRef<T> ref_;\n+\n+#ifndef NDEBUG\n+    // In debug builds we track the number of futures created from a promise to\n+    // detect when a promise for a move-only type can be accidentally shared by\n+    // multiple futures. We wrap the counter into shared pointer because promise\n+    // for a unique future is still copyable, but only one future can be created\n+    // from all the copies.\n+    std::shared_ptr<std::atomic<int64_t>> num_futures_ =\n+        std::make_shared<std::atomic<int64_t>>(0);\n+#endif\n   };\n \n   PjRtFutureBase() = default;\n@@ -348,8 +361,15 @@ class PjRtFuture : public internal::PjRtFutureBase<T> {\n       Promise promise,\n       PjRtFutureHelpers::OnBlockStartFn on_block_start = nullptr,\n       PjRtFutureHelpers::OnBlockEndFn on_block_end = nullptr)\n-      : Base(std::move(promise).ExtractRef(), std::move(on_block_start),\n-             std::move(on_block_end)) {}\n+      : Base(promise.release(), std::move(on_block_start),\n+             std::move(on_block_end)) {\n+#ifndef NDEBUG\n+    if constexpr (Base::is_unique()) {\n+      DCHECK_EQ(promise.AddFuture(), 0)\n+          << \"Unique PjRtFuture cannot share a promise object\";\n+    }\n+#endif\n+  }\n \n   // Blocks the calling thread until the future is ready, then returns the\n   // final value.\n@@ -467,7 +487,7 @@ class PjRtFuture<void> : public internal::PjRtFutureBase<std::nullopt_t> {\n       Promise promise,\n       PjRtFutureHelpers::OnBlockStartFn on_block_start = nullptr,\n       PjRtFutureHelpers::OnBlockEndFn on_block_end = nullptr)\n-      : Base(std::move(promise).ExtractRef(), std::move(on_block_start),\n+      : Base(promise.release(), std::move(on_block_start),\n              std::move(on_block_end)) {}\n \n   // Blocks the calling thread until the future is ready."
        }
    ]
},
{
    "Id": 398,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9b342f538c9de4ded35975803450e015b18f1029",
    "date": "2023-06-16T19:05:42-07:00",
    "message": "Part of a larger change to implement sliced prefetching.\n\nAdd a check to make sure repacking is disabled when slicing is enabled.\n\nPiperOrigin-RevId: 541069948",
    "label": "NO",
    "changes": [
        {
            "name": "memory_space_assignment.cc",
            "path": "tensorflow/compiler/xla/service/memory_space_assignment.cc",
            "patches": [
                {
                    "old_start": 3315,
                    "old_length": 9,
                    "new_start": 3315,
                    "new_length": 14,
                    "hunk": "@@ -3315,9 +3315,14 @@ HeapSimulator::Result<HloValue> AlternateMemoryBestFitHeap::Finish() {\n   if (options_.autotuning_config.has_value()) {\n     CHECK_EQ((*options_.autotuning_config).size(), buffer_intervals_.size());\n   }\n-\n-  // TODO(b/275905276): if slicing is turned on, ensure repacking is disabled,\n-  // i.e., max_repacks == 0\n+  // TODO(b/275905276): Add support to allow both slicing and repacking to be\n+  // enabled. When done, remove this check.\n+  CHECK(options_.sliced_prefetch_options.max_slices() < 2 ||\n+        options_.max_repacks == 0)\n+      << \"Repacking must be disabled when slicing is enabled.\";\n+  VLOG(1) << \"Slicing is \"\n+          << (options_.sliced_prefetch_options.max_slices() >= 2 ? \"enabled\"\n+                                                                 : \"disabled\");\n \n   AllocateReservedScopedAllocations();\n   std::vector<BufferInterval> sorted_buffer_intervals ="
                }
            ],
            "whole_deleted": "-\n-  // TODO(b/275905276): if slicing is turned on, ensure repacking is disabled,\n-  // i.e., max_repacks == 0\n",
            "whole_added": "+  // TODO(b/275905276): Add support to allow both slicing and repacking to be\n+  // enabled. When done, remove this check.\n+  CHECK(options_.sliced_prefetch_options.max_slices() < 2 ||\n+        options_.max_repacks == 0)\n+      << \"Repacking must be disabled when slicing is enabled.\";\n+  VLOG(1) << \"Slicing is \"\n+          << (options_.sliced_prefetch_options.max_slices() >= 2 ? \"enabled\"\n+                                                                 : \"disabled\");\n",
            "whole_hunk": "@@ -3315,9 +3315,14 @@ HeapSimulator::Result<HloValue> AlternateMemoryBestFitHeap::Finish() {\n   if (options_.autotuning_config.has_value()) {\n     CHECK_EQ((*options_.autotuning_config).size(), buffer_intervals_.size());\n   }\n-\n-  // TODO(b/275905276): if slicing is turned on, ensure repacking is disabled,\n-  // i.e., max_repacks == 0\n+  // TODO(b/275905276): Add support to allow both slicing and repacking to be\n+  // enabled. When done, remove this check.\n+  CHECK(options_.sliced_prefetch_options.max_slices() < 2 ||\n+        options_.max_repacks == 0)\n+      << \"Repacking must be disabled when slicing is enabled.\";\n+  VLOG(1) << \"Slicing is \"\n+          << (options_.sliced_prefetch_options.max_slices() >= 2 ? \"enabled\"\n+                                                                 : \"disabled\");\n \n   AllocateReservedScopedAllocations();\n   std::vector<BufferInterval> sorted_buffer_intervals ="
        }
    ]
},
{
    "Id": 49,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/61163051af7315f10c18d4fd99d7a6f40d8cd766",
    "date": "2024-05-10T09:52:04-07:00",
    "message": "Fix segment reduction overflow.\n\nThe `nsegments` argument type must be the same as the `segment_ids`\n(since the latter are just indices into `nsegments`).  For large\ninputs that would overflow `Index` (`long`) but still fit into `int64_t`,\nthis was previously causing a uncaught failure, since it would pass\nthe validation checks that used `int64_t`, but then would result in\na negative value when it's actually used.\n\nFixes #64023.\n\nPiperOrigin-RevId: 632523124",
    "label": "YES",
    "changes": [
        {
            "name": "segment_reduction_ops_impl.h",
            "path": "tensorflow/core/kernels/segment_reduction_ops_impl.h",
            "patches": [
                {
                    "old_start": 493,
                    "old_length": 7,
                    "new_start": 493,
                    "new_length": 7,
                    "hunk": "@@ -493,7 +493,7 @@ class UnsortedSegmentReductionOp : public OpKernel {\n                    internal::ValidateUnsortedSegmentReduction(\n                        this, context, data, segment_ids, num_segments));\n     const auto segment_flat = segment_ids.flat<Index>();\n-    const int64_t output_rows = internal::SubtleMustCopy(static_cast<int64_t>(\n+    const Index output_rows = internal::SubtleMustCopy(static_cast<Index>(\n         num_segments.dtype() == DT_INT32 ? num_segments.scalar<int32>()()\n                                          : num_segments.scalar<int64_t>()()));\n     OP_REQUIRES(context, output_rows >= 0,\n"
                }
            ],
            "whole_deleted": "-    const int64_t output_rows = internal::SubtleMustCopy(static_cast<int64_t>(\n",
            "whole_added": "+    const Index output_rows = internal::SubtleMustCopy(static_cast<Index>(\n",
            "whole_hunk": "@@ -493,7 +493,7 @@ class UnsortedSegmentReductionOp : public OpKernel {\n                    internal::ValidateUnsortedSegmentReduction(\n                        this, context, data, segment_ids, num_segments));\n     const auto segment_flat = segment_ids.flat<Index>();\n-    const int64_t output_rows = internal::SubtleMustCopy(static_cast<int64_t>(\n+    const Index output_rows = internal::SubtleMustCopy(static_cast<Index>(\n         num_segments.dtype() == DT_INT32 ? num_segments.scalar<int32>()()\n                                          : num_segments.scalar<int64_t>()()));\n     OP_REQUIRES(context, output_rows >= 0,\n"
        },
        {
            "name": "segment_reduction_ops_test.py",
            "path": "tensorflow/python/kernel_tests/math_ops/segment_reduction_ops_test.py",
            "patches": [
                {
                    "old_start": 601,
                    "old_length": 7,
                    "new_start": 601,
                    "new_length": 9,
                    "hunk": "@@ -601,7 +601,9 @@ class UnsortedSegmentTest(SegmentReductionHelper, parameterized.TestCase):\n       num_segments = 8327099846119777499\n       unsorted = math_ops.unsorted_segment_sum(\n           np.ones((3)), segment_ids=898042203, num_segments=num_segments)\n-      with self.assertRaisesOpError(\"Encountered overflow when multiplying\"):\n+      with self.assertRaisesOpError(\n+          \"Encountered overflow when multiplying | must not be negative\"\n+      ):\n         self.evaluate(unsorted)\n \n "
                }
            ],
            "whole_deleted": "-      with self.assertRaisesOpError(\"Encountered overflow when multiplying\"):\n",
            "whole_added": "+      with self.assertRaisesOpError(\n+          \"Encountered overflow when multiplying | must not be negative\"\n+      ):\n",
            "whole_hunk": "@@ -601,7 +601,9 @@ class UnsortedSegmentTest(SegmentReductionHelper, parameterized.TestCase):\n       num_segments = 8327099846119777499\n       unsorted = math_ops.unsorted_segment_sum(\n           np.ones((3)), segment_ids=898042203, num_segments=num_segments)\n-      with self.assertRaisesOpError(\"Encountered overflow when multiplying\"):\n+      with self.assertRaisesOpError(\n+          \"Encountered overflow when multiplying | must not be negative\"\n+      ):\n         self.evaluate(unsorted)\n \n "
        }
    ]
},
{
    "Id": 199,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/98fb4f6e48627cf30cd747848505b7eae28f1ded",
    "date": "2024-01-15T03:11:08-08:00",
    "message": "PR #8073: ReplaceInstructionWithDifferentShape should return false if old_instruction HasControlDependencies\n\nImported from GitHub PR https://github.com/openxla/xla/pull/8073\n\nThere are two `ReplaceInstructionWithDifferentShape` functions.\nThe default value for `relay_control_dependency` is `false`.\n```\n  StatusOr<bool> ReplaceInstructionWithDifferentShape(\n      HloInstruction* old_instruction,\n      HloInstruction* new_instruction,\n      bool preserve_sharding,\n      bool relay_control_dependency = false,\n      bool remove_unused_operands = true\n  );\n\n  StatusOr<bool> ReplaceInstructionWithDifferentShape(\n      HloInstruction* old_instruction,\n      HloInstruction* new_instruction\n  );\n```\nMany users use `ReplaceInstructionWithDifferentShape` functions providing only old and new instructions and leave all other parameters with default values.\n\nIf old_instruction has ControlDependencies then it will be impossible to replace it because ReplaceInstruction copies and drops ControlDependencies from old to new instruction only if `relay_control_dependency` is set to `true`.\nCurrently the function will Fail if we try to use it for the case when old_instruction has ControlDependencies. It will fail because `RemoveInstruction(old_instruction)` will fail because `IsSafelyRemovable` check will return false.\n\nInstead of failing, ReplaceInstructionWithDifferentShape should simply return false - indicating that the validation checks for ReplaceInstruction was not satisfied.\n\n@Tongfei-Guo @jurahul Can you have a look?\n\nCopybara import of the project:\n\n--\nfbe2fb2da6a51e73a6913b1f67c8d1587abbe049 by Alexander Pivovarov <pivovaa@amazon.com>:\n\nReplaceInstructionWithDifferentShape should return false if old_instruction HasControlDependencies\n\nMerging this change closes #8073\n\nPiperOrigin-RevId: 598560578",
    "label": "YES",
    "changes": [
        {
            "name": "hlo_computation.cc",
            "path": "third_party/xla/xla/hlo/ir/hlo_computation.cc",
            "patches": [
                {
                    "old_start": 1182,
                    "old_length": 6,
                    "new_start": 1182,
                    "new_length": 10,
                    "hunk": "@@ -1182,6 +1182,10 @@ StatusOr<bool> HloComputation::ReplaceInstructionWithDifferentShape(\n     TF_RETURN_IF_ERROR(\n         new_instruction->CopyAllControlDepsFrom(old_instruction));\n     TF_RETURN_IF_ERROR(old_instruction->DropAllControlDeps());\n+  } else if (old_instruction->HasControlDependencies()) {\n+    VLOG(10) << \"Skipping replacement because old instruction has \"\n+                \"control dependencies\";\n+    return false;\n   }\n   VLOG(10) << \"transformed \" << old_instruction->ToString() << \" to \"\n            << new_instruction->ToString();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  } else if (old_instruction->HasControlDependencies()) {\n+    VLOG(10) << \"Skipping replacement because old instruction has \"\n+                \"control dependencies\";\n+    return false;\n",
            "whole_hunk": "@@ -1182,6 +1182,10 @@ StatusOr<bool> HloComputation::ReplaceInstructionWithDifferentShape(\n     TF_RETURN_IF_ERROR(\n         new_instruction->CopyAllControlDepsFrom(old_instruction));\n     TF_RETURN_IF_ERROR(old_instruction->DropAllControlDeps());\n+  } else if (old_instruction->HasControlDependencies()) {\n+    VLOG(10) << \"Skipping replacement because old instruction has \"\n+                \"control dependencies\";\n+    return false;\n   }\n   VLOG(10) << \"transformed \" << old_instruction->ToString() << \" to \"\n            << new_instruction->ToString();"
        }
    ]
},
{
    "Id": 133,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e8cc764cd7dab7495b91e0c973a41d258f18c568",
    "date": "2024-02-29T15:25:27-08:00",
    "message": "Support mocking number of GPUs in CUDA plugin.\n\nAlso move reading jax config value to be right before the client is created. Previously they were read before calling register_plugin, which happens during import and before any call of jax.config.update.\n\nThe decorator in mock_gpu_test was used wrongly. jtu.run_on_devices will create the client before jax.config.update is called, which is not desired. Remove the decorator will not fail CPU/TPU tests because the mesh will check the num_shard and the number of devices in the client and skip it if it does not match.\n\ngenerate_pjrt_gpu_plugin_options is only used in places that do not require compatibility so do not need to update xla_client version.\n\nPiperOrigin-RevId: 611610915",
    "label": "YES",
    "changes": [
        {
            "name": "pjrt_c_api_gpu_internal.cc",
            "path": "third_party/xla/xla/pjrt/c/pjrt_c_api_gpu_internal.cc",
            "patches": [
                {
                    "old_start": 62,
                    "old_length": 17,
                    "new_start": 62,
                    "new_length": 18,
                    "hunk": "@@ -62,17 +62,18 @@ PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n       pjrt::ConvertFromPjRtNamedValueList(args->create_options,\n                                           args->num_options);\n   const auto kExpectedOptionNameAndTypes =\n-      absl::flat_hash_map<std::string, PJRT_NamedValue_Type>(\n-          {{\"platform_name\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n-           {\"allocator\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n-           {\"memory_fraction\", PJRT_NamedValue_Type::PJRT_NamedValue_kFloat},\n-           {\"preallocate\", PJRT_NamedValue_Type::PJRT_NamedValue_kBool},\n-           {\"collective_memory_size\",\n-            PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n-           {\"visible_devices\",\n-            PJRT_NamedValue_Type::PJRT_NamedValue_kInt64List},\n-           {\"node_id\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n-           {\"num_nodes\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64}});\n+      absl::flat_hash_map<std::string, PJRT_NamedValue_Type>({\n+          {\"platform_name\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n+          {\"allocator\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n+          {\"memory_fraction\", PJRT_NamedValue_Type::PJRT_NamedValue_kFloat},\n+          {\"preallocate\", PJRT_NamedValue_Type::PJRT_NamedValue_kBool},\n+          {\"collective_memory_size\",\n+           PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n+          {\"visible_devices\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64List},\n+          {\"node_id\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n+          {\"num_nodes\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n+          {\"enable_mock_nccl\", PJRT_NamedValue_Type::PJRT_NamedValue_kBool},\n+      });\n   PJRT_RETURN_IF_ERROR(\n       ValidateCreateOptions(create_options, kExpectedOptionNameAndTypes));\n \n"
                },
                {
                    "old_start": 125,
                    "old_length": 6,
                    "new_start": 126,
                    "new_length": 11,
                    "hunk": "@@ -125,6 +126,11 @@ PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n   if (auto it = create_options.find(\"num_nodes\"); it != create_options.end()) {\n     num_nodes = std::get<int64_t>(it->second);\n   }\n+  bool enable_mock_nccl = false;\n+  if (auto it = create_options.find(\"enable_mock_nccl\");\n+      it != create_options.end()) {\n+    enable_mock_nccl = std::get<bool>(it->second);\n+  }\n \n   xla::GpuClientOptions options;\n   options.allocator_config = allocator_config;\n"
                },
                {
                    "old_start": 135,
                    "old_length": 6,
                    "new_start": 141,
                    "new_length": 7,
                    "hunk": "@@ -135,6 +141,7 @@ PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n   options.kv_store =\n       pjrt::ToCppKeyValueStore(args->kv_get_callback, args->kv_get_user_arg,\n                                args->kv_put_callback, args->kv_put_user_arg);\n+  options.enable_mock_nccl = enable_mock_nccl;\n   PJRT_ASSIGN_OR_RETURN(std::unique_ptr<xla::PjRtClient> client,\n                         xla::GetStreamExecutorGpuClient(options));\n   args->client = pjrt::CreateWrapperClient(std::move(client));\n"
                }
            ],
            "whole_deleted": "-      absl::flat_hash_map<std::string, PJRT_NamedValue_Type>(\n-          {{\"platform_name\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n-           {\"allocator\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n-           {\"memory_fraction\", PJRT_NamedValue_Type::PJRT_NamedValue_kFloat},\n-           {\"preallocate\", PJRT_NamedValue_Type::PJRT_NamedValue_kBool},\n-           {\"collective_memory_size\",\n-            PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n-           {\"visible_devices\",\n-            PJRT_NamedValue_Type::PJRT_NamedValue_kInt64List},\n-           {\"node_id\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n-           {\"num_nodes\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64}});\n",
            "whole_added": "+      absl::flat_hash_map<std::string, PJRT_NamedValue_Type>({\n+          {\"platform_name\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n+          {\"allocator\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n+          {\"memory_fraction\", PJRT_NamedValue_Type::PJRT_NamedValue_kFloat},\n+          {\"preallocate\", PJRT_NamedValue_Type::PJRT_NamedValue_kBool},\n+          {\"collective_memory_size\",\n+           PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n+          {\"visible_devices\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64List},\n+          {\"node_id\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n+          {\"num_nodes\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n+          {\"enable_mock_nccl\", PJRT_NamedValue_Type::PJRT_NamedValue_kBool},\n+      });\n+  bool enable_mock_nccl = false;\n+  if (auto it = create_options.find(\"enable_mock_nccl\");\n+      it != create_options.end()) {\n+    enable_mock_nccl = std::get<bool>(it->second);\n+  }\n+  options.enable_mock_nccl = enable_mock_nccl;\n",
            "whole_hunk": "@@ -62,17 +62,18 @@ PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n       pjrt::ConvertFromPjRtNamedValueList(args->create_options,\n                                           args->num_options);\n   const auto kExpectedOptionNameAndTypes =\n-      absl::flat_hash_map<std::string, PJRT_NamedValue_Type>(\n-          {{\"platform_name\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n-           {\"allocator\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n-           {\"memory_fraction\", PJRT_NamedValue_Type::PJRT_NamedValue_kFloat},\n-           {\"preallocate\", PJRT_NamedValue_Type::PJRT_NamedValue_kBool},\n-           {\"collective_memory_size\",\n-            PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n-           {\"visible_devices\",\n-            PJRT_NamedValue_Type::PJRT_NamedValue_kInt64List},\n-           {\"node_id\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n-           {\"num_nodes\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64}});\n+      absl::flat_hash_map<std::string, PJRT_NamedValue_Type>({\n+          {\"platform_name\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n+          {\"allocator\", PJRT_NamedValue_Type::PJRT_NamedValue_kString},\n+          {\"memory_fraction\", PJRT_NamedValue_Type::PJRT_NamedValue_kFloat},\n+          {\"preallocate\", PJRT_NamedValue_Type::PJRT_NamedValue_kBool},\n+          {\"collective_memory_size\",\n+           PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n+          {\"visible_devices\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64List},\n+          {\"node_id\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n+          {\"num_nodes\", PJRT_NamedValue_Type::PJRT_NamedValue_kInt64},\n+          {\"enable_mock_nccl\", PJRT_NamedValue_Type::PJRT_NamedValue_kBool},\n+      });\n   PJRT_RETURN_IF_ERROR(\n       ValidateCreateOptions(create_options, kExpectedOptionNameAndTypes));\n \n@@ -125,6 +126,11 @@ PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n   if (auto it = create_options.find(\"num_nodes\"); it != create_options.end()) {\n     num_nodes = std::get<int64_t>(it->second);\n   }\n+  bool enable_mock_nccl = false;\n+  if (auto it = create_options.find(\"enable_mock_nccl\");\n+      it != create_options.end()) {\n+    enable_mock_nccl = std::get<bool>(it->second);\n+  }\n \n   xla::GpuClientOptions options;\n   options.allocator_config = allocator_config;\n@@ -135,6 +141,7 @@ PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n   options.kv_store =\n       pjrt::ToCppKeyValueStore(args->kv_get_callback, args->kv_get_user_arg,\n                                args->kv_put_callback, args->kv_put_user_arg);\n+  options.enable_mock_nccl = enable_mock_nccl;\n   PJRT_ASSIGN_OR_RETURN(std::unique_ptr<xla::PjRtClient> client,\n                         xla::GetStreamExecutorGpuClient(options));\n   args->client = pjrt::CreateWrapperClient(std::move(client));\n"
        },
        {
            "name": "xla_client.py",
            "path": "third_party/xla/xla/python/xla_client.py",
            "patches": [
                {
                    "old_start": 205,
                    "old_length": 21,
                    "new_start": 205,
                    "new_length": 14,
                    "hunk": "@@ -205,21 +205,14 @@ def make_tpu_client(library_path: Optional[str] = None):\n   return make_tfrt_tpu_c_api_client()\n \n \n-def generate_pjrt_gpu_plugin_options(\n-    visible_devices: str = 'all',\n-) -> _NameValueMapping:\n+def generate_pjrt_gpu_plugin_options() -> _NameValueMapping:\n   \"\"\"Generates the PjRt GPU plugin options.\n \n-  Args:\n-    visible_devices: A string of visible cuda devices.\n-\n   Returns:\n     A dictionary of plugin options.\n   \"\"\"\n \n   options = {}\n-  if visible_devices != 'all':\n-    options['visible_devices'] = [int(x) for x in visible_devices.split(',')]\n   options['platform_name'] = 'cuda'\n   allocator = os.getenv('XLA_PYTHON_CLIENT_ALLOCATOR', 'default').lower()\n   memory_fraction = os.getenv('XLA_PYTHON_CLIENT_MEM_FRACTION', '')\n"
                }
            ],
            "whole_deleted": "-def generate_pjrt_gpu_plugin_options(\n-    visible_devices: str = 'all',\n-) -> _NameValueMapping:\n-  Args:\n-    visible_devices: A string of visible cuda devices.\n-\n-  if visible_devices != 'all':\n-    options['visible_devices'] = [int(x) for x in visible_devices.split(',')]\n",
            "whole_added": "+def generate_pjrt_gpu_plugin_options() -> _NameValueMapping:\n",
            "whole_hunk": "@@ -205,21 +205,14 @@ def make_tpu_client(library_path: Optional[str] = None):\n   return make_tfrt_tpu_c_api_client()\n \n \n-def generate_pjrt_gpu_plugin_options(\n-    visible_devices: str = 'all',\n-) -> _NameValueMapping:\n+def generate_pjrt_gpu_plugin_options() -> _NameValueMapping:\n   \"\"\"Generates the PjRt GPU plugin options.\n \n-  Args:\n-    visible_devices: A string of visible cuda devices.\n-\n   Returns:\n     A dictionary of plugin options.\n   \"\"\"\n \n   options = {}\n-  if visible_devices != 'all':\n-    options['visible_devices'] = [int(x) for x in visible_devices.split(',')]\n   options['platform_name'] = 'cuda'\n   allocator = os.getenv('XLA_PYTHON_CLIENT_ALLOCATOR', 'default').lower()\n   memory_fraction = os.getenv('XLA_PYTHON_CLIENT_MEM_FRACTION', '')\n"
        },
        {
            "name": "xla_client.pyi",
            "path": "third_party/xla/xla/python/xla_client.pyi",
            "patches": [
                {
                    "old_start": 134,
                    "old_length": 9,
                    "new_start": 134,
                    "new_length": 7,
                    "hunk": "@@ -134,9 +134,7 @@ def pjrt_plugin_initialized(plugin_name: str) -> bool:\n def initialize_pjrt_plugin(plugin_name: str) -> None:\n   ...\n \n-def generate_pjrt_gpu_plugin_options(\n-    visible_devices: str = 'all',\n-) -> _NameValueMapping:\n+def generate_pjrt_gpu_plugin_options() -> _NameValueMapping:\n   ...\n \n class OpMetadata:"
                }
            ],
            "whole_deleted": "-def generate_pjrt_gpu_plugin_options(\n-    visible_devices: str = 'all',\n-) -> _NameValueMapping:\n",
            "whole_added": "+def generate_pjrt_gpu_plugin_options() -> _NameValueMapping:\n",
            "whole_hunk": "@@ -134,9 +134,7 @@ def pjrt_plugin_initialized(plugin_name: str) -> bool:\n def initialize_pjrt_plugin(plugin_name: str) -> None:\n   ...\n \n-def generate_pjrt_gpu_plugin_options(\n-    visible_devices: str = 'all',\n-) -> _NameValueMapping:\n+def generate_pjrt_gpu_plugin_options() -> _NameValueMapping:\n   ...\n \n class OpMetadata:"
        }
    ]
},
{
    "Id": 39,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/85ea63741928b4ff2182112f8cf7f12673cc9fe0",
    "date": "2024-05-23T07:01:54-07:00",
    "message": "Fix a crash in unoptimized builds.\n\ngetIntOrFloatBitWidth asserts that the type is an integer, but\nwe only check the type after calling it.\n\nPiperOrigin-RevId: 636531275",
    "label": "YES",
    "changes": [
        {
            "name": "lower_tensors.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/mlir/lower_tensors.cc",
            "patches": [
                {
                    "old_start": 463,
                    "old_length": 9,
                    "new_start": 463,
                    "new_length": 11,
                    "hunk": "@@ -463,9 +463,11 @@ struct RemoveUnusedIndexSwitchResults\n };\n \n bool IsAtomicIntegral(Type element_type) {\n+  if (!element_type.isInteger()) {\n+    return false;\n+  }\n   unsigned element_bitwidth = element_type.getIntOrFloatBitWidth();\n-  return element_type.isInteger() &&\n-         (element_bitwidth == 32 || element_bitwidth == 64);\n+  return element_bitwidth == 32 || element_bitwidth == 64;\n }\n \n Value CreateBitcast(mlir::ImplicitLocOpBuilder& b, Value value, Type ty) {"
                }
            ],
            "whole_deleted": "-  return element_type.isInteger() &&\n-         (element_bitwidth == 32 || element_bitwidth == 64);\n",
            "whole_added": "+  if (!element_type.isInteger()) {\n+    return false;\n+  }\n+  return element_bitwidth == 32 || element_bitwidth == 64;\n",
            "whole_hunk": "@@ -463,9 +463,11 @@ struct RemoveUnusedIndexSwitchResults\n };\n \n bool IsAtomicIntegral(Type element_type) {\n+  if (!element_type.isInteger()) {\n+    return false;\n+  }\n   unsigned element_bitwidth = element_type.getIntOrFloatBitWidth();\n-  return element_type.isInteger() &&\n-         (element_bitwidth == 32 || element_bitwidth == 64);\n+  return element_bitwidth == 32 || element_bitwidth == 64;\n }\n \n Value CreateBitcast(mlir::ImplicitLocOpBuilder& b, Value value, Type ty) {"
        }
    ]
},
{
    "Id": 208,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7494487575eecff08ddbffc12e038661876ee835",
    "date": "2024-01-08T18:57:48-08:00",
    "message": "Fix race condition in kernel caching.\n\nIf two identical kernels are created at the same time, they will both observe that the cache doesn't contain their cache_key, and both try to create the kernel at the same time. The first kernel to be created gets added to the cache, then the second kernel overwrites the first kernel in the cache.\n\nThis can cause issues for stateful kernels such as kernels that own resources. Destroying the first kernel in the above example could cause in-use resources to be destroyed.\n\nThis CL resolves the issue by checking if a cache entry already exists right before adding to the cache. If we create a new kernel but discover a cached entry has been added during the kernel creation, we now discard the new kernel and use the cached entry. This way we only ever use one kernel per cache key.\n\nPiperOrigin-RevId: 596762121",
    "label": "YES",
    "changes": [
        {
            "name": "context.cc",
            "path": "tensorflow/core/common_runtime/eager/context.cc",
            "patches": [
                {
                    "old_start": 1203,
                    "old_length": 10,
                    "new_start": 1203,
                    "new_length": 16,
                    "hunk": "@@ -1203,10 +1203,16 @@ Device* EagerContext::GetCachedDevice(Fprint128 device_cache_key) {\n   return iter->second;\n }\n \n-void EagerContext::AddKernelToCache(Fprint128 cache_key,\n-                                    KernelAndDevice* kernel) {\n+core::RefCountPtr<KernelAndDevice> EagerContext::AddKernelToCache(\n+    Fprint128 cache_key, core::RefCountPtr<KernelAndDevice> kernel) {\n   mutex_lock ml(cache_mu_);\n-  core::RefCountPtr<KernelAndDevice> new_ref(kernel);\n+  auto iter = kernel_cache_.find(cache_key);\n+  if (iter != kernel_cache_.end()) {\n+    core::RefCountPtr<KernelAndDevice> new_ref(iter->second.get());\n+    new_ref->Ref();\n+    return new_ref;\n+  }\n+  core::RefCountPtr<KernelAndDevice> new_ref(kernel.get());\n   new_ref->Ref();\n   kernel_cache_[cache_key] = std::move(new_ref);\n   auto* registered_function =\n"
                },
                {
                    "old_start": 1218,
                    "old_length": 6,
                    "new_start": 1224,
                    "new_length": 7,
                    "hunk": "@@ -1218,6 +1224,7 @@ void EagerContext::AddKernelToCache(Fprint128 cache_key,\n     VLOG(5) << \"Cached key size of kernel \" << kernel->name()\n             << \" is: \" << registered_function->cached_kernel_keys->size();\n   }\n+  return kernel;\n }\n \n void EagerContext::AddDeviceToCache(Fprint128 device_cache_key,\n"
                }
            ],
            "whole_deleted": "-void EagerContext::AddKernelToCache(Fprint128 cache_key,\n-                                    KernelAndDevice* kernel) {\n-  core::RefCountPtr<KernelAndDevice> new_ref(kernel);\n",
            "whole_added": "+core::RefCountPtr<KernelAndDevice> EagerContext::AddKernelToCache(\n+    Fprint128 cache_key, core::RefCountPtr<KernelAndDevice> kernel) {\n+  auto iter = kernel_cache_.find(cache_key);\n+  if (iter != kernel_cache_.end()) {\n+    core::RefCountPtr<KernelAndDevice> new_ref(iter->second.get());\n+    new_ref->Ref();\n+    return new_ref;\n+  }\n+  core::RefCountPtr<KernelAndDevice> new_ref(kernel.get());\n+  return kernel;\n",
            "whole_hunk": "@@ -1203,10 +1203,16 @@ Device* EagerContext::GetCachedDevice(Fprint128 device_cache_key) {\n   return iter->second;\n }\n \n-void EagerContext::AddKernelToCache(Fprint128 cache_key,\n-                                    KernelAndDevice* kernel) {\n+core::RefCountPtr<KernelAndDevice> EagerContext::AddKernelToCache(\n+    Fprint128 cache_key, core::RefCountPtr<KernelAndDevice> kernel) {\n   mutex_lock ml(cache_mu_);\n-  core::RefCountPtr<KernelAndDevice> new_ref(kernel);\n+  auto iter = kernel_cache_.find(cache_key);\n+  if (iter != kernel_cache_.end()) {\n+    core::RefCountPtr<KernelAndDevice> new_ref(iter->second.get());\n+    new_ref->Ref();\n+    return new_ref;\n+  }\n+  core::RefCountPtr<KernelAndDevice> new_ref(kernel.get());\n   new_ref->Ref();\n   kernel_cache_[cache_key] = std::move(new_ref);\n   auto* registered_function =\n@@ -1218,6 +1224,7 @@ void EagerContext::AddKernelToCache(Fprint128 cache_key,\n     VLOG(5) << \"Cached key size of kernel \" << kernel->name()\n             << \" is: \" << registered_function->cached_kernel_keys->size();\n   }\n+  return kernel;\n }\n \n void EagerContext::AddDeviceToCache(Fprint128 device_cache_key,\n"
        },
        {
            "name": "context.h",
            "path": "tensorflow/core/common_runtime/eager/context.h",
            "patches": [
                {
                    "old_start": 280,
                    "old_length": 7,
                    "new_start": 280,
                    "new_length": 8,
                    "hunk": "@@ -280,7 +280,8 @@ class EagerContext : public ImmediateExecutionContext, public core::RefCounted {\n   core::RefCountPtr<KernelAndDevice> GetCachedKernel(Fprint128 cache_key);\n   Device* GetCachedDevice(Fprint128 device_cache_key);\n \n-  void AddKernelToCache(Fprint128 cache_key, KernelAndDevice* kernel);\n+  core::RefCountPtr<KernelAndDevice> AddKernelToCache(\n+      Fprint128 cache_key, core::RefCountPtr<KernelAndDevice> kernel);\n   void AddDeviceToCache(Fprint128 device_cache_key, Device* device);\n \n   bool LogDevicePlacement() const { return log_device_placement_; }\n"
                }
            ],
            "whole_deleted": "-  void AddKernelToCache(Fprint128 cache_key, KernelAndDevice* kernel);\n",
            "whole_added": "+  core::RefCountPtr<KernelAndDevice> AddKernelToCache(\n+      Fprint128 cache_key, core::RefCountPtr<KernelAndDevice> kernel);\n",
            "whole_hunk": "@@ -280,7 +280,8 @@ class EagerContext : public ImmediateExecutionContext, public core::RefCounted {\n   core::RefCountPtr<KernelAndDevice> GetCachedKernel(Fprint128 cache_key);\n   Device* GetCachedDevice(Fprint128 device_cache_key);\n \n-  void AddKernelToCache(Fprint128 cache_key, KernelAndDevice* kernel);\n+  core::RefCountPtr<KernelAndDevice> AddKernelToCache(\n+      Fprint128 cache_key, core::RefCountPtr<KernelAndDevice> kernel);\n   void AddDeviceToCache(Fprint128 device_cache_key, Device* device);\n \n   bool LogDevicePlacement() const { return log_device_placement_; }\n"
        },
        {
            "name": "execute.cc",
            "path": "tensorflow/core/common_runtime/eager/execute.cc",
            "patches": [
                {
                    "old_start": 1559,
                    "old_length": 7,
                    "new_start": 1559,
                    "new_length": 9,
                    "hunk": "@@ -1559,7 +1559,9 @@ Status GetOrCreateKernelAndDevice(\n       // TODO(intel-tf): Implement an eviction policy to prevent potential\n       // memory growth (https://github.com/tensorflow/tensorflow/issues/58676)\n       VLOG(2) << \"Caching op \" << op->Name();\n-      ctx.AddKernelToCache(cache_key, kernel.get());\n+      // If the kernel is already in the cache, this discards the passed-in\n+      // kernel and returns the cached kernel.\n+      kernel = ctx.AddKernelToCache(cache_key, std::move(kernel));\n     }\n   }\n "
                }
            ],
            "whole_deleted": "-      ctx.AddKernelToCache(cache_key, kernel.get());\n",
            "whole_added": "+      // If the kernel is already in the cache, this discards the passed-in\n+      // kernel and returns the cached kernel.\n+      kernel = ctx.AddKernelToCache(cache_key, std::move(kernel));\n",
            "whole_hunk": "@@ -1559,7 +1559,9 @@ Status GetOrCreateKernelAndDevice(\n       // TODO(intel-tf): Implement an eviction policy to prevent potential\n       // memory growth (https://github.com/tensorflow/tensorflow/issues/58676)\n       VLOG(2) << \"Caching op \" << op->Name();\n-      ctx.AddKernelToCache(cache_key, kernel.get());\n+      // If the kernel is already in the cache, this discards the passed-in\n+      // kernel and returns the cached kernel.\n+      kernel = ctx.AddKernelToCache(cache_key, std::move(kernel));\n     }\n   }\n "
        }
    ]
},
{
    "Id": 6,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/49d64711af8a28aafb1eca4cdf20661a5511700e",
    "date": "2024-07-07T23:06:24-07:00",
    "message": "[xla:cpu] Do not check buffer slices in optimized builds\n\nOverheads of checking buffer slices add up, and incorrect buffer slice is a serious compiler bug that should be discovered in debug builds.\n\nname                                     old cpu/op   new cpu/op   delta\nBM_SelectAndScatterF32/128/process_time   965\u00b5s \u00b1 1%   806\u00b5s \u00b120%     ~\nBM_SelectAndScatterF32/256/process_time  3.79ms \u00b1 2%  2.93ms \u00b1 0%  -22.64%\nBM_SelectAndScatterF32/512/process_time  15.5ms \u00b1 2%  12.0ms \u00b1 2%  -22.50%\n\nPiperOrigin-RevId: 650128768",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/cpu/runtime/BUILD",
            "patches": [
                {
                    "old_start": 759,
                    "old_length": 6,
                    "new_start": 759,
                    "new_length": 7,
                    "hunk": "@@ -759,6 +759,7 @@ cc_library(\n     srcs = [\"kernel_thunk.cc\"],\n     hdrs = [\"kernel_thunk.h\"],\n     deps = [\n+        \":buffer_allocations\",\n         \":thunk\",\n         \"//xla:util\",\n         \"//xla/runtime:buffer_use\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \":buffer_allocations\",\n",
            "whole_hunk": "@@ -759,6 +759,7 @@ cc_library(\n     srcs = [\"kernel_thunk.cc\"],\n     hdrs = [\"kernel_thunk.h\"],\n     deps = [\n+        \":buffer_allocations\",\n         \":thunk\",\n         \"//xla:util\",\n         \"//xla/runtime:buffer_use\",\n"
        },
        {
            "name": "buffer_allocations.h",
            "path": "third_party/xla/xla/service/cpu/runtime/buffer_allocations.h",
            "patches": [
                {
                    "old_start": 35,
                    "old_length": 25,
                    "new_start": 35,
                    "new_length": 34,
                    "hunk": "@@ -35,25 +35,34 @@ namespace xla::cpu {\n // particular XLA execution. Buffers are indexed by the buffer allocation index.\n class BufferAllocations {\n  public:\n-  explicit inline BufferAllocations(\n-      absl::Span<const MaybeOwningDeviceMemory> buffers);\n+  explicit BufferAllocations(absl::Span<const MaybeOwningDeviceMemory> buffers);\n \n-  // Returns the device address of buffer `buffer_index`. `buffer_index` must be\n-  // a valid index, i.e., in [0, buffer_count).\n-  inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n-  GetDeviceAddress(BufferAllocation::Index buffer_index) const;\n+  // Returns the device address of buffer at the given index. Returns an error\n+  // if the index is out of range.\n+  absl::StatusOr<se::DeviceMemoryBase> GetDeviceAddress(\n+      BufferAllocation::Index index) const;\n \n   // Same as above, but also adjusts the returned address for the offset and\n   // size contained in the given slice.\n-  inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n-  GetDeviceAddress(const BufferAllocation::Slice& buffer_slice) const;\n+  absl::StatusOr<se::DeviceMemoryBase> GetDeviceAddress(\n+      const BufferAllocation::Slice& slice) const;\n+\n+  // Unchecked version of `GetDeviceAddress` that does not check the buffer\n+  // index and assumes it is valid.\n+  se::DeviceMemoryBase GetDeviceAddressUnchecked(\n+      BufferAllocation::Index buffer_index) const;\n+\n+  // Unchecked version of `GetDeviceAddress` that does not check the slice\n+  // buffer index, offset and size and assumes they all are valid.\n+  se::DeviceMemoryBase GetDeviceAddressUnchecked(\n+      const BufferAllocation::Slice& slice) const;\n \n  private:\n   std::vector<se::DeviceMemoryBase> buffers_;\n   size_t num_buffers_;\n };\n \n-BufferAllocations::BufferAllocations(\n+inline BufferAllocations::BufferAllocations(\n     absl::Span<const MaybeOwningDeviceMemory> buffers)\n     : buffers_(buffers.size()), num_buffers_(buffers_.size()) {\n   for (size_t i = 0; i < buffers.size(); ++i) {\n"
                },
                {
                    "old_start": 61,
                    "old_length": 8,
                    "new_start": 70,
                    "new_length": 8,
                    "hunk": "@@ -61,8 +70,8 @@ BufferAllocations::BufferAllocations(\n   }\n }\n \n-absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n-    BufferAllocation::Index index) const {\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n+BufferAllocations::GetDeviceAddress(BufferAllocation::Index index) const {\n   if (ABSL_PREDICT_FALSE(index < 0 || index >= num_buffers_)) {\n     return InvalidArgument(\n         \"Invalid buffer index %d. It must be in the range [0, %d)\", index,\n"
                },
                {
                    "old_start": 72,
                    "old_length": 16,
                    "new_start": 81,
                    "new_length": 17,
                    "hunk": "@@ -72,16 +81,17 @@ absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n   return buffers_[index];\n }\n \n-absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n-    const BufferAllocation::Slice& buffer_slice) const {\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n+BufferAllocations::GetDeviceAddress(\n+    const BufferAllocation::Slice& slice) const {\n   // Handle empty slices explicitly and return a null pointer device memory to\n   // guarantee that we do not accidentally write through the empty slice which\n   // would hide a real bug in the code.\n-  if (ABSL_PREDICT_FALSE(buffer_slice.size() == 0)) {\n+  if (ABSL_PREDICT_FALSE(slice.size() == 0)) {\n     return se::DeviceMemoryBase(nullptr, 0);\n   }\n \n-  int64_t index = buffer_slice.index();\n+  int64_t index = slice.index();\n   if (ABSL_PREDICT_FALSE(index < 0 || index >= num_buffers_)) {\n     return InvalidArgument(\n         \"Invalid buffer index %d. It must be in the range [0, %d)\", index,\n"
                },
                {
                    "old_start": 89,
                    "old_length": 8,
                    "new_start": 99,
                    "new_length": 8,
                    "hunk": "@@ -89,8 +99,8 @@ absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n   }\n   const se::DeviceMemoryBase& base = buffers_[index];\n \n-  int64_t offset = buffer_slice.offset();\n-  int64_t extent = offset + buffer_slice.size();\n+  int64_t offset = slice.offset();\n+  int64_t extent = offset + slice.size();\n \n   if (ABSL_PREDICT_FALSE(offset < 0)) {\n     return InvalidArgument(\"Buffer slice offset %d must be non-negative\",\n"
                },
                {
                    "old_start": 109,
                    "old_length": 7,
                    "new_start": 119,
                    "new_length": 21,
                    "hunk": "@@ -109,7 +119,21 @@ absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n         extent, index, base.size());\n   }\n \n-  return base.GetByteSlice(offset, buffer_slice.size());\n+  return base.GetByteSlice(offset, slice.size());\n+}\n+\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE se::DeviceMemoryBase\n+BufferAllocations::GetDeviceAddressUnchecked(\n+    BufferAllocation::Index buffer_index) const {\n+  return buffers_[buffer_index];\n+}\n+\n+// Unchecked version of `GetDeviceAddress` that does not check the slice\n+// buffer index, offset and size and assumes they are valid.\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE se::DeviceMemoryBase\n+BufferAllocations::GetDeviceAddressUnchecked(\n+    const BufferAllocation::Slice& slice) const {\n+  return buffers_[slice.index()].GetByteSlice(slice.offset(), slice.size());\n }\n \n }  // namespace xla::cpu\n"
                }
            ],
            "whole_deleted": "-  explicit inline BufferAllocations(\n-      absl::Span<const MaybeOwningDeviceMemory> buffers);\n-  // Returns the device address of buffer `buffer_index`. `buffer_index` must be\n-  // a valid index, i.e., in [0, buffer_count).\n-  inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n-  GetDeviceAddress(BufferAllocation::Index buffer_index) const;\n-  inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n-  GetDeviceAddress(const BufferAllocation::Slice& buffer_slice) const;\n-BufferAllocations::BufferAllocations(\n-absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n-    BufferAllocation::Index index) const {\n-absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n-    const BufferAllocation::Slice& buffer_slice) const {\n-  if (ABSL_PREDICT_FALSE(buffer_slice.size() == 0)) {\n-  int64_t index = buffer_slice.index();\n-  int64_t offset = buffer_slice.offset();\n-  int64_t extent = offset + buffer_slice.size();\n-  return base.GetByteSlice(offset, buffer_slice.size());\n",
            "whole_added": "+  explicit BufferAllocations(absl::Span<const MaybeOwningDeviceMemory> buffers);\n+  // Returns the device address of buffer at the given index. Returns an error\n+  // if the index is out of range.\n+  absl::StatusOr<se::DeviceMemoryBase> GetDeviceAddress(\n+      BufferAllocation::Index index) const;\n+  absl::StatusOr<se::DeviceMemoryBase> GetDeviceAddress(\n+      const BufferAllocation::Slice& slice) const;\n+\n+  // Unchecked version of `GetDeviceAddress` that does not check the buffer\n+  // index and assumes it is valid.\n+  se::DeviceMemoryBase GetDeviceAddressUnchecked(\n+      BufferAllocation::Index buffer_index) const;\n+\n+  // Unchecked version of `GetDeviceAddress` that does not check the slice\n+  // buffer index, offset and size and assumes they all are valid.\n+  se::DeviceMemoryBase GetDeviceAddressUnchecked(\n+      const BufferAllocation::Slice& slice) const;\n+inline BufferAllocations::BufferAllocations(\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n+BufferAllocations::GetDeviceAddress(BufferAllocation::Index index) const {\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n+BufferAllocations::GetDeviceAddress(\n+    const BufferAllocation::Slice& slice) const {\n+  if (ABSL_PREDICT_FALSE(slice.size() == 0)) {\n+  int64_t index = slice.index();\n+  int64_t offset = slice.offset();\n+  int64_t extent = offset + slice.size();\n+  return base.GetByteSlice(offset, slice.size());\n+}\n+\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE se::DeviceMemoryBase\n+BufferAllocations::GetDeviceAddressUnchecked(\n+    BufferAllocation::Index buffer_index) const {\n+  return buffers_[buffer_index];\n+}\n+\n+// Unchecked version of `GetDeviceAddress` that does not check the slice\n+// buffer index, offset and size and assumes they are valid.\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE se::DeviceMemoryBase\n+BufferAllocations::GetDeviceAddressUnchecked(\n+    const BufferAllocation::Slice& slice) const {\n+  return buffers_[slice.index()].GetByteSlice(slice.offset(), slice.size());\n",
            "whole_hunk": "@@ -35,25 +35,34 @@ namespace xla::cpu {\n // particular XLA execution. Buffers are indexed by the buffer allocation index.\n class BufferAllocations {\n  public:\n-  explicit inline BufferAllocations(\n-      absl::Span<const MaybeOwningDeviceMemory> buffers);\n+  explicit BufferAllocations(absl::Span<const MaybeOwningDeviceMemory> buffers);\n \n-  // Returns the device address of buffer `buffer_index`. `buffer_index` must be\n-  // a valid index, i.e., in [0, buffer_count).\n-  inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n-  GetDeviceAddress(BufferAllocation::Index buffer_index) const;\n+  // Returns the device address of buffer at the given index. Returns an error\n+  // if the index is out of range.\n+  absl::StatusOr<se::DeviceMemoryBase> GetDeviceAddress(\n+      BufferAllocation::Index index) const;\n \n   // Same as above, but also adjusts the returned address for the offset and\n   // size contained in the given slice.\n-  inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n-  GetDeviceAddress(const BufferAllocation::Slice& buffer_slice) const;\n+  absl::StatusOr<se::DeviceMemoryBase> GetDeviceAddress(\n+      const BufferAllocation::Slice& slice) const;\n+\n+  // Unchecked version of `GetDeviceAddress` that does not check the buffer\n+  // index and assumes it is valid.\n+  se::DeviceMemoryBase GetDeviceAddressUnchecked(\n+      BufferAllocation::Index buffer_index) const;\n+\n+  // Unchecked version of `GetDeviceAddress` that does not check the slice\n+  // buffer index, offset and size and assumes they all are valid.\n+  se::DeviceMemoryBase GetDeviceAddressUnchecked(\n+      const BufferAllocation::Slice& slice) const;\n \n  private:\n   std::vector<se::DeviceMemoryBase> buffers_;\n   size_t num_buffers_;\n };\n \n-BufferAllocations::BufferAllocations(\n+inline BufferAllocations::BufferAllocations(\n     absl::Span<const MaybeOwningDeviceMemory> buffers)\n     : buffers_(buffers.size()), num_buffers_(buffers_.size()) {\n   for (size_t i = 0; i < buffers.size(); ++i) {\n@@ -61,8 +70,8 @@ BufferAllocations::BufferAllocations(\n   }\n }\n \n-absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n-    BufferAllocation::Index index) const {\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n+BufferAllocations::GetDeviceAddress(BufferAllocation::Index index) const {\n   if (ABSL_PREDICT_FALSE(index < 0 || index >= num_buffers_)) {\n     return InvalidArgument(\n         \"Invalid buffer index %d. It must be in the range [0, %d)\", index,\n@@ -72,16 +81,17 @@ absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n   return buffers_[index];\n }\n \n-absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n-    const BufferAllocation::Slice& buffer_slice) const {\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n+BufferAllocations::GetDeviceAddress(\n+    const BufferAllocation::Slice& slice) const {\n   // Handle empty slices explicitly and return a null pointer device memory to\n   // guarantee that we do not accidentally write through the empty slice which\n   // would hide a real bug in the code.\n-  if (ABSL_PREDICT_FALSE(buffer_slice.size() == 0)) {\n+  if (ABSL_PREDICT_FALSE(slice.size() == 0)) {\n     return se::DeviceMemoryBase(nullptr, 0);\n   }\n \n-  int64_t index = buffer_slice.index();\n+  int64_t index = slice.index();\n   if (ABSL_PREDICT_FALSE(index < 0 || index >= num_buffers_)) {\n     return InvalidArgument(\n         \"Invalid buffer index %d. It must be in the range [0, %d)\", index,\n@@ -89,8 +99,8 @@ absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n   }\n   const se::DeviceMemoryBase& base = buffers_[index];\n \n-  int64_t offset = buffer_slice.offset();\n-  int64_t extent = offset + buffer_slice.size();\n+  int64_t offset = slice.offset();\n+  int64_t extent = offset + slice.size();\n \n   if (ABSL_PREDICT_FALSE(offset < 0)) {\n     return InvalidArgument(\"Buffer slice offset %d must be non-negative\",\n@@ -109,7 +119,21 @@ absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n         extent, index, base.size());\n   }\n \n-  return base.GetByteSlice(offset, buffer_slice.size());\n+  return base.GetByteSlice(offset, slice.size());\n+}\n+\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE se::DeviceMemoryBase\n+BufferAllocations::GetDeviceAddressUnchecked(\n+    BufferAllocation::Index buffer_index) const {\n+  return buffers_[buffer_index];\n+}\n+\n+// Unchecked version of `GetDeviceAddress` that does not check the slice\n+// buffer index, offset and size and assumes they are valid.\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE se::DeviceMemoryBase\n+BufferAllocations::GetDeviceAddressUnchecked(\n+    const BufferAllocation::Slice& slice) const {\n+  return buffers_[slice.index()].GetByteSlice(slice.offset(), slice.size());\n }\n \n }  // namespace xla::cpu\n"
        },
        {
            "name": "buffer_allocations_test.cc",
            "path": "third_party/xla/xla/service/cpu/runtime/buffer_allocations_test.cc",
            "patches": [
                {
                    "old_start": 49,
                    "old_length": 5,
                    "new_start": 49,
                    "new_length": 25,
                    "hunk": "@@ -49,5 +49,25 @@ TEST(BufferAllocationsTest, GetDeviceAddress) {\n   EXPECT_EQ(slice_mem.opaque(), &data[2]);\n }\n \n+TEST(BufferAllocationsTest, GetDeviceAddressUnchecked) {\n+  std::vector<MaybeOwningDeviceMemory> buffers;\n+  std::vector<float> data = {1.0, 2.0, 3.0, 4.0};\n+\n+  size_t size_in_bytes = data.size() * sizeof(float);\n+  buffers.emplace_back(se::DeviceMemoryBase(data.data(), size_in_bytes));\n+\n+  BufferAllocations allocations(buffers);\n+\n+  BufferAllocation alloc(0, size_in_bytes, 0);\n+  BufferAllocation::Slice slice(&alloc, /*offset=*/2 * sizeof(float),\n+                                /*size=*/sizeof(float));\n+\n+  se::DeviceMemoryBase alloc_mem = allocations.GetDeviceAddressUnchecked(0);\n+  EXPECT_EQ(alloc_mem.opaque(), &data[0]);\n+\n+  se::DeviceMemoryBase slice_mem = allocations.GetDeviceAddressUnchecked(slice);\n+  EXPECT_EQ(slice_mem.opaque(), &data[2]);\n+}\n+\n }  // namespace\n }  // namespace xla::cpu\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST(BufferAllocationsTest, GetDeviceAddressUnchecked) {\n+  std::vector<MaybeOwningDeviceMemory> buffers;\n+  std::vector<float> data = {1.0, 2.0, 3.0, 4.0};\n+\n+  size_t size_in_bytes = data.size() * sizeof(float);\n+  buffers.emplace_back(se::DeviceMemoryBase(data.data(), size_in_bytes));\n+\n+  BufferAllocations allocations(buffers);\n+\n+  BufferAllocation alloc(0, size_in_bytes, 0);\n+  BufferAllocation::Slice slice(&alloc, /*offset=*/2 * sizeof(float),\n+                                /*size=*/sizeof(float));\n+\n+  se::DeviceMemoryBase alloc_mem = allocations.GetDeviceAddressUnchecked(0);\n+  EXPECT_EQ(alloc_mem.opaque(), &data[0]);\n+\n+  se::DeviceMemoryBase slice_mem = allocations.GetDeviceAddressUnchecked(slice);\n+  EXPECT_EQ(slice_mem.opaque(), &data[2]);\n+}\n+\n",
            "whole_hunk": "@@ -49,5 +49,25 @@ TEST(BufferAllocationsTest, GetDeviceAddress) {\n   EXPECT_EQ(slice_mem.opaque(), &data[2]);\n }\n \n+TEST(BufferAllocationsTest, GetDeviceAddressUnchecked) {\n+  std::vector<MaybeOwningDeviceMemory> buffers;\n+  std::vector<float> data = {1.0, 2.0, 3.0, 4.0};\n+\n+  size_t size_in_bytes = data.size() * sizeof(float);\n+  buffers.emplace_back(se::DeviceMemoryBase(data.data(), size_in_bytes));\n+\n+  BufferAllocations allocations(buffers);\n+\n+  BufferAllocation alloc(0, size_in_bytes, 0);\n+  BufferAllocation::Slice slice(&alloc, /*offset=*/2 * sizeof(float),\n+                                /*size=*/sizeof(float));\n+\n+  se::DeviceMemoryBase alloc_mem = allocations.GetDeviceAddressUnchecked(0);\n+  EXPECT_EQ(alloc_mem.opaque(), &data[0]);\n+\n+  se::DeviceMemoryBase slice_mem = allocations.GetDeviceAddressUnchecked(slice);\n+  EXPECT_EQ(slice_mem.opaque(), &data[2]);\n+}\n+\n }  // namespace\n }  // namespace xla::cpu\n"
        },
        {
            "name": "kernel_thunk.cc",
            "path": "third_party/xla/xla/service/cpu/runtime/kernel_thunk.cc",
            "patches": [
                {
                    "old_start": 27,
                    "old_length": 6,
                    "new_start": 27,
                    "new_length": 7,
                    "hunk": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/base/optimization.h\"\n #include \"absl/memory/memory.h\"\n #include \"absl/numeric/bits.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n"
                },
                {
                    "old_start": 34,
                    "old_length": 6,
                    "new_start": 35,
                    "new_length": 7,
                    "hunk": "@@ -34,6 +35,7 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/cpu/runtime/buffer_allocations.h\"\n #include \"xla/service/cpu/runtime/thunk.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/host/host_kernel.h\"\n"
                },
                {
                    "old_start": 88,
                    "old_length": 6,
                    "new_start": 90,
                    "new_length": 8,
                    "hunk": "@@ -88,6 +90,8 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> KernelThunk::Execute(\n       kernel_name_, arguments_buffers_.size(), results_buffers_.size(),\n       thread_dim_.ToString());\n \n+  const BufferAllocations* allocations = params.buffer_allocations;\n+\n   // We use `llvm::SmallVector` instead of `absl::InlinedVector` because\n   // it allows to resize a vector without zero-initializing storage.\n   llvm::SmallVector<SE_HOST_KernelArg, 8> kernel_args;\n"
                },
                {
                    "old_start": 96,
                    "old_length": 37,
                    "new_start": 100,
                    "new_length": 38,
                    "hunk": "@@ -96,37 +100,38 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> KernelThunk::Execute(\n   int64_t kernel_arg_idx = 0;\n \n   for (BufferAllocation::Slice& buffer : arguments_buffers_) {\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase arg_data,\n-                        params.buffer_allocations->GetDeviceAddress(buffer));\n+    if constexpr (ShouldCheckBufferSlices()) {\n+      TF_ASSIGN_OR_RETURN(auto mem, allocations->GetDeviceAddress(buffer));\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    } else {\n+      auto mem = allocations->GetDeviceAddressUnchecked(buffer);\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    }\n+\n     VLOG(3) << absl::StreamFormat(\"  arg #%d: %s (%p)\", kernel_arg_idx,\n-                                  buffer.ToString(), arg_data.opaque());\n-    kernel_args[kernel_arg_idx++] =\n-        SE_HOST_KernelArg{arg_data.opaque(), arg_data.size()};\n+                                  buffer.ToString(),\n+                                  kernel_args[kernel_arg_idx].data);\n+    ++kernel_arg_idx;\n   }\n \n   for (BufferAllocation::Slice& buffer : results_buffers_) {\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase result_data,\n-                        params.buffer_allocations->GetDeviceAddress(buffer));\n-    VLOG(3) << absl::StreamFormat(\"  res #%d: %s (%p)\",\n-                                  kernel_arg_idx - arguments_buffers_.size(),\n-                                  buffer.ToString(), result_data.opaque());\n-    kernel_args[kernel_arg_idx++] =\n-        SE_HOST_KernelArg{result_data.opaque(), result_data.size()};\n+    if constexpr (ShouldCheckBufferSlices()) {\n+      TF_ASSIGN_OR_RETURN(auto mem, allocations->GetDeviceAddress(buffer));\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    } else {\n+      auto mem = allocations->GetDeviceAddressUnchecked(buffer);\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    }\n+\n+    VLOG(3) << absl::StreamFormat(\n+        \"  res #%d: %s (%p)\", kernel_arg_idx - arguments_buffers_.size(),\n+        buffer.ToString(), kernel_args[kernel_arg_idx].data);\n+    ++kernel_arg_idx;\n   }\n \n-  // Check that all buffers are aligned to the minimum alignment. We codegen\n-  // with the assumption that all buffers are aligned, and if they are not, we\n-  // will crash with a segmentation fault, or worse, produce incorrect results.\n-  if (min_alignment_.has_value()) {\n-    for (int64_t i = 0; i < num_kernel_args_; ++i) {\n-      auto ptr = reinterpret_cast<uintptr_t>(kernel_args[i].data);\n-      if (ABSL_PREDICT_FALSE((ptr & (*min_alignment_ - 1)) != 0)) {\n-        return Internal(\n-            \"Host kernel %s buffer argument #%d (%p) is not aligned to a \"\n-            \"required minimum alignment of %d bytes\",\n-            info().op_name, i, kernel_args[i].data, *min_alignment_);\n-      }\n-    }\n+  // \u0421heck that all resolved buffers are properly aligned.\n+  if constexpr (ShouldCheckBufferSlices()) {\n+    TF_RETURN_IF_ERROR(CheckBufferAlignment(kernel_args));\n   }\n \n   // TODO(ezhulenev): Kernel ptr should be loaded as a part of Thunk\n"
                },
                {
                    "old_start": 158,
                    "old_length": 6,
                    "new_start": 163,
                    "new_length": 22,
                    "hunk": "@@ -158,6 +163,22 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> KernelThunk::Execute(\n   return OkExecuteEvent();\n }\n \n+absl::Status KernelThunk::CheckBufferAlignment(\n+    absl::Span<const SE_HOST_KernelArg> kernel_args) {\n+  if (min_alignment_.has_value()) {\n+    for (int64_t i = 0; i < num_kernel_args_; ++i) {\n+      auto ptr = reinterpret_cast<uintptr_t>(kernel_args[i].data);\n+      if (ABSL_PREDICT_FALSE((ptr & (*min_alignment_ - 1)) != 0)) {\n+        return Internal(\n+            \"Host kernel %s buffer argument #%d (%p) is not aligned to a \"\n+            \"required minimum alignment of %d bytes\",\n+            info().op_name, i, kernel_args[i].data, *min_alignment_);\n+      }\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n KernelThunk::BufferUses KernelThunk::buffer_uses() const {\n   BufferUses buffer_uses;\n   for (const BufferAllocation::Slice& buffer : arguments_buffers_) {\n"
                }
            ],
            "whole_deleted": "-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase arg_data,\n-                        params.buffer_allocations->GetDeviceAddress(buffer));\n-                                  buffer.ToString(), arg_data.opaque());\n-    kernel_args[kernel_arg_idx++] =\n-        SE_HOST_KernelArg{arg_data.opaque(), arg_data.size()};\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase result_data,\n-                        params.buffer_allocations->GetDeviceAddress(buffer));\n-    VLOG(3) << absl::StreamFormat(\"  res #%d: %s (%p)\",\n-                                  kernel_arg_idx - arguments_buffers_.size(),\n-                                  buffer.ToString(), result_data.opaque());\n-    kernel_args[kernel_arg_idx++] =\n-        SE_HOST_KernelArg{result_data.opaque(), result_data.size()};\n-  // Check that all buffers are aligned to the minimum alignment. We codegen\n-  // with the assumption that all buffers are aligned, and if they are not, we\n-  // will crash with a segmentation fault, or worse, produce incorrect results.\n-  if (min_alignment_.has_value()) {\n-    for (int64_t i = 0; i < num_kernel_args_; ++i) {\n-      auto ptr = reinterpret_cast<uintptr_t>(kernel_args[i].data);\n-      if (ABSL_PREDICT_FALSE((ptr & (*min_alignment_ - 1)) != 0)) {\n-        return Internal(\n-            \"Host kernel %s buffer argument #%d (%p) is not aligned to a \"\n-            \"required minimum alignment of %d bytes\",\n-            info().op_name, i, kernel_args[i].data, *min_alignment_);\n-      }\n-    }\n",
            "whole_added": "+#include \"absl/status/status.h\"\n+#include \"xla/service/cpu/runtime/buffer_allocations.h\"\n+  const BufferAllocations* allocations = params.buffer_allocations;\n+\n+    if constexpr (ShouldCheckBufferSlices()) {\n+      TF_ASSIGN_OR_RETURN(auto mem, allocations->GetDeviceAddress(buffer));\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    } else {\n+      auto mem = allocations->GetDeviceAddressUnchecked(buffer);\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    }\n+\n+                                  buffer.ToString(),\n+                                  kernel_args[kernel_arg_idx].data);\n+    ++kernel_arg_idx;\n+    if constexpr (ShouldCheckBufferSlices()) {\n+      TF_ASSIGN_OR_RETURN(auto mem, allocations->GetDeviceAddress(buffer));\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    } else {\n+      auto mem = allocations->GetDeviceAddressUnchecked(buffer);\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    }\n+\n+    VLOG(3) << absl::StreamFormat(\n+        \"  res #%d: %s (%p)\", kernel_arg_idx - arguments_buffers_.size(),\n+        buffer.ToString(), kernel_args[kernel_arg_idx].data);\n+    ++kernel_arg_idx;\n+  // \u0421heck that all resolved buffers are properly aligned.\n+  if constexpr (ShouldCheckBufferSlices()) {\n+    TF_RETURN_IF_ERROR(CheckBufferAlignment(kernel_args));\n+absl::Status KernelThunk::CheckBufferAlignment(\n+    absl::Span<const SE_HOST_KernelArg> kernel_args) {\n+  if (min_alignment_.has_value()) {\n+    for (int64_t i = 0; i < num_kernel_args_; ++i) {\n+      auto ptr = reinterpret_cast<uintptr_t>(kernel_args[i].data);\n+      if (ABSL_PREDICT_FALSE((ptr & (*min_alignment_ - 1)) != 0)) {\n+        return Internal(\n+            \"Host kernel %s buffer argument #%d (%p) is not aligned to a \"\n+            \"required minimum alignment of %d bytes\",\n+            info().op_name, i, kernel_args[i].data, *min_alignment_);\n+      }\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n",
            "whole_hunk": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/base/optimization.h\"\n #include \"absl/memory/memory.h\"\n #include \"absl/numeric/bits.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n@@ -34,6 +35,7 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"xla/runtime/buffer_use.h\"\n #include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/cpu/runtime/buffer_allocations.h\"\n #include \"xla/service/cpu/runtime/thunk.h\"\n #include \"xla/stream_executor/device_memory.h\"\n #include \"xla/stream_executor/host/host_kernel.h\"\n@@ -88,6 +90,8 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> KernelThunk::Execute(\n       kernel_name_, arguments_buffers_.size(), results_buffers_.size(),\n       thread_dim_.ToString());\n \n+  const BufferAllocations* allocations = params.buffer_allocations;\n+\n   // We use `llvm::SmallVector` instead of `absl::InlinedVector` because\n   // it allows to resize a vector without zero-initializing storage.\n   llvm::SmallVector<SE_HOST_KernelArg, 8> kernel_args;\n@@ -96,37 +100,38 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> KernelThunk::Execute(\n   int64_t kernel_arg_idx = 0;\n \n   for (BufferAllocation::Slice& buffer : arguments_buffers_) {\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase arg_data,\n-                        params.buffer_allocations->GetDeviceAddress(buffer));\n+    if constexpr (ShouldCheckBufferSlices()) {\n+      TF_ASSIGN_OR_RETURN(auto mem, allocations->GetDeviceAddress(buffer));\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    } else {\n+      auto mem = allocations->GetDeviceAddressUnchecked(buffer);\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    }\n+\n     VLOG(3) << absl::StreamFormat(\"  arg #%d: %s (%p)\", kernel_arg_idx,\n-                                  buffer.ToString(), arg_data.opaque());\n-    kernel_args[kernel_arg_idx++] =\n-        SE_HOST_KernelArg{arg_data.opaque(), arg_data.size()};\n+                                  buffer.ToString(),\n+                                  kernel_args[kernel_arg_idx].data);\n+    ++kernel_arg_idx;\n   }\n \n   for (BufferAllocation::Slice& buffer : results_buffers_) {\n-    TF_ASSIGN_OR_RETURN(se::DeviceMemoryBase result_data,\n-                        params.buffer_allocations->GetDeviceAddress(buffer));\n-    VLOG(3) << absl::StreamFormat(\"  res #%d: %s (%p)\",\n-                                  kernel_arg_idx - arguments_buffers_.size(),\n-                                  buffer.ToString(), result_data.opaque());\n-    kernel_args[kernel_arg_idx++] =\n-        SE_HOST_KernelArg{result_data.opaque(), result_data.size()};\n+    if constexpr (ShouldCheckBufferSlices()) {\n+      TF_ASSIGN_OR_RETURN(auto mem, allocations->GetDeviceAddress(buffer));\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    } else {\n+      auto mem = allocations->GetDeviceAddressUnchecked(buffer);\n+      kernel_args[kernel_arg_idx] = SE_HOST_KernelArg{mem.opaque(), mem.size()};\n+    }\n+\n+    VLOG(3) << absl::StreamFormat(\n+        \"  res #%d: %s (%p)\", kernel_arg_idx - arguments_buffers_.size(),\n+        buffer.ToString(), kernel_args[kernel_arg_idx].data);\n+    ++kernel_arg_idx;\n   }\n \n-  // Check that all buffers are aligned to the minimum alignment. We codegen\n-  // with the assumption that all buffers are aligned, and if they are not, we\n-  // will crash with a segmentation fault, or worse, produce incorrect results.\n-  if (min_alignment_.has_value()) {\n-    for (int64_t i = 0; i < num_kernel_args_; ++i) {\n-      auto ptr = reinterpret_cast<uintptr_t>(kernel_args[i].data);\n-      if (ABSL_PREDICT_FALSE((ptr & (*min_alignment_ - 1)) != 0)) {\n-        return Internal(\n-            \"Host kernel %s buffer argument #%d (%p) is not aligned to a \"\n-            \"required minimum alignment of %d bytes\",\n-            info().op_name, i, kernel_args[i].data, *min_alignment_);\n-      }\n-    }\n+  // \u0421heck that all resolved buffers are properly aligned.\n+  if constexpr (ShouldCheckBufferSlices()) {\n+    TF_RETURN_IF_ERROR(CheckBufferAlignment(kernel_args));\n   }\n \n   // TODO(ezhulenev): Kernel ptr should be loaded as a part of Thunk\n@@ -158,6 +163,22 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> KernelThunk::Execute(\n   return OkExecuteEvent();\n }\n \n+absl::Status KernelThunk::CheckBufferAlignment(\n+    absl::Span<const SE_HOST_KernelArg> kernel_args) {\n+  if (min_alignment_.has_value()) {\n+    for (int64_t i = 0; i < num_kernel_args_; ++i) {\n+      auto ptr = reinterpret_cast<uintptr_t>(kernel_args[i].data);\n+      if (ABSL_PREDICT_FALSE((ptr & (*min_alignment_ - 1)) != 0)) {\n+        return Internal(\n+            \"Host kernel %s buffer argument #%d (%p) is not aligned to a \"\n+            \"required minimum alignment of %d bytes\",\n+            info().op_name, i, kernel_args[i].data, *min_alignment_);\n+      }\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n KernelThunk::BufferUses KernelThunk::buffer_uses() const {\n   BufferUses buffer_uses;\n   for (const BufferAllocation::Slice& buffer : arguments_buffers_) {\n"
        },
        {
            "name": "kernel_thunk.h",
            "path": "third_party/xla/xla/service/cpu/runtime/kernel_thunk.h",
            "patches": [
                {
                    "old_start": 25,
                    "old_length": 12,
                    "new_start": 25,
                    "new_length": 14,
                    "hunk": "@@ -25,12 +25,14 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/base/thread_annotations.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/cpu/runtime/thunk.h\"\n #include \"xla/stream_executor/host/host_kernel.h\"\n+#include \"xla/stream_executor/host/host_kernel_c_api.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n \n"
                },
                {
                    "old_start": 56,
                    "old_length": 6,
                    "new_start": 58,
                    "new_length": 12,
                    "hunk": "@@ -56,6 +58,12 @@ class KernelThunk final : public Thunk {\n               std::string kernel_name, se::ThreadDim thread_dim,\n               std::optional<uint64_t> min_alignment);\n \n+  // Checks that all buffers are aligned to the minimum alignment. We codegen\n+  // with the assumption that all buffers are aligned, and if they are not, we\n+  // will crash with a segmentation fault, or worse, produce incorrect results.\n+  absl::Status CheckBufferAlignment(\n+      absl::Span<const SE_HOST_KernelArg> kernel_args);\n+\n   std::vector<BufferAllocation::Slice> arguments_buffers_;\n   std::vector<BufferAllocation::Slice> results_buffers_;\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"absl/status/status.h\"\n+#include \"xla/stream_executor/host/host_kernel_c_api.h\"\n+  // Checks that all buffers are aligned to the minimum alignment. We codegen\n+  // with the assumption that all buffers are aligned, and if they are not, we\n+  // will crash with a segmentation fault, or worse, produce incorrect results.\n+  absl::Status CheckBufferAlignment(\n+      absl::Span<const SE_HOST_KernelArg> kernel_args);\n+\n",
            "whole_hunk": "@@ -25,12 +25,14 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/base/thread_annotations.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/cpu/runtime/thunk.h\"\n #include \"xla/stream_executor/host/host_kernel.h\"\n+#include \"xla/stream_executor/host/host_kernel_c_api.h\"\n #include \"xla/stream_executor/launch_dim.h\"\n #include \"xla/tsl/concurrency/async_value_ref.h\"\n \n@@ -56,6 +58,12 @@ class KernelThunk final : public Thunk {\n               std::string kernel_name, se::ThreadDim thread_dim,\n               std::optional<uint64_t> min_alignment);\n \n+  // Checks that all buffers are aligned to the minimum alignment. We codegen\n+  // with the assumption that all buffers are aligned, and if they are not, we\n+  // will crash with a segmentation fault, or worse, produce incorrect results.\n+  absl::Status CheckBufferAlignment(\n+      absl::Span<const SE_HOST_KernelArg> kernel_args);\n+\n   std::vector<BufferAllocation::Slice> arguments_buffers_;\n   std::vector<BufferAllocation::Slice> results_buffers_;\n \n"
        },
        {
            "name": "thunk.h",
            "path": "third_party/xla/xla/service/cpu/runtime/thunk.h",
            "patches": [
                {
                    "old_start": 225,
                    "old_length": 6,
                    "new_start": 225,
                    "new_length": 18,
                    "hunk": "@@ -225,6 +225,18 @@ class Thunk {\n   // Encodes thunk info into the TraceMe compatible format.\n   std::string TraceMeEncode() const;\n \n+  // Returns `true` if thunk should check buffer slices bounds, alignment, etc.\n+  // In optimized builds, we skip buffer slices checks, and assume that all\n+  // buffer slices are valid, as overhead of buffer slices checks adds up and\n+  // become measurable on a hot path of executing tiny thunks.\n+  static constexpr bool ShouldCheckBufferSlices() {\n+#ifdef NDEBUG\n+    return false;\n+#else\n+    return true;\n+#endif  // NDEBUG\n+  }\n+\n  private:\n   Kind kind_;\n   Info info_;\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // Returns `true` if thunk should check buffer slices bounds, alignment, etc.\n+  // In optimized builds, we skip buffer slices checks, and assume that all\n+  // buffer slices are valid, as overhead of buffer slices checks adds up and\n+  // become measurable on a hot path of executing tiny thunks.\n+  static constexpr bool ShouldCheckBufferSlices() {\n+#ifdef NDEBUG\n+    return false;\n+#else\n+    return true;\n+#endif  // NDEBUG\n+  }\n+\n",
            "whole_hunk": "@@ -225,6 +225,18 @@ class Thunk {\n   // Encodes thunk info into the TraceMe compatible format.\n   std::string TraceMeEncode() const;\n \n+  // Returns `true` if thunk should check buffer slices bounds, alignment, etc.\n+  // In optimized builds, we skip buffer slices checks, and assume that all\n+  // buffer slices are valid, as overhead of buffer slices checks adds up and\n+  // become measurable on a hot path of executing tiny thunks.\n+  static constexpr bool ShouldCheckBufferSlices() {\n+#ifdef NDEBUG\n+    return false;\n+#else\n+    return true;\n+#endif  // NDEBUG\n+  }\n+\n  private:\n   Kind kind_;\n   Info info_;\n"
        },
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/stream_executor/BUILD",
            "patches": [
                {
                    "old_start": 171,
                    "old_length": 7,
                    "new_start": 171,
                    "new_length": 10,
                    "hunk": "@@ -171,7 +171,10 @@ cc_library(\n cc_library(\n     name = \"device_memory\",\n     hdrs = [\"device_memory.h\"],\n-    deps = [\"@local_tsl//tsl/platform:logging\"],\n+    deps = [\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@local_tsl//tsl/platform:logging\",\n+    ],\n )\n \n cc_library(\n"
                }
            ],
            "whole_deleted": "-    deps = [\"@local_tsl//tsl/platform:logging\"],\n",
            "whole_added": "+    deps = [\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@local_tsl//tsl/platform:logging\",\n+    ],\n",
            "whole_hunk": "@@ -171,7 +171,10 @@ cc_library(\n cc_library(\n     name = \"device_memory\",\n     hdrs = [\"device_memory.h\"],\n-    deps = [\"@local_tsl//tsl/platform:logging\"],\n+    deps = [\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@local_tsl//tsl/platform:logging\",\n+    ],\n )\n \n cc_library(\n"
        },
        {
            "name": "device_memory.h",
            "path": "third_party/xla/xla/stream_executor/device_memory.h",
            "patches": [
                {
                    "old_start": 30,
                    "old_length": 6,
                    "new_start": 30,
                    "new_length": 7,
                    "hunk": "@@ -30,6 +30,7 @@ limitations under the License.\n #include <cstdint>\n #include <tuple>\n \n+#include \"absl/base/attributes.h\"\n #include \"tsl/platform/logging.h\"\n \n namespace stream_executor {\n"
                },
                {
                    "old_start": 101,
                    "old_length": 8,
                    "new_start": 102,
                    "new_length": 8,
                    "hunk": "@@ -101,8 +102,8 @@ class DeviceMemoryBase {\n \n   // Creates a memory region (slice) inside another allocated memory region.\n   // Offset and size are in bytes.\n-  DeviceMemoryBase GetByteSlice(uint64_t offset_bytes,\n-                                uint64_t size_bytes) const {\n+  ABSL_ATTRIBUTE_ALWAYS_INLINE DeviceMemoryBase\n+  GetByteSlice(uint64_t offset_bytes, uint64_t size_bytes) const {\n     DCHECK(offset_bytes + size_bytes <= size_)\n         << \"requested slice allocation (offset + size) is greater \"\n         << \"than parent allocation size: (\" << offset_bytes << \" + \""
                }
            ],
            "whole_deleted": "-  DeviceMemoryBase GetByteSlice(uint64_t offset_bytes,\n-                                uint64_t size_bytes) const {\n",
            "whole_added": "+#include \"absl/base/attributes.h\"\n+  ABSL_ATTRIBUTE_ALWAYS_INLINE DeviceMemoryBase\n+  GetByteSlice(uint64_t offset_bytes, uint64_t size_bytes) const {\n",
            "whole_hunk": "@@ -30,6 +30,7 @@ limitations under the License.\n #include <cstdint>\n #include <tuple>\n \n+#include \"absl/base/attributes.h\"\n #include \"tsl/platform/logging.h\"\n \n namespace stream_executor {\n@@ -101,8 +102,8 @@ class DeviceMemoryBase {\n \n   // Creates a memory region (slice) inside another allocated memory region.\n   // Offset and size are in bytes.\n-  DeviceMemoryBase GetByteSlice(uint64_t offset_bytes,\n-                                uint64_t size_bytes) const {\n+  ABSL_ATTRIBUTE_ALWAYS_INLINE DeviceMemoryBase\n+  GetByteSlice(uint64_t offset_bytes, uint64_t size_bytes) const {\n     DCHECK(offset_bytes + size_bytes <= size_)\n         << \"requested slice allocation (offset + size) is greater \"\n         << \"than parent allocation size: (\" << offset_bytes << \" + \""
        }
    ]
},
{
    "Id": 101,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f849de0732b5bc677311a3d4b9fe55ecf5b68adf",
    "date": "2024-03-20T05:44:31-07:00",
    "message": "[XLA:GPU] Refactor code that sets priorities into a function.\n\nThis change lookups and erases a few more elements from `reverse_map_`, but overall it shouldn't make a dent in the compile time.\n\nWith this change, we check if priority is negative before inserting into the queue, not after. This also doesn't affect compile time, but arguably makes the code easier to understand.\n\nPiperOrigin-RevId: 617478676",
    "label": "NO",
    "changes": [
        {
            "name": "priority_fusion.cc",
            "path": "third_party/xla/xla/service/gpu/priority_fusion.cc",
            "patches": [
                {
                    "old_start": 154,
                    "old_length": 12,
                    "new_start": 154,
                    "new_length": 36,
                    "hunk": "@@ -154,12 +154,36 @@ class GpuPriorityFusionQueue {\n       }\n       instructions.push_back(instruction);\n     }\n+\n+    ComputeAndSetPriorities(instructions);\n+  }\n+\n+  void ComputeAndSetPriorities(\n+      const std::vector<HloInstruction*>& instructions) {\n     std::vector<Priority> priorities = ComputePriorities(instructions);\n \n     for (auto [instruction, priority] : llvm::zip(instructions, priorities)) {\n-      auto emplace_result = producer_priority_queue_.emplace(\n-          std::make_pair(priority, instruction->unique_id()), instruction);\n-      CHECK(emplace_result.second);\n+      auto key = std::make_pair(priority, instruction->unique_id());\n+\n+      // Remove instruction with the old priority from the queue.\n+      auto reverse_it = reverse_map_.find(instruction);\n+      if (reverse_it != reverse_map_.end()) {\n+        const PriorityQueue::iterator& queue_it = reverse_it->second;\n+        // Priority didn't change. Nothing to do.\n+        if (key == queue_it->first) {\n+          continue;\n+        }\n+        producer_priority_queue_.erase(queue_it);\n+        reverse_map_.erase(reverse_it);\n+      }\n+\n+      // If the priority is negative, it's not helpful to perform fusion on this\n+      // instruction.\n+      if (priority < 0) {\n+        continue;\n+      }\n+\n+      auto emplace_result = producer_priority_queue_.emplace(key, instruction);\n       reverse_map_.emplace(instruction, emplace_result.first);\n     }\n   }\n"
                },
                {
                    "old_start": 195,
                    "old_length": 18,
                    "new_start": 219,
                    "new_length": 11,
                    "hunk": "@@ -195,18 +219,11 @@ class GpuPriorityFusionQueue {\n \n     while (!producer_priority_queue_.empty() && current_consumers_.empty()) {\n       auto next_it = std::prev(producer_priority_queue_.end());\n-      auto priority = next_it->first.first;\n \n       current_producer_ = next_it->second;\n       producer_priority_queue_.erase(next_it);\n       reverse_map_.erase(current_producer_);\n \n-      // If the priority is negative, it's not helpful to perform fusion on this\n-      // instruction.\n-      if (priority < 0) {\n-        continue;\n-      }\n-\n       current_consumers_ = current_producer_->users();\n \n       if (current_producer_->opcode() == HloOpcode::kBitcast) {\n"
                },
                {
                    "old_start": 229,
                    "old_length": 30,
                    "new_start": 246,
                    "new_length": 9,
                    "hunk": "@@ -229,30 +246,9 @@ class GpuPriorityFusionQueue {\n       TF_CHECK_OK(cost_analysis_.RevisitInstruction(instruction));\n     }\n \n-    std::vector<HloInstruction*> to_update_vector{to_update_priority_.begin(),\n-                                                  to_update_priority_.end()};\n-    std::vector<Priority> new_priorities = ComputePriorities(to_update_vector);\n+    ComputeAndSetPriorities(std::vector<HloInstruction*>{\n+        to_update_priority_.begin(), to_update_priority_.end()});\n \n-    for (auto [instruction, new_priority] :\n-         llvm::zip(to_update_vector, new_priorities)) {\n-      auto reverse_it = reverse_map_.find(instruction);\n-      const auto new_key =\n-          std::make_pair(new_priority, instruction->unique_id());\n-      if (reverse_it != reverse_map_.end()) {\n-        if (new_key == reverse_it->second->first) {\n-          continue;\n-        }\n-        producer_priority_queue_.erase(reverse_it->second);\n-      }\n-      auto emplace_result =\n-          producer_priority_queue_.emplace(new_key, instruction);\n-      CHECK(emplace_result.second);\n-      if (reverse_it != reverse_map_.end()) {\n-        reverse_it->second = emplace_result.first;\n-      } else {\n-        reverse_map_.emplace(instruction, emplace_result.first);\n-      }\n-    }\n     to_update_priority_.clear();\n   }\n "
                }
            ],
            "whole_deleted": "-      auto emplace_result = producer_priority_queue_.emplace(\n-          std::make_pair(priority, instruction->unique_id()), instruction);\n-      CHECK(emplace_result.second);\n-      auto priority = next_it->first.first;\n-      // If the priority is negative, it's not helpful to perform fusion on this\n-      // instruction.\n-      if (priority < 0) {\n-        continue;\n-      }\n-\n-    std::vector<HloInstruction*> to_update_vector{to_update_priority_.begin(),\n-                                                  to_update_priority_.end()};\n-    std::vector<Priority> new_priorities = ComputePriorities(to_update_vector);\n-    for (auto [instruction, new_priority] :\n-         llvm::zip(to_update_vector, new_priorities)) {\n-      auto reverse_it = reverse_map_.find(instruction);\n-      const auto new_key =\n-          std::make_pair(new_priority, instruction->unique_id());\n-      if (reverse_it != reverse_map_.end()) {\n-        if (new_key == reverse_it->second->first) {\n-          continue;\n-        }\n-        producer_priority_queue_.erase(reverse_it->second);\n-      }\n-      auto emplace_result =\n-          producer_priority_queue_.emplace(new_key, instruction);\n-      CHECK(emplace_result.second);\n-      if (reverse_it != reverse_map_.end()) {\n-        reverse_it->second = emplace_result.first;\n-      } else {\n-        reverse_map_.emplace(instruction, emplace_result.first);\n-      }\n-    }\n",
            "whole_added": "+\n+    ComputeAndSetPriorities(instructions);\n+  }\n+\n+  void ComputeAndSetPriorities(\n+      const std::vector<HloInstruction*>& instructions) {\n+      auto key = std::make_pair(priority, instruction->unique_id());\n+\n+      // Remove instruction with the old priority from the queue.\n+      auto reverse_it = reverse_map_.find(instruction);\n+      if (reverse_it != reverse_map_.end()) {\n+        const PriorityQueue::iterator& queue_it = reverse_it->second;\n+        // Priority didn't change. Nothing to do.\n+        if (key == queue_it->first) {\n+          continue;\n+        }\n+        producer_priority_queue_.erase(queue_it);\n+        reverse_map_.erase(reverse_it);\n+      }\n+\n+      // If the priority is negative, it's not helpful to perform fusion on this\n+      // instruction.\n+      if (priority < 0) {\n+        continue;\n+      }\n+\n+      auto emplace_result = producer_priority_queue_.emplace(key, instruction);\n+    ComputeAndSetPriorities(std::vector<HloInstruction*>{\n+        to_update_priority_.begin(), to_update_priority_.end()});\n",
            "whole_hunk": "@@ -154,12 +154,36 @@ class GpuPriorityFusionQueue {\n       }\n       instructions.push_back(instruction);\n     }\n+\n+    ComputeAndSetPriorities(instructions);\n+  }\n+\n+  void ComputeAndSetPriorities(\n+      const std::vector<HloInstruction*>& instructions) {\n     std::vector<Priority> priorities = ComputePriorities(instructions);\n \n     for (auto [instruction, priority] : llvm::zip(instructions, priorities)) {\n-      auto emplace_result = producer_priority_queue_.emplace(\n-          std::make_pair(priority, instruction->unique_id()), instruction);\n-      CHECK(emplace_result.second);\n+      auto key = std::make_pair(priority, instruction->unique_id());\n+\n+      // Remove instruction with the old priority from the queue.\n+      auto reverse_it = reverse_map_.find(instruction);\n+      if (reverse_it != reverse_map_.end()) {\n+        const PriorityQueue::iterator& queue_it = reverse_it->second;\n+        // Priority didn't change. Nothing to do.\n+        if (key == queue_it->first) {\n+          continue;\n+        }\n+        producer_priority_queue_.erase(queue_it);\n+        reverse_map_.erase(reverse_it);\n+      }\n+\n+      // If the priority is negative, it's not helpful to perform fusion on this\n+      // instruction.\n+      if (priority < 0) {\n+        continue;\n+      }\n+\n+      auto emplace_result = producer_priority_queue_.emplace(key, instruction);\n       reverse_map_.emplace(instruction, emplace_result.first);\n     }\n   }\n@@ -195,18 +219,11 @@ class GpuPriorityFusionQueue {\n \n     while (!producer_priority_queue_.empty() && current_consumers_.empty()) {\n       auto next_it = std::prev(producer_priority_queue_.end());\n-      auto priority = next_it->first.first;\n \n       current_producer_ = next_it->second;\n       producer_priority_queue_.erase(next_it);\n       reverse_map_.erase(current_producer_);\n \n-      // If the priority is negative, it's not helpful to perform fusion on this\n-      // instruction.\n-      if (priority < 0) {\n-        continue;\n-      }\n-\n       current_consumers_ = current_producer_->users();\n \n       if (current_producer_->opcode() == HloOpcode::kBitcast) {\n@@ -229,30 +246,9 @@ class GpuPriorityFusionQueue {\n       TF_CHECK_OK(cost_analysis_.RevisitInstruction(instruction));\n     }\n \n-    std::vector<HloInstruction*> to_update_vector{to_update_priority_.begin(),\n-                                                  to_update_priority_.end()};\n-    std::vector<Priority> new_priorities = ComputePriorities(to_update_vector);\n+    ComputeAndSetPriorities(std::vector<HloInstruction*>{\n+        to_update_priority_.begin(), to_update_priority_.end()});\n \n-    for (auto [instruction, new_priority] :\n-         llvm::zip(to_update_vector, new_priorities)) {\n-      auto reverse_it = reverse_map_.find(instruction);\n-      const auto new_key =\n-          std::make_pair(new_priority, instruction->unique_id());\n-      if (reverse_it != reverse_map_.end()) {\n-        if (new_key == reverse_it->second->first) {\n-          continue;\n-        }\n-        producer_priority_queue_.erase(reverse_it->second);\n-      }\n-      auto emplace_result =\n-          producer_priority_queue_.emplace(new_key, instruction);\n-      CHECK(emplace_result.second);\n-      if (reverse_it != reverse_map_.end()) {\n-        reverse_it->second = emplace_result.first;\n-      } else {\n-        reverse_map_.emplace(instruction, emplace_result.first);\n-      }\n-    }\n     to_update_priority_.clear();\n   }\n "
        }
    ]
},
{
    "Id": 589,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/6a6af6b4729ea1532a1eafe50149664347c45bdb",
    "date": "2023-01-11T17:23:16-08:00",
    "message": "- Simplify the logics to check if graph input arg is a capture\n- Fix the bug of by-ref captures are not identified\n\nPiperOrigin-RevId: 501419075",
    "label": "YES",
    "changes": [
        {
            "name": "polymorphic_function_test.py",
            "path": "tensorflow/python/eager/polymorphic_function/polymorphic_function_test.py",
            "patches": [
                {
                    "old_start": 4869,
                    "old_length": 6,
                    "new_start": 4869,
                    "new_length": 20,
                    "hunk": "@@ -4869,6 +4869,20 @@ class MultiDeviceTest(test.TestCase, parameterized.TestCase):\n     _ = f()\n     self.assertLen(total_function_cache(f), expected_len)\n \n+  def testByRefCaptureWithInputSignature(self):\n+\n+    @polymorphic_function.function(input_signature=[])\n+    def f():\n+      func = lambda: x\n+      return ops.get_default_graph()._experimental_capture_side_input_by_ref(  # pylint: disable=protected-access\n+          'lambda: x', func)\n+\n+    x = 1\n+    _ = f()\n+    x = 2\n+    _ = f()\n+    self.assertLen(total_function_cache(f), 2)\n+\n   def testFunctoolsLruCache(self):\n     self.skipTest(\n         \"b/194845243: inspect.getfullargspec doesn't unwrap Python decorators.\")\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  def testByRefCaptureWithInputSignature(self):\n+\n+    @polymorphic_function.function(input_signature=[])\n+    def f():\n+      func = lambda: x\n+      return ops.get_default_graph()._experimental_capture_side_input_by_ref(  # pylint: disable=protected-access\n+          'lambda: x', func)\n+\n+    x = 1\n+    _ = f()\n+    x = 2\n+    _ = f()\n+    self.assertLen(total_function_cache(f), 2)\n+\n",
            "whole_hunk": "@@ -4869,6 +4869,20 @@ class MultiDeviceTest(test.TestCase, parameterized.TestCase):\n     _ = f()\n     self.assertLen(total_function_cache(f), expected_len)\n \n+  def testByRefCaptureWithInputSignature(self):\n+\n+    @polymorphic_function.function(input_signature=[])\n+    def f():\n+      func = lambda: x\n+      return ops.get_default_graph()._experimental_capture_side_input_by_ref(  # pylint: disable=protected-access\n+          'lambda: x', func)\n+\n+    x = 1\n+    _ = f()\n+    x = 2\n+    _ = f()\n+    self.assertLen(total_function_cache(f), 2)\n+\n   def testFunctoolsLruCache(self):\n     self.skipTest(\n         \"b/194845243: inspect.getfullargspec doesn't unwrap Python decorators.\")\n"
        },
        {
            "name": "tracing_compiler.py",
            "path": "tensorflow/python/eager/polymorphic_function/tracing_compiler.py",
            "patches": [
                {
                    "old_start": 34,
                    "old_length": 7,
                    "new_start": 34,
                    "new_length": 6,
                    "hunk": "@@ -34,7 +34,6 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.profiler import trace\n from tensorflow.python.util import compat\n from tensorflow.python.util import lazy_loader\n-from tensorflow.python.util import object_identity\n from tensorflow.python.util import tf_decorator\n from tensorflow.python.util import tf_inspect\n \n"
                },
                {
                    "old_start": 201,
                    "old_length": 17,
                    "new_start": 200,
                    "new_length": 13,
                    "hunk": "@@ -201,17 +200,13 @@ class TracingCompiler:\n     with self._lock:\n       concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)\n       seen_names = set()\n-      captured = object_identity.ObjectIdentitySet(\n-          concrete_function.graph.internal_captures)\n-      # pylint: disable=protected-access\n-      concrete_function._arg_keywords = []\n+      concrete_function._arg_keywords = []  # pylint: disable=protected-access\n       prefix_counts = {}\n-      # pylint: enable=protected-access\n-      num_positional = 0\n-      for arg in concrete_function.graph.inputs:\n-        if arg in captured:\n-          break\n-        num_positional += 1\n+      graph = concrete_function.graph\n+      num_captures = len(\n+          graph.internal_captures + graph.deferred_internal_captures)\n+      num_positional = len(graph.inputs) - num_captures\n+      for arg in concrete_function.graph.inputs[:num_positional]:\n         user_arg_name = compat.as_str(arg.op.get_attr(\"_user_specified_name\"))\n         proposal = user_arg_name\n         while proposal in seen_names:"
                }
            ],
            "whole_deleted": "-from tensorflow.python.util import object_identity\n-      captured = object_identity.ObjectIdentitySet(\n-          concrete_function.graph.internal_captures)\n-      # pylint: disable=protected-access\n-      concrete_function._arg_keywords = []\n-      # pylint: enable=protected-access\n-      num_positional = 0\n-      for arg in concrete_function.graph.inputs:\n-        if arg in captured:\n-          break\n-        num_positional += 1\n",
            "whole_added": "+      concrete_function._arg_keywords = []  # pylint: disable=protected-access\n+      graph = concrete_function.graph\n+      num_captures = len(\n+          graph.internal_captures + graph.deferred_internal_captures)\n+      num_positional = len(graph.inputs) - num_captures\n+      for arg in concrete_function.graph.inputs[:num_positional]:\n",
            "whole_hunk": "@@ -34,7 +34,6 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.profiler import trace\n from tensorflow.python.util import compat\n from tensorflow.python.util import lazy_loader\n-from tensorflow.python.util import object_identity\n from tensorflow.python.util import tf_decorator\n from tensorflow.python.util import tf_inspect\n \n@@ -201,17 +200,13 @@ class TracingCompiler:\n     with self._lock:\n       concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)\n       seen_names = set()\n-      captured = object_identity.ObjectIdentitySet(\n-          concrete_function.graph.internal_captures)\n-      # pylint: disable=protected-access\n-      concrete_function._arg_keywords = []\n+      concrete_function._arg_keywords = []  # pylint: disable=protected-access\n       prefix_counts = {}\n-      # pylint: enable=protected-access\n-      num_positional = 0\n-      for arg in concrete_function.graph.inputs:\n-        if arg in captured:\n-          break\n-        num_positional += 1\n+      graph = concrete_function.graph\n+      num_captures = len(\n+          graph.internal_captures + graph.deferred_internal_captures)\n+      num_positional = len(graph.inputs) - num_captures\n+      for arg in concrete_function.graph.inputs[:num_positional]:\n         user_arg_name = compat.as_str(arg.op.get_attr(\"_user_specified_name\"))\n         proposal = user_arg_name\n         while proposal in seen_names:"
        }
    ]
},
{
    "Id": 468,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/685418cd85e09bc2117fa15bc1b6a75d21248348",
    "date": "2023-03-29T22:25:19-07:00",
    "message": "maxpooling op should check that ksize must be positive.\n\nPiperOrigin-RevId: 520539022",
    "label": "YES",
    "changes": [
        {
            "name": "pooling_ops_common.h",
            "path": "tensorflow/core/kernels/pooling_ops_common.h",
            "patches": [
                {
                    "old_start": 355,
                    "old_length": 6,
                    "new_start": 355,
                    "new_length": 10,
                    "hunk": "@@ -355,6 +355,10 @@ class MaxPoolingV2Op : public OpKernel {\n       OP_REQUIRES(context, ksize_.size() == 4,\n                   errors::InvalidArgument(\"Sliding window ksize field must \"\n                                           \"specify 4 dimensions\"));\n+      OP_REQUIRES(\n+          context,\n+          ksize_[0] > 0 && ksize_[1] > 0 && ksize_[2] > 0 && ksize_[3] > 0,\n+          errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n       OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n       OP_REQUIRES(context, stride_.size() == 4,\n                   errors::InvalidArgument(\"Sliding window stride field must \"\n"
                },
                {
                    "old_start": 387,
                    "old_length": 6,
                    "new_start": 391,
                    "new_length": 9,
                    "hunk": "@@ -387,6 +391,9 @@ class MaxPoolingV2Op : public OpKernel {\n     OP_REQUIRES(context, ksize.size() == 4,\n                 errors::InvalidArgument(\"Sliding window ksize field must \"\n                                         \"specify 4 dimensions\"));\n+    OP_REQUIRES(\n+        context, ksize[0] > 0 && ksize[1] > 0 && ksize[2] > 0 && ksize[3] > 0,\n+        errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n     OP_REQUIRES(context, stride.size() == 4,\n                 errors::InvalidArgument(\"Sliding window stride field must \"\n                                         \"specify 4 dimensions\"));"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      OP_REQUIRES(\n+          context,\n+          ksize_[0] > 0 && ksize_[1] > 0 && ksize_[2] > 0 && ksize_[3] > 0,\n+          errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n+    OP_REQUIRES(\n+        context, ksize[0] > 0 && ksize[1] > 0 && ksize[2] > 0 && ksize[3] > 0,\n+        errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n",
            "whole_hunk": "@@ -355,6 +355,10 @@ class MaxPoolingV2Op : public OpKernel {\n       OP_REQUIRES(context, ksize_.size() == 4,\n                   errors::InvalidArgument(\"Sliding window ksize field must \"\n                                           \"specify 4 dimensions\"));\n+      OP_REQUIRES(\n+          context,\n+          ksize_[0] > 0 && ksize_[1] > 0 && ksize_[2] > 0 && ksize_[3] > 0,\n+          errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n       OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n       OP_REQUIRES(context, stride_.size() == 4,\n                   errors::InvalidArgument(\"Sliding window stride field must \"\n@@ -387,6 +391,9 @@ class MaxPoolingV2Op : public OpKernel {\n     OP_REQUIRES(context, ksize.size() == 4,\n                 errors::InvalidArgument(\"Sliding window ksize field must \"\n                                         \"specify 4 dimensions\"));\n+    OP_REQUIRES(\n+        context, ksize[0] > 0 && ksize[1] > 0 && ksize[2] > 0 && ksize[3] > 0,\n+        errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n     OP_REQUIRES(context, stride.size() == 4,\n                 errors::InvalidArgument(\"Sliding window stride field must \"\n                                         \"specify 4 dimensions\"));"
        }
    ]
},
{
    "Id": 426,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7175c60d92eef9d4971157be27c17899b7b32d7c",
    "date": "2023-05-10T11:51:38-07:00",
    "message": "Add check to ensure number of parameters in HLO Computation in equal to number of op operands.\n\nPiperOrigin-RevId: 530963153",
    "label": "NO",
    "changes": [
        {
            "name": "tf2xla_rewriter.cc",
            "path": "tensorflow/compiler/mlir/tf2xla/transforms/tf2xla_rewriter.cc",
            "patches": [
                {
                    "old_start": 156,
                    "old_length": 6,
                    "new_start": 156,
                    "new_length": 13,
                    "hunk": "@@ -156,6 +156,13 @@ tsl::StatusOr<mhlo::TupleOp> Tf2XlaRewriter::ImportXlaComputation(\n     return tsl::errors::InvalidArgument(\"Imported XLA Root is not a tuple op\");\n   }\n \n+  if (op_->getNumOperands() !=\n+      hlo_module->entry_computation()->num_parameters()) {\n+    return tsl::errors::InvalidArgument(\n+        \"Entry computation does not have equal number of parameters to op \"\n+        \"operands\");\n+  }\n+\n   ModuleOp mlir_module = op_->getParentOfType<ModuleOp>();\n   mlir::OpBuilder builder(op_);\n   mlir::SymbolTable symbol_table(mlir_module);\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  if (op_->getNumOperands() !=\n+      hlo_module->entry_computation()->num_parameters()) {\n+    return tsl::errors::InvalidArgument(\n+        \"Entry computation does not have equal number of parameters to op \"\n+        \"operands\");\n+  }\n+\n",
            "whole_hunk": "@@ -156,6 +156,13 @@ tsl::StatusOr<mhlo::TupleOp> Tf2XlaRewriter::ImportXlaComputation(\n     return tsl::errors::InvalidArgument(\"Imported XLA Root is not a tuple op\");\n   }\n \n+  if (op_->getNumOperands() !=\n+      hlo_module->entry_computation()->num_parameters()) {\n+    return tsl::errors::InvalidArgument(\n+        \"Entry computation does not have equal number of parameters to op \"\n+        \"operands\");\n+  }\n+\n   ModuleOp mlir_module = op_->getParentOfType<ModuleOp>();\n   mlir::OpBuilder builder(op_);\n   mlir::SymbolTable symbol_table(mlir_module);\n"
        },
        {
            "name": "tf2xla_rewriter_test.cc",
            "path": "tensorflow/compiler/mlir/tf2xla/transforms/tf2xla_rewriter_test.cc",
            "patches": [
                {
                    "old_start": 75,
                    "old_length": 8,
                    "new_start": 75,
                    "new_length": 10,
                    "hunk": "@@ -75,8 +75,10 @@ module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, pr\n \n XlaComputation GetTestXlaComputation() {\n   XlaBuilder xla_builder(\"test\");\n-  XlaOp add = xla::Add(xla::ConstantR0<float>(&xla_builder, 1.0),\n-                       xla::ConstantR0<float>(&xla_builder, 2.0));\n+  auto param =\n+      Parameter(&xla_builder, 0, ShapeUtil::MakeScalarShape(xla::F32), \"a\");\n+\n+  XlaOp add = xla::Add(param, xla::ConstantR0<float>(&xla_builder, 2.0));\n \n   std::vector<XlaOp> tuple_values;\n   tuple_values.push_back(add);\n"
                },
                {
                    "old_start": 291,
                    "old_length": 7,
                    "new_start": 293,
                    "new_length": 7,
                    "hunk": "@@ -291,7 +293,7 @@ TEST_F(Tf2XlaRewriterTest, InsertsConstantParameters) {\n       LegalizeModule(/*use_tf2xla_hlo_importer=*/true, kModuleWithConstParam));\n }\n \n-TEST_F(Tf2XlaRewriterTest, DISABLED_ImportsPrivateFunctions) {\n+TEST_F(Tf2XlaRewriterTest, ErrorsWithInvalidNumberOfParametersToArgs) {\n   XlaBuilder builder(\"test_builder\");\n   XlaComputation to_apply;\n   {\n"
                },
                {
                    "old_start": 315,
                    "old_length": 9,
                    "new_start": 317,
                    "new_length": 9,
                    "hunk": "@@ -315,9 +317,9 @@ TEST_F(Tf2XlaRewriterTest, DISABLED_ImportsPrivateFunctions) {\n   EXPECT_EQ(computation.proto().computations_size(), 2);\n \n   TF_ASSERT_OK(CreateMlirModule());\n-  TF_ASSERT_OK_AND_ASSIGN(TupleOp root_tuple,\n-                          ImportXlaComputationIntoModule(computation));\n-  EXPECT_TRUE(root_tuple);\n+  tsl::StatusOr<TupleOp> status_or_tuple_op =\n+      ImportXlaComputationIntoModule(computation);\n+  EXPECT_FALSE(status_or_tuple_op.ok());\n }\n \n }  // namespace mhlo"
                }
            ],
            "whole_deleted": "-  XlaOp add = xla::Add(xla::ConstantR0<float>(&xla_builder, 1.0),\n-                       xla::ConstantR0<float>(&xla_builder, 2.0));\n-TEST_F(Tf2XlaRewriterTest, DISABLED_ImportsPrivateFunctions) {\n-  TF_ASSERT_OK_AND_ASSIGN(TupleOp root_tuple,\n-                          ImportXlaComputationIntoModule(computation));\n-  EXPECT_TRUE(root_tuple);\n",
            "whole_added": "+  auto param =\n+      Parameter(&xla_builder, 0, ShapeUtil::MakeScalarShape(xla::F32), \"a\");\n+\n+  XlaOp add = xla::Add(param, xla::ConstantR0<float>(&xla_builder, 2.0));\n+TEST_F(Tf2XlaRewriterTest, ErrorsWithInvalidNumberOfParametersToArgs) {\n+  tsl::StatusOr<TupleOp> status_or_tuple_op =\n+      ImportXlaComputationIntoModule(computation);\n+  EXPECT_FALSE(status_or_tuple_op.ok());\n",
            "whole_hunk": "@@ -75,8 +75,10 @@ module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, pr\n \n XlaComputation GetTestXlaComputation() {\n   XlaBuilder xla_builder(\"test\");\n-  XlaOp add = xla::Add(xla::ConstantR0<float>(&xla_builder, 1.0),\n-                       xla::ConstantR0<float>(&xla_builder, 2.0));\n+  auto param =\n+      Parameter(&xla_builder, 0, ShapeUtil::MakeScalarShape(xla::F32), \"a\");\n+\n+  XlaOp add = xla::Add(param, xla::ConstantR0<float>(&xla_builder, 2.0));\n \n   std::vector<XlaOp> tuple_values;\n   tuple_values.push_back(add);\n@@ -291,7 +293,7 @@ TEST_F(Tf2XlaRewriterTest, InsertsConstantParameters) {\n       LegalizeModule(/*use_tf2xla_hlo_importer=*/true, kModuleWithConstParam));\n }\n \n-TEST_F(Tf2XlaRewriterTest, DISABLED_ImportsPrivateFunctions) {\n+TEST_F(Tf2XlaRewriterTest, ErrorsWithInvalidNumberOfParametersToArgs) {\n   XlaBuilder builder(\"test_builder\");\n   XlaComputation to_apply;\n   {\n@@ -315,9 +317,9 @@ TEST_F(Tf2XlaRewriterTest, DISABLED_ImportsPrivateFunctions) {\n   EXPECT_EQ(computation.proto().computations_size(), 2);\n \n   TF_ASSERT_OK(CreateMlirModule());\n-  TF_ASSERT_OK_AND_ASSIGN(TupleOp root_tuple,\n-                          ImportXlaComputationIntoModule(computation));\n-  EXPECT_TRUE(root_tuple);\n+  tsl::StatusOr<TupleOp> status_or_tuple_op =\n+      ImportXlaComputationIntoModule(computation);\n+  EXPECT_FALSE(status_or_tuple_op.ok());\n }\n \n }  // namespace mhlo"
        }
    ]
},
{
    "Id": 417,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/03d38ef5265d84d486d99aae9edd2040f79328af",
    "date": "2023-05-30T09:39:09-07:00",
    "message": "Remove duplicate checks. `std::optional<T>::operator bool` and `std::optional<T>::has_value` (HloInputOutputAliasConfig::OutputHasAlias) are equivalent.\n\nPiperOrigin-RevId: 536425731",
    "label": "NO",
    "changes": [
        {
            "name": "hlo_input_output_alias_config.cc",
            "path": "tensorflow/compiler/xla/hlo/ir/hlo_input_output_alias_config.cc",
            "patches": [
                {
                    "old_start": 34,
                    "old_length": 8,
                    "new_start": 34,
                    "new_length": 6,
                    "hunk": "@@ -34,8 +34,6 @@ Status HloInputOutputAliasConfig::SetUpAlias(\n       << \" which is an invalid index for shape \"\n       << ShapeUtil::HumanString(alias_.shape());\n   TF_RET_CHECK(param_number >= 0) << param_number;\n-  TF_RET_CHECK(!OutputHasAlias(output_index))\n-      << \"Output index \" << output_index << \" already has an alias setup\";\n   // Output can't be aliased with multiple parameters.\n   TF_RET_CHECK(!alias_.element(output_index)) << absl::StrFormat(\n       \"Trying to set up output alias for param %lld at %s but failed: output \""
                }
            ],
            "whole_deleted": "-  TF_RET_CHECK(!OutputHasAlias(output_index))\n-      << \"Output index \" << output_index << \" already has an alias setup\";\n",
            "whole_added": "",
            "whole_hunk": "@@ -34,8 +34,6 @@ Status HloInputOutputAliasConfig::SetUpAlias(\n       << \" which is an invalid index for shape \"\n       << ShapeUtil::HumanString(alias_.shape());\n   TF_RET_CHECK(param_number >= 0) << param_number;\n-  TF_RET_CHECK(!OutputHasAlias(output_index))\n-      << \"Output index \" << output_index << \" already has an alias setup\";\n   // Output can't be aliased with multiple parameters.\n   TF_RET_CHECK(!alias_.element(output_index)) << absl::StrFormat(\n       \"Trying to set up output alias for param %lld at %s but failed: output \""
        }
    ]
},
{
    "Id": 332,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c2da6c528594f361d03fb106e6db0279ea9fec22",
    "date": "2023-08-07T17:08:03-07:00",
    "message": "Update isinstance checks in TF-NumPy to use core.Tensor instead of tensor.Tensor.\nThis allows WeakTensor to pass the instance checks.\n\nPiperOrigin-RevId: 554636220",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/python/ops/numpy_ops/BUILD",
            "patches": [
                {
                    "old_start": 71,
                    "old_length": 6,
                    "new_start": 71,
                    "new_length": 7,
                    "hunk": "@@ -71,6 +71,7 @@ py_strict_library(\n         \"//tensorflow/python/ops:manip_ops\",\n         \"//tensorflow/python/ops:math_ops\",\n         \"//tensorflow/python/ops:sort_ops\",\n+        \"//tensorflow/python/types:core\",\n         \"//tensorflow/python/util:nest\",\n         \"//tensorflow/python/util:tf_export\",\n         \"//third_party/py/numpy\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//tensorflow/python/types:core\",\n",
            "whole_hunk": "@@ -71,6 +71,7 @@ py_strict_library(\n         \"//tensorflow/python/ops:manip_ops\",\n         \"//tensorflow/python/ops:math_ops\",\n         \"//tensorflow/python/ops:sort_ops\",\n+        \"//tensorflow/python/types:core\",\n         \"//tensorflow/python/util:nest\",\n         \"//tensorflow/python/util:tf_export\",\n         \"//third_party/py/numpy\",\n"
        },
        {
            "name": "np_array_ops.py",
            "path": "tensorflow/python/ops/numpy_ops/np_array_ops.py",
            "patches": [
                {
                    "old_start": 39,
                    "old_length": 6,
                    "new_start": 39,
                    "new_length": 7,
                    "hunk": "@@ -39,6 +39,7 @@ from tensorflow.python.ops import sort_ops\n from tensorflow.python.ops.numpy_ops import np_arrays\n from tensorflow.python.ops.numpy_ops import np_dtypes\n from tensorflow.python.ops.numpy_ops import np_utils\n+from tensorflow.python.types import core as core_tf_types\n from tensorflow.python.util import nest\n from tensorflow.python.util import tf_export\n \n"
                },
                {
                    "old_start": 2045,
                    "old_length": 7,
                    "new_start": 2046,
                    "new_length": 7,
                    "hunk": "@@ -2045,7 +2046,7 @@ def _getitem(self, slice_spec):\n   if (\n       isinstance(slice_spec, bool)\n       or (\n-          isinstance(slice_spec, tensor_lib.Tensor)\n+          isinstance(slice_spec, core_tf_types.Tensor)\n           and slice_spec.dtype == dtypes.bool\n       )\n       or (\n"
                },
                {
                    "old_start": 2067,
                    "old_length": 7,
                    "new_start": 2068,
                    "new_length": 7,
                    "hunk": "@@ -2067,7 +2068,7 @@ def _with_index_update_helper(update_method, a, slice_spec, updates):\n   if (\n       isinstance(slice_spec, bool)\n       or (\n-          isinstance(slice_spec, tensor_lib.Tensor)\n+          isinstance(slice_spec, core_tf_types.Tensor)\n           and slice_spec.dtype == dtypes.bool\n       )\n       or ("
                }
            ],
            "whole_deleted": "-          isinstance(slice_spec, tensor_lib.Tensor)\n-          isinstance(slice_spec, tensor_lib.Tensor)\n",
            "whole_added": "+from tensorflow.python.types import core as core_tf_types\n+          isinstance(slice_spec, core_tf_types.Tensor)\n+          isinstance(slice_spec, core_tf_types.Tensor)\n",
            "whole_hunk": "@@ -39,6 +39,7 @@ from tensorflow.python.ops import sort_ops\n from tensorflow.python.ops.numpy_ops import np_arrays\n from tensorflow.python.ops.numpy_ops import np_dtypes\n from tensorflow.python.ops.numpy_ops import np_utils\n+from tensorflow.python.types import core as core_tf_types\n from tensorflow.python.util import nest\n from tensorflow.python.util import tf_export\n \n@@ -2045,7 +2046,7 @@ def _getitem(self, slice_spec):\n   if (\n       isinstance(slice_spec, bool)\n       or (\n-          isinstance(slice_spec, tensor_lib.Tensor)\n+          isinstance(slice_spec, core_tf_types.Tensor)\n           and slice_spec.dtype == dtypes.bool\n       )\n       or (\n@@ -2067,7 +2068,7 @@ def _with_index_update_helper(update_method, a, slice_spec, updates):\n   if (\n       isinstance(slice_spec, bool)\n       or (\n-          isinstance(slice_spec, tensor_lib.Tensor)\n+          isinstance(slice_spec, core_tf_types.Tensor)\n           and slice_spec.dtype == dtypes.bool\n       )\n       or ("
        }
    ]
},
{
    "Id": 687,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2aa9e7e4c7581cb47fc69324d98883c252bee263",
    "date": "2022-09-20T10:06:48-07:00",
    "message": "[XLA:GPU] Allow partially uncoalesced reads in transpose->reduction fusion merging.\n\nProfiling of real models and small test cases shows that merging fusions with a transpose and fusions resulting in a reduction may be beneficial for not materializing intermediate results even though reads become uncoalesced. The check for transposes is improved therefore to estimate what percentage of the data of the producer fusion is being transposed.\n\nTest GpuFusibleTest.DoNotFuseLayoutChangingOpWithReduce was removed for being a duplicate of InstructionFusionTest.DoNotFuseLayoutChangingOpWithReduce: it is about instruction fusion, whereas gpu_fusible checks are used for both instruction fusion and fusion merging.\n\nTest GpuFusibleTest.TransposingCopyNotFused was removed for being almost a duplicate of FusionMergerTest.WillNotMergeReduceUnfriendlyLayouts: it is about fusion merging, whereas again gpu_fusible checks are common.\n\nPiperOrigin-RevId: 475580193",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 1356,
                    "old_length": 12,
                    "new_start": 1356,
                    "new_length": 10,
                    "hunk": "@@ -1356,12 +1356,10 @@ cc_library(\n         \"//tensorflow/compiler/xla:shape_util\",\n         \"//tensorflow/compiler/xla:util\",\n         \"//tensorflow/compiler/xla/service:hlo\",\n-        \"//tensorflow/compiler/xla/service:hlo_cost_analysis\",\n         \"//tensorflow/compiler/xla/service:hlo_graph_dumper\",\n         \"//tensorflow/compiler/xla/service:hlo_pass\",\n         \"//tensorflow/compiler/xla/service/llvm_ir:fused_ir_emitter\",\n         \"//tensorflow/core:lib\",\n-        \"//tensorflow/tsl/platform:logging\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/strings\",\n     ],\n"
                }
            ],
            "whole_deleted": "-        \"//tensorflow/compiler/xla/service:hlo_cost_analysis\",\n-        \"//tensorflow/tsl/platform:logging\",\n",
            "whole_added": "",
            "whole_hunk": "@@ -1356,12 +1356,10 @@ cc_library(\n         \"//tensorflow/compiler/xla:shape_util\",\n         \"//tensorflow/compiler/xla:util\",\n         \"//tensorflow/compiler/xla/service:hlo\",\n-        \"//tensorflow/compiler/xla/service:hlo_cost_analysis\",\n         \"//tensorflow/compiler/xla/service:hlo_graph_dumper\",\n         \"//tensorflow/compiler/xla/service:hlo_pass\",\n         \"//tensorflow/compiler/xla/service/llvm_ir:fused_ir_emitter\",\n         \"//tensorflow/core:lib\",\n-        \"//tensorflow/tsl/platform:logging\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/strings\",\n     ],\n"
        },
        {
            "name": "fusion_merger.cc",
            "path": "tensorflow/compiler/xla/service/gpu/fusion_merger.cc",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 8,
                    "new_start": 23,
                    "new_length": 8,
                    "hunk": "@@ -23,8 +23,8 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"tensorflow/compiler/xla/service/gpu/gpu_fusible.h\"\n #include \"tensorflow/compiler/xla/service/gpu/instruction_fusion.h\"\n-#include \"tensorflow/compiler/xla/service/hlo_cost_analysis.h\"\n #include \"tensorflow/compiler/xla/service/hlo_graph_dumper.h\"\n+#include \"tensorflow/compiler/xla/service/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/fused_ir_emitter.h\"\n #include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/util.h\"\n"
                },
                {
                    "old_start": 160,
                    "old_length": 6,
                    "new_start": 160,
                    "new_length": 7,
                    "hunk": "@@ -160,6 +160,7 @@ class FusionInstructionMerger {\n   int num_fail_net_bytes_transferred_ratio_ = 0;\n   int num_fail_inefficient_fusion_emitter_ = 0;\n   int num_fail_fusion_too_large_ = 0;\n+  int num_fail_uncoalesced_read_ = 0;\n \n   FusionInstructionMerger(const FusionInstructionMerger&) = delete;\n   FusionInstructionMerger& operator=(const FusionInstructionMerger&) = delete;\n"
                },
                {
                    "old_start": 239,
                    "old_length": 6,
                    "new_start": 240,
                    "new_length": 7,
                    "hunk": "@@ -239,6 +240,7 @@ Status FusionInstructionMerger::Run() {\n           << \" not_loop_fusion: \" << num_fail_not_loop_fusion_\n           << \" merge_all_users: \" << num_fail_merge_all_users_\n           << \" expensive_instruction: \" << num_fail_expensive_fused_instruction_\n+          << \" uncoalesced_read: \" << num_fail_uncoalesced_read_\n           << \" net_bytes_transferred: \" << num_fail_net_bytes_transferred_ratio_\n           << \" inefficient_fusion_emitter: \"\n           << num_fail_inefficient_fusion_emitter_\n"
                },
                {
                    "old_start": 246,
                    "old_length": 6,
                    "new_start": 248,
                    "new_length": 23,
                    "hunk": "@@ -246,6 +248,23 @@ Status FusionInstructionMerger::Run() {\n   return OkStatus();\n }\n \n+bool TransposesMostData(const HloInstruction& fusion) {\n+  float score = 0;\n+\n+  for (const HloInstruction* instr : fusion.fused_instructions()) {\n+    if (IsPhysicallyTransposing(*instr)) {\n+      score += 1.0 * ShapeUtil::ElementsIn(instr->shape()) /\n+               ShapeUtil::ElementsIn(fusion.shape());\n+      if (score >= 0.5) {\n+        VLOG(3) << fusion.ToString() << \" transpose ratio exceeds \" << score;\n+        return true;\n+      }\n+    }\n+  }\n+\n+  return false;\n+}\n+\n FusionDecision FusionInstructionMerger::HandleFusion(HloInstruction* fusion) {\n   ++total_visited_;\n \n"
                },
                {
                    "old_start": 264,
                    "old_length": 14,
                    "new_start": 283,
                    "new_length": 27,
                    "hunk": "@@ -264,14 +283,27 @@ FusionDecision FusionInstructionMerger::HandleFusion(HloInstruction* fusion) {\n     return \"not a loop fusion\";\n   }\n \n+  bool has_reduction_user = false;\n   for (const HloInstruction* user : fusion->users()) {\n-    FusionDecision fusible = IsProducerConsumerFusible(*fusion, *user)\n-                                 .And({user->opcode() != HloOpcode::kBitcast,\n-                                       \"not fusing bitcast ops\"});\n+    if (user->opcode() == HloOpcode::kBitcast) {\n+      ++num_fail_merge_all_users_;\n+      return \"not fusing bitcast ops\";\n+    }\n+    FusionDecision fusible = IsProducerConsumerFusible(*fusion, *user);\n     if (!fusible) {\n       ++num_fail_merge_all_users_;\n       return fusible;\n     }\n+    if (IsInputFusibleReduction(*user)) {\n+      has_reduction_user = true;\n+    }\n+  }\n+\n+  // We do not want to worsen reduction's memory access pattern by connecting\n+  // it to a producer which transposes most data.\n+  if (has_reduction_user && TransposesMostData(*fusion)) {\n+    ++num_fail_uncoalesced_read_;\n+    return \"would read mostly uncoalesced\";\n   }\n \n   // Skip 'fusion' instruction if merging it into all users would result in a\n"
                }
            ],
            "whole_deleted": "-#include \"tensorflow/compiler/xla/service/hlo_cost_analysis.h\"\n-    FusionDecision fusible = IsProducerConsumerFusible(*fusion, *user)\n-                                 .And({user->opcode() != HloOpcode::kBitcast,\n-                                       \"not fusing bitcast ops\"});\n",
            "whole_added": "+#include \"tensorflow/compiler/xla/service/hlo_instruction.h\"\n+  int num_fail_uncoalesced_read_ = 0;\n+          << \" uncoalesced_read: \" << num_fail_uncoalesced_read_\n+bool TransposesMostData(const HloInstruction& fusion) {\n+  float score = 0;\n+\n+  for (const HloInstruction* instr : fusion.fused_instructions()) {\n+    if (IsPhysicallyTransposing(*instr)) {\n+      score += 1.0 * ShapeUtil::ElementsIn(instr->shape()) /\n+               ShapeUtil::ElementsIn(fusion.shape());\n+      if (score >= 0.5) {\n+        VLOG(3) << fusion.ToString() << \" transpose ratio exceeds \" << score;\n+        return true;\n+      }\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+  bool has_reduction_user = false;\n+    if (user->opcode() == HloOpcode::kBitcast) {\n+      ++num_fail_merge_all_users_;\n+      return \"not fusing bitcast ops\";\n+    }\n+    FusionDecision fusible = IsProducerConsumerFusible(*fusion, *user);\n+    if (IsInputFusibleReduction(*user)) {\n+      has_reduction_user = true;\n+    }\n+  }\n+\n+  // We do not want to worsen reduction's memory access pattern by connecting\n+  // it to a producer which transposes most data.\n+  if (has_reduction_user && TransposesMostData(*fusion)) {\n+    ++num_fail_uncoalesced_read_;\n+    return \"would read mostly uncoalesced\";\n",
            "whole_hunk": "@@ -23,8 +23,8 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"tensorflow/compiler/xla/service/gpu/gpu_fusible.h\"\n #include \"tensorflow/compiler/xla/service/gpu/instruction_fusion.h\"\n-#include \"tensorflow/compiler/xla/service/hlo_cost_analysis.h\"\n #include \"tensorflow/compiler/xla/service/hlo_graph_dumper.h\"\n+#include \"tensorflow/compiler/xla/service/hlo_instruction.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/fused_ir_emitter.h\"\n #include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/util.h\"\n@@ -160,6 +160,7 @@ class FusionInstructionMerger {\n   int num_fail_net_bytes_transferred_ratio_ = 0;\n   int num_fail_inefficient_fusion_emitter_ = 0;\n   int num_fail_fusion_too_large_ = 0;\n+  int num_fail_uncoalesced_read_ = 0;\n \n   FusionInstructionMerger(const FusionInstructionMerger&) = delete;\n   FusionInstructionMerger& operator=(const FusionInstructionMerger&) = delete;\n@@ -239,6 +240,7 @@ Status FusionInstructionMerger::Run() {\n           << \" not_loop_fusion: \" << num_fail_not_loop_fusion_\n           << \" merge_all_users: \" << num_fail_merge_all_users_\n           << \" expensive_instruction: \" << num_fail_expensive_fused_instruction_\n+          << \" uncoalesced_read: \" << num_fail_uncoalesced_read_\n           << \" net_bytes_transferred: \" << num_fail_net_bytes_transferred_ratio_\n           << \" inefficient_fusion_emitter: \"\n           << num_fail_inefficient_fusion_emitter_\n@@ -246,6 +248,23 @@ Status FusionInstructionMerger::Run() {\n   return OkStatus();\n }\n \n+bool TransposesMostData(const HloInstruction& fusion) {\n+  float score = 0;\n+\n+  for (const HloInstruction* instr : fusion.fused_instructions()) {\n+    if (IsPhysicallyTransposing(*instr)) {\n+      score += 1.0 * ShapeUtil::ElementsIn(instr->shape()) /\n+               ShapeUtil::ElementsIn(fusion.shape());\n+      if (score >= 0.5) {\n+        VLOG(3) << fusion.ToString() << \" transpose ratio exceeds \" << score;\n+        return true;\n+      }\n+    }\n+  }\n+\n+  return false;\n+}\n+\n FusionDecision FusionInstructionMerger::HandleFusion(HloInstruction* fusion) {\n   ++total_visited_;\n \n@@ -264,14 +283,27 @@ FusionDecision FusionInstructionMerger::HandleFusion(HloInstruction* fusion) {\n     return \"not a loop fusion\";\n   }\n \n+  bool has_reduction_user = false;\n   for (const HloInstruction* user : fusion->users()) {\n-    FusionDecision fusible = IsProducerConsumerFusible(*fusion, *user)\n-                                 .And({user->opcode() != HloOpcode::kBitcast,\n-                                       \"not fusing bitcast ops\"});\n+    if (user->opcode() == HloOpcode::kBitcast) {\n+      ++num_fail_merge_all_users_;\n+      return \"not fusing bitcast ops\";\n+    }\n+    FusionDecision fusible = IsProducerConsumerFusible(*fusion, *user);\n     if (!fusible) {\n       ++num_fail_merge_all_users_;\n       return fusible;\n     }\n+    if (IsInputFusibleReduction(*user)) {\n+      has_reduction_user = true;\n+    }\n+  }\n+\n+  // We do not want to worsen reduction's memory access pattern by connecting\n+  // it to a producer which transposes most data.\n+  if (has_reduction_user && TransposesMostData(*fusion)) {\n+    ++num_fail_uncoalesced_read_;\n+    return \"would read mostly uncoalesced\";\n   }\n \n   // Skip 'fusion' instruction if merging it into all users would result in a\n"
        },
        {
            "name": "fusion_merger_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/fusion_merger_test.cc",
            "patches": [
                {
                    "old_start": 283,
                    "old_length": 6,
                    "new_start": 283,
                    "new_length": 8,
                    "hunk": "@@ -283,6 +283,8 @@ TEST_F(FusionMergerTest, WillMergeIntoUnfusedConsumer) {\n }\n \n TEST_F(FusionMergerTest, WillNotMergeReduceUnfriendlyLayouts) {\n+  // TODO(b/247762001): the case here does not represent the problem -\n+  // profiling shows that it works faster if merged (even on larger dimensions).\n   auto module = ParseAndReturnVerifiedModule(R\"(\n     HloModule m\n \n"
                },
                {
                    "old_start": 315,
                    "old_length": 6,
                    "new_start": 317,
                    "new_length": 42,
                    "hunk": "@@ -315,6 +317,42 @@ TEST_F(FusionMergerTest, WillNotMergeReduceUnfriendlyLayouts) {\n   EXPECT_FALSE(FusionMerger().Run(module.get()).value());\n }\n \n+TEST_F(FusionMergerTest, WillMergeReduceNotTooUnfriendlyLayouts) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule m\n+\n+    f1_computation {\n+      f1_p0 = f32[16,16,256]{0,1,2} parameter(0)\n+      slice1 = f32[5,16,256]{0,1,2} slice(f1_p0), slice={[0:5], [0:16], [0:256]}\n+      // Here the copy changes the layout only of a part of the data.\n+      f1_copy = f32[5,16,256]{2,1,0} copy(slice1)\n+      slice2 = f32[11,16,256]{0,1,2} slice(f1_p0), slice={[0:11], [0:16], [0:256]}\n+      bitcast = f32[11,16,256]{2,1,0} bitcast(slice2)\n+      ROOT f1_root = f32[16,16,256]{2,1,0} concatenate(f1_copy, bitcast), dimensions={0}\n+    }\n+\n+    add_computation {\n+      add_lhs = f32[] parameter(0)\n+      add_rhs = f32[] parameter(1)\n+      ROOT add_root = f32[] add(add_lhs, add_rhs)\n+    }\n+\n+    f2_computation {\n+      f2_p0 = f32[16,16,256]{2,1,0} parameter(0)\n+      f2_zero = f32[] constant(0)\n+      ROOT f2_root = f32[] reduce(f2_p0, f2_zero), dimensions={0,1,2},\n+             to_apply=add_computation\n+    }\n+\n+    ENTRY entry {\n+      p0 = f32[16,16,256]{0,1,2} parameter(0)\n+      f1 = f32[16,16,256]{2,1,0} fusion(p0), kind=kLoop, calls=f1_computation\n+      ROOT f2 = f32[] fusion(f1), kind=kInput, calls=f2_computation\n+    })\")\n+                    .value();\n+  EXPECT_TRUE(FusionMerger().Run(module.get()).value());\n+}\n+\n // Check that we limit the number of operands to fusions we create.\n TEST_F(FusionMergerTest, AvoidsLargeFusion) {\n   constexpr int64_t kNumParams = MaxOperandsAndOutputsPerFusion() + 1;\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // TODO(b/247762001): the case here does not represent the problem -\n+  // profiling shows that it works faster if merged (even on larger dimensions).\n+TEST_F(FusionMergerTest, WillMergeReduceNotTooUnfriendlyLayouts) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule m\n+\n+    f1_computation {\n+      f1_p0 = f32[16,16,256]{0,1,2} parameter(0)\n+      slice1 = f32[5,16,256]{0,1,2} slice(f1_p0), slice={[0:5], [0:16], [0:256]}\n+      // Here the copy changes the layout only of a part of the data.\n+      f1_copy = f32[5,16,256]{2,1,0} copy(slice1)\n+      slice2 = f32[11,16,256]{0,1,2} slice(f1_p0), slice={[0:11], [0:16], [0:256]}\n+      bitcast = f32[11,16,256]{2,1,0} bitcast(slice2)\n+      ROOT f1_root = f32[16,16,256]{2,1,0} concatenate(f1_copy, bitcast), dimensions={0}\n+    }\n+\n+    add_computation {\n+      add_lhs = f32[] parameter(0)\n+      add_rhs = f32[] parameter(1)\n+      ROOT add_root = f32[] add(add_lhs, add_rhs)\n+    }\n+\n+    f2_computation {\n+      f2_p0 = f32[16,16,256]{2,1,0} parameter(0)\n+      f2_zero = f32[] constant(0)\n+      ROOT f2_root = f32[] reduce(f2_p0, f2_zero), dimensions={0,1,2},\n+             to_apply=add_computation\n+    }\n+\n+    ENTRY entry {\n+      p0 = f32[16,16,256]{0,1,2} parameter(0)\n+      f1 = f32[16,16,256]{2,1,0} fusion(p0), kind=kLoop, calls=f1_computation\n+      ROOT f2 = f32[] fusion(f1), kind=kInput, calls=f2_computation\n+    })\")\n+                    .value();\n+  EXPECT_TRUE(FusionMerger().Run(module.get()).value());\n+}\n+\n",
            "whole_hunk": "@@ -283,6 +283,8 @@ TEST_F(FusionMergerTest, WillMergeIntoUnfusedConsumer) {\n }\n \n TEST_F(FusionMergerTest, WillNotMergeReduceUnfriendlyLayouts) {\n+  // TODO(b/247762001): the case here does not represent the problem -\n+  // profiling shows that it works faster if merged (even on larger dimensions).\n   auto module = ParseAndReturnVerifiedModule(R\"(\n     HloModule m\n \n@@ -315,6 +317,42 @@ TEST_F(FusionMergerTest, WillNotMergeReduceUnfriendlyLayouts) {\n   EXPECT_FALSE(FusionMerger().Run(module.get()).value());\n }\n \n+TEST_F(FusionMergerTest, WillMergeReduceNotTooUnfriendlyLayouts) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule m\n+\n+    f1_computation {\n+      f1_p0 = f32[16,16,256]{0,1,2} parameter(0)\n+      slice1 = f32[5,16,256]{0,1,2} slice(f1_p0), slice={[0:5], [0:16], [0:256]}\n+      // Here the copy changes the layout only of a part of the data.\n+      f1_copy = f32[5,16,256]{2,1,0} copy(slice1)\n+      slice2 = f32[11,16,256]{0,1,2} slice(f1_p0), slice={[0:11], [0:16], [0:256]}\n+      bitcast = f32[11,16,256]{2,1,0} bitcast(slice2)\n+      ROOT f1_root = f32[16,16,256]{2,1,0} concatenate(f1_copy, bitcast), dimensions={0}\n+    }\n+\n+    add_computation {\n+      add_lhs = f32[] parameter(0)\n+      add_rhs = f32[] parameter(1)\n+      ROOT add_root = f32[] add(add_lhs, add_rhs)\n+    }\n+\n+    f2_computation {\n+      f2_p0 = f32[16,16,256]{2,1,0} parameter(0)\n+      f2_zero = f32[] constant(0)\n+      ROOT f2_root = f32[] reduce(f2_p0, f2_zero), dimensions={0,1,2},\n+             to_apply=add_computation\n+    }\n+\n+    ENTRY entry {\n+      p0 = f32[16,16,256]{0,1,2} parameter(0)\n+      f1 = f32[16,16,256]{2,1,0} fusion(p0), kind=kLoop, calls=f1_computation\n+      ROOT f2 = f32[] fusion(f1), kind=kInput, calls=f2_computation\n+    })\")\n+                    .value();\n+  EXPECT_TRUE(FusionMerger().Run(module.get()).value());\n+}\n+\n // Check that we limit the number of operands to fusions we create.\n TEST_F(FusionMergerTest, AvoidsLargeFusion) {\n   constexpr int64_t kNumParams = MaxOperandsAndOutputsPerFusion() + 1;\n"
        },
        {
            "name": "gpu_fusible.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible.cc",
            "patches": [
                {
                    "old_start": 238,
                    "old_length": 12,
                    "new_start": 238,
                    "new_length": 6,
                    "hunk": "@@ -238,12 +238,6 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n     return \"the fusion would create a heavy computation\";\n   }\n \n-  // Do not fuse into fusions if the resulting kernel would suffer from\n-  // uncoalesced reads due to a transposed memory access pattern.\n-  if (IsInputFusibleReduction(consumer) && IsPhysicallyTransposing(producer)) {\n-    return \"fusing the producer would break read coalescing\";\n-  }\n-\n   // Fuse scalar constants into loop fusion nodes. This reduces the number of\n   // parameters and makes matching scalar broadcasts easier.\n   //\n"
                }
            ],
            "whole_deleted": "-  // Do not fuse into fusions if the resulting kernel would suffer from\n-  // uncoalesced reads due to a transposed memory access pattern.\n-  if (IsInputFusibleReduction(consumer) && IsPhysicallyTransposing(producer)) {\n-    return \"fusing the producer would break read coalescing\";\n-  }\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -238,12 +238,6 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n     return \"the fusion would create a heavy computation\";\n   }\n \n-  // Do not fuse into fusions if the resulting kernel would suffer from\n-  // uncoalesced reads due to a transposed memory access pattern.\n-  if (IsInputFusibleReduction(consumer) && IsPhysicallyTransposing(producer)) {\n-    return \"fusing the producer would break read coalescing\";\n-  }\n-\n   // Fuse scalar constants into loop fusion nodes. This reduces the number of\n   // parameters and makes matching scalar broadcasts easier.\n   //\n"
        },
        {
            "name": "gpu_fusible.h",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible.h",
            "patches": [
                {
                    "old_start": 115,
                    "old_length": 6,
                    "new_start": 115,
                    "new_length": 7,
                    "hunk": "@@ -115,6 +115,7 @@ bool ShapesCompatibleForMultiOutputFusion(const HloInstruction& instr1,\n // Whether the instructions are compatible for producer-consumer fusion\n // i.e. whether the producer and consumer are loop/input fusible and\n // they are not library calls.\n+// Used both by instruction fusion and fusion-fusion merging.\n FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n                                          const HloInstruction& consumer);\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// Used both by instruction fusion and fusion-fusion merging.\n",
            "whole_hunk": "@@ -115,6 +115,7 @@ bool ShapesCompatibleForMultiOutputFusion(const HloInstruction& instr1,\n // Whether the instructions are compatible for producer-consumer fusion\n // i.e. whether the producer and consumer are loop/input fusible and\n // they are not library calls.\n+// Used both by instruction fusion and fusion-fusion merging.\n FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n                                          const HloInstruction& consumer);\n \n"
        },
        {
            "name": "gpu_fusible_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible_test.cc",
            "patches": [
                {
                    "old_start": 962,
                    "old_length": 69,
                    "new_start": 962,
                    "new_length": 6,
                    "hunk": "@@ -962,69 +962,6 @@ TEST_F(GpuFusibleTest, NonscalarConstantsNotFused) {\n       static_cast<bool>(IsProducerConsumerFusible(*producer2, *consumer2)));\n }\n \n-TEST_F(GpuFusibleTest, TransposingCopyNotFused) {\n-  auto module = ParseAndReturnVerifiedModule(R\"(\n-    HloModule test_module\n-\n-    add {\n-      lhs = f32[] parameter(0)\n-      rhs = f32[] parameter(1)\n-      ROOT add = f32[] add(lhs, rhs)\n-    }\n-\n-    fused_producer {\n-      p = f16[32,64,128]{2,1,0} parameter(0)\n-      c = f32[32, 64, 128]{2,1,0} convert(p)\n-      copy = f32[32, 64, 128]{0,2,1} copy(c)\n-      ROOT bitcast = f32[32, 64, 128]{2,1,0} bitcast(copy)\n-    }\n-\n-    fused_consumer {\n-      p = f32[32, 64, 128]{2,1,0} parameter(0)\n-      zero = f32[] constant(0)\n-      ROOT out = f32[32, 64]{1,0} reduce(p, zero), dimensions={2}, to_apply=add\n-    }\n-\n-    ENTRY BroadcastIntoReduce {\n-      p = f16[32,64,128]{2,1,0} parameter(0)\n-      producer = f32[32, 64, 128]{2,1,0} fusion(p), kind=kLoop, calls=fused_producer\n-      ROOT consumer = f32[32, 64]{1,0} fusion(producer), kind=kInput, calls=fused_consumer\n-    })\")\n-                    .value();\n-  // Check that the transposing copy is not fusible into a reduction.\n-  const HloInstruction* root = module->entry_computation()->root_instruction();\n-  const HloInstruction* consumer =\n-      module->entry_computation()->root_instruction();\n-  const HloInstruction* producer = root->operand(0);\n-  EXPECT_FALSE(\n-      static_cast<bool>(IsProducerConsumerFusible(*producer, *consumer)));\n-}\n-\n-TEST_F(GpuFusibleTest, DoNotFuseLayoutChangingOpWithReduce) {\n-  auto module = ParseAndReturnVerifiedModule(R\"(\n-    HloModule test_module\n-\n-    add {\n-      lhs = f32[] parameter(0)\n-      rhs = f32[] parameter(1)\n-      ROOT add = f32[] add(lhs, rhs)\n-    }\n-\n-    ENTRY entry {\n-      p0 = f32[16,16,16,16]{3,2,1,0} parameter(0)\n-      copy = f32[16,16,16,16]{0,1,2,3} copy(p0)\n-      constant.1 = f32[] constant(0)\n-      ROOT reduce = f32[16] reduce(copy, constant.1), dimensions={0,1,2}, to_apply=add\n-    })\")\n-                    .value();\n-\n-  const HloInstruction* consumer =\n-      module->entry_computation()->root_instruction();\n-  const HloInstruction* producer = consumer->operand(0);\n-  EXPECT_FALSE(\n-      static_cast<bool>(IsProducerConsumerFusible(*producer, *consumer)));\n-}\n-\n TEST_F(GpuFusibleTest, FuseLayoutChangingOpWithElementwise) {\n   auto module = ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n"
                }
            ],
            "whole_deleted": "-TEST_F(GpuFusibleTest, TransposingCopyNotFused) {\n-  auto module = ParseAndReturnVerifiedModule(R\"(\n-    HloModule test_module\n-\n-    add {\n-      lhs = f32[] parameter(0)\n-      rhs = f32[] parameter(1)\n-      ROOT add = f32[] add(lhs, rhs)\n-    }\n-\n-    fused_producer {\n-      p = f16[32,64,128]{2,1,0} parameter(0)\n-      c = f32[32, 64, 128]{2,1,0} convert(p)\n-      copy = f32[32, 64, 128]{0,2,1} copy(c)\n-      ROOT bitcast = f32[32, 64, 128]{2,1,0} bitcast(copy)\n-    }\n-\n-    fused_consumer {\n-      p = f32[32, 64, 128]{2,1,0} parameter(0)\n-      zero = f32[] constant(0)\n-      ROOT out = f32[32, 64]{1,0} reduce(p, zero), dimensions={2}, to_apply=add\n-    }\n-\n-    ENTRY BroadcastIntoReduce {\n-      p = f16[32,64,128]{2,1,0} parameter(0)\n-      producer = f32[32, 64, 128]{2,1,0} fusion(p), kind=kLoop, calls=fused_producer\n-      ROOT consumer = f32[32, 64]{1,0} fusion(producer), kind=kInput, calls=fused_consumer\n-    })\")\n-                    .value();\n-  // Check that the transposing copy is not fusible into a reduction.\n-  const HloInstruction* root = module->entry_computation()->root_instruction();\n-  const HloInstruction* consumer =\n-      module->entry_computation()->root_instruction();\n-  const HloInstruction* producer = root->operand(0);\n-  EXPECT_FALSE(\n-      static_cast<bool>(IsProducerConsumerFusible(*producer, *consumer)));\n-}\n-\n-TEST_F(GpuFusibleTest, DoNotFuseLayoutChangingOpWithReduce) {\n-  auto module = ParseAndReturnVerifiedModule(R\"(\n-    HloModule test_module\n-\n-    add {\n-      lhs = f32[] parameter(0)\n-      rhs = f32[] parameter(1)\n-      ROOT add = f32[] add(lhs, rhs)\n-    }\n-\n-    ENTRY entry {\n-      p0 = f32[16,16,16,16]{3,2,1,0} parameter(0)\n-      copy = f32[16,16,16,16]{0,1,2,3} copy(p0)\n-      constant.1 = f32[] constant(0)\n-      ROOT reduce = f32[16] reduce(copy, constant.1), dimensions={0,1,2}, to_apply=add\n-    })\")\n-                    .value();\n-\n-  const HloInstruction* consumer =\n-      module->entry_computation()->root_instruction();\n-  const HloInstruction* producer = consumer->operand(0);\n-  EXPECT_FALSE(\n-      static_cast<bool>(IsProducerConsumerFusible(*producer, *consumer)));\n-}\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -962,69 +962,6 @@ TEST_F(GpuFusibleTest, NonscalarConstantsNotFused) {\n       static_cast<bool>(IsProducerConsumerFusible(*producer2, *consumer2)));\n }\n \n-TEST_F(GpuFusibleTest, TransposingCopyNotFused) {\n-  auto module = ParseAndReturnVerifiedModule(R\"(\n-    HloModule test_module\n-\n-    add {\n-      lhs = f32[] parameter(0)\n-      rhs = f32[] parameter(1)\n-      ROOT add = f32[] add(lhs, rhs)\n-    }\n-\n-    fused_producer {\n-      p = f16[32,64,128]{2,1,0} parameter(0)\n-      c = f32[32, 64, 128]{2,1,0} convert(p)\n-      copy = f32[32, 64, 128]{0,2,1} copy(c)\n-      ROOT bitcast = f32[32, 64, 128]{2,1,0} bitcast(copy)\n-    }\n-\n-    fused_consumer {\n-      p = f32[32, 64, 128]{2,1,0} parameter(0)\n-      zero = f32[] constant(0)\n-      ROOT out = f32[32, 64]{1,0} reduce(p, zero), dimensions={2}, to_apply=add\n-    }\n-\n-    ENTRY BroadcastIntoReduce {\n-      p = f16[32,64,128]{2,1,0} parameter(0)\n-      producer = f32[32, 64, 128]{2,1,0} fusion(p), kind=kLoop, calls=fused_producer\n-      ROOT consumer = f32[32, 64]{1,0} fusion(producer), kind=kInput, calls=fused_consumer\n-    })\")\n-                    .value();\n-  // Check that the transposing copy is not fusible into a reduction.\n-  const HloInstruction* root = module->entry_computation()->root_instruction();\n-  const HloInstruction* consumer =\n-      module->entry_computation()->root_instruction();\n-  const HloInstruction* producer = root->operand(0);\n-  EXPECT_FALSE(\n-      static_cast<bool>(IsProducerConsumerFusible(*producer, *consumer)));\n-}\n-\n-TEST_F(GpuFusibleTest, DoNotFuseLayoutChangingOpWithReduce) {\n-  auto module = ParseAndReturnVerifiedModule(R\"(\n-    HloModule test_module\n-\n-    add {\n-      lhs = f32[] parameter(0)\n-      rhs = f32[] parameter(1)\n-      ROOT add = f32[] add(lhs, rhs)\n-    }\n-\n-    ENTRY entry {\n-      p0 = f32[16,16,16,16]{3,2,1,0} parameter(0)\n-      copy = f32[16,16,16,16]{0,1,2,3} copy(p0)\n-      constant.1 = f32[] constant(0)\n-      ROOT reduce = f32[16] reduce(copy, constant.1), dimensions={0,1,2}, to_apply=add\n-    })\")\n-                    .value();\n-\n-  const HloInstruction* consumer =\n-      module->entry_computation()->root_instruction();\n-  const HloInstruction* producer = consumer->operand(0);\n-  EXPECT_FALSE(\n-      static_cast<bool>(IsProducerConsumerFusible(*producer, *consumer)));\n-}\n-\n TEST_F(GpuFusibleTest, FuseLayoutChangingOpWithElementwise) {\n   auto module = ParseAndReturnVerifiedModule(R\"(\n     HloModule test_module\n"
        },
        {
            "name": "instruction_fusion.cc",
            "path": "tensorflow/compiler/xla/service/gpu/instruction_fusion.cc",
            "patches": [
                {
                    "old_start": 70,
                    "old_length": 6,
                    "new_start": 70,
                    "new_length": 13,
                    "hunk": "@@ -70,6 +70,13 @@ FusionDecision GpuInstructionFusion::ShouldFuseInexpensiveChecks(\n     return \"the producer is expensive, and the consumer reuses inputs\";\n   }\n \n+  // Do not fuse into fusions if the resulting kernel would suffer from\n+  // uncoalesced reads due to a transposed memory access pattern.\n+  if (IsInputFusibleReduction(*consumer) &&\n+      IsPhysicallyTransposing(*producer)) {\n+    return \"fusing the producer would break read coalescing\";\n+  }\n+\n   if (NoFusionPossible fusible =\n           !IsProducerConsumerFusible(*producer, *consumer)) {\n     return !fusible;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // Do not fuse into fusions if the resulting kernel would suffer from\n+  // uncoalesced reads due to a transposed memory access pattern.\n+  if (IsInputFusibleReduction(*consumer) &&\n+      IsPhysicallyTransposing(*producer)) {\n+    return \"fusing the producer would break read coalescing\";\n+  }\n+\n",
            "whole_hunk": "@@ -70,6 +70,13 @@ FusionDecision GpuInstructionFusion::ShouldFuseInexpensiveChecks(\n     return \"the producer is expensive, and the consumer reuses inputs\";\n   }\n \n+  // Do not fuse into fusions if the resulting kernel would suffer from\n+  // uncoalesced reads due to a transposed memory access pattern.\n+  if (IsInputFusibleReduction(*consumer) &&\n+      IsPhysicallyTransposing(*producer)) {\n+    return \"fusing the producer would break read coalescing\";\n+  }\n+\n   if (NoFusionPossible fusible =\n           !IsProducerConsumerFusible(*producer, *consumer)) {\n     return !fusible;"
        }
    ]
},
{
    "Id": 281,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/422d09c0badb4bc56fb554a8a50e7e7346a2c972",
    "date": "2023-09-22T04:51:41-07:00",
    "message": "[XLA:GPU][NFC] Always profile the cuBLAS version of GEMMs in TritonAutotuner\n\nEven if no validation is requested.\nThis is a preparation for a later CL.\n\nPiperOrigin-RevId: 567587347",
    "label": "NO",
    "changes": [
        {
            "name": "triton_autotuner.cc",
            "path": "third_party/xla/xla/service/gpu/triton_autotuner.cc",
            "patches": [
                {
                    "old_start": 165,
                    "old_length": 7,
                    "new_start": 165,
                    "new_length": 7,
                    "hunk": "@@ -165,7 +165,7 @@ struct ExecutableCandidate {\n // This contains all alternative executables related to one fusion.\n struct ExecutableSet {\n   std::vector<ExecutableCandidate> candidates;\n-  // This is nullptr iff correctness check is disabled.\n+  // Not nullptr.\n   std::unique_ptr<Executable> reference;\n };\n \n"
                },
                {
                    "old_start": 397,
                    "old_length": 9,
                    "new_start": 397,
                    "new_length": 8,
                    "hunk": "@@ -397,9 +397,8 @@ CompileMany(const AutotuneConfig& config, AutotunerCompileUtil& util,\n     const GemmConfigSet& gemm_config_set = key_value.second;\n     config_count += gemm_config_set.configs.size();\n   }\n-  if (config.should_check_correctness()) {\n-    config_count += gemm_config_sets.size();\n-  }\n+  // The cuBLAS configs:\n+  config_count += gemm_config_sets.size();\n \n   std::atomic<int> done_count = 0;\n   std::atomic<int> good_count = 0;\n"
                },
                {
                    "old_start": 490,
                    "old_length": 14,
                    "new_start": 489,
                    "new_length": 12,
                    "hunk": "@@ -490,14 +489,12 @@ CompileMany(const AutotuneConfig& config, AutotunerCompileUtil& util,\n         });\n       }\n \n-      if (config.should_check_correctness()) {\n-        thread_pool->Schedule([&, fusion] {\n-          StatusOr<bool> has_executable = compile_reference_executable(fusion);\n-          TF_CHECK_OK(has_executable.status());\n-          log(has_executable.value());\n-          counter.DecrementCount();\n-        });\n-      }\n+      thread_pool->Schedule([&, fusion] {\n+        StatusOr<bool> has_executable = compile_reference_executable(fusion);\n+        TF_CHECK_OK(has_executable.status());\n+        log(has_executable.value());\n+        counter.DecrementCount();\n+      });\n     }\n     counter.Wait();\n   } else {\n"
                },
                {
                    "old_start": 521,
                    "old_length": 11,
                    "new_start": 518,
                    "new_length": 9,
                    "hunk": "@@ -521,11 +518,9 @@ CompileMany(const AutotuneConfig& config, AutotunerCompileUtil& util,\n         log(has_executable);\n       }\n \n-      if (config.should_check_correctness()) {\n-        TF_ASSIGN_OR_RETURN(bool has_executable,\n-                            compile_reference_executable(fusion));\n-        log(has_executable);\n-      }\n+      TF_ASSIGN_OR_RETURN(bool has_executable,\n+                          compile_reference_executable(fusion));\n+      log(has_executable);\n     }\n   }\n \n"
                },
                {
                    "old_start": 534,
                    "old_length": 9,
                    "new_start": 529,
                    "new_length": 9,
                    "hunk": "@@ -534,9 +529,9 @@ CompileMany(const AutotuneConfig& config, AutotunerCompileUtil& util,\n   return executable_sets;\n }\n \n-// Runs matmul fusion contents without Triton - with cuBLAS, to generate\n-// a reference output.\n-StatusOr<ScopedShapedBuffer> RunMatmulWithCublas(\n+// Runs matmul fusion contents without Triton - with cuBLAS, to measure time and\n+// generate a reference output.\n+StatusOr<ProfilingOutput> RunMatmulWithCublas(\n     AutotunerCompileUtil& util, se::Stream* stream, Executable& executable,\n     absl::Span<se::DeviceMemoryBase const> input_buffers,\n     absl::Span<Shape const> input_shapes) {\n"
                },
                {
                    "old_start": 544,
                    "old_length": 7,
                    "new_start": 539,
                    "new_length": 7,
                    "hunk": "@@ -544,7 +539,7 @@ StatusOr<ScopedShapedBuffer> RunMatmulWithCublas(\n       std::optional<ProfilingOutput> output,\n       util.ProfileExecutable(&executable, stream, input_buffers, input_shapes));\n   TF_RET_CHECK(output.has_value());\n-  return std::move(output->output);\n+  return std::move(output.value());\n }\n \n StatusOr<AutotuneResult> Execute(const AutotuneConfig& config,\n"
                },
                {
                    "old_start": 569,
                    "old_length": 7,
                    "new_start": 564,
                    "new_length": 6,
                    "hunk": "@@ -569,7 +564,6 @@ StatusOr<AutotuneResult> Execute(const AutotuneConfig& config,\n       se::RedzoneAllocator rz_allocator,\n       AutotunerUtil::CreateRedzoneAllocator(config, debug_opts));\n \n-  std::optional<ScopedShapedBuffer> reference_buffer;\n   const HloInstruction& root = *fusion_computation->root_instruction();\n   BufferComparator comparator(root.shape(),\n                               fusion_computation->parent()->config());\n"
                },
                {
                    "old_start": 588,
                    "old_length": 13,
                    "new_start": 582,
                    "new_length": 21,
                    "hunk": "@@ -588,13 +582,21 @@ StatusOr<AutotuneResult> Execute(const AutotuneConfig& config,\n     input_shapes.push_back(param->shape());\n   }\n \n-  if (config.should_check_correctness()) {\n+  // Run with cuBLAS.\n+  std::optional<ScopedShapedBuffer> reference_buffer;\n+  absl::Duration cublas_duration;\n+  {\n     TF_RET_CHECK(executable_set.reference != nullptr);\n     TF_ASSIGN_OR_RETURN(\n-        reference_buffer,\n+        ProfilingOutput output,\n         RunMatmulWithCublas(util, stream, *executable_set.reference, inputs,\n                             input_shapes));\n+    if (config.should_check_correctness()) {\n+      reference_buffer = std::move(output.output);\n+    }\n+    cublas_duration = output.duration;\n   }\n+  VLOG(3) << \"Running with cuBLAS took: \" << cublas_duration;\n \n   const int log_every_n = GetLogEveryN();\n   int64_t executable_count ="
                }
            ],
            "whole_deleted": "-  // This is nullptr iff correctness check is disabled.\n-  if (config.should_check_correctness()) {\n-    config_count += gemm_config_sets.size();\n-  }\n-      if (config.should_check_correctness()) {\n-        thread_pool->Schedule([&, fusion] {\n-          StatusOr<bool> has_executable = compile_reference_executable(fusion);\n-          TF_CHECK_OK(has_executable.status());\n-          log(has_executable.value());\n-          counter.DecrementCount();\n-        });\n-      }\n-      if (config.should_check_correctness()) {\n-        TF_ASSIGN_OR_RETURN(bool has_executable,\n-                            compile_reference_executable(fusion));\n-        log(has_executable);\n-      }\n-// Runs matmul fusion contents without Triton - with cuBLAS, to generate\n-// a reference output.\n-StatusOr<ScopedShapedBuffer> RunMatmulWithCublas(\n-  return std::move(output->output);\n-  std::optional<ScopedShapedBuffer> reference_buffer;\n-  if (config.should_check_correctness()) {\n-        reference_buffer,\n",
            "whole_added": "+  // Not nullptr.\n+  // The cuBLAS configs:\n+  config_count += gemm_config_sets.size();\n+      thread_pool->Schedule([&, fusion] {\n+        StatusOr<bool> has_executable = compile_reference_executable(fusion);\n+        TF_CHECK_OK(has_executable.status());\n+        log(has_executable.value());\n+        counter.DecrementCount();\n+      });\n+      TF_ASSIGN_OR_RETURN(bool has_executable,\n+                          compile_reference_executable(fusion));\n+      log(has_executable);\n+// Runs matmul fusion contents without Triton - with cuBLAS, to measure time and\n+// generate a reference output.\n+StatusOr<ProfilingOutput> RunMatmulWithCublas(\n+  return std::move(output.value());\n+  // Run with cuBLAS.\n+  std::optional<ScopedShapedBuffer> reference_buffer;\n+  absl::Duration cublas_duration;\n+  {\n+        ProfilingOutput output,\n+    if (config.should_check_correctness()) {\n+      reference_buffer = std::move(output.output);\n+    }\n+    cublas_duration = output.duration;\n+  VLOG(3) << \"Running with cuBLAS took: \" << cublas_duration;\n",
            "whole_hunk": "@@ -165,7 +165,7 @@ struct ExecutableCandidate {\n // This contains all alternative executables related to one fusion.\n struct ExecutableSet {\n   std::vector<ExecutableCandidate> candidates;\n-  // This is nullptr iff correctness check is disabled.\n+  // Not nullptr.\n   std::unique_ptr<Executable> reference;\n };\n \n@@ -397,9 +397,8 @@ CompileMany(const AutotuneConfig& config, AutotunerCompileUtil& util,\n     const GemmConfigSet& gemm_config_set = key_value.second;\n     config_count += gemm_config_set.configs.size();\n   }\n-  if (config.should_check_correctness()) {\n-    config_count += gemm_config_sets.size();\n-  }\n+  // The cuBLAS configs:\n+  config_count += gemm_config_sets.size();\n \n   std::atomic<int> done_count = 0;\n   std::atomic<int> good_count = 0;\n@@ -490,14 +489,12 @@ CompileMany(const AutotuneConfig& config, AutotunerCompileUtil& util,\n         });\n       }\n \n-      if (config.should_check_correctness()) {\n-        thread_pool->Schedule([&, fusion] {\n-          StatusOr<bool> has_executable = compile_reference_executable(fusion);\n-          TF_CHECK_OK(has_executable.status());\n-          log(has_executable.value());\n-          counter.DecrementCount();\n-        });\n-      }\n+      thread_pool->Schedule([&, fusion] {\n+        StatusOr<bool> has_executable = compile_reference_executable(fusion);\n+        TF_CHECK_OK(has_executable.status());\n+        log(has_executable.value());\n+        counter.DecrementCount();\n+      });\n     }\n     counter.Wait();\n   } else {\n@@ -521,11 +518,9 @@ CompileMany(const AutotuneConfig& config, AutotunerCompileUtil& util,\n         log(has_executable);\n       }\n \n-      if (config.should_check_correctness()) {\n-        TF_ASSIGN_OR_RETURN(bool has_executable,\n-                            compile_reference_executable(fusion));\n-        log(has_executable);\n-      }\n+      TF_ASSIGN_OR_RETURN(bool has_executable,\n+                          compile_reference_executable(fusion));\n+      log(has_executable);\n     }\n   }\n \n@@ -534,9 +529,9 @@ CompileMany(const AutotuneConfig& config, AutotunerCompileUtil& util,\n   return executable_sets;\n }\n \n-// Runs matmul fusion contents without Triton - with cuBLAS, to generate\n-// a reference output.\n-StatusOr<ScopedShapedBuffer> RunMatmulWithCublas(\n+// Runs matmul fusion contents without Triton - with cuBLAS, to measure time and\n+// generate a reference output.\n+StatusOr<ProfilingOutput> RunMatmulWithCublas(\n     AutotunerCompileUtil& util, se::Stream* stream, Executable& executable,\n     absl::Span<se::DeviceMemoryBase const> input_buffers,\n     absl::Span<Shape const> input_shapes) {\n@@ -544,7 +539,7 @@ StatusOr<ScopedShapedBuffer> RunMatmulWithCublas(\n       std::optional<ProfilingOutput> output,\n       util.ProfileExecutable(&executable, stream, input_buffers, input_shapes));\n   TF_RET_CHECK(output.has_value());\n-  return std::move(output->output);\n+  return std::move(output.value());\n }\n \n StatusOr<AutotuneResult> Execute(const AutotuneConfig& config,\n@@ -569,7 +564,6 @@ StatusOr<AutotuneResult> Execute(const AutotuneConfig& config,\n       se::RedzoneAllocator rz_allocator,\n       AutotunerUtil::CreateRedzoneAllocator(config, debug_opts));\n \n-  std::optional<ScopedShapedBuffer> reference_buffer;\n   const HloInstruction& root = *fusion_computation->root_instruction();\n   BufferComparator comparator(root.shape(),\n                               fusion_computation->parent()->config());\n@@ -588,13 +582,21 @@ StatusOr<AutotuneResult> Execute(const AutotuneConfig& config,\n     input_shapes.push_back(param->shape());\n   }\n \n-  if (config.should_check_correctness()) {\n+  // Run with cuBLAS.\n+  std::optional<ScopedShapedBuffer> reference_buffer;\n+  absl::Duration cublas_duration;\n+  {\n     TF_RET_CHECK(executable_set.reference != nullptr);\n     TF_ASSIGN_OR_RETURN(\n-        reference_buffer,\n+        ProfilingOutput output,\n         RunMatmulWithCublas(util, stream, *executable_set.reference, inputs,\n                             input_shapes));\n+    if (config.should_check_correctness()) {\n+      reference_buffer = std::move(output.output);\n+    }\n+    cublas_duration = output.duration;\n   }\n+  VLOG(3) << \"Running with cuBLAS took: \" << cublas_duration;\n \n   const int log_every_n = GetLogEveryN();\n   int64_t executable_count ="
        }
    ]
},
{
    "Id": 41,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/30b8166b0ed8e61260e961e3cd60a61678544316",
    "date": "2024-05-22T03:19:45-07:00",
    "message": "Fix bug in FusionCanShareBufferHint() for transpose multi-output fusions.\n\nWe did not correctly check whether a fusion root is accessed in two different\niteration orders. If there is another root that has a transpose hero, the\niteration orders will be different.\n\nPiperOrigin-RevId: 636096047",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 4549,
                    "old_length": 6,
                    "new_start": 4549,
                    "new_length": 7,
                    "hunk": "@@ -4549,6 +4549,7 @@ cc_library(\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@llvm-project//llvm:Support\",\n     ],\n )\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"@llvm-project//llvm:Support\",\n",
            "whole_hunk": "@@ -4549,6 +4549,7 @@ cc_library(\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/log:check\",\n+        \"@llvm-project//llvm:Support\",\n     ],\n )\n \n"
        },
        {
            "name": "buffer_sharing.cc",
            "path": "third_party/xla/xla/service/gpu/buffer_sharing.cc",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 6,
                    "new_start": 23,
                    "new_length": 7,
                    "hunk": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n"
                },
                {
                    "old_start": 182,
                    "old_length": 15,
                    "new_start": 183,
                    "new_length": 35,
                    "hunk": "@@ -182,15 +183,35 @@ std::optional<bool> FusionCanShareBufferHint(const HloInstruction* user,\n       }\n     }\n   }\n-  // Special case: multi-output fusions with Scatter or DynamicUpdateSlice. For\n-  // Scatter, we currently do not support multi-output fusions anyway, but still\n-  // handle it here. To be on the safe side, check for !IsElementwise() instead\n-  // of checking whether it is Scatter or DynamicUpdateSlice.\n-  if (user->IsMultiOutputFusion() && !non_bitcast_root->IsElementwise()) {\n-    // Check if any other fusion output was reached. If yes, we cannot share,\n-    // because the order in which the output is written might be different.\n-    for (HloInstruction* operand : user->fused_expression_root()->operands()) {\n-      if (operand != output && visited.find(operand) != visited.end()) {\n+  if (user->IsMultiOutputFusion()) {\n+    // Check if any other fusion output was reached. If yes, we need to check\n+    // whether that fusion output has the same iteration order. If not, we\n+    // cannot share.\n+    bool other_root_was_reached = false;\n+    bool reached_root_has_transpose_hero = false;\n+    for (auto [root, hero] :\n+         llvm::zip(analysis.fusion_roots(), analysis.fusion_heroes())) {\n+      if (visited.find(&root.instruction()) != visited.end()) {\n+        if (&root.instruction() != output) {\n+          other_root_was_reached = true;\n+        }\n+        if (hero.opcode() == HloOpcode::kTranspose ||\n+            hero.opcode() == HloOpcode::kCopy) {\n+          reached_root_has_transpose_hero = true;\n+        }\n+      }\n+    }\n+    if (other_root_was_reached) {\n+      // If a root was reached that has a transpose hero, that root will use the\n+      // iteration order of the transpose operand. The other root will have a\n+      // different iteration order, so we cannot share the buffer.\n+      // Special case: multi-output fusions with Scatter or DynamicUpdateSlice.\n+      // For Scatter, we currently do not support multi-output fusions anyway,\n+      // but still handle it here. To be on the safe side, check for\n+      // !IsElementwise() instead of checking whether it is Scatter or\n+      // DynamicUpdateSlice.\n+      if (!non_bitcast_root->IsElementwise() ||\n+          reached_root_has_transpose_hero) {\n         return false;\n       }\n     }\n"
                }
            ],
            "whole_deleted": "-  // Special case: multi-output fusions with Scatter or DynamicUpdateSlice. For\n-  // Scatter, we currently do not support multi-output fusions anyway, but still\n-  // handle it here. To be on the safe side, check for !IsElementwise() instead\n-  // of checking whether it is Scatter or DynamicUpdateSlice.\n-  if (user->IsMultiOutputFusion() && !non_bitcast_root->IsElementwise()) {\n-    // Check if any other fusion output was reached. If yes, we cannot share,\n-    // because the order in which the output is written might be different.\n-    for (HloInstruction* operand : user->fused_expression_root()->operands()) {\n-      if (operand != output && visited.find(operand) != visited.end()) {\n",
            "whole_added": "+#include \"llvm/ADT/STLExtras.h\"\n+  if (user->IsMultiOutputFusion()) {\n+    // Check if any other fusion output was reached. If yes, we need to check\n+    // whether that fusion output has the same iteration order. If not, we\n+    // cannot share.\n+    bool other_root_was_reached = false;\n+    bool reached_root_has_transpose_hero = false;\n+    for (auto [root, hero] :\n+         llvm::zip(analysis.fusion_roots(), analysis.fusion_heroes())) {\n+      if (visited.find(&root.instruction()) != visited.end()) {\n+        if (&root.instruction() != output) {\n+          other_root_was_reached = true;\n+        }\n+        if (hero.opcode() == HloOpcode::kTranspose ||\n+            hero.opcode() == HloOpcode::kCopy) {\n+          reached_root_has_transpose_hero = true;\n+        }\n+      }\n+    }\n+    if (other_root_was_reached) {\n+      // If a root was reached that has a transpose hero, that root will use the\n+      // iteration order of the transpose operand. The other root will have a\n+      // different iteration order, so we cannot share the buffer.\n+      // Special case: multi-output fusions with Scatter or DynamicUpdateSlice.\n+      // For Scatter, we currently do not support multi-output fusions anyway,\n+      // but still handle it here. To be on the safe side, check for\n+      // !IsElementwise() instead of checking whether it is Scatter or\n+      // DynamicUpdateSlice.\n+      if (!non_bitcast_root->IsElementwise() ||\n+          reached_root_has_transpose_hero) {\n",
            "whole_hunk": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n #include \"xla/hlo/ir/hlo_casting_utils.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n@@ -182,15 +183,35 @@ std::optional<bool> FusionCanShareBufferHint(const HloInstruction* user,\n       }\n     }\n   }\n-  // Special case: multi-output fusions with Scatter or DynamicUpdateSlice. For\n-  // Scatter, we currently do not support multi-output fusions anyway, but still\n-  // handle it here. To be on the safe side, check for !IsElementwise() instead\n-  // of checking whether it is Scatter or DynamicUpdateSlice.\n-  if (user->IsMultiOutputFusion() && !non_bitcast_root->IsElementwise()) {\n-    // Check if any other fusion output was reached. If yes, we cannot share,\n-    // because the order in which the output is written might be different.\n-    for (HloInstruction* operand : user->fused_expression_root()->operands()) {\n-      if (operand != output && visited.find(operand) != visited.end()) {\n+  if (user->IsMultiOutputFusion()) {\n+    // Check if any other fusion output was reached. If yes, we need to check\n+    // whether that fusion output has the same iteration order. If not, we\n+    // cannot share.\n+    bool other_root_was_reached = false;\n+    bool reached_root_has_transpose_hero = false;\n+    for (auto [root, hero] :\n+         llvm::zip(analysis.fusion_roots(), analysis.fusion_heroes())) {\n+      if (visited.find(&root.instruction()) != visited.end()) {\n+        if (&root.instruction() != output) {\n+          other_root_was_reached = true;\n+        }\n+        if (hero.opcode() == HloOpcode::kTranspose ||\n+            hero.opcode() == HloOpcode::kCopy) {\n+          reached_root_has_transpose_hero = true;\n+        }\n+      }\n+    }\n+    if (other_root_was_reached) {\n+      // If a root was reached that has a transpose hero, that root will use the\n+      // iteration order of the transpose operand. The other root will have a\n+      // different iteration order, so we cannot share the buffer.\n+      // Special case: multi-output fusions with Scatter or DynamicUpdateSlice.\n+      // For Scatter, we currently do not support multi-output fusions anyway,\n+      // but still handle it here. To be on the safe side, check for\n+      // !IsElementwise() instead of checking whether it is Scatter or\n+      // DynamicUpdateSlice.\n+      if (!non_bitcast_root->IsElementwise() ||\n+          reached_root_has_transpose_hero) {\n         return false;\n       }\n     }\n"
        },
        {
            "name": "gpu_copy_insertion_test.cc",
            "path": "third_party/xla/xla/service/gpu/gpu_copy_insertion_test.cc",
            "patches": [
                {
                    "old_start": 726,
                    "old_length": 6,
                    "new_start": 726,
                    "new_length": 42,
                    "hunk": "@@ -726,6 +726,42 @@ ENTRY main {\n   ExpectOptionalTrue(FusionCanShareBufferHint(fusion, fusion->operand(0), {}));\n }\n \n+TEST_F(FusionCanShareBufferHintTest,\n+       BufferCannotBeSharedWhenOtherUserIsTransposeUser) {\n+  const char* const kModuleString = R\"(\n+HloModule fusion\n+\n+fused_computation {\n+  p0 = f32[100,110,120]{2,1,0} parameter(0)\n+  p1 = f32[120,110,100]{2,1,0} parameter(1)\n+  zero = f32[] constant(0.0)\n+  broadcast = f32[120,110,100]{2,1,0} broadcast(zero), dimensions={}\n+  maximum = f32[120,110,100]{2,1,0} maximum(broadcast, p1)\n+  t = f32[120,110,100]{2,1,0} transpose(p0), dimensions={2,1,0}\n+  add = f32[120,110,100]{2,1,0} add(t, maximum)\n+  ROOT res = (f32[120,110,100]{2,1,0}, f32[120,110,100]{2,1,0}) tuple(add, maximum)\n+}\n+\n+ENTRY main {\n+  param_0 = f32[100,110,120]{2,1,0} parameter(0)\n+  param_1 = f32[120,110,100]{2,1,0} parameter(1)\n+  ROOT fusion = (f32[120,110,100]{2,1,0}, f32[120,110,100]{2,1,0}) fusion(param_0, param_1), kind=kInput, calls=fused_computation\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<xla::HloModule> module,\n+                          ParseAndReturnVerifiedModule(kModuleString));\n+  HloInstruction* fusion = module->entry_computation()->root_instruction();\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(0), {0}));\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(0), {1}));\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(1), {0}));\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(1), {1}));\n+}\n+\n TEST_F(FusionCanShareBufferHintTest,\n        BufferCannotBeSharedDynamicUpdateSliceAndOtherUser) {\n   // This is a fusion that we would normally not create because it cannot be"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(FusionCanShareBufferHintTest,\n+       BufferCannotBeSharedWhenOtherUserIsTransposeUser) {\n+  const char* const kModuleString = R\"(\n+HloModule fusion\n+\n+fused_computation {\n+  p0 = f32[100,110,120]{2,1,0} parameter(0)\n+  p1 = f32[120,110,100]{2,1,0} parameter(1)\n+  zero = f32[] constant(0.0)\n+  broadcast = f32[120,110,100]{2,1,0} broadcast(zero), dimensions={}\n+  maximum = f32[120,110,100]{2,1,0} maximum(broadcast, p1)\n+  t = f32[120,110,100]{2,1,0} transpose(p0), dimensions={2,1,0}\n+  add = f32[120,110,100]{2,1,0} add(t, maximum)\n+  ROOT res = (f32[120,110,100]{2,1,0}, f32[120,110,100]{2,1,0}) tuple(add, maximum)\n+}\n+\n+ENTRY main {\n+  param_0 = f32[100,110,120]{2,1,0} parameter(0)\n+  param_1 = f32[120,110,100]{2,1,0} parameter(1)\n+  ROOT fusion = (f32[120,110,100]{2,1,0}, f32[120,110,100]{2,1,0}) fusion(param_0, param_1), kind=kInput, calls=fused_computation\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<xla::HloModule> module,\n+                          ParseAndReturnVerifiedModule(kModuleString));\n+  HloInstruction* fusion = module->entry_computation()->root_instruction();\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(0), {0}));\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(0), {1}));\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(1), {0}));\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(1), {1}));\n+}\n+\n",
            "whole_hunk": "@@ -726,6 +726,42 @@ ENTRY main {\n   ExpectOptionalTrue(FusionCanShareBufferHint(fusion, fusion->operand(0), {}));\n }\n \n+TEST_F(FusionCanShareBufferHintTest,\n+       BufferCannotBeSharedWhenOtherUserIsTransposeUser) {\n+  const char* const kModuleString = R\"(\n+HloModule fusion\n+\n+fused_computation {\n+  p0 = f32[100,110,120]{2,1,0} parameter(0)\n+  p1 = f32[120,110,100]{2,1,0} parameter(1)\n+  zero = f32[] constant(0.0)\n+  broadcast = f32[120,110,100]{2,1,0} broadcast(zero), dimensions={}\n+  maximum = f32[120,110,100]{2,1,0} maximum(broadcast, p1)\n+  t = f32[120,110,100]{2,1,0} transpose(p0), dimensions={2,1,0}\n+  add = f32[120,110,100]{2,1,0} add(t, maximum)\n+  ROOT res = (f32[120,110,100]{2,1,0}, f32[120,110,100]{2,1,0}) tuple(add, maximum)\n+}\n+\n+ENTRY main {\n+  param_0 = f32[100,110,120]{2,1,0} parameter(0)\n+  param_1 = f32[120,110,100]{2,1,0} parameter(1)\n+  ROOT fusion = (f32[120,110,100]{2,1,0}, f32[120,110,100]{2,1,0}) fusion(param_0, param_1), kind=kInput, calls=fused_computation\n+}\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<xla::HloModule> module,\n+                          ParseAndReturnVerifiedModule(kModuleString));\n+  HloInstruction* fusion = module->entry_computation()->root_instruction();\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(0), {0}));\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(0), {1}));\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(1), {0}));\n+  ExpectOptionalFalse(\n+      FusionCanShareBufferHint(fusion, fusion->operand(1), {1}));\n+}\n+\n TEST_F(FusionCanShareBufferHintTest,\n        BufferCannotBeSharedDynamicUpdateSliceAndOtherUser) {\n   // This is a fusion that we would normally not create because it cannot be"
        }
    ]
},
{
    "Id": 108,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/98830e91ac878d1ee15b7539dc72bb06adb2fc2f",
    "date": "2024-03-18T15:10:23-07:00",
    "message": "[xla:hlo] Use llvm::BitVector instead of a set when checking reachability\n\nname                               old cpu/op   new cpu/op   delta\nBM_HloDfsReachabilityBuild/1        109ns \u00b1 4%   111ns \u00b1 4%     ~\nBM_HloDfsReachabilityBuild/64      1.71\u00b5s \u00b1 6%  1.71\u00b5s \u00b1 4%     ~\nBM_HloDfsReachabilityBuild/128     3.38\u00b5s \u00b1 3%  3.43\u00b5s \u00b1 3%   +1.54%\nBM_HloDfsReachabilityBuild/256     6.80\u00b5s \u00b1 4%  6.95\u00b5s \u00b1 5%   +2.25%\nBM_HloDfsReachabilityBuild/512     13.8\u00b5s \u00b1 4%  14.2\u00b5s \u00b1 6%   +2.63%\nBM_HloDfsReachabilityBuild/4096     155\u00b5s \u00b1 4%   157\u00b5s \u00b1 4%     ~\nBM_HloDfsReachabilityBuild/32768   1.42ms \u00b1 5%  1.45ms \u00b1 3%   +1.94%\nBM_HloDfsReachabilityBuild/262144  32.2ms \u00b1 4%  32.1ms \u00b1 4%     ~\nBM_HloDfsReachabilityCheck/1       7.37ns \u00b1 3%  7.41ns \u00b1 4%     ~\nBM_HloDfsReachabilityCheck/64       295ns \u00b1 5%   139ns \u00b1 8%  -52.78%\nBM_HloDfsReachabilityCheck/128      679ns \u00b1 3%   278ns \u00b1 7%  -59.05%\nBM_HloDfsReachabilityCheck/256     1.53\u00b5s \u00b1 5%  0.61\u00b5s \u00b1 6%  -60.06%\nBM_HloDfsReachabilityCheck/512     3.06\u00b5s \u00b1 5%  1.31\u00b5s \u00b1 6%  -57.27%\nBM_HloDfsReachabilityCheck/4096    30.2\u00b5s \u00b1 7%  17.9\u00b5s \u00b1 4%  -40.53%\nBM_HloDfsReachabilityCheck/32768    532\u00b5s \u00b1 4%   327\u00b5s \u00b1 5%  -38.52%\nBM_HloDfsReachabilityCheck/262144  8.72ms \u00b1 3%  6.66ms \u00b1 4%  -23.59%\n\nPiperOrigin-RevId: 616956892",
    "label": "NO",
    "changes": [
        {
            "name": "hlo_dfs_reachability.cc",
            "path": "third_party/xla/xla/hlo/ir/hlo_dfs_reachability.cc",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 7,
                    "new_start": 20,
                    "new_length": 7,
                    "hunk": "@@ -20,7 +20,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n-#include \"llvm/ADT/DenseSet.h\"\n+#include \"llvm/ADT/BitVector.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n"
                },
                {
                    "old_start": 45,
                    "old_length": 16,
                    "new_start": 45,
                    "new_length": 20,
                    "hunk": "@@ -45,16 +45,20 @@ bool HloDfsReachability::IsReachable(const HloInstruction* from,\n \n   // Note that the DFS goes from the \"uses\" root towards the \"defs\", i.e. from\n   // `to` node to `from` node, so the node indices are decreasing.\n-  if (target_node_idx > dfs_root_idx) {\n+  if (dfs_root_idx < target_node_idx) {\n     return false;\n   }\n \n-  // We use LLVM support library here because it has stack-allocated maps (in\n-  // contrast to absl) which significantly improves performance by avoiding heap\n-  // allocations when instructions are reachable via a short chain.\n-  llvm::SmallDenseSet<size_t, 8> visited_idxs{dfs_root_idx};\n+  // We use LLVM support library here because it has stack-allocated bit vector\n+  // which significantly improves performance by avoiding heap allocations when\n+  // instructions are reachable via a short chain.\n   llvm::SmallVector<const HloInstruction*> stack{to};\n \n+  // We will visit instructions in the [target_node_idx, dfs_root_idx] range, so\n+  // we can construct a smaller bit vector.\n+  llvm::BitVector visited_idxs(1 + (dfs_root_idx - target_node_idx));\n+  visited_idxs.set(dfs_root_idx - target_node_idx);\n+\n   auto check_and_enqueue = [&](const HloInstruction* instr) {\n     if (instr == from) {\n       return true;\n"
                },
                {
                    "old_start": 63,
                    "old_length": 9,
                    "new_start": 67,
                    "new_length": 11,
                    "hunk": "@@ -63,9 +67,11 @@ bool HloDfsReachability::IsReachable(const HloInstruction* from,\n     if (instr_idx < target_node_idx) {\n       return false;\n     }\n-    if (auto [_, inserted] = visited_idxs.insert(instr_idx); !inserted) {\n+    size_t visited_idx = instr_idx - target_node_idx;\n+    if (visited_idxs.test(visited_idx)) {\n       return false;\n     }\n+    visited_idxs.set(visited_idx);\n     stack.push_back(instr);\n     return false;\n   };"
                }
            ],
            "whole_deleted": "-#include \"llvm/ADT/DenseSet.h\"\n-  if (target_node_idx > dfs_root_idx) {\n-  // We use LLVM support library here because it has stack-allocated maps (in\n-  // contrast to absl) which significantly improves performance by avoiding heap\n-  // allocations when instructions are reachable via a short chain.\n-  llvm::SmallDenseSet<size_t, 8> visited_idxs{dfs_root_idx};\n-    if (auto [_, inserted] = visited_idxs.insert(instr_idx); !inserted) {\n",
            "whole_added": "+#include \"llvm/ADT/BitVector.h\"\n+  if (dfs_root_idx < target_node_idx) {\n+  // We use LLVM support library here because it has stack-allocated bit vector\n+  // which significantly improves performance by avoiding heap allocations when\n+  // instructions are reachable via a short chain.\n+  // We will visit instructions in the [target_node_idx, dfs_root_idx] range, so\n+  // we can construct a smaller bit vector.\n+  llvm::BitVector visited_idxs(1 + (dfs_root_idx - target_node_idx));\n+  visited_idxs.set(dfs_root_idx - target_node_idx);\n+\n+    size_t visited_idx = instr_idx - target_node_idx;\n+    if (visited_idxs.test(visited_idx)) {\n+    visited_idxs.set(visited_idx);\n",
            "whole_hunk": "@@ -20,7 +20,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n-#include \"llvm/ADT/DenseSet.h\"\n+#include \"llvm/ADT/BitVector.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n@@ -45,16 +45,20 @@ bool HloDfsReachability::IsReachable(const HloInstruction* from,\n \n   // Note that the DFS goes from the \"uses\" root towards the \"defs\", i.e. from\n   // `to` node to `from` node, so the node indices are decreasing.\n-  if (target_node_idx > dfs_root_idx) {\n+  if (dfs_root_idx < target_node_idx) {\n     return false;\n   }\n \n-  // We use LLVM support library here because it has stack-allocated maps (in\n-  // contrast to absl) which significantly improves performance by avoiding heap\n-  // allocations when instructions are reachable via a short chain.\n-  llvm::SmallDenseSet<size_t, 8> visited_idxs{dfs_root_idx};\n+  // We use LLVM support library here because it has stack-allocated bit vector\n+  // which significantly improves performance by avoiding heap allocations when\n+  // instructions are reachable via a short chain.\n   llvm::SmallVector<const HloInstruction*> stack{to};\n \n+  // We will visit instructions in the [target_node_idx, dfs_root_idx] range, so\n+  // we can construct a smaller bit vector.\n+  llvm::BitVector visited_idxs(1 + (dfs_root_idx - target_node_idx));\n+  visited_idxs.set(dfs_root_idx - target_node_idx);\n+\n   auto check_and_enqueue = [&](const HloInstruction* instr) {\n     if (instr == from) {\n       return true;\n@@ -63,9 +67,11 @@ bool HloDfsReachability::IsReachable(const HloInstruction* from,\n     if (instr_idx < target_node_idx) {\n       return false;\n     }\n-    if (auto [_, inserted] = visited_idxs.insert(instr_idx); !inserted) {\n+    size_t visited_idx = instr_idx - target_node_idx;\n+    if (visited_idxs.test(visited_idx)) {\n       return false;\n     }\n+    visited_idxs.set(visited_idx);\n     stack.push_back(instr);\n     return false;\n   };"
        }
    ]
},
{
    "Id": 431,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/924f80a4fdb34230965a7a8a4476901847463645",
    "date": "2023-05-08T16:20:25-04:00",
    "message": "Add stricter type checking for tf.math.real\n\nFix for tf.math.real so that it only accepts tensors with numeric entries as input. This makes it consistent with its documentation at https://www.tensorflow.org/api_docs/python/tf/math/real and raises a TypeError saying input must have numeric entries when called incorrectly.",
    "label": "NO",
    "changes": [
        {
            "name": "math_ops.py",
            "path": "tensorflow/python/ops/math_ops.py",
            "patches": [
                {
                    "old_start": 822,
                    "old_length": 8,
                    "new_start": 822,
                    "new_length": 10,
                    "hunk": "@@ -822,8 +822,10 @@ def real(input, name=None):\n     if input.dtype.is_complex:\n       real_dtype = input.dtype.real_dtype\n       return gen_math_ops.real(input, Tout=real_dtype, name=name)\n-    else:\n+    elif tf.debugging.is_numeric_tensor(input):\n       return input\n+    else:\n+      raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))\n \n \n @tf_export(\"math.imag\", v1=[\"math.imag\", \"imag\"])"
                }
            ],
            "whole_deleted": "-    else:\n",
            "whole_added": "+    elif tf.debugging.is_numeric_tensor(input):\n+    else:\n+      raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))\n",
            "whole_hunk": "@@ -822,8 +822,10 @@ def real(input, name=None):\n     if input.dtype.is_complex:\n       real_dtype = input.dtype.real_dtype\n       return gen_math_ops.real(input, Tout=real_dtype, name=name)\n-    else:\n+    elif tf.debugging.is_numeric_tensor(input):\n       return input\n+    else:\n+      raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))\n \n \n @tf_export(\"math.imag\", v1=[\"math.imag\", \"imag\"])"
        }
    ]
},
{
    "Id": 56,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/89f97d1aa691a1042705134ddf70a51d8a86e591",
    "date": "2024-05-08T04:56:06-07:00",
    "message": "Move fuel consumption point to an earlier stage in the cudnn fused convolution rewriter\n\nThe call to `ConsumeFuel` happened after `EnsureIfConvBiasActivation` had already rewritten\nthe convolution instruction. In order to effectively use `ConsumeFuel` for model\nbisection I'm moving the check to before any changes are made to the instruction.\n\nPiperOrigin-RevId: 631755068",
    "label": "NO",
    "changes": [
        {
            "name": "cudnn_fused_conv_rewriter.cc",
            "path": "third_party/xla/xla/service/gpu/cudnn_fused_conv_rewriter.cc",
            "patches": [
                {
                    "old_start": 782,
                    "old_length": 6,
                    "new_start": 782,
                    "new_length": 12,
                    "hunk": "@@ -782,6 +782,12 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {\n       continue;\n     }\n \n+    if (!ConsumeFuel(\"cudnn-fused-convolution-rewriter\", [&] {\n+          return absl::StrCat(\"FuseBiasOrSideInput: \", conv->ToString());\n+        })) {\n+      continue;\n+    }\n+\n     // If it's a vanilla forward conv, upgrade it to a bias-activation conv.  We\n     // only want to do this if the fusion will succeed, but we're guaranteed\n     // that it will, because the only reason we'll bail at this point is if\n"
                },
                {
                    "old_start": 848,
                    "old_length": 12,
                    "new_start": 854,
                    "new_length": 6,
                    "hunk": "@@ -848,12 +854,6 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {\n       continue;\n     }\n \n-    if (!ConsumeFuel(\"cudnn-fused-convolution-rewriter\", [&] {\n-          return absl::StrCat(\"FuseBiasOrSideInput: \", conv->ToString());\n-        })) {\n-      continue;\n-    }\n-\n     HloInstruction* new_conv = comp->AddInstruction(\n         conv->CloneWithNewOperands(conv->shape(), new_operands));\n     comp->parent()->SetAndUniquifyInstrName(new_conv, conv->name());"
                }
            ],
            "whole_deleted": "-    if (!ConsumeFuel(\"cudnn-fused-convolution-rewriter\", [&] {\n-          return absl::StrCat(\"FuseBiasOrSideInput: \", conv->ToString());\n-        })) {\n-      continue;\n-    }\n-\n",
            "whole_added": "+    if (!ConsumeFuel(\"cudnn-fused-convolution-rewriter\", [&] {\n+          return absl::StrCat(\"FuseBiasOrSideInput: \", conv->ToString());\n+        })) {\n+      continue;\n+    }\n+\n",
            "whole_hunk": "@@ -782,6 +782,12 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {\n       continue;\n     }\n \n+    if (!ConsumeFuel(\"cudnn-fused-convolution-rewriter\", [&] {\n+          return absl::StrCat(\"FuseBiasOrSideInput: \", conv->ToString());\n+        })) {\n+      continue;\n+    }\n+\n     // If it's a vanilla forward conv, upgrade it to a bias-activation conv.  We\n     // only want to do this if the fusion will succeed, but we're guaranteed\n     // that it will, because the only reason we'll bail at this point is if\n@@ -848,12 +854,6 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {\n       continue;\n     }\n \n-    if (!ConsumeFuel(\"cudnn-fused-convolution-rewriter\", [&] {\n-          return absl::StrCat(\"FuseBiasOrSideInput: \", conv->ToString());\n-        })) {\n-      continue;\n-    }\n-\n     HloInstruction* new_conv = comp->AddInstruction(\n         conv->CloneWithNewOperands(conv->shape(), new_operands));\n     comp->parent()->SetAndUniquifyInstrName(new_conv, conv->name());"
        }
    ]
},
{
    "Id": 306,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/5322fd40cd58cfa8c551e602fede7a3be19fff95",
    "date": "2023-08-24T16:12:36-07:00",
    "message": "[PJRT] Fix checking for output sharding\n\nOutput sharding for empty tuple needs to have one \"replicated\" element.\n\nPiperOrigin-RevId: 559899447",
    "label": "YES",
    "changes": [
        {
            "name": "pjrt_executable.cc",
            "path": "tensorflow/compiler/xla/python/pjrt_ifrt/pjrt_executable.cc",
            "patches": [
                {
                    "old_start": 286,
                    "old_length": 6,
                    "new_start": 286,
                    "new_length": 18,
                    "hunk": "@@ -286,6 +286,18 @@ PjRtLoadedExecutable::CreateInternal(\n     output_shapes.push_back(Shape({}));\n     output_shardings.push_back(OpaqueSharding::Create(devices, MemoryKind()));\n   };\n+  auto check_tuple_output_sharding_condition =\n+      [](const xla::Shape& shape, const xla::HloSharding& sharding) {\n+        // Check that the HLO sharding of the result is a tuple and that it has\n+        // the same number of elements as the output tuple shape. If the output\n+        // is an empty tuple then the output sharding will have a single element\n+        // for the tuple as a special case, so we will have to allow that by\n+        // checking this condition specifically.\n+        return sharding.IsTuple() && (shape.tuple_shapes().size() ==\n+                                          sharding.tuple_elements().size() ||\n+                                      (shape.tuple_shapes().empty() &&\n+                                       sharding.tuple_elements().size() == 1));\n+      };\n \n   if (result_shape.IsArray()) {\n     output_dtypes.reserve(1);\n"
                },
                {
                    "old_start": 313,
                    "old_length": 9,
                    "new_start": 325,
                    "new_length": 8,
                    "hunk": "@@ -313,9 +325,8 @@ PjRtLoadedExecutable::CreateInternal(\n     output_shapes.reserve(result_shape.tuple_shapes().size());\n     output_shardings.reserve(result_shape.tuple_shapes().size());\n     if (result_hlo_sharding.has_value() &&\n-        (!result_hlo_sharding->IsTuple() ||\n-         result_hlo_sharding->tuple_elements().size() !=\n-             result_shape.tuple_shapes().size())) {\n+        !check_tuple_output_sharding_condition(result_shape,\n+                                               *result_hlo_sharding)) {\n       return FailedPrecondition(\n           \"Output sharding is inconsistent with the tuple result\");\n     }"
                }
            ],
            "whole_deleted": "-        (!result_hlo_sharding->IsTuple() ||\n-         result_hlo_sharding->tuple_elements().size() !=\n-             result_shape.tuple_shapes().size())) {\n",
            "whole_added": "+  auto check_tuple_output_sharding_condition =\n+      [](const xla::Shape& shape, const xla::HloSharding& sharding) {\n+        // Check that the HLO sharding of the result is a tuple and that it has\n+        // the same number of elements as the output tuple shape. If the output\n+        // is an empty tuple then the output sharding will have a single element\n+        // for the tuple as a special case, so we will have to allow that by\n+        // checking this condition specifically.\n+        return sharding.IsTuple() && (shape.tuple_shapes().size() ==\n+                                          sharding.tuple_elements().size() ||\n+                                      (shape.tuple_shapes().empty() &&\n+                                       sharding.tuple_elements().size() == 1));\n+      };\n+        !check_tuple_output_sharding_condition(result_shape,\n+                                               *result_hlo_sharding)) {\n",
            "whole_hunk": "@@ -286,6 +286,18 @@ PjRtLoadedExecutable::CreateInternal(\n     output_shapes.push_back(Shape({}));\n     output_shardings.push_back(OpaqueSharding::Create(devices, MemoryKind()));\n   };\n+  auto check_tuple_output_sharding_condition =\n+      [](const xla::Shape& shape, const xla::HloSharding& sharding) {\n+        // Check that the HLO sharding of the result is a tuple and that it has\n+        // the same number of elements as the output tuple shape. If the output\n+        // is an empty tuple then the output sharding will have a single element\n+        // for the tuple as a special case, so we will have to allow that by\n+        // checking this condition specifically.\n+        return sharding.IsTuple() && (shape.tuple_shapes().size() ==\n+                                          sharding.tuple_elements().size() ||\n+                                      (shape.tuple_shapes().empty() &&\n+                                       sharding.tuple_elements().size() == 1));\n+      };\n \n   if (result_shape.IsArray()) {\n     output_dtypes.reserve(1);\n@@ -313,9 +325,8 @@ PjRtLoadedExecutable::CreateInternal(\n     output_shapes.reserve(result_shape.tuple_shapes().size());\n     output_shardings.reserve(result_shape.tuple_shapes().size());\n     if (result_hlo_sharding.has_value() &&\n-        (!result_hlo_sharding->IsTuple() ||\n-         result_hlo_sharding->tuple_elements().size() !=\n-             result_shape.tuple_shapes().size())) {\n+        !check_tuple_output_sharding_condition(result_shape,\n+                                               *result_hlo_sharding)) {\n       return FailedPrecondition(\n           \"Output sharding is inconsistent with the tuple result\");\n     }"
        }
    ]
},
{
    "Id": 518,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ab60b0ee51a8924a0f02b0152cd6a78ba64d3e94",
    "date": "2023-02-22T11:57:10-08:00",
    "message": "[tfg] Fix named-attribute token check.\n\nSince the name tokens are being indexed directly, we should check that list of tokens is not empty to prevent an out-of-bounds error.\n\nPiperOrigin-RevId: 511553573",
    "label": "YES",
    "changes": [
        {
            "name": "convert_attributes.cc",
            "path": "tensorflow/core/ir/importexport/convert_attributes.cc",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 6,
                    "new_start": 16,
                    "new_length": 7,
                    "hunk": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"tensorflow/core/ir/importexport/convert_attributes.h\"\n \n #include <string>\n+#include <vector>\n \n #include \"llvm/ADT/StringSet.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n"
                },
                {
                    "old_start": 264,
                    "old_length": 6,
                    "new_start": 265,
                    "new_length": 7,
                    "hunk": "@@ -264,6 +265,7 @@ Status ConvertAttributes(ArrayRef<NamedAttribute> attrs,\n     // calls.\n     std::vector<std::string> name_tokens =\n         absl::StrSplit(name, '.', absl::SkipEmpty());\n+    TF_RET_CHECK(!name_tokens.empty());\n     TF_RET_CHECK(name_tokens.size() <= 2);\n     auto it = func_call_attrs.find(name_tokens[0]);\n     if (it == func_call_attrs.end())"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <vector>\n+    TF_RET_CHECK(!name_tokens.empty());\n",
            "whole_hunk": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"tensorflow/core/ir/importexport/convert_attributes.h\"\n \n #include <string>\n+#include <vector>\n \n #include \"llvm/ADT/StringSet.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n@@ -264,6 +265,7 @@ Status ConvertAttributes(ArrayRef<NamedAttribute> attrs,\n     // calls.\n     std::vector<std::string> name_tokens =\n         absl::StrSplit(name, '.', absl::SkipEmpty());\n+    TF_RET_CHECK(!name_tokens.empty());\n     TF_RET_CHECK(name_tokens.size() <= 2);\n     auto it = func_call_attrs.find(name_tokens[0]);\n     if (it == func_call_attrs.end())"
        }
    ]
},
{
    "Id": 24,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ba451bde470e0d5cfe6d40c42f17035b17fd9538",
    "date": "2024-06-18T02:36:27-07:00",
    "message": "PR #13310: [NVIDIA GPU] Added a rewrite logic in gpu_windowned_einsum_handler to handle all2all\n\nImported from GitHub PR https://github.com/openxla/xla/pull/13310\n\nAdded a rewrite logic in gpu_windowned_einsum_handler to decompose a2a+gemm or gemm+a2a into smaller a2a+gemm/gemm+a2a to hide communuication overhead. Partial results will be aggregated at the end.\nAn example will be:\n```\n\ninput\n   |\na2a{replica_groups={{0,1}}}\n   |\ngemm\n\n```\n\ndecomposed into\n```\n\n                  input\n                 /         \\\n           slice1      slice2\n              /               \\\n           a2a1             a2a2\n            /                  \\\n       gemm1                 gemm2\n                \\           /\n                    add\n```\n\nAll partial gemms will be dispatched to parallel streams too to achieve gemm-gemm overlap.\n\nPerformance metrics:\nFor an unit with just a2a+gemm or gemm+a2a, we see from 5-15% speedup depending on the size by doing this type of composition.\nCopybara import of the project:\n\n--\n557c540df51b3c238f87ae01262f1a6000ee4499 by TJ <tjx@nvidia.com>:\n\nAdded a rewrite logic in gpu_windowned_einsum_handler to decompose\na2a+gemm or gemm+a2a into smaller a2a+gemm/gemm+a2a to hide\ncommunuication overhead.\n\n--\nd3e9b2fc28b484f263609c21b6177ea948aa8e01 by TJ <tjx@nvidia.com>:\n\nChanged testing to use file check\n\n--\n3de9fbb962438837e77353c3c0b2a96e3e0d397e by TJ Xu <tjx@nvidia.com>:\n\nAdded e2e tests\naddress recent changes to thunk emission with execution stream id\n\n--\nd7790ed5e206c5e1ebf33afa8e34d7faedff4d47 by TJ Xu <tjx@nvidia.com>:\n\nAdded file check to BUILD file\n\nMerging this change closes #13310\n\nPiperOrigin-RevId: 644291310",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 6084,
                    "old_length": 6,
                    "new_start": 6084,
                    "new_length": 9,
                    "hunk": "@@ -6084,6 +6084,9 @@ cc_library(\n     hdrs = [\"gpu_windowed_einsum_handler.h\"],\n     deps = [\n         \":backend_configs_cc\",\n+        \"//xla:literal_util\",\n+        \"//xla:status\",\n+        \"//xla:statusor\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n"
                },
                {
                    "old_start": 6091,
                    "old_length": 6,
                    "new_start": 6094,
                    "new_length": 8,
                    "hunk": "@@ -6091,6 +6094,8 @@ cc_library(\n         \"//xla/service:hlo_creation_utils\",\n         \"//xla/service:hlo_pass\",\n         \"//xla/service:pattern_matcher\",\n+        \"//xla/service:shape_inference\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n"
                },
                {
                    "old_start": 6110,
                    "old_length": 6,
                    "new_start": 6115,
                    "new_length": 7,
                    "hunk": "@@ -6110,6 +6115,7 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service:pattern_matcher_gmock\",\n+        \"//xla/tests:filecheck\",\n         \"//xla/tests:hlo_test_base\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//xla:literal_util\",\n+        \"//xla:status\",\n+        \"//xla:statusor\",\n+        \"//xla/service:shape_inference\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"//xla/tests:filecheck\",\n",
            "whole_hunk": "@@ -6084,6 +6084,9 @@ cc_library(\n     hdrs = [\"gpu_windowed_einsum_handler.h\"],\n     deps = [\n         \":backend_configs_cc\",\n+        \"//xla:literal_util\",\n+        \"//xla:status\",\n+        \"//xla:statusor\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/ir:hlo\",\n@@ -6091,6 +6094,8 @@ cc_library(\n         \"//xla/service:hlo_creation_utils\",\n         \"//xla/service:hlo_pass\",\n         \"//xla/service:pattern_matcher\",\n+        \"//xla/service:shape_inference\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n@@ -6110,6 +6115,7 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service:pattern_matcher_gmock\",\n+        \"//xla/tests:filecheck\",\n         \"//xla/tests:hlo_test_base\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\",\n"
        },
        {
            "name": "gpu_windowed_einsum_handler.cc",
            "path": "third_party/xla/xla/service/gpu/gpu_windowed_einsum_handler.cc",
            "patches": [
                {
                    "old_start": 27,
                    "old_length": 9,
                    "new_start": 27,
                    "new_length": 11,
                    "hunk": "@@ -27,9 +27,11 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n+#include \"xla/literal_util.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/hlo_creation_utils.h\"\n #include \"xla/service/pattern_matcher.h\"\n+#include \"xla/service/shape_inference.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/errors.h\"\n"
                },
                {
                    "old_start": 454,
                    "old_length": 17,
                    "new_start": 455,
                    "new_length": 150,
                    "hunk": "@@ -454,17 +455,150 @@ absl::Status ProcessWindowedEinsumLoopForActivationCaching(\n   return absl::OkStatus();\n }\n \n+bool HasReplicaGroups(const HloInstruction* inst) {\n+  return inst->replica_groups().size() > 0;\n+}\n+\n+bool ShouldAddToChain(const HloInstruction* inst) {\n+  switch (inst->opcode()) {\n+    case HloOpcode::kTranspose:\n+    case HloOpcode::kReshape:\n+    case HloOpcode::kCopy:\n+      return inst->user_count() == 1;\n+    default:\n+      return false;\n+  }\n+}\n+\n+struct MatchedGemmA2aResult {\n+  HloInstruction* producer_gemm;\n+  HloInstruction* lhs;\n+  HloInstruction* rhs;\n+  HloInstruction* a2a_replacement = nullptr;\n+  bool matched = false;\n+};\n+\n class WindowedEinsumVisitor : public DfsHloRewriteVisitor {\n  public:\n   explicit WindowedEinsumVisitor(\n       std::vector<GpuWindowedEinsumHandler::WindowedEinsumAgLoops>&\n           all_ag_loops)\n       : all_ag_loops_(all_ag_loops) {}\n-  // Rewrites a allgather-dot pattern that shares the same operand\n-  // with a windowed einsum loop to consume the output of the loop\n-  // and remove the all-gather.\n+  absl::StatusOr<bool> MatchA2aGemmWithIntermediateReshapes(\n+      HloInstruction* dot, HloInstruction** lhs, HloInstruction** rhs) {\n+    if (Match(dot, m::Dot(m::AllToAll(lhs).WithOneUse().WithPredicate(\n+                              HasReplicaGroups),\n+                          m::Op(rhs))) &&\n+        !DynCast<HloAllToAllInstruction>((*lhs))->constrain_layout() &&\n+        !(*lhs)->shape().IsTuple()) {\n+      return true;\n+    }\n+    std::vector<HloInstruction*> allowed_intermediate_ops(\n+        {dot->mutable_operand(0)});\n+\n+    HloAllToAllInstruction* matched_a2a = nullptr;\n+    // We keep pushing until an unmet condition or we have found the a2a.\n+    while (true) {\n+      HloInstruction* curr = allowed_intermediate_ops.back();\n+      if (ShouldAddToChain(curr)) {\n+        allowed_intermediate_ops.insert(allowed_intermediate_ops.end(),\n+                                        std::begin(curr->operands()),\n+                                        std::end(curr->operands()));\n+      } else if (curr->opcode() == HloOpcode::kAllToAll &&\n+                 curr->user_count() == 1) {\n+        matched_a2a = DynCast<HloAllToAllInstruction>(curr);\n+        allowed_intermediate_ops.pop_back();\n+        break;\n+      } else {\n+        return false;\n+      }\n+    }\n+    CHECK(matched_a2a != nullptr);\n+    if (matched_a2a->constrain_layout() || matched_a2a->shape().IsTuple() ||\n+        !HasReplicaGroups(matched_a2a) || !matched_a2a->split_dimension()) {\n+      return false;\n+    }\n+    // We need to create a new a2a that's a direct producer of the dot and\n+    // replace it with the original a2a. A new reshape will be added to the\n+    // orginal a2a's input. We first need to determine the new split dimension\n+    // after all the reshape ops.\n+    int64_t split_dimension = *matched_a2a->split_dimension();\n+    for (int64_t i = allowed_intermediate_ops.size() - 1; i >= 0; i--) {\n+      HloInstruction* current_op = allowed_intermediate_ops[i];\n+      if (current_op->opcode() == HloOpcode::kReshape) {\n+        std::vector<std::pair<int64_t, int64_t>> unmodified_dims =\n+            ShapeUtil::DimensionsUnmodifiedByReshape(\n+                current_op->operand(0)->shape(), current_op->shape());\n+        auto it = absl::c_find_if(\n+            unmodified_dims,\n+            [&split_dimension](std::pair<int64_t, int64_t>& dim_pair) {\n+              return dim_pair.first == split_dimension;\n+            });\n+        // Split dimension of a2a has been modified, we cannot deduce the new\n+        // split dim easily, so skip decomposition.\n+        if (it == unmodified_dims.end()) {\n+          VLOG(5) << \"Split dimension of: \" << matched_a2a->ToShortString()\n+                  << \" has been modified by reshapes. Skip process it for \"\n+                     \"decomposition.\";\n+          return false;\n+        }\n+        // Assign the new split dim.\n+        split_dimension = it->second;\n+      } else if (current_op->opcode() == HloOpcode::kTranspose) {\n+        const auto& transpose_dims = current_op->dimensions();\n+        for (int64_t j = 0; j < transpose_dims.size(); j++) {\n+          if ((int64_t)transpose_dims[j] == split_dimension) {\n+            split_dimension = j;\n+            break;\n+          }\n+        }\n+      }\n+    }\n+    TF_RETURN_IF_ERROR(allowed_intermediate_ops.back()->ReplaceOperandWith(\n+        0, matched_a2a->mutable_operand(0)));\n+    HloInstruction* new_a2a =\n+        matched_a2a->parent()->AddInstruction(HloInstruction::CreateAllToAll(\n+            allowed_intermediate_ops.front()->shape(),\n+            {allowed_intermediate_ops.front()}, matched_a2a->replica_groups(),\n+            false, hlo_query::NextChannelId(*matched_a2a->GetModule()),\n+            split_dimension));\n+\n+    TF_RETURN_IF_ERROR(dot->ReplaceOperandWith(0, new_a2a));\n+    TF_RETURN_IF_ERROR(\n+        matched_a2a->parent()->RemoveInstructionAndUnusedOperands(matched_a2a));\n+    MarkAsChanged();\n+    *lhs = new_a2a;\n+    *rhs = dot->mutable_operand(1);\n+    return true;\n+  }\n+\n   absl::Status HandleDot(HloInstruction* dot) override {\n     CHECK_EQ(dot->opcode(), HloOpcode::kDot);\n+    HloComputation* comp = dot->parent();\n+    // Rewrites a allgather-dot pattern that shares the same operand\n+    // with a windowed einsum loop to consume the output of the loop\n+    // and remove the all-gather.\n+    // Now that we have processed all loops, we can check if there are any\n+    // allgather-dot pattern that we can optimize. We'd want to transform:\n+    //                       input\n+    //                       /    |\n+    //                      /     |\n+    //                     AG    windowed loop\n+    //                     /\n+    //                    /\n+    //                   dot\n+    // to:\n+    //                       input\n+    //                       |\n+    //                       |\n+    //                     windowed loop\n+    //                       |\n+    //                       |\n+    //                      dot\n+    // The windowed einsum loop will also be rewritten to output the full input\n+    // to be consumed by the dot. This is advantageous since the chained dot can\n+    // fully utilize all the resources on the GPU while comm is hidden by the\n+    // first collective matmul loop.\n     for (GpuWindowedEinsumHandler::WindowedEinsumAgLoops ag_loop :\n          all_ag_loops_) {\n       HloInstruction* loop = ag_loop.loop;\n"
                },
                {
                    "old_start": 487,
                    "old_length": 7,
                    "new_start": 621,
                    "new_length": 6,
                    "hunk": "@@ -487,7 +621,6 @@ class WindowedEinsumVisitor : public DfsHloRewriteVisitor {\n                    \"windowed einsum loop : \"\n                 << loop->ToString();\n         int64_t cache_output_index = dot->operand_index(ag_with_shared_operand);\n-        HloComputation* comp = dot->parent();\n         HloInstruction* new_gte = comp->AddInstruction(\n             HloInstruction::CreateGetTupleElement(loop, 3));\n         TF_RETURN_IF_ERROR(\n"
                },
                {
                    "old_start": 500,
                    "old_length": 6,
                    "new_start": 633,
                    "new_length": 374,
                    "hunk": "@@ -500,6 +633,374 @@ class WindowedEinsumVisitor : public DfsHloRewriteVisitor {\n         }\n       }\n     }\n+    // Rewrites an all-to-all+gemm into multiple independent partial a2a+gemms\n+    // to minimize communication overhead. To do this, the original input will\n+    // be sliced into replica_group size and perform all-to-all+gemm.\n+    HloInstruction* lhs;\n+    HloInstruction* rhs;\n+    std::vector<xla::ReplicaGroup> replica_groups;\n+    TF_ASSIGN_OR_RETURN(bool matched,\n+                        MatchA2aGemmWithIntermediateReshapes(dot, &lhs, &rhs));\n+    if (matched) {\n+      replica_groups = lhs->replica_groups();\n+      // We split the a2a+gemm along the contracting dimension into multiple\n+      // a2a+gemms and perform partial dots, partial results are added to the\n+      // final output buffer.\n+      int64_t group_size = replica_groups[0].replica_ids_size();\n+      if (absl::c_find_if(replica_groups, [&](ReplicaGroup& group) {\n+            return group.replica_ids_size() != group_size;\n+          }) != replica_groups.end()) {\n+        VLOG(5) << \"All-to-all split groups don't have the same number of \"\n+                   \"replicas.\";\n+        return absl::OkStatus();\n+      }\n+\n+      // Get the dimension to slice for lhs and rhs, we slice on the contracting\n+      // dimensions to calculate partial results\n+      const DotDimensionNumbers& original_dot_dnums =\n+          dot->dot_dimension_numbers();\n+      const PrecisionConfig& original_precision = dot->precision_config();\n+      const auto& lhs_contracting_dims =\n+          dot->dot_dimension_numbers().lhs_contracting_dimensions();\n+      const auto& rhs_contracting_dims =\n+          dot->dot_dimension_numbers().rhs_contracting_dimensions();\n+\n+      if (lhs_contracting_dims.size() != 1 ||\n+          rhs_contracting_dims.size() != 1) {\n+        VLOG(5) << \"Contracting dimensions have multiple elements, all-to-all \"\n+                   \"sharding will be skipped.\";\n+        return absl::OkStatus();\n+      }\n+      int64_t lhs_contracting_dim = lhs_contracting_dims[0];\n+      int64_t rhs_contracting_dim = rhs_contracting_dims[0];\n+      HloAllToAllInstruction* a2a = DynCast<HloAllToAllInstruction>(lhs);\n+      int64_t contracting_dim_value =\n+          rhs->shape().dimensions()[rhs_contracting_dim];\n+\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      std::vector<int64_t> lhs_slice_sizes(a2a->shape().rank(), 0);\n+      std::vector<int64_t> lhs_slice_increments(a2a->shape().rank(), 1);\n+      std::vector<int64_t> lhs_slice_max_range(\n+          a2a->shape().dimensions().begin(), a2a->shape().dimensions().end());\n+\n+      std::vector<int64_t> rhs_slice_sizes(rhs->shape().rank(), 0);\n+      std::vector<int64_t> rhs_slice_increments(rhs->shape().rank(), 1);\n+      std::vector<int64_t> rhs_slice_max_range(\n+          rhs->shape().dimensions().begin(), rhs->shape().dimensions().end());\n+\n+      // Create a zero-valued buffer to hold output.\n+      HloInstruction* output_buffer =\n+          comp->AddInstruction(HloInstruction::CreateBroadcast(\n+              dot->shape(),\n+              comp->AddInstruction(HloInstruction::CreateConstant(\n+                  LiteralUtil::Zero(dot->shape().element_type()))),\n+              {}));\n+      HloInstruction* a2a_operand = a2a->mutable_operand(0);\n+      if (contracting_dim_value % group_size) {\n+        VLOG(5) << absl::StrFormat(\n+            \"Contracting dimension %d needs to be divisible by group_size %d\",\n+            contracting_dim_value, group_size);\n+        return absl::OkStatus();\n+      }\n+      int64_t size_per_split = contracting_dim_value / group_size;\n+\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      lhs_slice_max_range[lhs_contracting_dim] = size_per_split;\n+      rhs_slice_max_range[rhs_contracting_dim] = size_per_split;\n+\n+      Shape lhs_slice_shape = a2a->shape();\n+      Shape rhs_slice_shape = rhs->shape();\n+\n+      lhs_slice_shape.set_dimensions(lhs_contracting_dim, size_per_split);\n+      rhs_slice_shape.set_dimensions(rhs_contracting_dim, size_per_split);\n+\n+      HloInstruction* lhs_slice;\n+      HloInstruction* rhs_slice;\n+\n+      HloInstruction* partial_result = output_buffer;\n+\n+      Shape partial_all_to_all_shape = lhs_slice_shape;\n+\n+      TF_ASSIGN_OR_RETURN(\n+          Shape partial_dot_shape,\n+          ShapeInference::InferDotOpShape(\n+              partial_all_to_all_shape, rhs_slice_shape, original_dot_dnums,\n+              /*preferred_element_type=*/std::nullopt));\n+      int64_t stream_id = hlo_query::NextChannelId(*a2a->GetModule());\n+      for (int64_t i = 0; i < group_size; ++i) {\n+        lhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            lhs_slice_shape, a2a_operand, lhs_slice_sizes, lhs_slice_max_range,\n+            lhs_slice_increments));\n+        a2a->SetupDerivedInstruction(lhs_slice);\n+        lhs_slice_sizes[lhs_contracting_dim] =\n+            lhs_slice_max_range[lhs_contracting_dim];\n+        lhs_slice_max_range[lhs_contracting_dim] += size_per_split;\n+\n+        rhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            rhs_slice_shape, rhs, rhs_slice_sizes, rhs_slice_max_range,\n+            rhs_slice_increments));\n+        a2a->SetupDerivedInstruction(rhs_slice);\n+        rhs_slice_sizes[rhs_contracting_dim] =\n+            rhs_slice_max_range[rhs_contracting_dim];\n+        rhs_slice_max_range[rhs_contracting_dim] += size_per_split;\n+\n+        HloInstruction* partial_all_to_all =\n+            comp->AddInstruction(HloInstruction::CreateAllToAll(\n+                partial_all_to_all_shape, {lhs_slice}, a2a->device_list(),\n+                false, hlo_query::NextChannelId(*a2a->GetModule()),\n+                a2a->split_dimension()));\n+        a2a->SetupDerivedInstruction(partial_all_to_all);\n+\n+        HloInstruction* partial_dot =\n+            comp->AddInstruction(HloInstruction::CreateDot(\n+                partial_dot_shape, partial_all_to_all, rhs_slice,\n+                original_dot_dnums, original_precision));\n+        partial_result = comp->AddInstruction(\n+            HloInstruction::CreateBinary(partial_dot->shape(), HloOpcode::kAdd,\n+                                         partial_dot, partial_result));\n+        a2a->SetupDerivedInstruction(partial_result);\n+        TF_RETURN_IF_ERROR(\n+            UpdateDotAndConsumerConfig(partial_dot, stream_id++));\n+      }\n+      TF_RETURN_IF_ERROR(ReplaceInstruction(dot, partial_result));\n+    }\n+    return absl::OkStatus();\n+  }\n+\n+  absl::StatusOr<MatchedGemmA2aResult> MatchGemmA2aWithIntermediateReshapes(\n+      HloInstruction* inst) {\n+    MatchedGemmA2aResult result;\n+    HloAllToAllInstruction* a2a = DynCast<HloAllToAllInstruction>(inst);\n+    if (!HasReplicaGroups(a2a) || a2a->constrain_layout() ||\n+        a2a->shape().IsTuple()) {\n+      return result;\n+    }\n+    if (Match(a2a, m::AllToAll(m::Dot(&result.producer_gemm, m::Op(&result.lhs),\n+                                      m::Op(&result.rhs))\n+                                   .WithOneUse()))) {\n+      result.matched = true;\n+      return result;\n+    }\n+    std::vector<HloInstruction*> allowed_intermediate_ops(\n+        {a2a->mutable_operand(0)});\n+\n+    HloInstruction* matched_dot = nullptr;\n+    // We keep pushing until an unmet condition or we have found the producer\n+    // dot.\n+    while (true) {\n+      HloInstruction* curr = allowed_intermediate_ops.back();\n+      if (ShouldAddToChain(curr)) {\n+        allowed_intermediate_ops.insert(allowed_intermediate_ops.end(),\n+                                        std::begin(curr->operands()),\n+                                        std::end(curr->operands()));\n+      } else if (curr->opcode() == HloOpcode::kDot && curr->user_count() == 1) {\n+        matched_dot = curr;\n+        allowed_intermediate_ops.pop_back();\n+        break;\n+      } else {\n+        return result;\n+      }\n+    }\n+    CHECK(matched_dot != nullptr);\n+    // We need to create a new a2a that's a direct consumer of the dot and\n+    // replace it with the original a2a. A new reshape will be added to the\n+    // orginal a2a's output. We first need to determine the new split dimension\n+    // after all the reshape ops.\n+    int64_t split_dimension = *a2a->split_dimension();\n+    for (int64_t i = 0; i < allowed_intermediate_ops.size(); i++) {\n+      HloInstruction* current_op = allowed_intermediate_ops[i];\n+      if (current_op->opcode() == HloOpcode::kReshape) {\n+        std::vector<std::pair<int64_t, int64_t>> unmodified_dims =\n+            ShapeUtil::DimensionsUnmodifiedByReshape(\n+                current_op->operand(0)->shape(), current_op->shape());\n+        auto it = absl::c_find_if(\n+            unmodified_dims,\n+            [&split_dimension](std::pair<int64_t, int64_t>& dim_pair) {\n+              return dim_pair.second == split_dimension;\n+            });\n+        // Split dimension of a2a has been modified, we cannot deduce the new\n+        // split dim easily, so skip decomposition.\n+        if (it == unmodified_dims.end()) {\n+          VLOG(5) << \"Split dimension of: \" << a2a->ToShortString()\n+                  << \" has been modified by reshapes. Skip process it for \"\n+                     \"decomposition.\";\n+          return result;\n+        }\n+        // Assign the new split dim.\n+        split_dimension = it->first;\n+      } else if (current_op->opcode() == HloOpcode::kTranspose) {\n+        const auto& transpose_dims = current_op->dimensions();\n+        split_dimension = transpose_dims[split_dimension];\n+      }\n+    }\n+    result.a2a_replacement =\n+        matched_dot->parent()->AddInstruction(HloInstruction::CreateAllToAll(\n+            matched_dot->shape(), {matched_dot}, a2a->replica_groups(), false,\n+            hlo_query::NextChannelId(*matched_dot->GetModule()),\n+            split_dimension));\n+    TF_RETURN_IF_ERROR(allowed_intermediate_ops.back()->ReplaceOperandWith(\n+        0, result.a2a_replacement));\n+    inst->SetupDerivedInstruction(result.a2a_replacement);\n+\n+    TF_RETURN_IF_ERROR(\n+        ReplaceInstruction(inst, allowed_intermediate_ops.front()));\n+    result.lhs = matched_dot->mutable_operand(0);\n+    result.rhs = matched_dot->mutable_operand(1);\n+    result.producer_gemm = matched_dot;\n+    result.matched = true;\n+    return result;\n+  }\n+\n+  // Rewrites an gemm+all-to-all into multiple independent partial gemm+a2a's\n+  // to minimize communication overhead. To do this, the original input will be\n+  // sliced into replica_group size and perform gemm+all-to-all.\n+  absl::Status HandleAllToAll(HloInstruction* inst) override {\n+    CHECK_EQ(inst->opcode(), HloOpcode::kAllToAll);\n+    HloComputation* comp = inst->parent();\n+    // Rewrites a gemm+alltoall into multiple independent partial gemm+a2as\n+    // to minimize communication overhead.\n+    std::vector<xla::ReplicaGroup> replica_groups;\n+    TF_ASSIGN_OR_RETURN(MatchedGemmA2aResult matched_result,\n+                        MatchGemmA2aWithIntermediateReshapes(inst));\n+    if (matched_result.matched) {\n+      HloInstruction* a2a = inst;\n+      if (matched_result.a2a_replacement) {\n+        a2a = matched_result.a2a_replacement;\n+      }\n+      replica_groups = a2a->replica_groups();\n+      // Similar to a2a+gemm, we split along contracting dimensions\n+      // and aggregate result at each step.\n+      int64_t group_size = replica_groups[0].replica_ids_size();\n+\n+      if (absl::c_find_if(replica_groups, [&](ReplicaGroup& group) {\n+            return group.replica_ids_size() != group_size;\n+          }) != replica_groups.end()) {\n+        VLOG(5) << \"All-to-all split groups don't have the same number of \"\n+                   \"replicas.\";\n+        return absl::OkStatus();\n+      }\n+\n+      // Get the dimension to slice for lhs and rhs, we slice on the contracting\n+      // dimensions to calculate partial results\n+      const DotDimensionNumbers& original_dot_dnums =\n+          matched_result.producer_gemm->dot_dimension_numbers();\n+      const PrecisionConfig& original_precision =\n+          matched_result.producer_gemm->precision_config();\n+      const auto& lhs_contracting_dims =\n+          matched_result.producer_gemm->dot_dimension_numbers()\n+              .lhs_contracting_dimensions();\n+      const auto& rhs_contracting_dims =\n+          matched_result.producer_gemm->dot_dimension_numbers()\n+              .rhs_contracting_dimensions();\n+\n+      if (lhs_contracting_dims.size() != 1 ||\n+          rhs_contracting_dims.size() != 1) {\n+        VLOG(5) << \"Contracting dimensions have multiple elements, all-to-all \"\n+                   \"sharding will be skipped.\";\n+        return absl::OkStatus();\n+      }\n+      int64_t lhs_contracting_dim = lhs_contracting_dims[0];\n+      int64_t rhs_contracting_dim = rhs_contracting_dims[0];\n+      HloAllToAllInstruction* all_to_all = DynCast<HloAllToAllInstruction>(a2a);\n+      int64_t contracting_dim_value =\n+          matched_result.rhs->shape().dimensions()[rhs_contracting_dim];\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      std::vector<int64_t> lhs_slice_sizes(matched_result.lhs->shape().rank(),\n+                                           0);\n+      std::vector<int64_t> lhs_slice_increments(\n+          matched_result.lhs->shape().rank(), 1);\n+      std::vector<int64_t> lhs_slice_max_range(\n+          matched_result.lhs->shape().dimensions().begin(),\n+          matched_result.lhs->shape().dimensions().end());\n+\n+      std::vector<int64_t> rhs_slice_sizes(matched_result.rhs->shape().rank(),\n+                                           0);\n+      std::vector<int64_t> rhs_slice_increments(\n+          matched_result.rhs->shape().rank(), 1);\n+      std::vector<int64_t> rhs_slice_max_range(\n+          matched_result.rhs->shape().dimensions().begin(),\n+          matched_result.rhs->shape().dimensions().end());\n+\n+      // Create a zero-valued buffer to hold output.\n+      HloInstruction* output_buffer =\n+          comp->AddInstruction(HloInstruction::CreateBroadcast(\n+              all_to_all->shape(),\n+              comp->AddInstruction(HloInstruction::CreateConstant(\n+                  LiteralUtil::Zero(all_to_all->shape().element_type()))),\n+              {}));\n+      if (contracting_dim_value % group_size) {\n+        VLOG(5) << absl::StrFormat(\n+            \"Contracting dimension %d needs to be divisible by group_size %d\",\n+            contracting_dim_value, group_size);\n+        return absl::OkStatus();\n+      }\n+\n+      int64_t size_per_split = contracting_dim_value / group_size;\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      lhs_slice_max_range[lhs_contracting_dim] = size_per_split;\n+      rhs_slice_max_range[rhs_contracting_dim] = size_per_split;\n+\n+      Shape lhs_slice_shape = matched_result.lhs->shape();\n+      Shape rhs_slice_shape = matched_result.rhs->shape();\n+\n+      lhs_slice_shape.set_dimensions(lhs_contracting_dim, size_per_split);\n+      rhs_slice_shape.set_dimensions(rhs_contracting_dim, size_per_split);\n+\n+      HloInstruction* lhs_slice;\n+      HloInstruction* rhs_slice;\n+\n+      HloInstruction* partial_result = output_buffer;\n+      Shape partial_all_to_all_shape = all_to_all->shape();\n+\n+      TF_ASSIGN_OR_RETURN(\n+          Shape partial_dot_shape,\n+          ShapeInference::InferDotOpShape(\n+              lhs_slice_shape, rhs_slice_shape, original_dot_dnums,\n+              /*preferred_element_type=*/std::nullopt));\n+      int64_t stream_id = hlo_query::NextChannelId(*all_to_all->GetModule());\n+      for (int64_t i = 0; i < group_size; ++i) {\n+        lhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            lhs_slice_shape, matched_result.lhs, lhs_slice_sizes,\n+            lhs_slice_max_range, lhs_slice_increments));\n+        all_to_all->SetupDerivedInstruction(lhs_slice);\n+        lhs_slice_sizes[lhs_contracting_dim] =\n+            lhs_slice_max_range[lhs_contracting_dim];\n+        lhs_slice_max_range[lhs_contracting_dim] += size_per_split;\n+\n+        rhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            rhs_slice_shape, matched_result.rhs, rhs_slice_sizes,\n+            rhs_slice_max_range, rhs_slice_increments));\n+\n+        all_to_all->SetupDerivedInstruction(rhs_slice);\n+        rhs_slice_sizes[rhs_contracting_dim] =\n+            rhs_slice_max_range[rhs_contracting_dim];\n+        rhs_slice_max_range[rhs_contracting_dim] += size_per_split;\n+\n+        HloInstruction* partial_dot = comp->AddInstruction(\n+            HloInstruction::CreateDot(partial_dot_shape, lhs_slice, rhs_slice,\n+                                      original_dot_dnums, original_precision));\n+\n+        HloInstruction* partial_all_to_all =\n+            comp->AddInstruction(HloInstruction::CreateAllToAll(\n+                partial_all_to_all_shape, {partial_dot},\n+                all_to_all->device_list(), false,\n+                hlo_query::NextChannelId(*all_to_all->GetModule()),\n+                all_to_all->split_dimension()));\n+        all_to_all->SetupDerivedInstruction(partial_all_to_all);\n+        partial_result = comp->AddInstruction(HloInstruction::CreateBinary(\n+            partial_all_to_all_shape, HloOpcode::kAdd, partial_all_to_all,\n+            partial_result));\n+        all_to_all->SetupDerivedInstruction(partial_result);\n+        TF_RETURN_IF_ERROR(\n+            UpdateDotAndConsumerConfig(partial_dot, stream_id++));\n+      }\n+      TF_RETURN_IF_ERROR(ReplaceInstruction(all_to_all, partial_result));\n+    }\n+\n     return absl::OkStatus();\n   }\n \n"
                },
                {
                    "old_start": 533,
                    "old_length": 28,
                    "new_start": 1034,
                    "new_length": 6,
                    "hunk": "@@ -533,28 +1034,6 @@ absl::StatusOr<bool> GpuWindowedEinsumHandler::Run(\n       changed = comp_result;\n     }\n   }\n-  // Now that we have processed all loops, we can check if there are any\n-  // allgather-dot pattern that we can optimize. We'd want to transform:\n-  //                       input\n-  //                       /    |\n-  //                      /     |\n-  //                     AG    windowed loop\n-  //                     /\n-  //                    /\n-  //                   dot\n-  // to:\n-  //                       input\n-  //                       |\n-  //                       |\n-  //                     windowed loop\n-  //                       |\n-  //                       |\n-  //                      dot\n-  // The windowed einsum loop will also be rewritten to output the full input to\n-  // be consumed by the dot.\n-  // This is advantageous since the chained dot can fully utilize all the\n-  // resources on the GPU while comm is hidden by the first collective matmul\n-  // loop.\n   for (HloComputation* comp :\n        module->MakeNonfusionComputations(execution_threads)) {\n     WindowedEinsumVisitor visitor(all_ag_loops_);\n"
                }
            ],
            "whole_deleted": "-  // Rewrites a allgather-dot pattern that shares the same operand\n-  // with a windowed einsum loop to consume the output of the loop\n-  // and remove the all-gather.\n-        HloComputation* comp = dot->parent();\n-  // Now that we have processed all loops, we can check if there are any\n-  // allgather-dot pattern that we can optimize. We'd want to transform:\n-  //                       input\n-  //                       /    |\n-  //                      /     |\n-  //                     AG    windowed loop\n-  //                     /\n-  //                    /\n-  //                   dot\n-  // to:\n-  //                       input\n-  //                       |\n-  //                       |\n-  //                     windowed loop\n-  //                       |\n-  //                       |\n-  //                      dot\n-  // The windowed einsum loop will also be rewritten to output the full input to\n-  // be consumed by the dot.\n-  // This is advantageous since the chained dot can fully utilize all the\n-  // resources on the GPU while comm is hidden by the first collective matmul\n-  // loop.\n",
            "whole_added": "+#include \"xla/literal_util.h\"\n+#include \"xla/service/shape_inference.h\"\n+bool HasReplicaGroups(const HloInstruction* inst) {\n+  return inst->replica_groups().size() > 0;\n+}\n+\n+bool ShouldAddToChain(const HloInstruction* inst) {\n+  switch (inst->opcode()) {\n+    case HloOpcode::kTranspose:\n+    case HloOpcode::kReshape:\n+    case HloOpcode::kCopy:\n+      return inst->user_count() == 1;\n+    default:\n+      return false;\n+  }\n+}\n+\n+struct MatchedGemmA2aResult {\n+  HloInstruction* producer_gemm;\n+  HloInstruction* lhs;\n+  HloInstruction* rhs;\n+  HloInstruction* a2a_replacement = nullptr;\n+  bool matched = false;\n+};\n+\n+  absl::StatusOr<bool> MatchA2aGemmWithIntermediateReshapes(\n+      HloInstruction* dot, HloInstruction** lhs, HloInstruction** rhs) {\n+    if (Match(dot, m::Dot(m::AllToAll(lhs).WithOneUse().WithPredicate(\n+                              HasReplicaGroups),\n+                          m::Op(rhs))) &&\n+        !DynCast<HloAllToAllInstruction>((*lhs))->constrain_layout() &&\n+        !(*lhs)->shape().IsTuple()) {\n+      return true;\n+    }\n+    std::vector<HloInstruction*> allowed_intermediate_ops(\n+        {dot->mutable_operand(0)});\n+\n+    HloAllToAllInstruction* matched_a2a = nullptr;\n+    // We keep pushing until an unmet condition or we have found the a2a.\n+    while (true) {\n+      HloInstruction* curr = allowed_intermediate_ops.back();\n+      if (ShouldAddToChain(curr)) {\n+        allowed_intermediate_ops.insert(allowed_intermediate_ops.end(),\n+                                        std::begin(curr->operands()),\n+                                        std::end(curr->operands()));\n+      } else if (curr->opcode() == HloOpcode::kAllToAll &&\n+                 curr->user_count() == 1) {\n+        matched_a2a = DynCast<HloAllToAllInstruction>(curr);\n+        allowed_intermediate_ops.pop_back();\n+        break;\n+      } else {\n+        return false;\n+      }\n+    }\n+    CHECK(matched_a2a != nullptr);\n+    if (matched_a2a->constrain_layout() || matched_a2a->shape().IsTuple() ||\n+        !HasReplicaGroups(matched_a2a) || !matched_a2a->split_dimension()) {\n+      return false;\n+    }\n+    // We need to create a new a2a that's a direct producer of the dot and\n+    // replace it with the original a2a. A new reshape will be added to the\n+    // orginal a2a's input. We first need to determine the new split dimension\n+    // after all the reshape ops.\n+    int64_t split_dimension = *matched_a2a->split_dimension();\n+    for (int64_t i = allowed_intermediate_ops.size() - 1; i >= 0; i--) {\n+      HloInstruction* current_op = allowed_intermediate_ops[i];\n+      if (current_op->opcode() == HloOpcode::kReshape) {\n+        std::vector<std::pair<int64_t, int64_t>> unmodified_dims =\n+            ShapeUtil::DimensionsUnmodifiedByReshape(\n+                current_op->operand(0)->shape(), current_op->shape());\n+        auto it = absl::c_find_if(\n+            unmodified_dims,\n+            [&split_dimension](std::pair<int64_t, int64_t>& dim_pair) {\n+              return dim_pair.first == split_dimension;\n+            });\n+        // Split dimension of a2a has been modified, we cannot deduce the new\n+        // split dim easily, so skip decomposition.\n+        if (it == unmodified_dims.end()) {\n+          VLOG(5) << \"Split dimension of: \" << matched_a2a->ToShortString()\n+                  << \" has been modified by reshapes. Skip process it for \"\n+                     \"decomposition.\";\n+          return false;\n+        }\n+        // Assign the new split dim.\n+        split_dimension = it->second;\n+      } else if (current_op->opcode() == HloOpcode::kTranspose) {\n+        const auto& transpose_dims = current_op->dimensions();\n+        for (int64_t j = 0; j < transpose_dims.size(); j++) {\n+          if ((int64_t)transpose_dims[j] == split_dimension) {\n+            split_dimension = j;\n+            break;\n+          }\n+        }\n+      }\n+    }\n+    TF_RETURN_IF_ERROR(allowed_intermediate_ops.back()->ReplaceOperandWith(\n+        0, matched_a2a->mutable_operand(0)));\n+    HloInstruction* new_a2a =\n+        matched_a2a->parent()->AddInstruction(HloInstruction::CreateAllToAll(\n+            allowed_intermediate_ops.front()->shape(),\n+            {allowed_intermediate_ops.front()}, matched_a2a->replica_groups(),\n+            false, hlo_query::NextChannelId(*matched_a2a->GetModule()),\n+            split_dimension));\n+\n+    TF_RETURN_IF_ERROR(dot->ReplaceOperandWith(0, new_a2a));\n+    TF_RETURN_IF_ERROR(\n+        matched_a2a->parent()->RemoveInstructionAndUnusedOperands(matched_a2a));\n+    MarkAsChanged();\n+    *lhs = new_a2a;\n+    *rhs = dot->mutable_operand(1);\n+    return true;\n+  }\n+\n+    HloComputation* comp = dot->parent();\n+    // Rewrites a allgather-dot pattern that shares the same operand\n+    // with a windowed einsum loop to consume the output of the loop\n+    // and remove the all-gather.\n+    // Now that we have processed all loops, we can check if there are any\n+    // allgather-dot pattern that we can optimize. We'd want to transform:\n+    //                       input\n+    //                       /    |\n+    //                      /     |\n+    //                     AG    windowed loop\n+    //                     /\n+    //                    /\n+    //                   dot\n+    // to:\n+    //                       input\n+    //                       |\n+    //                       |\n+    //                     windowed loop\n+    //                       |\n+    //                       |\n+    //                      dot\n+    // The windowed einsum loop will also be rewritten to output the full input\n+    // to be consumed by the dot. This is advantageous since the chained dot can\n+    // fully utilize all the resources on the GPU while comm is hidden by the\n+    // first collective matmul loop.\n+    // Rewrites an all-to-all+gemm into multiple independent partial a2a+gemms\n+    // to minimize communication overhead. To do this, the original input will\n+    // be sliced into replica_group size and perform all-to-all+gemm.\n+    HloInstruction* lhs;\n+    HloInstruction* rhs;\n+    std::vector<xla::ReplicaGroup> replica_groups;\n+    TF_ASSIGN_OR_RETURN(bool matched,\n+                        MatchA2aGemmWithIntermediateReshapes(dot, &lhs, &rhs));\n+    if (matched) {\n+      replica_groups = lhs->replica_groups();\n+      // We split the a2a+gemm along the contracting dimension into multiple\n+      // a2a+gemms and perform partial dots, partial results are added to the\n+      // final output buffer.\n+      int64_t group_size = replica_groups[0].replica_ids_size();\n+      if (absl::c_find_if(replica_groups, [&](ReplicaGroup& group) {\n+            return group.replica_ids_size() != group_size;\n+          }) != replica_groups.end()) {\n+        VLOG(5) << \"All-to-all split groups don't have the same number of \"\n+                   \"replicas.\";\n+        return absl::OkStatus();\n+      }\n+\n+      // Get the dimension to slice for lhs and rhs, we slice on the contracting\n+      // dimensions to calculate partial results\n+      const DotDimensionNumbers& original_dot_dnums =\n+          dot->dot_dimension_numbers();\n+      const PrecisionConfig& original_precision = dot->precision_config();\n+      const auto& lhs_contracting_dims =\n+          dot->dot_dimension_numbers().lhs_contracting_dimensions();\n+      const auto& rhs_contracting_dims =\n+          dot->dot_dimension_numbers().rhs_contracting_dimensions();\n+\n+      if (lhs_contracting_dims.size() != 1 ||\n+          rhs_contracting_dims.size() != 1) {\n+        VLOG(5) << \"Contracting dimensions have multiple elements, all-to-all \"\n+                   \"sharding will be skipped.\";\n+        return absl::OkStatus();\n+      }\n+      int64_t lhs_contracting_dim = lhs_contracting_dims[0];\n+      int64_t rhs_contracting_dim = rhs_contracting_dims[0];\n+      HloAllToAllInstruction* a2a = DynCast<HloAllToAllInstruction>(lhs);\n+      int64_t contracting_dim_value =\n+          rhs->shape().dimensions()[rhs_contracting_dim];\n+\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      std::vector<int64_t> lhs_slice_sizes(a2a->shape().rank(), 0);\n+      std::vector<int64_t> lhs_slice_increments(a2a->shape().rank(), 1);\n+      std::vector<int64_t> lhs_slice_max_range(\n+          a2a->shape().dimensions().begin(), a2a->shape().dimensions().end());\n+\n+      std::vector<int64_t> rhs_slice_sizes(rhs->shape().rank(), 0);\n+      std::vector<int64_t> rhs_slice_increments(rhs->shape().rank(), 1);\n+      std::vector<int64_t> rhs_slice_max_range(\n+          rhs->shape().dimensions().begin(), rhs->shape().dimensions().end());\n+\n+      // Create a zero-valued buffer to hold output.\n+      HloInstruction* output_buffer =\n+          comp->AddInstruction(HloInstruction::CreateBroadcast(\n+              dot->shape(),\n+              comp->AddInstruction(HloInstruction::CreateConstant(\n+                  LiteralUtil::Zero(dot->shape().element_type()))),\n+              {}));\n+      HloInstruction* a2a_operand = a2a->mutable_operand(0);\n+      if (contracting_dim_value % group_size) {\n+        VLOG(5) << absl::StrFormat(\n+            \"Contracting dimension %d needs to be divisible by group_size %d\",\n+            contracting_dim_value, group_size);\n+        return absl::OkStatus();\n+      }\n+      int64_t size_per_split = contracting_dim_value / group_size;\n+\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      lhs_slice_max_range[lhs_contracting_dim] = size_per_split;\n+      rhs_slice_max_range[rhs_contracting_dim] = size_per_split;\n+\n+      Shape lhs_slice_shape = a2a->shape();\n+      Shape rhs_slice_shape = rhs->shape();\n+\n+      lhs_slice_shape.set_dimensions(lhs_contracting_dim, size_per_split);\n+      rhs_slice_shape.set_dimensions(rhs_contracting_dim, size_per_split);\n+\n+      HloInstruction* lhs_slice;\n+      HloInstruction* rhs_slice;\n+\n+      HloInstruction* partial_result = output_buffer;\n+\n+      Shape partial_all_to_all_shape = lhs_slice_shape;\n+\n+      TF_ASSIGN_OR_RETURN(\n+          Shape partial_dot_shape,\n+          ShapeInference::InferDotOpShape(\n+              partial_all_to_all_shape, rhs_slice_shape, original_dot_dnums,\n+              /*preferred_element_type=*/std::nullopt));\n+      int64_t stream_id = hlo_query::NextChannelId(*a2a->GetModule());\n+      for (int64_t i = 0; i < group_size; ++i) {\n+        lhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            lhs_slice_shape, a2a_operand, lhs_slice_sizes, lhs_slice_max_range,\n+            lhs_slice_increments));\n+        a2a->SetupDerivedInstruction(lhs_slice);\n+        lhs_slice_sizes[lhs_contracting_dim] =\n+            lhs_slice_max_range[lhs_contracting_dim];\n+        lhs_slice_max_range[lhs_contracting_dim] += size_per_split;\n+\n+        rhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            rhs_slice_shape, rhs, rhs_slice_sizes, rhs_slice_max_range,\n+            rhs_slice_increments));\n+        a2a->SetupDerivedInstruction(rhs_slice);\n+        rhs_slice_sizes[rhs_contracting_dim] =\n+            rhs_slice_max_range[rhs_contracting_dim];\n+        rhs_slice_max_range[rhs_contracting_dim] += size_per_split;\n+\n+        HloInstruction* partial_all_to_all =\n+            comp->AddInstruction(HloInstruction::CreateAllToAll(\n+                partial_all_to_all_shape, {lhs_slice}, a2a->device_list(),\n+                false, hlo_query::NextChannelId(*a2a->GetModule()),\n+                a2a->split_dimension()));\n+        a2a->SetupDerivedInstruction(partial_all_to_all);\n+\n+        HloInstruction* partial_dot =\n+            comp->AddInstruction(HloInstruction::CreateDot(\n+                partial_dot_shape, partial_all_to_all, rhs_slice,\n+                original_dot_dnums, original_precision));\n+        partial_result = comp->AddInstruction(\n+            HloInstruction::CreateBinary(partial_dot->shape(), HloOpcode::kAdd,\n+                                         partial_dot, partial_result));\n+        a2a->SetupDerivedInstruction(partial_result);\n+        TF_RETURN_IF_ERROR(\n+            UpdateDotAndConsumerConfig(partial_dot, stream_id++));\n+      }\n+      TF_RETURN_IF_ERROR(ReplaceInstruction(dot, partial_result));\n+    }\n+    return absl::OkStatus();\n+  }\n+\n+  absl::StatusOr<MatchedGemmA2aResult> MatchGemmA2aWithIntermediateReshapes(\n+      HloInstruction* inst) {\n+    MatchedGemmA2aResult result;\n+    HloAllToAllInstruction* a2a = DynCast<HloAllToAllInstruction>(inst);\n+    if (!HasReplicaGroups(a2a) || a2a->constrain_layout() ||\n+        a2a->shape().IsTuple()) {\n+      return result;\n+    }\n+    if (Match(a2a, m::AllToAll(m::Dot(&result.producer_gemm, m::Op(&result.lhs),\n+                                      m::Op(&result.rhs))\n+                                   .WithOneUse()))) {\n+      result.matched = true;\n+      return result;\n+    }\n+    std::vector<HloInstruction*> allowed_intermediate_ops(\n+        {a2a->mutable_operand(0)});\n+\n+    HloInstruction* matched_dot = nullptr;\n+    // We keep pushing until an unmet condition or we have found the producer\n+    // dot.\n+    while (true) {\n+      HloInstruction* curr = allowed_intermediate_ops.back();\n+      if (ShouldAddToChain(curr)) {\n+        allowed_intermediate_ops.insert(allowed_intermediate_ops.end(),\n+                                        std::begin(curr->operands()),\n+                                        std::end(curr->operands()));\n+      } else if (curr->opcode() == HloOpcode::kDot && curr->user_count() == 1) {\n+        matched_dot = curr;\n+        allowed_intermediate_ops.pop_back();\n+        break;\n+      } else {\n+        return result;\n+      }\n+    }\n+    CHECK(matched_dot != nullptr);\n+    // We need to create a new a2a that's a direct consumer of the dot and\n+    // replace it with the original a2a. A new reshape will be added to the\n+    // orginal a2a's output. We first need to determine the new split dimension\n+    // after all the reshape ops.\n+    int64_t split_dimension = *a2a->split_dimension();\n+    for (int64_t i = 0; i < allowed_intermediate_ops.size(); i++) {\n+      HloInstruction* current_op = allowed_intermediate_ops[i];\n+      if (current_op->opcode() == HloOpcode::kReshape) {\n+        std::vector<std::pair<int64_t, int64_t>> unmodified_dims =\n+            ShapeUtil::DimensionsUnmodifiedByReshape(\n+                current_op->operand(0)->shape(), current_op->shape());\n+        auto it = absl::c_find_if(\n+            unmodified_dims,\n+            [&split_dimension](std::pair<int64_t, int64_t>& dim_pair) {\n+              return dim_pair.second == split_dimension;\n+            });\n+        // Split dimension of a2a has been modified, we cannot deduce the new\n+        // split dim easily, so skip decomposition.\n+        if (it == unmodified_dims.end()) {\n+          VLOG(5) << \"Split dimension of: \" << a2a->ToShortString()\n+                  << \" has been modified by reshapes. Skip process it for \"\n+                     \"decomposition.\";\n+          return result;\n+        }\n+        // Assign the new split dim.\n+        split_dimension = it->first;\n+      } else if (current_op->opcode() == HloOpcode::kTranspose) {\n+        const auto& transpose_dims = current_op->dimensions();\n+        split_dimension = transpose_dims[split_dimension];\n+      }\n+    }\n+    result.a2a_replacement =\n+        matched_dot->parent()->AddInstruction(HloInstruction::CreateAllToAll(\n+            matched_dot->shape(), {matched_dot}, a2a->replica_groups(), false,\n+            hlo_query::NextChannelId(*matched_dot->GetModule()),\n+            split_dimension));\n+    TF_RETURN_IF_ERROR(allowed_intermediate_ops.back()->ReplaceOperandWith(\n+        0, result.a2a_replacement));\n+    inst->SetupDerivedInstruction(result.a2a_replacement);\n+\n+    TF_RETURN_IF_ERROR(\n+        ReplaceInstruction(inst, allowed_intermediate_ops.front()));\n+    result.lhs = matched_dot->mutable_operand(0);\n+    result.rhs = matched_dot->mutable_operand(1);\n+    result.producer_gemm = matched_dot;\n+    result.matched = true;\n+    return result;\n+  }\n+\n+  // Rewrites an gemm+all-to-all into multiple independent partial gemm+a2a's\n+  // to minimize communication overhead. To do this, the original input will be\n+  // sliced into replica_group size and perform gemm+all-to-all.\n+  absl::Status HandleAllToAll(HloInstruction* inst) override {\n+    CHECK_EQ(inst->opcode(), HloOpcode::kAllToAll);\n+    HloComputation* comp = inst->parent();\n+    // Rewrites a gemm+alltoall into multiple independent partial gemm+a2as\n+    // to minimize communication overhead.\n+    std::vector<xla::ReplicaGroup> replica_groups;\n+    TF_ASSIGN_OR_RETURN(MatchedGemmA2aResult matched_result,\n+                        MatchGemmA2aWithIntermediateReshapes(inst));\n+    if (matched_result.matched) {\n+      HloInstruction* a2a = inst;\n+      if (matched_result.a2a_replacement) {\n+        a2a = matched_result.a2a_replacement;\n+      }\n+      replica_groups = a2a->replica_groups();\n+      // Similar to a2a+gemm, we split along contracting dimensions\n+      // and aggregate result at each step.\n+      int64_t group_size = replica_groups[0].replica_ids_size();\n+\n+      if (absl::c_find_if(replica_groups, [&](ReplicaGroup& group) {\n+            return group.replica_ids_size() != group_size;\n+          }) != replica_groups.end()) {\n+        VLOG(5) << \"All-to-all split groups don't have the same number of \"\n+                   \"replicas.\";\n+        return absl::OkStatus();\n+      }\n+\n+      // Get the dimension to slice for lhs and rhs, we slice on the contracting\n+      // dimensions to calculate partial results\n+      const DotDimensionNumbers& original_dot_dnums =\n+          matched_result.producer_gemm->dot_dimension_numbers();\n+      const PrecisionConfig& original_precision =\n+          matched_result.producer_gemm->precision_config();\n+      const auto& lhs_contracting_dims =\n+          matched_result.producer_gemm->dot_dimension_numbers()\n+              .lhs_contracting_dimensions();\n+      const auto& rhs_contracting_dims =\n+          matched_result.producer_gemm->dot_dimension_numbers()\n+              .rhs_contracting_dimensions();\n+\n+      if (lhs_contracting_dims.size() != 1 ||\n+          rhs_contracting_dims.size() != 1) {\n+        VLOG(5) << \"Contracting dimensions have multiple elements, all-to-all \"\n+                   \"sharding will be skipped.\";\n+        return absl::OkStatus();\n+      }\n+      int64_t lhs_contracting_dim = lhs_contracting_dims[0];\n+      int64_t rhs_contracting_dim = rhs_contracting_dims[0];\n+      HloAllToAllInstruction* all_to_all = DynCast<HloAllToAllInstruction>(a2a);\n+      int64_t contracting_dim_value =\n+          matched_result.rhs->shape().dimensions()[rhs_contracting_dim];\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      std::vector<int64_t> lhs_slice_sizes(matched_result.lhs->shape().rank(),\n+                                           0);\n+      std::vector<int64_t> lhs_slice_increments(\n+          matched_result.lhs->shape().rank(), 1);\n+      std::vector<int64_t> lhs_slice_max_range(\n+          matched_result.lhs->shape().dimensions().begin(),\n+          matched_result.lhs->shape().dimensions().end());\n+\n+      std::vector<int64_t> rhs_slice_sizes(matched_result.rhs->shape().rank(),\n+                                           0);\n+      std::vector<int64_t> rhs_slice_increments(\n+          matched_result.rhs->shape().rank(), 1);\n+      std::vector<int64_t> rhs_slice_max_range(\n+          matched_result.rhs->shape().dimensions().begin(),\n+          matched_result.rhs->shape().dimensions().end());\n+\n+      // Create a zero-valued buffer to hold output.\n+      HloInstruction* output_buffer =\n+          comp->AddInstruction(HloInstruction::CreateBroadcast(\n+              all_to_all->shape(),\n+              comp->AddInstruction(HloInstruction::CreateConstant(\n+                  LiteralUtil::Zero(all_to_all->shape().element_type()))),\n+              {}));\n+      if (contracting_dim_value % group_size) {\n+        VLOG(5) << absl::StrFormat(\n+            \"Contracting dimension %d needs to be divisible by group_size %d\",\n+            contracting_dim_value, group_size);\n+        return absl::OkStatus();\n+      }\n+\n+      int64_t size_per_split = contracting_dim_value / group_size;\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      lhs_slice_max_range[lhs_contracting_dim] = size_per_split;\n+      rhs_slice_max_range[rhs_contracting_dim] = size_per_split;\n+\n+      Shape lhs_slice_shape = matched_result.lhs->shape();\n+      Shape rhs_slice_shape = matched_result.rhs->shape();\n+\n+      lhs_slice_shape.set_dimensions(lhs_contracting_dim, size_per_split);\n+      rhs_slice_shape.set_dimensions(rhs_contracting_dim, size_per_split);\n+\n+      HloInstruction* lhs_slice;\n+      HloInstruction* rhs_slice;\n+\n+      HloInstruction* partial_result = output_buffer;\n+      Shape partial_all_to_all_shape = all_to_all->shape();\n+\n+      TF_ASSIGN_OR_RETURN(\n+          Shape partial_dot_shape,\n+          ShapeInference::InferDotOpShape(\n+              lhs_slice_shape, rhs_slice_shape, original_dot_dnums,\n+              /*preferred_element_type=*/std::nullopt));\n+      int64_t stream_id = hlo_query::NextChannelId(*all_to_all->GetModule());\n+      for (int64_t i = 0; i < group_size; ++i) {\n+        lhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            lhs_slice_shape, matched_result.lhs, lhs_slice_sizes,\n+            lhs_slice_max_range, lhs_slice_increments));\n+        all_to_all->SetupDerivedInstruction(lhs_slice);\n+        lhs_slice_sizes[lhs_contracting_dim] =\n+            lhs_slice_max_range[lhs_contracting_dim];\n+        lhs_slice_max_range[lhs_contracting_dim] += size_per_split;\n+\n+        rhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            rhs_slice_shape, matched_result.rhs, rhs_slice_sizes,\n+            rhs_slice_max_range, rhs_slice_increments));\n+\n+        all_to_all->SetupDerivedInstruction(rhs_slice);\n+        rhs_slice_sizes[rhs_contracting_dim] =\n+            rhs_slice_max_range[rhs_contracting_dim];\n+        rhs_slice_max_range[rhs_contracting_dim] += size_per_split;\n+\n+        HloInstruction* partial_dot = comp->AddInstruction(\n+            HloInstruction::CreateDot(partial_dot_shape, lhs_slice, rhs_slice,\n+                                      original_dot_dnums, original_precision));\n+\n+        HloInstruction* partial_all_to_all =\n+            comp->AddInstruction(HloInstruction::CreateAllToAll(\n+                partial_all_to_all_shape, {partial_dot},\n+                all_to_all->device_list(), false,\n+                hlo_query::NextChannelId(*all_to_all->GetModule()),\n+                all_to_all->split_dimension()));\n+        all_to_all->SetupDerivedInstruction(partial_all_to_all);\n+        partial_result = comp->AddInstruction(HloInstruction::CreateBinary(\n+            partial_all_to_all_shape, HloOpcode::kAdd, partial_all_to_all,\n+            partial_result));\n+        all_to_all->SetupDerivedInstruction(partial_result);\n+        TF_RETURN_IF_ERROR(\n+            UpdateDotAndConsumerConfig(partial_dot, stream_id++));\n+      }\n+      TF_RETURN_IF_ERROR(ReplaceInstruction(all_to_all, partial_result));\n+    }\n+\n",
            "whole_hunk": "@@ -27,9 +27,11 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n+#include \"xla/literal_util.h\"\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/hlo_creation_utils.h\"\n #include \"xla/service/pattern_matcher.h\"\n+#include \"xla/service/shape_inference.h\"\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/errors.h\"\n@@ -454,17 +455,150 @@ absl::Status ProcessWindowedEinsumLoopForActivationCaching(\n   return absl::OkStatus();\n }\n \n+bool HasReplicaGroups(const HloInstruction* inst) {\n+  return inst->replica_groups().size() > 0;\n+}\n+\n+bool ShouldAddToChain(const HloInstruction* inst) {\n+  switch (inst->opcode()) {\n+    case HloOpcode::kTranspose:\n+    case HloOpcode::kReshape:\n+    case HloOpcode::kCopy:\n+      return inst->user_count() == 1;\n+    default:\n+      return false;\n+  }\n+}\n+\n+struct MatchedGemmA2aResult {\n+  HloInstruction* producer_gemm;\n+  HloInstruction* lhs;\n+  HloInstruction* rhs;\n+  HloInstruction* a2a_replacement = nullptr;\n+  bool matched = false;\n+};\n+\n class WindowedEinsumVisitor : public DfsHloRewriteVisitor {\n  public:\n   explicit WindowedEinsumVisitor(\n       std::vector<GpuWindowedEinsumHandler::WindowedEinsumAgLoops>&\n           all_ag_loops)\n       : all_ag_loops_(all_ag_loops) {}\n-  // Rewrites a allgather-dot pattern that shares the same operand\n-  // with a windowed einsum loop to consume the output of the loop\n-  // and remove the all-gather.\n+  absl::StatusOr<bool> MatchA2aGemmWithIntermediateReshapes(\n+      HloInstruction* dot, HloInstruction** lhs, HloInstruction** rhs) {\n+    if (Match(dot, m::Dot(m::AllToAll(lhs).WithOneUse().WithPredicate(\n+                              HasReplicaGroups),\n+                          m::Op(rhs))) &&\n+        !DynCast<HloAllToAllInstruction>((*lhs))->constrain_layout() &&\n+        !(*lhs)->shape().IsTuple()) {\n+      return true;\n+    }\n+    std::vector<HloInstruction*> allowed_intermediate_ops(\n+        {dot->mutable_operand(0)});\n+\n+    HloAllToAllInstruction* matched_a2a = nullptr;\n+    // We keep pushing until an unmet condition or we have found the a2a.\n+    while (true) {\n+      HloInstruction* curr = allowed_intermediate_ops.back();\n+      if (ShouldAddToChain(curr)) {\n+        allowed_intermediate_ops.insert(allowed_intermediate_ops.end(),\n+                                        std::begin(curr->operands()),\n+                                        std::end(curr->operands()));\n+      } else if (curr->opcode() == HloOpcode::kAllToAll &&\n+                 curr->user_count() == 1) {\n+        matched_a2a = DynCast<HloAllToAllInstruction>(curr);\n+        allowed_intermediate_ops.pop_back();\n+        break;\n+      } else {\n+        return false;\n+      }\n+    }\n+    CHECK(matched_a2a != nullptr);\n+    if (matched_a2a->constrain_layout() || matched_a2a->shape().IsTuple() ||\n+        !HasReplicaGroups(matched_a2a) || !matched_a2a->split_dimension()) {\n+      return false;\n+    }\n+    // We need to create a new a2a that's a direct producer of the dot and\n+    // replace it with the original a2a. A new reshape will be added to the\n+    // orginal a2a's input. We first need to determine the new split dimension\n+    // after all the reshape ops.\n+    int64_t split_dimension = *matched_a2a->split_dimension();\n+    for (int64_t i = allowed_intermediate_ops.size() - 1; i >= 0; i--) {\n+      HloInstruction* current_op = allowed_intermediate_ops[i];\n+      if (current_op->opcode() == HloOpcode::kReshape) {\n+        std::vector<std::pair<int64_t, int64_t>> unmodified_dims =\n+            ShapeUtil::DimensionsUnmodifiedByReshape(\n+                current_op->operand(0)->shape(), current_op->shape());\n+        auto it = absl::c_find_if(\n+            unmodified_dims,\n+            [&split_dimension](std::pair<int64_t, int64_t>& dim_pair) {\n+              return dim_pair.first == split_dimension;\n+            });\n+        // Split dimension of a2a has been modified, we cannot deduce the new\n+        // split dim easily, so skip decomposition.\n+        if (it == unmodified_dims.end()) {\n+          VLOG(5) << \"Split dimension of: \" << matched_a2a->ToShortString()\n+                  << \" has been modified by reshapes. Skip process it for \"\n+                     \"decomposition.\";\n+          return false;\n+        }\n+        // Assign the new split dim.\n+        split_dimension = it->second;\n+      } else if (current_op->opcode() == HloOpcode::kTranspose) {\n+        const auto& transpose_dims = current_op->dimensions();\n+        for (int64_t j = 0; j < transpose_dims.size(); j++) {\n+          if ((int64_t)transpose_dims[j] == split_dimension) {\n+            split_dimension = j;\n+            break;\n+          }\n+        }\n+      }\n+    }\n+    TF_RETURN_IF_ERROR(allowed_intermediate_ops.back()->ReplaceOperandWith(\n+        0, matched_a2a->mutable_operand(0)));\n+    HloInstruction* new_a2a =\n+        matched_a2a->parent()->AddInstruction(HloInstruction::CreateAllToAll(\n+            allowed_intermediate_ops.front()->shape(),\n+            {allowed_intermediate_ops.front()}, matched_a2a->replica_groups(),\n+            false, hlo_query::NextChannelId(*matched_a2a->GetModule()),\n+            split_dimension));\n+\n+    TF_RETURN_IF_ERROR(dot->ReplaceOperandWith(0, new_a2a));\n+    TF_RETURN_IF_ERROR(\n+        matched_a2a->parent()->RemoveInstructionAndUnusedOperands(matched_a2a));\n+    MarkAsChanged();\n+    *lhs = new_a2a;\n+    *rhs = dot->mutable_operand(1);\n+    return true;\n+  }\n+\n   absl::Status HandleDot(HloInstruction* dot) override {\n     CHECK_EQ(dot->opcode(), HloOpcode::kDot);\n+    HloComputation* comp = dot->parent();\n+    // Rewrites a allgather-dot pattern that shares the same operand\n+    // with a windowed einsum loop to consume the output of the loop\n+    // and remove the all-gather.\n+    // Now that we have processed all loops, we can check if there are any\n+    // allgather-dot pattern that we can optimize. We'd want to transform:\n+    //                       input\n+    //                       /    |\n+    //                      /     |\n+    //                     AG    windowed loop\n+    //                     /\n+    //                    /\n+    //                   dot\n+    // to:\n+    //                       input\n+    //                       |\n+    //                       |\n+    //                     windowed loop\n+    //                       |\n+    //                       |\n+    //                      dot\n+    // The windowed einsum loop will also be rewritten to output the full input\n+    // to be consumed by the dot. This is advantageous since the chained dot can\n+    // fully utilize all the resources on the GPU while comm is hidden by the\n+    // first collective matmul loop.\n     for (GpuWindowedEinsumHandler::WindowedEinsumAgLoops ag_loop :\n          all_ag_loops_) {\n       HloInstruction* loop = ag_loop.loop;\n@@ -487,7 +621,6 @@ class WindowedEinsumVisitor : public DfsHloRewriteVisitor {\n                    \"windowed einsum loop : \"\n                 << loop->ToString();\n         int64_t cache_output_index = dot->operand_index(ag_with_shared_operand);\n-        HloComputation* comp = dot->parent();\n         HloInstruction* new_gte = comp->AddInstruction(\n             HloInstruction::CreateGetTupleElement(loop, 3));\n         TF_RETURN_IF_ERROR(\n@@ -500,6 +633,374 @@ class WindowedEinsumVisitor : public DfsHloRewriteVisitor {\n         }\n       }\n     }\n+    // Rewrites an all-to-all+gemm into multiple independent partial a2a+gemms\n+    // to minimize communication overhead. To do this, the original input will\n+    // be sliced into replica_group size and perform all-to-all+gemm.\n+    HloInstruction* lhs;\n+    HloInstruction* rhs;\n+    std::vector<xla::ReplicaGroup> replica_groups;\n+    TF_ASSIGN_OR_RETURN(bool matched,\n+                        MatchA2aGemmWithIntermediateReshapes(dot, &lhs, &rhs));\n+    if (matched) {\n+      replica_groups = lhs->replica_groups();\n+      // We split the a2a+gemm along the contracting dimension into multiple\n+      // a2a+gemms and perform partial dots, partial results are added to the\n+      // final output buffer.\n+      int64_t group_size = replica_groups[0].replica_ids_size();\n+      if (absl::c_find_if(replica_groups, [&](ReplicaGroup& group) {\n+            return group.replica_ids_size() != group_size;\n+          }) != replica_groups.end()) {\n+        VLOG(5) << \"All-to-all split groups don't have the same number of \"\n+                   \"replicas.\";\n+        return absl::OkStatus();\n+      }\n+\n+      // Get the dimension to slice for lhs and rhs, we slice on the contracting\n+      // dimensions to calculate partial results\n+      const DotDimensionNumbers& original_dot_dnums =\n+          dot->dot_dimension_numbers();\n+      const PrecisionConfig& original_precision = dot->precision_config();\n+      const auto& lhs_contracting_dims =\n+          dot->dot_dimension_numbers().lhs_contracting_dimensions();\n+      const auto& rhs_contracting_dims =\n+          dot->dot_dimension_numbers().rhs_contracting_dimensions();\n+\n+      if (lhs_contracting_dims.size() != 1 ||\n+          rhs_contracting_dims.size() != 1) {\n+        VLOG(5) << \"Contracting dimensions have multiple elements, all-to-all \"\n+                   \"sharding will be skipped.\";\n+        return absl::OkStatus();\n+      }\n+      int64_t lhs_contracting_dim = lhs_contracting_dims[0];\n+      int64_t rhs_contracting_dim = rhs_contracting_dims[0];\n+      HloAllToAllInstruction* a2a = DynCast<HloAllToAllInstruction>(lhs);\n+      int64_t contracting_dim_value =\n+          rhs->shape().dimensions()[rhs_contracting_dim];\n+\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      std::vector<int64_t> lhs_slice_sizes(a2a->shape().rank(), 0);\n+      std::vector<int64_t> lhs_slice_increments(a2a->shape().rank(), 1);\n+      std::vector<int64_t> lhs_slice_max_range(\n+          a2a->shape().dimensions().begin(), a2a->shape().dimensions().end());\n+\n+      std::vector<int64_t> rhs_slice_sizes(rhs->shape().rank(), 0);\n+      std::vector<int64_t> rhs_slice_increments(rhs->shape().rank(), 1);\n+      std::vector<int64_t> rhs_slice_max_range(\n+          rhs->shape().dimensions().begin(), rhs->shape().dimensions().end());\n+\n+      // Create a zero-valued buffer to hold output.\n+      HloInstruction* output_buffer =\n+          comp->AddInstruction(HloInstruction::CreateBroadcast(\n+              dot->shape(),\n+              comp->AddInstruction(HloInstruction::CreateConstant(\n+                  LiteralUtil::Zero(dot->shape().element_type()))),\n+              {}));\n+      HloInstruction* a2a_operand = a2a->mutable_operand(0);\n+      if (contracting_dim_value % group_size) {\n+        VLOG(5) << absl::StrFormat(\n+            \"Contracting dimension %d needs to be divisible by group_size %d\",\n+            contracting_dim_value, group_size);\n+        return absl::OkStatus();\n+      }\n+      int64_t size_per_split = contracting_dim_value / group_size;\n+\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      lhs_slice_max_range[lhs_contracting_dim] = size_per_split;\n+      rhs_slice_max_range[rhs_contracting_dim] = size_per_split;\n+\n+      Shape lhs_slice_shape = a2a->shape();\n+      Shape rhs_slice_shape = rhs->shape();\n+\n+      lhs_slice_shape.set_dimensions(lhs_contracting_dim, size_per_split);\n+      rhs_slice_shape.set_dimensions(rhs_contracting_dim, size_per_split);\n+\n+      HloInstruction* lhs_slice;\n+      HloInstruction* rhs_slice;\n+\n+      HloInstruction* partial_result = output_buffer;\n+\n+      Shape partial_all_to_all_shape = lhs_slice_shape;\n+\n+      TF_ASSIGN_OR_RETURN(\n+          Shape partial_dot_shape,\n+          ShapeInference::InferDotOpShape(\n+              partial_all_to_all_shape, rhs_slice_shape, original_dot_dnums,\n+              /*preferred_element_type=*/std::nullopt));\n+      int64_t stream_id = hlo_query::NextChannelId(*a2a->GetModule());\n+      for (int64_t i = 0; i < group_size; ++i) {\n+        lhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            lhs_slice_shape, a2a_operand, lhs_slice_sizes, lhs_slice_max_range,\n+            lhs_slice_increments));\n+        a2a->SetupDerivedInstruction(lhs_slice);\n+        lhs_slice_sizes[lhs_contracting_dim] =\n+            lhs_slice_max_range[lhs_contracting_dim];\n+        lhs_slice_max_range[lhs_contracting_dim] += size_per_split;\n+\n+        rhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            rhs_slice_shape, rhs, rhs_slice_sizes, rhs_slice_max_range,\n+            rhs_slice_increments));\n+        a2a->SetupDerivedInstruction(rhs_slice);\n+        rhs_slice_sizes[rhs_contracting_dim] =\n+            rhs_slice_max_range[rhs_contracting_dim];\n+        rhs_slice_max_range[rhs_contracting_dim] += size_per_split;\n+\n+        HloInstruction* partial_all_to_all =\n+            comp->AddInstruction(HloInstruction::CreateAllToAll(\n+                partial_all_to_all_shape, {lhs_slice}, a2a->device_list(),\n+                false, hlo_query::NextChannelId(*a2a->GetModule()),\n+                a2a->split_dimension()));\n+        a2a->SetupDerivedInstruction(partial_all_to_all);\n+\n+        HloInstruction* partial_dot =\n+            comp->AddInstruction(HloInstruction::CreateDot(\n+                partial_dot_shape, partial_all_to_all, rhs_slice,\n+                original_dot_dnums, original_precision));\n+        partial_result = comp->AddInstruction(\n+            HloInstruction::CreateBinary(partial_dot->shape(), HloOpcode::kAdd,\n+                                         partial_dot, partial_result));\n+        a2a->SetupDerivedInstruction(partial_result);\n+        TF_RETURN_IF_ERROR(\n+            UpdateDotAndConsumerConfig(partial_dot, stream_id++));\n+      }\n+      TF_RETURN_IF_ERROR(ReplaceInstruction(dot, partial_result));\n+    }\n+    return absl::OkStatus();\n+  }\n+\n+  absl::StatusOr<MatchedGemmA2aResult> MatchGemmA2aWithIntermediateReshapes(\n+      HloInstruction* inst) {\n+    MatchedGemmA2aResult result;\n+    HloAllToAllInstruction* a2a = DynCast<HloAllToAllInstruction>(inst);\n+    if (!HasReplicaGroups(a2a) || a2a->constrain_layout() ||\n+        a2a->shape().IsTuple()) {\n+      return result;\n+    }\n+    if (Match(a2a, m::AllToAll(m::Dot(&result.producer_gemm, m::Op(&result.lhs),\n+                                      m::Op(&result.rhs))\n+                                   .WithOneUse()))) {\n+      result.matched = true;\n+      return result;\n+    }\n+    std::vector<HloInstruction*> allowed_intermediate_ops(\n+        {a2a->mutable_operand(0)});\n+\n+    HloInstruction* matched_dot = nullptr;\n+    // We keep pushing until an unmet condition or we have found the producer\n+    // dot.\n+    while (true) {\n+      HloInstruction* curr = allowed_intermediate_ops.back();\n+      if (ShouldAddToChain(curr)) {\n+        allowed_intermediate_ops.insert(allowed_intermediate_ops.end(),\n+                                        std::begin(curr->operands()),\n+                                        std::end(curr->operands()));\n+      } else if (curr->opcode() == HloOpcode::kDot && curr->user_count() == 1) {\n+        matched_dot = curr;\n+        allowed_intermediate_ops.pop_back();\n+        break;\n+      } else {\n+        return result;\n+      }\n+    }\n+    CHECK(matched_dot != nullptr);\n+    // We need to create a new a2a that's a direct consumer of the dot and\n+    // replace it with the original a2a. A new reshape will be added to the\n+    // orginal a2a's output. We first need to determine the new split dimension\n+    // after all the reshape ops.\n+    int64_t split_dimension = *a2a->split_dimension();\n+    for (int64_t i = 0; i < allowed_intermediate_ops.size(); i++) {\n+      HloInstruction* current_op = allowed_intermediate_ops[i];\n+      if (current_op->opcode() == HloOpcode::kReshape) {\n+        std::vector<std::pair<int64_t, int64_t>> unmodified_dims =\n+            ShapeUtil::DimensionsUnmodifiedByReshape(\n+                current_op->operand(0)->shape(), current_op->shape());\n+        auto it = absl::c_find_if(\n+            unmodified_dims,\n+            [&split_dimension](std::pair<int64_t, int64_t>& dim_pair) {\n+              return dim_pair.second == split_dimension;\n+            });\n+        // Split dimension of a2a has been modified, we cannot deduce the new\n+        // split dim easily, so skip decomposition.\n+        if (it == unmodified_dims.end()) {\n+          VLOG(5) << \"Split dimension of: \" << a2a->ToShortString()\n+                  << \" has been modified by reshapes. Skip process it for \"\n+                     \"decomposition.\";\n+          return result;\n+        }\n+        // Assign the new split dim.\n+        split_dimension = it->first;\n+      } else if (current_op->opcode() == HloOpcode::kTranspose) {\n+        const auto& transpose_dims = current_op->dimensions();\n+        split_dimension = transpose_dims[split_dimension];\n+      }\n+    }\n+    result.a2a_replacement =\n+        matched_dot->parent()->AddInstruction(HloInstruction::CreateAllToAll(\n+            matched_dot->shape(), {matched_dot}, a2a->replica_groups(), false,\n+            hlo_query::NextChannelId(*matched_dot->GetModule()),\n+            split_dimension));\n+    TF_RETURN_IF_ERROR(allowed_intermediate_ops.back()->ReplaceOperandWith(\n+        0, result.a2a_replacement));\n+    inst->SetupDerivedInstruction(result.a2a_replacement);\n+\n+    TF_RETURN_IF_ERROR(\n+        ReplaceInstruction(inst, allowed_intermediate_ops.front()));\n+    result.lhs = matched_dot->mutable_operand(0);\n+    result.rhs = matched_dot->mutable_operand(1);\n+    result.producer_gemm = matched_dot;\n+    result.matched = true;\n+    return result;\n+  }\n+\n+  // Rewrites an gemm+all-to-all into multiple independent partial gemm+a2a's\n+  // to minimize communication overhead. To do this, the original input will be\n+  // sliced into replica_group size and perform gemm+all-to-all.\n+  absl::Status HandleAllToAll(HloInstruction* inst) override {\n+    CHECK_EQ(inst->opcode(), HloOpcode::kAllToAll);\n+    HloComputation* comp = inst->parent();\n+    // Rewrites a gemm+alltoall into multiple independent partial gemm+a2as\n+    // to minimize communication overhead.\n+    std::vector<xla::ReplicaGroup> replica_groups;\n+    TF_ASSIGN_OR_RETURN(MatchedGemmA2aResult matched_result,\n+                        MatchGemmA2aWithIntermediateReshapes(inst));\n+    if (matched_result.matched) {\n+      HloInstruction* a2a = inst;\n+      if (matched_result.a2a_replacement) {\n+        a2a = matched_result.a2a_replacement;\n+      }\n+      replica_groups = a2a->replica_groups();\n+      // Similar to a2a+gemm, we split along contracting dimensions\n+      // and aggregate result at each step.\n+      int64_t group_size = replica_groups[0].replica_ids_size();\n+\n+      if (absl::c_find_if(replica_groups, [&](ReplicaGroup& group) {\n+            return group.replica_ids_size() != group_size;\n+          }) != replica_groups.end()) {\n+        VLOG(5) << \"All-to-all split groups don't have the same number of \"\n+                   \"replicas.\";\n+        return absl::OkStatus();\n+      }\n+\n+      // Get the dimension to slice for lhs and rhs, we slice on the contracting\n+      // dimensions to calculate partial results\n+      const DotDimensionNumbers& original_dot_dnums =\n+          matched_result.producer_gemm->dot_dimension_numbers();\n+      const PrecisionConfig& original_precision =\n+          matched_result.producer_gemm->precision_config();\n+      const auto& lhs_contracting_dims =\n+          matched_result.producer_gemm->dot_dimension_numbers()\n+              .lhs_contracting_dimensions();\n+      const auto& rhs_contracting_dims =\n+          matched_result.producer_gemm->dot_dimension_numbers()\n+              .rhs_contracting_dimensions();\n+\n+      if (lhs_contracting_dims.size() != 1 ||\n+          rhs_contracting_dims.size() != 1) {\n+        VLOG(5) << \"Contracting dimensions have multiple elements, all-to-all \"\n+                   \"sharding will be skipped.\";\n+        return absl::OkStatus();\n+      }\n+      int64_t lhs_contracting_dim = lhs_contracting_dims[0];\n+      int64_t rhs_contracting_dim = rhs_contracting_dims[0];\n+      HloAllToAllInstruction* all_to_all = DynCast<HloAllToAllInstruction>(a2a);\n+      int64_t contracting_dim_value =\n+          matched_result.rhs->shape().dimensions()[rhs_contracting_dim];\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      std::vector<int64_t> lhs_slice_sizes(matched_result.lhs->shape().rank(),\n+                                           0);\n+      std::vector<int64_t> lhs_slice_increments(\n+          matched_result.lhs->shape().rank(), 1);\n+      std::vector<int64_t> lhs_slice_max_range(\n+          matched_result.lhs->shape().dimensions().begin(),\n+          matched_result.lhs->shape().dimensions().end());\n+\n+      std::vector<int64_t> rhs_slice_sizes(matched_result.rhs->shape().rank(),\n+                                           0);\n+      std::vector<int64_t> rhs_slice_increments(\n+          matched_result.rhs->shape().rank(), 1);\n+      std::vector<int64_t> rhs_slice_max_range(\n+          matched_result.rhs->shape().dimensions().begin(),\n+          matched_result.rhs->shape().dimensions().end());\n+\n+      // Create a zero-valued buffer to hold output.\n+      HloInstruction* output_buffer =\n+          comp->AddInstruction(HloInstruction::CreateBroadcast(\n+              all_to_all->shape(),\n+              comp->AddInstruction(HloInstruction::CreateConstant(\n+                  LiteralUtil::Zero(all_to_all->shape().element_type()))),\n+              {}));\n+      if (contracting_dim_value % group_size) {\n+        VLOG(5) << absl::StrFormat(\n+            \"Contracting dimension %d needs to be divisible by group_size %d\",\n+            contracting_dim_value, group_size);\n+        return absl::OkStatus();\n+      }\n+\n+      int64_t size_per_split = contracting_dim_value / group_size;\n+      // Each split is sliced out of the input buffer, we need to determine the\n+      // slice sizes and increments.\n+      lhs_slice_max_range[lhs_contracting_dim] = size_per_split;\n+      rhs_slice_max_range[rhs_contracting_dim] = size_per_split;\n+\n+      Shape lhs_slice_shape = matched_result.lhs->shape();\n+      Shape rhs_slice_shape = matched_result.rhs->shape();\n+\n+      lhs_slice_shape.set_dimensions(lhs_contracting_dim, size_per_split);\n+      rhs_slice_shape.set_dimensions(rhs_contracting_dim, size_per_split);\n+\n+      HloInstruction* lhs_slice;\n+      HloInstruction* rhs_slice;\n+\n+      HloInstruction* partial_result = output_buffer;\n+      Shape partial_all_to_all_shape = all_to_all->shape();\n+\n+      TF_ASSIGN_OR_RETURN(\n+          Shape partial_dot_shape,\n+          ShapeInference::InferDotOpShape(\n+              lhs_slice_shape, rhs_slice_shape, original_dot_dnums,\n+              /*preferred_element_type=*/std::nullopt));\n+      int64_t stream_id = hlo_query::NextChannelId(*all_to_all->GetModule());\n+      for (int64_t i = 0; i < group_size; ++i) {\n+        lhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            lhs_slice_shape, matched_result.lhs, lhs_slice_sizes,\n+            lhs_slice_max_range, lhs_slice_increments));\n+        all_to_all->SetupDerivedInstruction(lhs_slice);\n+        lhs_slice_sizes[lhs_contracting_dim] =\n+            lhs_slice_max_range[lhs_contracting_dim];\n+        lhs_slice_max_range[lhs_contracting_dim] += size_per_split;\n+\n+        rhs_slice = comp->AddInstruction(HloInstruction::CreateSlice(\n+            rhs_slice_shape, matched_result.rhs, rhs_slice_sizes,\n+            rhs_slice_max_range, rhs_slice_increments));\n+\n+        all_to_all->SetupDerivedInstruction(rhs_slice);\n+        rhs_slice_sizes[rhs_contracting_dim] =\n+            rhs_slice_max_range[rhs_contracting_dim];\n+        rhs_slice_max_range[rhs_contracting_dim] += size_per_split;\n+\n+        HloInstruction* partial_dot = comp->AddInstruction(\n+            HloInstruction::CreateDot(partial_dot_shape, lhs_slice, rhs_slice,\n+                                      original_dot_dnums, original_precision));\n+\n+        HloInstruction* partial_all_to_all =\n+            comp->AddInstruction(HloInstruction::CreateAllToAll(\n+                partial_all_to_all_shape, {partial_dot},\n+                all_to_all->device_list(), false,\n+                hlo_query::NextChannelId(*all_to_all->GetModule()),\n+                all_to_all->split_dimension()));\n+        all_to_all->SetupDerivedInstruction(partial_all_to_all);\n+        partial_result = comp->AddInstruction(HloInstruction::CreateBinary(\n+            partial_all_to_all_shape, HloOpcode::kAdd, partial_all_to_all,\n+            partial_result));\n+        all_to_all->SetupDerivedInstruction(partial_result);\n+        TF_RETURN_IF_ERROR(\n+            UpdateDotAndConsumerConfig(partial_dot, stream_id++));\n+      }\n+      TF_RETURN_IF_ERROR(ReplaceInstruction(all_to_all, partial_result));\n+    }\n+\n     return absl::OkStatus();\n   }\n \n@@ -533,28 +1034,6 @@ absl::StatusOr<bool> GpuWindowedEinsumHandler::Run(\n       changed = comp_result;\n     }\n   }\n-  // Now that we have processed all loops, we can check if there are any\n-  // allgather-dot pattern that we can optimize. We'd want to transform:\n-  //                       input\n-  //                       /    |\n-  //                      /     |\n-  //                     AG    windowed loop\n-  //                     /\n-  //                    /\n-  //                   dot\n-  // to:\n-  //                       input\n-  //                       |\n-  //                       |\n-  //                     windowed loop\n-  //                       |\n-  //                       |\n-  //                      dot\n-  // The windowed einsum loop will also be rewritten to output the full input to\n-  // be consumed by the dot.\n-  // This is advantageous since the chained dot can fully utilize all the\n-  // resources on the GPU while comm is hidden by the first collective matmul\n-  // loop.\n   for (HloComputation* comp :\n        module->MakeNonfusionComputations(execution_threads)) {\n     WindowedEinsumVisitor visitor(all_ag_loops_);\n"
        },
        {
            "name": "gpu_windowed_einsum_handler_test.cc",
            "path": "third_party/xla/xla/service/gpu/gpu_windowed_einsum_handler_test.cc",
            "patches": [
                {
                    "old_start": 25,
                    "old_length": 6,
                    "new_start": 25,
                    "new_length": 7,
                    "hunk": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/pattern_matcher.h\"\n #include \"xla/service/pattern_matcher_gmock.h\"\n+#include \"xla/tests/filecheck.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"tsl/platform/statusor.h\"\n \n"
                },
                {
                    "old_start": 287,
                    "old_length": 6,
                    "new_start": 287,
                    "new_length": 316,
                    "hunk": "@@ -287,6 +287,316 @@ ENTRY main.12_spmd {\n                       m::Op(), m::Op(), m::Op(), m::Op()),\n                   m::Op())));\n }\n+TEST_F(GpuWindowedEinsumHanlderTest, A2aGemmHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,8192,32768]{2,1,0}, bf16[1,4,2048,8192]{3,2,1,0})->bf16[1,4,2048,32768]{3,2,1,0}}, num_partitions=8\n+\n+ENTRY main.9_spmd {\n+  param0 = bf16[1,8192,32768]{2,1,0} parameter(0)\n+  param1 = bf16[1,4,2048,8192]{3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(param1), channel_id=4, replica_groups={{0,1,2,3},{4,5,6,7}}, dimensions={1}\n+  ROOT dot.12 = bf16[1,4,2048,32768]{3,2,1,0} dot(all-to-all, param0), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} parameter(1)\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [6144:8192]}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE0]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,8192,32768]{2,1,0} parameter(0)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [6144:8192], [0:32768]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A0:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"8\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [4096:6144]}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE1]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [4096:6144], [0:32768]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A1:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"7\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [2048:4096]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE2]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [2048:4096], [0:32768]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A2:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"6\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [0:2048]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE3]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:2048], [0:32768]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A3:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"5\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,32768]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT0:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[BROADCAST:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"5\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT1:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD0:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"6\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT2:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD1:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"7\"],\"force_earliest_schedule\":false}\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT3:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD2:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n+\n+TEST_F(GpuWindowedEinsumHanlderTest, GemmA2aHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,8192,32768]{2,1,0}, bf16[1,4,2048,32768]{3,2,1,0})->bf16[1,4,2048,8192]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,8192,32768]{2,1,0} parameter(0)\n+  param.10 = bf16[1,4,2048,32768]{3,2,1,0} parameter(1)\n+  dot.12 = bf16[1,4,2048,8192]{3,2,1,0} dot(param.10, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}\n+  ROOT all-to-all = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(dot.12), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} parameter(1)\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [24576:32768]}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,8192,32768]{2,1,0} parameter(0)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [24576:32768]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE0:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"8\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT0:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [16384:24576]}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [16384:24576]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE1:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"7\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT1:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [8192:16384]}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [8192:16384]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE2:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"6\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT2:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [0:8192]}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [0:8192]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE3:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"5\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT3:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,8192]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A0:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[BROADCAST:.*]])\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A1:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD0:.*]])\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A2:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD1:.*]])\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A3:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD2:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n+\n+TEST_F(GpuWindowedEinsumHanlderTest, A2aTransposeLoopsHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,8192,32768]{2,1,0}, bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0})->bf16[1,4,2048,32768]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,8192,32768]{2,1,0} parameter(0)\n+  param.10 = bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} all-to-all(param.10), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={3}\n+  transpose.15 = bf16[1,4,1,8192,1,2048]{5,4,1,3,2,0} transpose(all-to-all), dimensions={0,3,1,2,4,5}\n+  reshape.2170 = bf16[1,4,8192,1,2048]{4,3,2,1,0} reshape(transpose.15)\n+  reshape.2173 = bf16[4,8192,1,2048]{3,2,1,0} reshape(reshape.2170)\n+  transpose.16 = bf16[1,4,2048,8192]{2,0,3,1} transpose(reshape.2173), dimensions={2,0,3,1}\n+  copy.53 = bf16[1,4,2048,8192]{3,2,1,0} copy(transpose.16)\n+  ROOT dot.12 = bf16[1,4,2048,32768]{3,2,1,0} dot(copy.53, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} parameter(1)\n+CHECK-DAG: %[[TRANSPOSE0:.*]] = bf16[1,4,1,8192,1,2048]{5,4,1,3,2,0} transpose(bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} %[[P1:.*]]), dimensions={0,3,1,2,4,5}\n+CHECK-DAG: %[[RESHAPE0:.*]] = bf16[1,4,8192,1,2048]{4,3,2,1,0} reshape(bf16[1,4,1,8192,1,2048]{5,4,1,3,2,0} %[[TRANSPOSE0:.*]])\n+CHECK-DAG: %[[RESHAPE1:.*]] = bf16[4,8192,1,2048]{3,2,1,0} reshape(bf16[1,4,8192,1,2048]{4,3,2,1,0} %[[RESHAPE0:.*]])\n+CHECK-DAG: %[[TRANSPOSE1:.*]] = bf16[1,4,2048,8192]{2,0,3,1} transpose(bf16[4,8192,1,2048]{3,2,1,0} %[[RESHAPE1:.*]]), dimensions={2,0,3,1}\n+CHECK-DAG: %[[COPY:.*]] = bf16[1,4,2048,8192]{3,2,1,0} copy(bf16[1,4,2048,8192]{2,0,3,1} %[[TRANSPOSE1:.*]])\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [6144:8192]}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE0]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,8192,32768]{2,1,0} parameter(0)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [6144:8192], [0:32768]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A0:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"9\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [4096:6144]}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE1]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [4096:6144], [0:32768]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A1:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"8\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [2048:4096]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE2]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [2048:4096], [0:32768]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A2:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"7\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [0:2048]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE3]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:2048], [0:32768]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A3:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"6\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,32768]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT0:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[BROADCAST:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"6\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT1:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD0:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"7\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT2:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD1:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"8\"],\"force_earliest_schedule\":false}\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT3:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD2:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  EXPECT_TRUE(changed);\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n+\n+TEST_F(GpuWindowedEinsumHanlderTest, GemmA2aTransposeLoopsHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,4,2048,32768]{3,2,1,0}, bf16[1,32768,8192]{2,1,0})->bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,4,2048,32768]{3,2,1,0} parameter(0)\n+  param.10 = bf16[1,32768,8192]{2,1,0} parameter(1)\n+  dot.13 = bf16[1,4,2048,8192]{3,2,1,0} dot(param.9, param.10), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+  copy.55 = bf16[1,4,2048,8192]{3,2,1,0} copy(dot.13)\n+  transpose.17 = bf16[4,1,2048,8192]{3,2,0,1} transpose(copy.55), dimensions={1,0,2,3}\n+  copy.56 = bf16[4,1,2048,8192]{3,2,1,0} copy(transpose.17)\n+  reshape.2216 = bf16[1,4,1,2048,8192]{4,3,2,1,0} reshape(copy.56)\n+  reshape.2219 = bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0} reshape(reshape.2216)\n+  ROOT all-to-all.1 = bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0} all-to-all(reshape.2219), channel_id=7, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} parameter(0)\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [24576:32768]}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,32768,8192]{2,1,0} parameter(1)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [24576:32768], [0:8192]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE0:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"12\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT0:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [16384:24576]}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [16384:24576], [0:8192]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE1:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"11\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT1:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [8192:16384]}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [8192:16384], [0:8192]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE2:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"10\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT2:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [0:8192]}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [0:8192]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE3:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"9\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT3:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,8192]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A0:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[BROADCAST:.*]])\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A1:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD0:.*]])\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A2:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD1:.*]])\n+CHECK-DAG: %[[ADD3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A3:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD2:.*]])\n+\n+CHECK-DAG: %[[COPY:.*]] = bf16[1,4,2048,8192]{3,2,1,0} copy(bf16[1,4,2048,8192]{3,2,1,0} %[[ADD3:.*]])\n+CHECK-DAG: %[[TRANSPOSE0:.*]] = bf16[4,1,2048,8192]{3,2,0,1} transpose(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), dimensions={1,0,2,3}\n+CHECK-DAG: %[[COPY1:.*]] = bf16[4,1,2048,8192]{3,2,1,0} copy(bf16[4,1,2048,8192]{3,2,0,1} %[[TRANSPOSE0:.*]])\n+CHECK-DAG: %[[RESHAPE0:.*]] = bf16[1,4,1,2048,8192]{4,3,2,1,0} reshape(bf16[4,1,2048,8192]{3,2,1,0} %[[COPY1:.*]])\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0} reshape(bf16[1,4,1,2048,8192]{4,3,2,1,0} %[[RESHAPE0:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  EXPECT_TRUE(changed);\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n \n TEST_F(GpuWindowedEinsumHanlderTest, AllGatherF8) {\n   constexpr absl::string_view kHloString = R\"(\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"xla/tests/filecheck.h\"\n+TEST_F(GpuWindowedEinsumHanlderTest, A2aGemmHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,8192,32768]{2,1,0}, bf16[1,4,2048,8192]{3,2,1,0})->bf16[1,4,2048,32768]{3,2,1,0}}, num_partitions=8\n+\n+ENTRY main.9_spmd {\n+  param0 = bf16[1,8192,32768]{2,1,0} parameter(0)\n+  param1 = bf16[1,4,2048,8192]{3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(param1), channel_id=4, replica_groups={{0,1,2,3},{4,5,6,7}}, dimensions={1}\n+  ROOT dot.12 = bf16[1,4,2048,32768]{3,2,1,0} dot(all-to-all, param0), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} parameter(1)\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [6144:8192]}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE0]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,8192,32768]{2,1,0} parameter(0)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [6144:8192], [0:32768]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A0:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"8\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [4096:6144]}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE1]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [4096:6144], [0:32768]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A1:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"7\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [2048:4096]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE2]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [2048:4096], [0:32768]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A2:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"6\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [0:2048]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE3]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:2048], [0:32768]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A3:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"5\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,32768]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT0:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[BROADCAST:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"5\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT1:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD0:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"6\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT2:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD1:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"7\"],\"force_earliest_schedule\":false}\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT3:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD2:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n+\n+TEST_F(GpuWindowedEinsumHanlderTest, GemmA2aHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,8192,32768]{2,1,0}, bf16[1,4,2048,32768]{3,2,1,0})->bf16[1,4,2048,8192]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,8192,32768]{2,1,0} parameter(0)\n+  param.10 = bf16[1,4,2048,32768]{3,2,1,0} parameter(1)\n+  dot.12 = bf16[1,4,2048,8192]{3,2,1,0} dot(param.10, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}\n+  ROOT all-to-all = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(dot.12), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} parameter(1)\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [24576:32768]}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,8192,32768]{2,1,0} parameter(0)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [24576:32768]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE0:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"8\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT0:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [16384:24576]}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [16384:24576]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE1:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"7\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT1:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [8192:16384]}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [8192:16384]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE2:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"6\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT2:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [0:8192]}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [0:8192]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE3:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"5\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT3:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,8192]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A0:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[BROADCAST:.*]])\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A1:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD0:.*]])\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A2:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD1:.*]])\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A3:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD2:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n+\n+TEST_F(GpuWindowedEinsumHanlderTest, A2aTransposeLoopsHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,8192,32768]{2,1,0}, bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0})->bf16[1,4,2048,32768]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,8192,32768]{2,1,0} parameter(0)\n+  param.10 = bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} all-to-all(param.10), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={3}\n+  transpose.15 = bf16[1,4,1,8192,1,2048]{5,4,1,3,2,0} transpose(all-to-all), dimensions={0,3,1,2,4,5}\n+  reshape.2170 = bf16[1,4,8192,1,2048]{4,3,2,1,0} reshape(transpose.15)\n+  reshape.2173 = bf16[4,8192,1,2048]{3,2,1,0} reshape(reshape.2170)\n+  transpose.16 = bf16[1,4,2048,8192]{2,0,3,1} transpose(reshape.2173), dimensions={2,0,3,1}\n+  copy.53 = bf16[1,4,2048,8192]{3,2,1,0} copy(transpose.16)\n+  ROOT dot.12 = bf16[1,4,2048,32768]{3,2,1,0} dot(copy.53, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} parameter(1)\n+CHECK-DAG: %[[TRANSPOSE0:.*]] = bf16[1,4,1,8192,1,2048]{5,4,1,3,2,0} transpose(bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} %[[P1:.*]]), dimensions={0,3,1,2,4,5}\n+CHECK-DAG: %[[RESHAPE0:.*]] = bf16[1,4,8192,1,2048]{4,3,2,1,0} reshape(bf16[1,4,1,8192,1,2048]{5,4,1,3,2,0} %[[TRANSPOSE0:.*]])\n+CHECK-DAG: %[[RESHAPE1:.*]] = bf16[4,8192,1,2048]{3,2,1,0} reshape(bf16[1,4,8192,1,2048]{4,3,2,1,0} %[[RESHAPE0:.*]])\n+CHECK-DAG: %[[TRANSPOSE1:.*]] = bf16[1,4,2048,8192]{2,0,3,1} transpose(bf16[4,8192,1,2048]{3,2,1,0} %[[RESHAPE1:.*]]), dimensions={2,0,3,1}\n+CHECK-DAG: %[[COPY:.*]] = bf16[1,4,2048,8192]{3,2,1,0} copy(bf16[1,4,2048,8192]{2,0,3,1} %[[TRANSPOSE1:.*]])\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [6144:8192]}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE0]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,8192,32768]{2,1,0} parameter(0)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [6144:8192], [0:32768]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A0:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"9\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [4096:6144]}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE1]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [4096:6144], [0:32768]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A1:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"8\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [2048:4096]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE2]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [2048:4096], [0:32768]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A2:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"7\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [0:2048]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE3]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:2048], [0:32768]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A3:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"6\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,32768]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT0:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[BROADCAST:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"6\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT1:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD0:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"7\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT2:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD1:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"8\"],\"force_earliest_schedule\":false}\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT3:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD2:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  EXPECT_TRUE(changed);\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n+\n+TEST_F(GpuWindowedEinsumHanlderTest, GemmA2aTransposeLoopsHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,4,2048,32768]{3,2,1,0}, bf16[1,32768,8192]{2,1,0})->bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,4,2048,32768]{3,2,1,0} parameter(0)\n+  param.10 = bf16[1,32768,8192]{2,1,0} parameter(1)\n+  dot.13 = bf16[1,4,2048,8192]{3,2,1,0} dot(param.9, param.10), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+  copy.55 = bf16[1,4,2048,8192]{3,2,1,0} copy(dot.13)\n+  transpose.17 = bf16[4,1,2048,8192]{3,2,0,1} transpose(copy.55), dimensions={1,0,2,3}\n+  copy.56 = bf16[4,1,2048,8192]{3,2,1,0} copy(transpose.17)\n+  reshape.2216 = bf16[1,4,1,2048,8192]{4,3,2,1,0} reshape(copy.56)\n+  reshape.2219 = bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0} reshape(reshape.2216)\n+  ROOT all-to-all.1 = bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0} all-to-all(reshape.2219), channel_id=7, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} parameter(0)\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [24576:32768]}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,32768,8192]{2,1,0} parameter(1)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [24576:32768], [0:8192]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE0:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"12\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT0:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [16384:24576]}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [16384:24576], [0:8192]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE1:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"11\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT1:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [8192:16384]}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [8192:16384], [0:8192]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE2:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"10\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT2:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [0:8192]}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [0:8192]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE3:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"9\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT3:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,8192]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A0:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[BROADCAST:.*]])\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A1:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD0:.*]])\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A2:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD1:.*]])\n+CHECK-DAG: %[[ADD3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A3:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD2:.*]])\n+\n+CHECK-DAG: %[[COPY:.*]] = bf16[1,4,2048,8192]{3,2,1,0} copy(bf16[1,4,2048,8192]{3,2,1,0} %[[ADD3:.*]])\n+CHECK-DAG: %[[TRANSPOSE0:.*]] = bf16[4,1,2048,8192]{3,2,0,1} transpose(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), dimensions={1,0,2,3}\n+CHECK-DAG: %[[COPY1:.*]] = bf16[4,1,2048,8192]{3,2,1,0} copy(bf16[4,1,2048,8192]{3,2,0,1} %[[TRANSPOSE0:.*]])\n+CHECK-DAG: %[[RESHAPE0:.*]] = bf16[1,4,1,2048,8192]{4,3,2,1,0} reshape(bf16[4,1,2048,8192]{3,2,1,0} %[[COPY1:.*]])\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0} reshape(bf16[1,4,1,2048,8192]{4,3,2,1,0} %[[RESHAPE0:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  EXPECT_TRUE(changed);\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n",
            "whole_hunk": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"xla/service/gpu/backend_configs.pb.h\"\n #include \"xla/service/pattern_matcher.h\"\n #include \"xla/service/pattern_matcher_gmock.h\"\n+#include \"xla/tests/filecheck.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"tsl/platform/statusor.h\"\n \n@@ -287,6 +287,316 @@ ENTRY main.12_spmd {\n                       m::Op(), m::Op(), m::Op(), m::Op()),\n                   m::Op())));\n }\n+TEST_F(GpuWindowedEinsumHanlderTest, A2aGemmHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,8192,32768]{2,1,0}, bf16[1,4,2048,8192]{3,2,1,0})->bf16[1,4,2048,32768]{3,2,1,0}}, num_partitions=8\n+\n+ENTRY main.9_spmd {\n+  param0 = bf16[1,8192,32768]{2,1,0} parameter(0)\n+  param1 = bf16[1,4,2048,8192]{3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(param1), channel_id=4, replica_groups={{0,1,2,3},{4,5,6,7}}, dimensions={1}\n+  ROOT dot.12 = bf16[1,4,2048,32768]{3,2,1,0} dot(all-to-all, param0), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} parameter(1)\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [6144:8192]}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE0]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,8192,32768]{2,1,0} parameter(0)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [6144:8192], [0:32768]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A0:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"8\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [4096:6144]}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE1]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [4096:6144], [0:32768]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A1:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"7\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [2048:4096]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE2]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [2048:4096], [0:32768]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A2:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"6\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [0:2048]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE3]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3},{4,5,6,7}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:2048], [0:32768]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A3:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"5\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,32768]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT0:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[BROADCAST:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"5\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT1:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD0:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"6\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT2:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD1:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"7\"],\"force_earliest_schedule\":false}\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT3:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD2:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n+\n+TEST_F(GpuWindowedEinsumHanlderTest, GemmA2aHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,8192,32768]{2,1,0}, bf16[1,4,2048,32768]{3,2,1,0})->bf16[1,4,2048,8192]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,8192,32768]{2,1,0} parameter(0)\n+  param.10 = bf16[1,4,2048,32768]{3,2,1,0} parameter(1)\n+  dot.12 = bf16[1,4,2048,8192]{3,2,1,0} dot(param.10, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}\n+  ROOT all-to-all = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(dot.12), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} parameter(1)\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [24576:32768]}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,8192,32768]{2,1,0} parameter(0)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [24576:32768]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE0:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"8\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT0:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [16384:24576]}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [16384:24576]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE1:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"7\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT1:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [8192:16384]}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [8192:16384]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE2:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"6\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT2:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [0:8192]}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [0:8192]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE3:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}, backend_config={\"operation_queue_id\":\"5\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT3:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,8192]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A0:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[BROADCAST:.*]])\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A1:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD0:.*]])\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A2:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD1:.*]])\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A3:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD2:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n+\n+TEST_F(GpuWindowedEinsumHanlderTest, A2aTransposeLoopsHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,8192,32768]{2,1,0}, bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0})->bf16[1,4,2048,32768]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,8192,32768]{2,1,0} parameter(0)\n+  param.10 = bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} all-to-all(param.10), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={3}\n+  transpose.15 = bf16[1,4,1,8192,1,2048]{5,4,1,3,2,0} transpose(all-to-all), dimensions={0,3,1,2,4,5}\n+  reshape.2170 = bf16[1,4,8192,1,2048]{4,3,2,1,0} reshape(transpose.15)\n+  reshape.2173 = bf16[4,8192,1,2048]{3,2,1,0} reshape(reshape.2170)\n+  transpose.16 = bf16[1,4,2048,8192]{2,0,3,1} transpose(reshape.2173), dimensions={2,0,3,1}\n+  copy.53 = bf16[1,4,2048,8192]{3,2,1,0} copy(transpose.16)\n+  ROOT dot.12 = bf16[1,4,2048,32768]{3,2,1,0} dot(copy.53, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} parameter(1)\n+CHECK-DAG: %[[TRANSPOSE0:.*]] = bf16[1,4,1,8192,1,2048]{5,4,1,3,2,0} transpose(bf16[1,1,8192,4,1,2048]{5,4,3,2,1,0} %[[P1:.*]]), dimensions={0,3,1,2,4,5}\n+CHECK-DAG: %[[RESHAPE0:.*]] = bf16[1,4,8192,1,2048]{4,3,2,1,0} reshape(bf16[1,4,1,8192,1,2048]{5,4,1,3,2,0} %[[TRANSPOSE0:.*]])\n+CHECK-DAG: %[[RESHAPE1:.*]] = bf16[4,8192,1,2048]{3,2,1,0} reshape(bf16[1,4,8192,1,2048]{4,3,2,1,0} %[[RESHAPE0:.*]])\n+CHECK-DAG: %[[TRANSPOSE1:.*]] = bf16[1,4,2048,8192]{2,0,3,1} transpose(bf16[4,8192,1,2048]{3,2,1,0} %[[RESHAPE1:.*]]), dimensions={2,0,3,1}\n+CHECK-DAG: %[[COPY:.*]] = bf16[1,4,2048,8192]{3,2,1,0} copy(bf16[1,4,2048,8192]{2,0,3,1} %[[TRANSPOSE1:.*]])\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [6144:8192]}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE0]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,8192,32768]{2,1,0} parameter(0)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [6144:8192], [0:32768]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A0:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"9\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [4096:6144]}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE1]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [4096:6144], [0:32768]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A1:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"8\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [2048:4096]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE2]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [2048:4096], [0:32768]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A2:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"7\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,2048]{3,2,1,0} slice(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), slice={[0:1], [0:4], [0:2048], [0:2048]}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,2048]{3,2,1,0} all-to-all(bf16[1,4,2048,2048]{3,2,1,0} %[[SLICE3]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,2048,32768]{2,1,0} slice(bf16[1,8192,32768]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:2048], [0:32768]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,32768]{3,2,1,0} dot(bf16[1,4,2048,2048]{3,2,1,0} %[[A2A3:.*]], bf16[1,2048,32768]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"6\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,32768]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT0:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[BROADCAST:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"6\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT1:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD0:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"7\"],\"force_earliest_schedule\":false}\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT2:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD1:.*]]), backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[\"8\"],\"force_earliest_schedule\":false}\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,2048,32768]{3,2,1,0} add(bf16[1,4,2048,32768]{3,2,1,0} %[[DOT3:.*]], bf16[1,4,2048,32768]{3,2,1,0} %[[ADD2:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  EXPECT_TRUE(changed);\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n+\n+TEST_F(GpuWindowedEinsumHanlderTest, GemmA2aTransposeLoopsHaveStreamIds) {\n+  constexpr absl::string_view kHloString = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,4,2048,32768]{3,2,1,0}, bf16[1,32768,8192]{2,1,0})->bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,4,2048,32768]{3,2,1,0} parameter(0)\n+  param.10 = bf16[1,32768,8192]{2,1,0} parameter(1)\n+  dot.13 = bf16[1,4,2048,8192]{3,2,1,0} dot(param.9, param.10), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+  copy.55 = bf16[1,4,2048,8192]{3,2,1,0} copy(dot.13)\n+  transpose.17 = bf16[4,1,2048,8192]{3,2,0,1} transpose(copy.55), dimensions={1,0,2,3}\n+  copy.56 = bf16[4,1,2048,8192]{3,2,1,0} copy(transpose.17)\n+  reshape.2216 = bf16[1,4,1,2048,8192]{4,3,2,1,0} reshape(copy.56)\n+  reshape.2219 = bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0} reshape(reshape.2216)\n+  ROOT all-to-all.1 = bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0} all-to-all(reshape.2219), channel_id=7, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  const char* kExpected = R\"(\n+CHECK: ENTRY\n+CHECK-DAG: %[[P1:.*]] = bf16[1,4,2048,32768]{3,2,1,0} parameter(0)\n+\n+CHECK-DAG: %[[SLICE0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [24576:32768]}\n+CHECK-DAG: %[[P0:.*]] = bf16[1,32768,8192]{2,1,0} parameter(1)\n+CHECK-DAG: %[[SLICE4:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [24576:32768], [0:8192]}\n+CHECK-DAG: %[[DOT0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE0:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE4:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"12\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT0:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [16384:24576]}\n+CHECK-DAG: %[[SLICE5:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [16384:24576], [0:8192]}\n+CHECK-DAG: %[[DOT1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE1:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE5:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"11\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT1:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [8192:16384]}\n+CHECK-DAG: %[[SLICE6:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [8192:16384], [0:8192]}\n+CHECK-DAG: %[[DOT2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE2:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE6:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"10\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT2:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+\n+CHECK-DAG: %[[SLICE3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} slice(bf16[1,4,2048,32768]{3,2,1,0} %[[P1]]), slice={[0:1], [0:4], [0:2048], [0:8192]}\n+CHECK-DAG: %[[SLICE7:.*]] = bf16[1,8192,8192]{2,1,0} slice(bf16[1,32768,8192]{2,1,0} %[[P0:.*]]), slice={[0:1], [0:8192], [0:8192]}\n+CHECK-DAG: %[[DOT3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} dot(bf16[1,4,2048,8192]{3,2,1,0} %[[SLICE3:.*]], bf16[1,8192,8192]{2,1,0} %[[SLICE7:.*]]), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, backend_config={\"operation_queue_id\":\"9\",\"wait_on_operation_queues\":[],\"force_earliest_schedule\":false}\n+CHECK: %[[A2A2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} all-to-all(bf16[1,4,2048,8192]{3,2,1,0} %[[DOT3:.*]]),\n+CHECK: replica_groups={\n+CHECK:     {0,1,2,3}\n+CHECK: }\n+CHECK: dimensions={1}\n+CHECK-DAG: %[[CONSTANT:.*]] = bf16[] constant(0)\n+CHECK-DAG: %[[BROADCAST:.*]] = bf16[1,4,2048,8192]{3,2,1,0} broadcast(bf16[] %[[CONSTANT:.*]]), dimensions={}\n+CHECK-DAG: %[[ADD0:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A0:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[BROADCAST:.*]])\n+CHECK-DAG: %[[ADD1:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A1:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD0:.*]])\n+CHECK-DAG: %[[ADD2:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A2:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD1:.*]])\n+CHECK-DAG: %[[ADD3:.*]] = bf16[1,4,2048,8192]{3,2,1,0} add(bf16[1,4,2048,8192]{3,2,1,0} %[[A2A3:.*]], bf16[1,4,2048,8192]{3,2,1,0} %[[ADD2:.*]])\n+\n+CHECK-DAG: %[[COPY:.*]] = bf16[1,4,2048,8192]{3,2,1,0} copy(bf16[1,4,2048,8192]{3,2,1,0} %[[ADD3:.*]])\n+CHECK-DAG: %[[TRANSPOSE0:.*]] = bf16[4,1,2048,8192]{3,2,0,1} transpose(bf16[1,4,2048,8192]{3,2,1,0} %[[COPY:.*]]), dimensions={1,0,2,3}\n+CHECK-DAG: %[[COPY1:.*]] = bf16[4,1,2048,8192]{3,2,1,0} copy(bf16[4,1,2048,8192]{3,2,0,1} %[[TRANSPOSE0:.*]])\n+CHECK-DAG: %[[RESHAPE0:.*]] = bf16[1,4,1,2048,8192]{4,3,2,1,0} reshape(bf16[4,1,2048,8192]{3,2,1,0} %[[COPY1:.*]])\n+\n+CHECK: ROOT {{.*}} = bf16[1,4,1,1,2048,8192]{5,4,3,2,1,0} reshape(bf16[1,4,1,2048,8192]{4,3,2,1,0} %[[RESHAPE0:.*]])\n+)\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kHloString));\n+\n+  GpuWindowedEinsumHandler gpu_handler;\n+  bool changed;\n+  TF_ASSERT_OK_AND_ASSIGN(changed, gpu_handler.Run(module.get()));\n+  EXPECT_TRUE(changed);\n+  TF_ASSERT_OK_AND_ASSIGN(bool filecheck_matched,\n+                          RunFileCheck(module->ToString(), kExpected));\n+  EXPECT_TRUE(filecheck_matched);\n+}\n \n TEST_F(GpuWindowedEinsumHanlderTest, AllGatherF8) {\n   constexpr absl::string_view kHloString = R\"(\n"
        },
        {
            "name": "ir_emitter_unnested.cc",
            "path": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "patches": [
                {
                    "old_start": 1708,
                    "old_length": 6,
                    "new_start": 1708,
                    "new_length": 49,
                    "hunk": "@@ -1708,6 +1708,49 @@ absl::Status IrEmitterUnnested::EmitFusion(const HloFusionInstruction* instr) {\n   return absl::OkStatus();\n }\n \n+absl::Status IrEmitterUnnested::EmitAsyncCustomCallStart(\n+    const HloInstruction* instr) {\n+  const HloInstruction* wrapped = instr->async_wrapped_instruction();\n+  auto* async_start = Cast<HloAsyncInstruction>(instr);\n+  const ExecutionStreamAssignment& stream_assignment =\n+      ir_emitter_context_->execution_stream_assignment();\n+  TF_ASSIGN_OR_RETURN(\n+      ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n+      stream_assignment.GetAsyncExecutionStreamIds(async_start));\n+  AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n+      Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      streams.destination_stream_id, streams.source_stream_id));\n+  TF_ASSIGN_OR_RETURN(ExecutionStreamId execution_stream_id,\n+                      stream_assignment.GetSyncExecutionStreamId(wrapped));\n+\n+  auto* custom_call = Cast<HloCustomCallInstruction>(wrapped);\n+  if (IsLegacyCublasMatmul(*wrapped)) {\n+    auto status = EmitGemmThunk(custom_call);\n+    if (status.ok()) {\n+      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n+    }\n+    return status;\n+  }\n+#if GOOGLE_CUDA || TF_HIPBLASLT\n+  if (IsCublasLtMatmul(*wrapped)) {\n+    auto status = EmitGemmThunk(custom_call);\n+    if (status.ok()) {\n+      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n+    }\n+    return status;\n+  }\n+  if (IsCublasLtMatmulF8(*wrapped)) {\n+    auto status = EmitGemmThunk(custom_call);\n+    if (status.ok()) {\n+      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n+    }\n+    return status;\n+  }\n+#endif  // GOOGLE_CUDA || TF_HIPBLASLT\n+  return Internal(\"Unsupported async custom call instruction: %s\",\n+                  HloOpcodeString(wrapped->opcode()));\n+}\n+\n absl::Status IrEmitterUnnested::AssertNonDeterminismIsOkay(\n     const std::string& op_name) {\n   if (ir_emitter_context_->debug_options().xla_gpu_deterministic_ops() ||\n"
                },
                {
                    "old_start": 2771,
                    "old_length": 7,
                    "new_start": 2814,
                    "new_length": 8,
                    "hunk": "@@ -2771,7 +2814,8 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n           return EmitNcclAsyncDone(Thunk::kNcclAllToAllDone, instr);\n         case HloOpcode::kCollectiveBroadcast:\n           return EmitNcclAsyncDone(Thunk::kNcclCollectiveBroadcastDone, instr);\n-        case HloOpcode::kFusion: {\n+        case HloOpcode::kFusion:\n+        case HloOpcode::kCustomCall: {\n           // Wait until the concurrent stream has finished.\n           auto* async_done = Cast<HloAsyncInstruction>(instr);\n           const ExecutionStreamAssignment& stream_assignment =\n"
                },
                {
                    "old_start": 2829,
                    "old_length": 6,
                    "new_start": 2873,
                    "new_length": 9,
                    "hunk": "@@ -2829,6 +2873,9 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n               streams.destination_stream_id, streams.source_stream_id));\n           return EmitFusion(Cast<HloFusionInstruction>(wrapped));\n         }\n+        case HloOpcode::kCustomCall: {\n+          return EmitAsyncCustomCallStart(instr);\n+        }\n         default:\n           return Internal(\"Unsupported async start wrapped instruction: %s\",\n                           HloOpcodeString(wrapped->opcode()));\n"
                }
            ],
            "whole_deleted": "-        case HloOpcode::kFusion: {\n",
            "whole_added": "+absl::Status IrEmitterUnnested::EmitAsyncCustomCallStart(\n+    const HloInstruction* instr) {\n+  const HloInstruction* wrapped = instr->async_wrapped_instruction();\n+  auto* async_start = Cast<HloAsyncInstruction>(instr);\n+  const ExecutionStreamAssignment& stream_assignment =\n+      ir_emitter_context_->execution_stream_assignment();\n+  TF_ASSIGN_OR_RETURN(\n+      ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n+      stream_assignment.GetAsyncExecutionStreamIds(async_start));\n+  AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n+      Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      streams.destination_stream_id, streams.source_stream_id));\n+  TF_ASSIGN_OR_RETURN(ExecutionStreamId execution_stream_id,\n+                      stream_assignment.GetSyncExecutionStreamId(wrapped));\n+\n+  auto* custom_call = Cast<HloCustomCallInstruction>(wrapped);\n+  if (IsLegacyCublasMatmul(*wrapped)) {\n+    auto status = EmitGemmThunk(custom_call);\n+    if (status.ok()) {\n+      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n+    }\n+    return status;\n+  }\n+#if GOOGLE_CUDA || TF_HIPBLASLT\n+  if (IsCublasLtMatmul(*wrapped)) {\n+    auto status = EmitGemmThunk(custom_call);\n+    if (status.ok()) {\n+      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n+    }\n+    return status;\n+  }\n+  if (IsCublasLtMatmulF8(*wrapped)) {\n+    auto status = EmitGemmThunk(custom_call);\n+    if (status.ok()) {\n+      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n+    }\n+    return status;\n+  }\n+#endif  // GOOGLE_CUDA || TF_HIPBLASLT\n+  return Internal(\"Unsupported async custom call instruction: %s\",\n+                  HloOpcodeString(wrapped->opcode()));\n+}\n+\n+        case HloOpcode::kFusion:\n+        case HloOpcode::kCustomCall: {\n+        case HloOpcode::kCustomCall: {\n+          return EmitAsyncCustomCallStart(instr);\n+        }\n",
            "whole_hunk": "@@ -1708,6 +1708,49 @@ absl::Status IrEmitterUnnested::EmitFusion(const HloFusionInstruction* instr) {\n   return absl::OkStatus();\n }\n \n+absl::Status IrEmitterUnnested::EmitAsyncCustomCallStart(\n+    const HloInstruction* instr) {\n+  const HloInstruction* wrapped = instr->async_wrapped_instruction();\n+  auto* async_start = Cast<HloAsyncInstruction>(instr);\n+  const ExecutionStreamAssignment& stream_assignment =\n+      ir_emitter_context_->execution_stream_assignment();\n+  TF_ASSIGN_OR_RETURN(\n+      ExecutionStreamAssignment::AsyncExecutionStreamIds streams,\n+      stream_assignment.GetAsyncExecutionStreamIds(async_start));\n+  AddThunkToThunkSequence(std::make_unique<WaitForStreamsThunk>(\n+      Thunk::ThunkInfo::WithProfileAnnotation(instr),\n+      streams.destination_stream_id, streams.source_stream_id));\n+  TF_ASSIGN_OR_RETURN(ExecutionStreamId execution_stream_id,\n+                      stream_assignment.GetSyncExecutionStreamId(wrapped));\n+\n+  auto* custom_call = Cast<HloCustomCallInstruction>(wrapped);\n+  if (IsLegacyCublasMatmul(*wrapped)) {\n+    auto status = EmitGemmThunk(custom_call);\n+    if (status.ok()) {\n+      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n+    }\n+    return status;\n+  }\n+#if GOOGLE_CUDA || TF_HIPBLASLT\n+  if (IsCublasLtMatmul(*wrapped)) {\n+    auto status = EmitGemmThunk(custom_call);\n+    if (status.ok()) {\n+      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n+    }\n+    return status;\n+  }\n+  if (IsCublasLtMatmulF8(*wrapped)) {\n+    auto status = EmitGemmThunk(custom_call);\n+    if (status.ok()) {\n+      thunk_sequence_.back()->set_execution_stream_id(execution_stream_id);\n+    }\n+    return status;\n+  }\n+#endif  // GOOGLE_CUDA || TF_HIPBLASLT\n+  return Internal(\"Unsupported async custom call instruction: %s\",\n+                  HloOpcodeString(wrapped->opcode()));\n+}\n+\n absl::Status IrEmitterUnnested::AssertNonDeterminismIsOkay(\n     const std::string& op_name) {\n   if (ir_emitter_context_->debug_options().xla_gpu_deterministic_ops() ||\n@@ -2771,7 +2814,8 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n           return EmitNcclAsyncDone(Thunk::kNcclAllToAllDone, instr);\n         case HloOpcode::kCollectiveBroadcast:\n           return EmitNcclAsyncDone(Thunk::kNcclCollectiveBroadcastDone, instr);\n-        case HloOpcode::kFusion: {\n+        case HloOpcode::kFusion:\n+        case HloOpcode::kCustomCall: {\n           // Wait until the concurrent stream has finished.\n           auto* async_done = Cast<HloAsyncInstruction>(instr);\n           const ExecutionStreamAssignment& stream_assignment =\n@@ -2829,6 +2873,9 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(\n               streams.destination_stream_id, streams.source_stream_id));\n           return EmitFusion(Cast<HloFusionInstruction>(wrapped));\n         }\n+        case HloOpcode::kCustomCall: {\n+          return EmitAsyncCustomCallStart(instr);\n+        }\n         default:\n           return Internal(\"Unsupported async start wrapped instruction: %s\",\n                           HloOpcodeString(wrapped->opcode()));\n"
        },
        {
            "name": "ir_emitter_unnested.h",
            "path": "third_party/xla/xla/service/gpu/ir_emitter_unnested.h",
            "patches": [
                {
                    "old_start": 155,
                    "old_length": 6,
                    "new_start": 155,
                    "new_length": 7,
                    "hunk": "@@ -155,6 +155,7 @@ class IrEmitterUnnested : public IrEmitter {\n   absl::Status EmitCustomCallThunk(const HloCustomCallInstruction* instr);\n   absl::Status EmitFftThunk(const HloFftInstruction* instr);\n   absl::Status EmitFusion(const HloFusionInstruction* instr);\n+  absl::Status EmitAsyncCustomCallStart(const HloInstruction* instr);\n   absl::Status EmitSelectAndScatter(\n       const HloSelectAndScatterInstruction* instr);\n   absl::Status EmitWhile(const HloInstruction* instr);\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  absl::Status EmitAsyncCustomCallStart(const HloInstruction* instr);\n",
            "whole_hunk": "@@ -155,6 +155,7 @@ class IrEmitterUnnested : public IrEmitter {\n   absl::Status EmitCustomCallThunk(const HloCustomCallInstruction* instr);\n   absl::Status EmitFftThunk(const HloFftInstruction* instr);\n   absl::Status EmitFusion(const HloFusionInstruction* instr);\n+  absl::Status EmitAsyncCustomCallStart(const HloInstruction* instr);\n   absl::Status EmitSelectAndScatter(\n       const HloSelectAndScatterInstruction* instr);\n   absl::Status EmitWhile(const HloInstruction* instr);\n"
        },
        {
            "name": "collective_ops_test_e2e.cc",
            "path": "third_party/xla/xla/tests/collective_ops_test_e2e.cc",
            "patches": [
                {
                    "old_start": 781,
                    "old_length": 5,
                    "new_start": 781,
                    "new_length": 79,
                    "hunk": "@@ -781,5 +781,79 @@ ENTRY main.12 {\n \n   CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n }\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EAllToAllDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,128,64]{2,1,0}, bf16[1,4,64,128]{3,2,1,0})->bf16[1,4,64,64]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param0 = bf16[1,128,64]{2,1,0} parameter(0)\n+  param1 = bf16[1,4,64,128]{3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,4,64,128]{3,2,1,0} all-to-all(param1), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={1}\n+  ROOT dot.12 = bf16[1,4,64,64]{3,2,1,0} dot(all-to-all, param0), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EAllToAllTransposeDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,64,128]{2,1,0}, bf16[1,1,64,4,1,32]{5,4,3,2,1,0})->bf16[1,4,32,128]{3,2,1,0}}, num_partitions=4\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,64,128]{2,1,0} parameter(0)\n+  param.10 = bf16[1,1,64,4,1,32]{5,4,3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,1,64,4,1,32]{5,4,3,2,1,0} all-to-all(param.10), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={3}\n+  transpose.15 = bf16[1,4,1,64,1,32]{5,4,1,3,2,0} transpose(all-to-all), dimensions={0,3,1,2,4,5}\n+  reshape.2170 = bf16[1,4,64,1,32]{4,3,2,1,0} reshape(transpose.15)\n+  reshape.2173 = bf16[4,64,1,32]{3,2,1,0} reshape(reshape.2170)\n+  transpose.16 = bf16[1,4,32,64]{2,0,3,1} transpose(reshape.2173), dimensions={2,0,3,1}\n+  copy.53 = bf16[1,4,32,64]{3,2,1,0} copy(transpose.16)\n+  ROOT dot.12 = bf16[1,4,32,128]{3,2,1,0} dot(copy.53, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EGemmAllToAllDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,64,128]{2,1,0}, bf16[1,4,32,128]{3,2,1,0})->bf16[1,4,32,64]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,64,128]{2,1,0} parameter(0)\n+  param.10 = bf16[1,4,32,128]{3,2,1,0} parameter(1)\n+  dot.12 = bf16[1,4,32,64]{3,2,1,0} dot(param.10, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}\n+  ROOT all-to-all = bf16[1,4,32,64]{3,2,1,0} all-to-all(dot.12), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EGemmAllToAllTransposeDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,4,32,128]{3,2,1,0}, bf16[1,128,64]{2,1,0})->bf16[1,4,1,1,32,64]{5,4,3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,4,32,128]{3,2,1,0} parameter(0)\n+  param.10 = bf16[1,128,64]{2,1,0} parameter(1)\n+  dot.13 = bf16[1,4,32,64]{3,2,1,0} dot(param.9, param.10), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+  copy.55 = bf16[1,4,32,64]{3,2,1,0} copy(dot.13)\n+  transpose.17 = bf16[4,1,32,64]{3,2,0,1} transpose(copy.55), dimensions={1,0,2,3}\n+  copy.56 = bf16[4,1,32,64]{3,2,1,0} copy(transpose.17)\n+  reshape.2216 = bf16[1,4,1,32,64]{4,3,2,1,0} reshape(copy.56)\n+  reshape.2219 = bf16[1,4,1,1,32,64]{5,4,3,2,1,0} reshape(reshape.2216)\n+  ROOT all-to-all.1 = bf16[1,4,1,1,32,64]{5,4,3,2,1,0} all-to-all(reshape.2219), channel_id=7, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n }  // namespace\n }  // namespace xla"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EAllToAllDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,128,64]{2,1,0}, bf16[1,4,64,128]{3,2,1,0})->bf16[1,4,64,64]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param0 = bf16[1,128,64]{2,1,0} parameter(0)\n+  param1 = bf16[1,4,64,128]{3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,4,64,128]{3,2,1,0} all-to-all(param1), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={1}\n+  ROOT dot.12 = bf16[1,4,64,64]{3,2,1,0} dot(all-to-all, param0), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EAllToAllTransposeDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,64,128]{2,1,0}, bf16[1,1,64,4,1,32]{5,4,3,2,1,0})->bf16[1,4,32,128]{3,2,1,0}}, num_partitions=4\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,64,128]{2,1,0} parameter(0)\n+  param.10 = bf16[1,1,64,4,1,32]{5,4,3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,1,64,4,1,32]{5,4,3,2,1,0} all-to-all(param.10), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={3}\n+  transpose.15 = bf16[1,4,1,64,1,32]{5,4,1,3,2,0} transpose(all-to-all), dimensions={0,3,1,2,4,5}\n+  reshape.2170 = bf16[1,4,64,1,32]{4,3,2,1,0} reshape(transpose.15)\n+  reshape.2173 = bf16[4,64,1,32]{3,2,1,0} reshape(reshape.2170)\n+  transpose.16 = bf16[1,4,32,64]{2,0,3,1} transpose(reshape.2173), dimensions={2,0,3,1}\n+  copy.53 = bf16[1,4,32,64]{3,2,1,0} copy(transpose.16)\n+  ROOT dot.12 = bf16[1,4,32,128]{3,2,1,0} dot(copy.53, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EGemmAllToAllDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,64,128]{2,1,0}, bf16[1,4,32,128]{3,2,1,0})->bf16[1,4,32,64]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,64,128]{2,1,0} parameter(0)\n+  param.10 = bf16[1,4,32,128]{3,2,1,0} parameter(1)\n+  dot.12 = bf16[1,4,32,64]{3,2,1,0} dot(param.10, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}\n+  ROOT all-to-all = bf16[1,4,32,64]{3,2,1,0} all-to-all(dot.12), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EGemmAllToAllTransposeDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,4,32,128]{3,2,1,0}, bf16[1,128,64]{2,1,0})->bf16[1,4,1,1,32,64]{5,4,3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,4,32,128]{3,2,1,0} parameter(0)\n+  param.10 = bf16[1,128,64]{2,1,0} parameter(1)\n+  dot.13 = bf16[1,4,32,64]{3,2,1,0} dot(param.9, param.10), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+  copy.55 = bf16[1,4,32,64]{3,2,1,0} copy(dot.13)\n+  transpose.17 = bf16[4,1,32,64]{3,2,0,1} transpose(copy.55), dimensions={1,0,2,3}\n+  copy.56 = bf16[4,1,32,64]{3,2,1,0} copy(transpose.17)\n+  reshape.2216 = bf16[1,4,1,32,64]{4,3,2,1,0} reshape(copy.56)\n+  reshape.2219 = bf16[1,4,1,1,32,64]{5,4,3,2,1,0} reshape(reshape.2216)\n+  ROOT all-to-all.1 = bf16[1,4,1,1,32,64]{5,4,3,2,1,0} all-to-all(reshape.2219), channel_id=7, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n",
            "whole_hunk": "@@ -781,5 +781,79 @@ ENTRY main.12 {\n \n   CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n }\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EAllToAllDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,128,64]{2,1,0}, bf16[1,4,64,128]{3,2,1,0})->bf16[1,4,64,64]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param0 = bf16[1,128,64]{2,1,0} parameter(0)\n+  param1 = bf16[1,4,64,128]{3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,4,64,128]{3,2,1,0} all-to-all(param1), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={1}\n+  ROOT dot.12 = bf16[1,4,64,64]{3,2,1,0} dot(all-to-all, param0), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EAllToAllTransposeDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,64,128]{2,1,0}, bf16[1,1,64,4,1,32]{5,4,3,2,1,0})->bf16[1,4,32,128]{3,2,1,0}}, num_partitions=4\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,64,128]{2,1,0} parameter(0)\n+  param.10 = bf16[1,1,64,4,1,32]{5,4,3,2,1,0} parameter(1)\n+  all-to-all = bf16[1,1,64,4,1,32]{5,4,3,2,1,0} all-to-all(param.10), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={3}\n+  transpose.15 = bf16[1,4,1,64,1,32]{5,4,1,3,2,0} transpose(all-to-all), dimensions={0,3,1,2,4,5}\n+  reshape.2170 = bf16[1,4,64,1,32]{4,3,2,1,0} reshape(transpose.15)\n+  reshape.2173 = bf16[4,64,1,32]{3,2,1,0} reshape(reshape.2170)\n+  transpose.16 = bf16[1,4,32,64]{2,0,3,1} transpose(reshape.2173), dimensions={2,0,3,1}\n+  copy.53 = bf16[1,4,32,64]{3,2,1,0} copy(transpose.16)\n+  ROOT dot.12 = bf16[1,4,32,128]{3,2,1,0} dot(copy.53, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EGemmAllToAllDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,64,128]{2,1,0}, bf16[1,4,32,128]{3,2,1,0})->bf16[1,4,32,64]{3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,64,128]{2,1,0} parameter(0)\n+  param.10 = bf16[1,4,32,128]{3,2,1,0} parameter(1)\n+  dot.12 = bf16[1,4,32,64]{3,2,1,0} dot(param.10, param.9), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={2}\n+  ROOT all-to-all = bf16[1,4,32,64]{3,2,1,0} all-to-all(dot.12), channel_id=4, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n+TEST_F(CollectiveOpsTestE2EWindowedNonWindowed,\n+       WindowedEinsumE2EGemmAllToAllTransposeDecompose) {\n+  absl::string_view kModuleReplicatedStr = R\"(\n+HloModule pjit__unnamed_wrapped_function_, entry_computation_layout={(bf16[1,4,32,128]{3,2,1,0}, bf16[1,128,64]{2,1,0})->bf16[1,4,1,1,32,64]{5,4,3,2,1,0}}, num_partitions=4\n+\n+ENTRY main.9_spmd {\n+  param.9 = bf16[1,4,32,128]{3,2,1,0} parameter(0)\n+  param.10 = bf16[1,128,64]{2,1,0} parameter(1)\n+  dot.13 = bf16[1,4,32,64]{3,2,1,0} dot(param.9, param.10), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n+  copy.55 = bf16[1,4,32,64]{3,2,1,0} copy(dot.13)\n+  transpose.17 = bf16[4,1,32,64]{3,2,0,1} transpose(copy.55), dimensions={1,0,2,3}\n+  copy.56 = bf16[4,1,32,64]{3,2,1,0} copy(transpose.17)\n+  reshape.2216 = bf16[1,4,1,32,64]{4,3,2,1,0} reshape(copy.56)\n+  reshape.2219 = bf16[1,4,1,1,32,64]{5,4,3,2,1,0} reshape(reshape.2216)\n+  ROOT all-to-all.1 = bf16[1,4,1,1,32,64]{5,4,3,2,1,0} all-to-all(reshape.2219), channel_id=7, replica_groups={{0,1,2,3}}, dimensions={1}\n+}\n+)\";\n+\n+  CollectiveOpsCompareWindowedNonWindowed(kModuleReplicatedStr);\n+}\n+\n }  // namespace\n }  // namespace xla"
        }
    ]
},
{
    "Id": 429,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/84d7bf6f64fd9c8677f7f26511ce3031fe8d35a6",
    "date": "2023-05-09T14:44:28-04:00",
    "message": "Add is_numeric to dtypes.cc to check whether a data type is numeric",
    "label": "NO",
    "changes": [
        {
            "name": "dtypes.cc",
            "path": "tensorflow/python/framework/dtypes.cc",
            "patches": [
                {
                    "old_start": 120,
                    "old_length": 6,
                    "new_start": 120,
                    "new_length": 12,
                    "hunk": "@@ -120,6 +120,12 @@ PYBIND11_MODULE(_dtypes, m) {\n             return tensorflow::BaseType(self) == tensorflow::DT_BOOL;\n           },\n           \"Returns whether this is a boolean data type.\")\n+      .def_property_readonly(\n+          \"is_numeric\",\n+          [](tensorflow::DataType self) {\n+            return tensorflow::DataTypeIsNumeric(tensorflow::BaseType(self));\n+          },\n+          \"Returns whether this is a numeric data type.\")\n       .def_property_readonly(\n           \"is_complex\",\n           [](tensorflow::DataType self) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      .def_property_readonly(\n+          \"is_numeric\",\n+          [](tensorflow::DataType self) {\n+            return tensorflow::DataTypeIsNumeric(tensorflow::BaseType(self));\n+          },\n+          \"Returns whether this is a numeric data type.\")\n",
            "whole_hunk": "@@ -120,6 +120,12 @@ PYBIND11_MODULE(_dtypes, m) {\n             return tensorflow::BaseType(self) == tensorflow::DT_BOOL;\n           },\n           \"Returns whether this is a boolean data type.\")\n+      .def_property_readonly(\n+          \"is_numeric\",\n+          [](tensorflow::DataType self) {\n+            return tensorflow::DataTypeIsNumeric(tensorflow::BaseType(self));\n+          },\n+          \"Returns whether this is a numeric data type.\")\n       .def_property_readonly(\n           \"is_complex\",\n           [](tensorflow::DataType self) {"
        }
    ]
},
{
    "Id": 165,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fc24a5a3b3b79cb6eec10d39990d4eea3412b45b",
    "date": "2024-02-06T11:44:09-08:00",
    "message": "[XLA] Propagate side effects for async ops\n\nSince *-{update,done} ops no longer have a direct reference to the wrapped computation, the existing check for op side effects isn't able to trivially find the computation. As a side effect of this, DCE ends up purging these async ops.\n\nDeffer side effect checks for async ops to their respective wrapped op.\n\nPiperOrigin-RevId: 604707924",
    "label": "NO",
    "changes": [
        {
            "name": "hlo_instruction.h",
            "path": "third_party/xla/xla/hlo/ir/hlo_instruction.h",
            "patches": [
                {
                    "old_start": 1369,
                    "old_length": 7,
                    "new_start": 1369,
                    "new_length": 7,
                    "hunk": "@@ -1369,7 +1369,7 @@ class HloInstruction {\n   // Returns true if this instruction has a side effect. An instruction has a\n   // side effect if it uses certain opcodes or calls a computation with a side\n   // effect.\n-  bool HasSideEffect() const;\n+  virtual bool HasSideEffect() const;\n \n   // Returns the result shape of this instruction.\n   const Shape& shape() const;\n"
                }
            ],
            "whole_deleted": "-  bool HasSideEffect() const;\n",
            "whole_added": "+  virtual bool HasSideEffect() const;\n",
            "whole_hunk": "@@ -1369,7 +1369,7 @@ class HloInstruction {\n   // Returns true if this instruction has a side effect. An instruction has a\n   // side effect if it uses certain opcodes or calls a computation with a side\n   // effect.\n-  bool HasSideEffect() const;\n+  virtual bool HasSideEffect() const;\n \n   // Returns the result shape of this instruction.\n   const Shape& shape() const;\n"
        },
        {
            "name": "hlo_instructions.h",
            "path": "third_party/xla/xla/hlo/ir/hlo_instructions.h",
            "patches": [
                {
                    "old_start": 264,
                    "old_length": 6,
                    "new_start": 264,
                    "new_length": 10,
                    "hunk": "@@ -264,6 +264,10 @@ class HloAsyncInstruction : public HloInstruction {\n   // *end(GetAsyncChain()) is the async-done op.\n   std::vector<HloAsyncInstruction*> GetAsyncChain() const;\n \n+  bool HasSideEffect() const override {\n+    return async_wrapped_instruction()->HasSideEffect();\n+  }\n+\n  protected:\n   // Helper to constructs async-{start,update,done}.\n   HloAsyncInstruction(HloOpcode opcode, const Shape& shape,\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  bool HasSideEffect() const override {\n+    return async_wrapped_instruction()->HasSideEffect();\n+  }\n+\n",
            "whole_hunk": "@@ -264,6 +264,10 @@ class HloAsyncInstruction : public HloInstruction {\n   // *end(GetAsyncChain()) is the async-done op.\n   std::vector<HloAsyncInstruction*> GetAsyncChain() const;\n \n+  bool HasSideEffect() const override {\n+    return async_wrapped_instruction()->HasSideEffect();\n+  }\n+\n  protected:\n   // Helper to constructs async-{start,update,done}.\n   HloAsyncInstruction(HloOpcode opcode, const Shape& shape,\n"
        },
        {
            "name": "hlo_dce_test.cc",
            "path": "third_party/xla/xla/service/hlo_dce_test.cc",
            "patches": [
                {
                    "old_start": 111,
                    "old_length": 6,
                    "new_start": 111,
                    "new_length": 30,
                    "hunk": "@@ -111,6 +111,30 @@ TEST_F(HloDceTest, CustomCallInstructionsWithSideEffect) {\n   EXPECT_FALSE(result);\n }\n \n+TEST_F(HloDceTest, AsyncCustomCallInstructionsWithSideEffect) {\n+  // Verify that custom call instruction with side-effect is not removed.\n+  auto builder = HloComputation::Builder(TestName());\n+  auto instr = Cast<HloCustomCallInstruction>(builder.AddInstruction(\n+      HloInstruction::CreateCustomCall(ShapeUtil::MakeShape(F32, {}),\n+                                       /*operands=*/{},\n+                                       /*custom_call_target=*/\"foo\")));\n+  instr->set_custom_call_has_side_effect(true);\n+  builder.AddInstruction(HloInstruction::CreateTuple({}));\n+\n+  auto module = CreateNewVerifiedModule();\n+  module->AddEntryComputation(builder.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN([[maybe_unused]] HloInstruction * async_done,\n+                          module->entry_computation()->CreateAsyncInstructions(\n+                              instr, {{ShapeUtil::MakeScalarShape(U32)}},\n+                              HloInstruction::kMainExecutionThread,\n+                              /*replace=*/true, /*override_names=*/true));\n+\n+  HloDCE dce;\n+  TF_ASSERT_OK_AND_ASSIGN(bool result, RunHloPass(&dce, module.get()));\n+  EXPECT_FALSE(result);\n+}\n+\n TEST_F(HloDceTest, CustomCallInstructionsWithoutSideEffect) {\n   // Verify that custom call instruction without side-effect is removed.\n   auto builder = HloComputation::Builder(TestName());"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(HloDceTest, AsyncCustomCallInstructionsWithSideEffect) {\n+  // Verify that custom call instruction with side-effect is not removed.\n+  auto builder = HloComputation::Builder(TestName());\n+  auto instr = Cast<HloCustomCallInstruction>(builder.AddInstruction(\n+      HloInstruction::CreateCustomCall(ShapeUtil::MakeShape(F32, {}),\n+                                       /*operands=*/{},\n+                                       /*custom_call_target=*/\"foo\")));\n+  instr->set_custom_call_has_side_effect(true);\n+  builder.AddInstruction(HloInstruction::CreateTuple({}));\n+\n+  auto module = CreateNewVerifiedModule();\n+  module->AddEntryComputation(builder.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN([[maybe_unused]] HloInstruction * async_done,\n+                          module->entry_computation()->CreateAsyncInstructions(\n+                              instr, {{ShapeUtil::MakeScalarShape(U32)}},\n+                              HloInstruction::kMainExecutionThread,\n+                              /*replace=*/true, /*override_names=*/true));\n+\n+  HloDCE dce;\n+  TF_ASSERT_OK_AND_ASSIGN(bool result, RunHloPass(&dce, module.get()));\n+  EXPECT_FALSE(result);\n+}\n+\n",
            "whole_hunk": "@@ -111,6 +111,30 @@ TEST_F(HloDceTest, CustomCallInstructionsWithSideEffect) {\n   EXPECT_FALSE(result);\n }\n \n+TEST_F(HloDceTest, AsyncCustomCallInstructionsWithSideEffect) {\n+  // Verify that custom call instruction with side-effect is not removed.\n+  auto builder = HloComputation::Builder(TestName());\n+  auto instr = Cast<HloCustomCallInstruction>(builder.AddInstruction(\n+      HloInstruction::CreateCustomCall(ShapeUtil::MakeShape(F32, {}),\n+                                       /*operands=*/{},\n+                                       /*custom_call_target=*/\"foo\")));\n+  instr->set_custom_call_has_side_effect(true);\n+  builder.AddInstruction(HloInstruction::CreateTuple({}));\n+\n+  auto module = CreateNewVerifiedModule();\n+  module->AddEntryComputation(builder.Build());\n+\n+  TF_ASSERT_OK_AND_ASSIGN([[maybe_unused]] HloInstruction * async_done,\n+                          module->entry_computation()->CreateAsyncInstructions(\n+                              instr, {{ShapeUtil::MakeScalarShape(U32)}},\n+                              HloInstruction::kMainExecutionThread,\n+                              /*replace=*/true, /*override_names=*/true));\n+\n+  HloDCE dce;\n+  TF_ASSERT_OK_AND_ASSIGN(bool result, RunHloPass(&dce, module.get()));\n+  EXPECT_FALSE(result);\n+}\n+\n TEST_F(HloDceTest, CustomCallInstructionsWithoutSideEffect) {\n   // Verify that custom call instruction without side-effect is removed.\n   auto builder = HloComputation::Builder(TestName());"
        }
    ]
},
{
    "Id": 234,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9da5526ee0e967a829c730f9851f7d83871a1dde",
    "date": "2023-11-29T15:19:42-08:00",
    "message": "Remove math_ops.py's indirect dependency on resource_variable_ops.py by replacing the isinstance checks with checks in the C++ layer.\n\nPiperOrigin-RevId: 586466978",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/python/ops/BUILD",
            "patches": [
                {
                    "old_start": 2081,
                    "old_length": 6,
                    "new_start": 2081,
                    "new_length": 7,
                    "hunk": "@@ -2081,6 +2081,7 @@ py_strict_library(\n         \"//tensorflow/python/framework:tensor_util\",\n         \"//tensorflow/python/ops/numpy_ops:np_dtypes\",\n         \"//tensorflow/python/platform:tf_logging\",\n+        \"//tensorflow/python/util:_pywrap_utils\",\n         \"//tensorflow/python/util:compat\",\n         \"//tensorflow/python/util:deprecation\",\n         \"//tensorflow/python/util:dispatch\",\n"
                },
                {
                    "old_start": 2115,
                    "old_length": 7,
                    "new_start": 2116,
                    "new_length": 6,
                    "hunk": "@@ -2115,7 +2116,6 @@ py_strict_library(\n         \":array_ops\",\n         \":array_ops_gen\",\n         \":handle_data_util\",\n-        \":math_ops\",\n         \":resource_variable_ops_gen\",\n         \":state_ops\",\n         \":state_ops_gen\",\n"
                }
            ],
            "whole_deleted": "-        \":math_ops\",\n",
            "whole_added": "+        \"//tensorflow/python/util:_pywrap_utils\",\n",
            "whole_hunk": "@@ -2081,6 +2081,7 @@ py_strict_library(\n         \"//tensorflow/python/framework:tensor_util\",\n         \"//tensorflow/python/ops/numpy_ops:np_dtypes\",\n         \"//tensorflow/python/platform:tf_logging\",\n+        \"//tensorflow/python/util:_pywrap_utils\",\n         \"//tensorflow/python/util:compat\",\n         \"//tensorflow/python/util:deprecation\",\n         \"//tensorflow/python/util:dispatch\",\n@@ -2115,7 +2116,6 @@ py_strict_library(\n         \":array_ops\",\n         \":array_ops_gen\",\n         \":handle_data_util\",\n-        \":math_ops\",\n         \":resource_variable_ops_gen\",\n         \":state_ops\",\n         \":state_ops_gen\",\n"
        },
        {
            "name": "math_ops.py",
            "path": "tensorflow/python/ops/math_ops.py",
            "patches": [
                {
                    "old_start": 95,
                    "old_length": 6,
                    "new_start": 95,
                    "new_length": 7,
                    "hunk": "@@ -95,6 +95,7 @@ from tensorflow.python.ops.gen_math_ops import *\n # pylint: enable=wildcard-import\n from tensorflow.python.ops.numpy_ops import np_dtypes\n from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util import _pywrap_utils\n from tensorflow.python.util import compat\n from tensorflow.python.util import deprecation\n from tensorflow.python.util import dispatch\n"
                },
                {
                    "old_start": 233,
                    "old_length": 11,
                    "new_start": 234,
                    "new_length": 6,
                    "hunk": "@@ -233,11 +234,6 @@ tf_export(v1=[\"arg_max\"])(dispatch.add_dispatch_support(arg_max))\n tf_export(v1=[\"arg_min\"])(dispatch.add_dispatch_support(arg_min))\n \n \n-# This is set by resource_variable_ops.py. It is included in this way since\n-# there is a circular dependency between math_ops and resource_variable_ops\n-_resource_variable_type = None\n-\n-\n def _set_doc(doc):\n \n   def _decorator(func):\n"
                },
                {
                    "old_start": 997,
                    "old_length": 8,
                    "new_start": 993,
                    "new_length": 9,
                    "hunk": "@@ -997,8 +993,9 @@ def cast(x, dtype, name=None):\n \n   \"\"\"\n   base_type = dtypes.as_dtype(dtype).base_dtype\n-  if isinstance(\n-      x, (tensor_lib.Tensor, _resource_variable_type)) and base_type == x.dtype:\n+  if (\n+      isinstance(x, tensor_lib.Tensor) or _pywrap_utils.IsResourceVariable(x)\n+  ) and base_type == x.dtype:\n     return x\n   with ops.name_scope(name, \"Cast\", [x]) as name:\n     if isinstance(x, sparse_tensor.SparseTensor):\n"
                },
                {
                    "old_start": 3755,
                    "old_length": 9,
                    "new_start": 3752,
                    "new_length": 12,
                    "hunk": "@@ -3755,9 +3752,12 @@ def matmul(a,\n           f\"`adjoint_b`={adjoint_b}.\")\n \n     if context.executing_eagerly():\n-      if not isinstance(a, (ops.EagerTensor, _resource_variable_type)):\n+      if not (\n+          isinstance(a, ops.EagerTensor) or _pywrap_utils.IsResourceVariable(a)\n+      ):\n         a = ops.convert_to_tensor(a, name=\"a\")\n-      if not isinstance(b, (ops.EagerTensor, _resource_variable_type)):\n+      if not isinstance(b, ops.EagerTensor) or _pywrap_utils.IsResourceVariable(\n+          b):\n         b = ops.convert_to_tensor(b, dtype_hint=a.dtype.base_dtype, name=\"b\")\n     else:\n       a = ops.convert_to_tensor(a, name=\"a\")\n"
                }
            ],
            "whole_deleted": "-# This is set by resource_variable_ops.py. It is included in this way since\n-# there is a circular dependency between math_ops and resource_variable_ops\n-_resource_variable_type = None\n-\n-\n-  if isinstance(\n-      x, (tensor_lib.Tensor, _resource_variable_type)) and base_type == x.dtype:\n-      if not isinstance(a, (ops.EagerTensor, _resource_variable_type)):\n-      if not isinstance(b, (ops.EagerTensor, _resource_variable_type)):\n",
            "whole_added": "+from tensorflow.python.util import _pywrap_utils\n+  if (\n+      isinstance(x, tensor_lib.Tensor) or _pywrap_utils.IsResourceVariable(x)\n+  ) and base_type == x.dtype:\n+      if not (\n+          isinstance(a, ops.EagerTensor) or _pywrap_utils.IsResourceVariable(a)\n+      ):\n+      if not isinstance(b, ops.EagerTensor) or _pywrap_utils.IsResourceVariable(\n+          b):\n",
            "whole_hunk": "@@ -95,6 +95,7 @@ from tensorflow.python.ops.gen_math_ops import *\n # pylint: enable=wildcard-import\n from tensorflow.python.ops.numpy_ops import np_dtypes\n from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util import _pywrap_utils\n from tensorflow.python.util import compat\n from tensorflow.python.util import deprecation\n from tensorflow.python.util import dispatch\n@@ -233,11 +234,6 @@ tf_export(v1=[\"arg_max\"])(dispatch.add_dispatch_support(arg_max))\n tf_export(v1=[\"arg_min\"])(dispatch.add_dispatch_support(arg_min))\n \n \n-# This is set by resource_variable_ops.py. It is included in this way since\n-# there is a circular dependency between math_ops and resource_variable_ops\n-_resource_variable_type = None\n-\n-\n def _set_doc(doc):\n \n   def _decorator(func):\n@@ -997,8 +993,9 @@ def cast(x, dtype, name=None):\n \n   \"\"\"\n   base_type = dtypes.as_dtype(dtype).base_dtype\n-  if isinstance(\n-      x, (tensor_lib.Tensor, _resource_variable_type)) and base_type == x.dtype:\n+  if (\n+      isinstance(x, tensor_lib.Tensor) or _pywrap_utils.IsResourceVariable(x)\n+  ) and base_type == x.dtype:\n     return x\n   with ops.name_scope(name, \"Cast\", [x]) as name:\n     if isinstance(x, sparse_tensor.SparseTensor):\n@@ -3755,9 +3752,12 @@ def matmul(a,\n           f\"`adjoint_b`={adjoint_b}.\")\n \n     if context.executing_eagerly():\n-      if not isinstance(a, (ops.EagerTensor, _resource_variable_type)):\n+      if not (\n+          isinstance(a, ops.EagerTensor) or _pywrap_utils.IsResourceVariable(a)\n+      ):\n         a = ops.convert_to_tensor(a, name=\"a\")\n-      if not isinstance(b, (ops.EagerTensor, _resource_variable_type)):\n+      if not isinstance(b, ops.EagerTensor) or _pywrap_utils.IsResourceVariable(\n+          b):\n         b = ops.convert_to_tensor(b, dtype_hint=a.dtype.base_dtype, name=\"b\")\n     else:\n       a = ops.convert_to_tensor(a, name=\"a\")\n"
        },
        {
            "name": "resource_variable_ops.py",
            "path": "tensorflow/python/ops/resource_variable_ops.py",
            "patches": [
                {
                    "old_start": 49,
                    "old_length": 7,
                    "new_start": 49,
                    "new_length": 6,
                    "hunk": "@@ -49,7 +49,6 @@ from tensorflow.python.ops import gen_array_ops\n from tensorflow.python.ops import gen_resource_variable_ops\n from tensorflow.python.ops import gen_state_ops\n from tensorflow.python.ops import handle_data_util\n-from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import state_ops\n from tensorflow.python.ops import variables\n # go/tf-wildcard-import\n"
                },
                {
                    "old_start": 2333,
                    "old_length": 7,
                    "new_start": 2332,
                    "new_length": 6,
                    "hunk": "@@ -2333,7 +2332,6 @@ class UninitializedVariable(BaseResourceVariable):\n \n \n _pywrap_utils.RegisterType(\"ResourceVariable\", ResourceVariable)\n-math_ops._resource_variable_type = ResourceVariable  # pylint: disable=protected-access\n \n \n def _dense_var_to_tensor(var, dtype=None, name=None, as_ref=False):"
                }
            ],
            "whole_deleted": "-from tensorflow.python.ops import math_ops\n-math_ops._resource_variable_type = ResourceVariable  # pylint: disable=protected-access\n",
            "whole_added": "",
            "whole_hunk": "@@ -49,7 +49,6 @@ from tensorflow.python.ops import gen_array_ops\n from tensorflow.python.ops import gen_resource_variable_ops\n from tensorflow.python.ops import gen_state_ops\n from tensorflow.python.ops import handle_data_util\n-from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import state_ops\n from tensorflow.python.ops import variables\n # go/tf-wildcard-import\n@@ -2333,7 +2332,6 @@ class UninitializedVariable(BaseResourceVariable):\n \n \n _pywrap_utils.RegisterType(\"ResourceVariable\", ResourceVariable)\n-math_ops._resource_variable_type = ResourceVariable  # pylint: disable=protected-access\n \n \n def _dense_var_to_tensor(var, dtype=None, name=None, as_ref=False):"
        }
    ]
},
{
    "Id": 265,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/86aa7ef960146497a436ffb0bc3369706f930a40",
    "date": "2023-10-12T17:53:05-07:00",
    "message": "[PJRT C API] Change CheckMatchingStructSizes to be ActualStructSizeIsGreaterOrEqual.\n\nWith this change, it is no longer required that the plugin has the same struct size as the framework. Because ActualStructSizeIsGreaterOrEqual is called in the plugin, this means it will check whether framework has a greater or equal struct size than the plugin (meaning the framework is newer than the plugin).\n\nPiperOrigin-RevId: 573059095",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/pjrt/c/BUILD",
            "patches": [
                {
                    "old_start": 118,
                    "old_length": 6,
                    "new_start": 118,
                    "new_length": 8,
                    "hunk": "@@ -118,6 +118,8 @@ cc_library(\n         \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n         \"@local_tsl//tsl/platform:errors\",\n+        \"@local_tsl//tsl/platform:logging\",\n+        \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"@local_tsl//tsl/platform:logging\",\n+        \"@local_tsl//tsl/platform:status\",\n",
            "whole_hunk": "@@ -118,6 +118,8 @@ cc_library(\n         \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n         \"@local_tsl//tsl/platform:errors\",\n+        \"@local_tsl//tsl/platform:logging\",\n+        \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n"
        },
        {
            "name": "pjrt_c_api_cpu_internal.cc",
            "path": "third_party/xla/xla/pjrt/c/pjrt_c_api_cpu_internal.cc",
            "patches": [
                {
                    "old_start": 28,
                    "old_length": 7,
                    "new_start": 28,
                    "new_length": 7,
                    "hunk": "@@ -28,7 +28,7 @@ namespace pjrt {\n namespace cpu_plugin {\n \n PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_Create_Args\", PJRT_Client_Create_Args_STRUCT_SIZE,\n       args->struct_size));\n \n"
                }
            ],
            "whole_deleted": "-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n",
            "whole_added": "+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n",
            "whole_hunk": "@@ -28,7 +28,7 @@ namespace pjrt {\n namespace cpu_plugin {\n \n PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_Create_Args\", PJRT_Client_Create_Args_STRUCT_SIZE,\n       args->struct_size));\n \n"
        },
        {
            "name": "pjrt_c_api_gpu_internal.cc",
            "path": "third_party/xla/xla/pjrt/c/pjrt_c_api_gpu_internal.cc",
            "patches": [
                {
                    "old_start": 41,
                    "old_length": 7,
                    "new_start": 41,
                    "new_length": 7,
                    "hunk": "@@ -41,7 +41,7 @@ namespace gpu_plugin {\n #define PJRT_GPU_PLUGIN_PLATFORM_NAME \"CUDA\"\n \n PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_Create_Args\", PJRT_Client_Create_Args_STRUCT_SIZE,\n       args->struct_size));\n \n"
                },
                {
                    "old_start": 122,
                    "old_length": 7,
                    "new_start": 122,
                    "new_length": 7,
                    "hunk": "@@ -122,7 +122,7 @@ PJRT_Error* PJRT_GpuDeviceTopology_Create(\n \n PJRT_Error* PJRT_Gpu_Register_Custom_Call(\n     PJRT_Gpu_Register_Custom_Call_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Gpu_Register_Custom_Call_Args\",\n       PJRT_Gpu_Register_Custom_Call_Args_STRUCT_SIZE, args->struct_size));\n   std::string function_name(args->function_name, args->function_name_size);\n"
                }
            ],
            "whole_deleted": "-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n",
            "whole_added": "+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n",
            "whole_hunk": "@@ -41,7 +41,7 @@ namespace gpu_plugin {\n #define PJRT_GPU_PLUGIN_PLATFORM_NAME \"CUDA\"\n \n PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_Create_Args\", PJRT_Client_Create_Args_STRUCT_SIZE,\n       args->struct_size));\n \n@@ -122,7 +122,7 @@ PJRT_Error* PJRT_GpuDeviceTopology_Create(\n \n PJRT_Error* PJRT_Gpu_Register_Custom_Call(\n     PJRT_Gpu_Register_Custom_Call_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Gpu_Register_Custom_Call_Args\",\n       PJRT_Gpu_Register_Custom_Call_Args_STRUCT_SIZE, args->struct_size));\n   std::string function_name(args->function_name, args->function_name_size);\n"
        },
        {
            "name": "pjrt_c_api_helpers.cc",
            "path": "third_party/xla/xla/pjrt/c/pjrt_c_api_helpers.cc",
            "patches": [
                {
                    "old_start": 44,
                    "old_length": 6,
                    "new_start": 44,
                    "new_length": 8,
                    "hunk": "@@ -44,6 +44,8 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/errors.h\"\n+#include \"tsl/platform/logging.h\"\n+#include \"tsl/platform/status.h\"\n #include \"tsl/platform/statusor.h\"\n \n namespace pjrt {\n"
                },
                {
                    "old_start": 574,
                    "old_length": 12,
                    "new_start": 576,
                    "new_length": 16,
                    "hunk": "@@ -574,12 +576,16 @@ static std::string StructSizeErrorMsg(absl::string_view struct_name,\n   return error_msg;\n }\n \n-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size) {\n-  if (expected_size != actual_size) {\n+xla::Status ActualStructSizeIsGreaterOrEqual(absl::string_view struct_name,\n+                                             size_t expected_size,\n+                                             size_t actual_size) {\n+  if (actual_size < expected_size) {\n     return tsl::errors::InvalidArgument(\n         StructSizeErrorMsg(struct_name, expected_size, actual_size));\n   }\n+  if (actual_size > expected_size) {\n+    VLOG(2) << StructSizeErrorMsg(struct_name, expected_size, actual_size);\n+  }\n   return tsl::OkStatus();\n }\n \n"
                }
            ],
            "whole_deleted": "-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size) {\n-  if (expected_size != actual_size) {\n",
            "whole_added": "+#include \"tsl/platform/logging.h\"\n+#include \"tsl/platform/status.h\"\n+xla::Status ActualStructSizeIsGreaterOrEqual(absl::string_view struct_name,\n+                                             size_t expected_size,\n+                                             size_t actual_size) {\n+  if (actual_size < expected_size) {\n+  if (actual_size > expected_size) {\n+    VLOG(2) << StructSizeErrorMsg(struct_name, expected_size, actual_size);\n+  }\n",
            "whole_hunk": "@@ -44,6 +44,8 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/errors.h\"\n+#include \"tsl/platform/logging.h\"\n+#include \"tsl/platform/status.h\"\n #include \"tsl/platform/statusor.h\"\n \n namespace pjrt {\n@@ -574,12 +576,16 @@ static std::string StructSizeErrorMsg(absl::string_view struct_name,\n   return error_msg;\n }\n \n-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size) {\n-  if (expected_size != actual_size) {\n+xla::Status ActualStructSizeIsGreaterOrEqual(absl::string_view struct_name,\n+                                             size_t expected_size,\n+                                             size_t actual_size) {\n+  if (actual_size < expected_size) {\n     return tsl::errors::InvalidArgument(\n         StructSizeErrorMsg(struct_name, expected_size, actual_size));\n   }\n+  if (actual_size > expected_size) {\n+    VLOG(2) << StructSizeErrorMsg(struct_name, expected_size, actual_size);\n+  }\n   return tsl::OkStatus();\n }\n \n"
        },
        {
            "name": "pjrt_c_api_helpers.h",
            "path": "third_party/xla/xla/pjrt/c/pjrt_c_api_helpers.h",
            "patches": [
                {
                    "old_start": 152,
                    "old_length": 11,
                    "new_start": 152,
                    "new_length": 13,
                    "hunk": "@@ -152,11 +152,13 @@ xla::Status ValidateCreateOptions(\n     const absl::flat_hash_map<std::string, PJRT_NamedValue_Type>&\n         expected_name_and_types);\n \n-// Helper function for checking C API argument struct sizes. Returns a non-OK\n-// status if the expected and actual sizes aren't equal (i.e. no ABI\n-// compatibility guarantees).\n-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size);\n+// Helper function for checking the actual C API argument struct size is greater\n+// than or equal to the expected size. The actual struct size can be larger if\n+// it comes from a forwards-compatible caller built at a later version than this\n+// check. Returns a non-OK status if the expected is smaller.\n+xla::Status ActualStructSizeIsGreaterOrEqual(absl::string_view struct_name,\n+                                             size_t expected_size,\n+                                             size_t actual_size);\n \n absl::string_view GetPlatformVersion(PJRT_Client* client, const PJRT_Api* api);\n absl::string_view GetPlatformName(PJRT_Client* client, const PJRT_Api* api);\n"
                }
            ],
            "whole_deleted": "-// Helper function for checking C API argument struct sizes. Returns a non-OK\n-// status if the expected and actual sizes aren't equal (i.e. no ABI\n-// compatibility guarantees).\n-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size);\n",
            "whole_added": "+// Helper function for checking the actual C API argument struct size is greater\n+// than or equal to the expected size. The actual struct size can be larger if\n+// it comes from a forwards-compatible caller built at a later version than this\n+// check. Returns a non-OK status if the expected is smaller.\n+xla::Status ActualStructSizeIsGreaterOrEqual(absl::string_view struct_name,\n+                                             size_t expected_size,\n+                                             size_t actual_size);\n",
            "whole_hunk": "@@ -152,11 +152,13 @@ xla::Status ValidateCreateOptions(\n     const absl::flat_hash_map<std::string, PJRT_NamedValue_Type>&\n         expected_name_and_types);\n \n-// Helper function for checking C API argument struct sizes. Returns a non-OK\n-// status if the expected and actual sizes aren't equal (i.e. no ABI\n-// compatibility guarantees).\n-xla::Status CheckMatchingStructSizes(absl::string_view struct_name,\n-                                     size_t expected_size, size_t actual_size);\n+// Helper function for checking the actual C API argument struct size is greater\n+// than or equal to the expected size. The actual struct size can be larger if\n+// it comes from a forwards-compatible caller built at a later version than this\n+// check. Returns a non-OK status if the expected is smaller.\n+xla::Status ActualStructSizeIsGreaterOrEqual(absl::string_view struct_name,\n+                                             size_t expected_size,\n+                                             size_t actual_size);\n \n absl::string_view GetPlatformVersion(PJRT_Client* client, const PJRT_Api* api);\n absl::string_view GetPlatformName(PJRT_Client* client, const PJRT_Api* api);\n"
        },
        {
            "name": "pjrt_c_api_wrapper_impl.cc",
            "path": "third_party/xla/xla/pjrt/c/pjrt_c_api_wrapper_impl.cc",
            "patches": [
                {
                    "old_start": 285,
                    "old_length": 7,
                    "new_start": 285,
                    "new_length": 7,
                    "hunk": "@@ -285,7 +285,7 @@ xla::PjRtClient::KeyValuePutCallback ToCppKeyValuePutCallback(\n // ---------------------------------- Errors -----------------------------------\n \n void PJRT_Error_Destroy(PJRT_Error_Destroy_Args* args) {\n-  xla::Status struct_size_check = CheckMatchingStructSizes(\n+  xla::Status struct_size_check = ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Error_Destroy_Args\", PJRT_Error_Destroy_Args_STRUCT_SIZE,\n       args->struct_size);\n   if (!struct_size_check.ok()) {\n"
                },
                {
                    "old_start": 297,
                    "old_length": 7,
                    "new_start": 297,
                    "new_length": 7,
                    "hunk": "@@ -297,7 +297,7 @@ void PJRT_Error_Destroy(PJRT_Error_Destroy_Args* args) {\n }\n \n void PJRT_Error_Message(PJRT_Error_Message_Args* args) {\n-  xla::Status struct_size_check = CheckMatchingStructSizes(\n+  xla::Status struct_size_check = ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Error_Message_Args\", PJRT_Error_Message_Args_STRUCT_SIZE,\n       args->struct_size);\n   if (!struct_size_check.ok()) {\n"
                },
                {
                    "old_start": 311,
                    "old_length": 7,
                    "new_start": 311,
                    "new_length": 7,
                    "hunk": "@@ -311,7 +311,7 @@ void PJRT_Error_Message(PJRT_Error_Message_Args* args) {\n }\n \n PJRT_Error* PJRT_Error_GetCode(PJRT_Error_GetCode_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Error_GetCode_Args\", PJRT_Error_GetCode_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->code = StatusCodeToPjrtErrorCode(\n"
                },
                {
                    "old_start": 322,
                    "old_length": 7,
                    "new_start": 322,
                    "new_length": 7,
                    "hunk": "@@ -322,7 +322,7 @@ PJRT_Error* PJRT_Error_GetCode(PJRT_Error_GetCode_Args* args) {\n // ---------------------------------- Plugin -----------------------------------\n \n PJRT_Error* PJRT_Plugin_Attributes(PJRT_Plugin_Attributes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Plugin_Attributes_Args\", PJRT_Plugin_Attributes_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->num_attributes = 0;\n"
                },
                {
                    "old_start": 330,
                    "old_length": 7,
                    "new_start": 330,
                    "new_length": 7,
                    "hunk": "@@ -330,7 +330,7 @@ PJRT_Error* PJRT_Plugin_Attributes(PJRT_Plugin_Attributes_Args* args) {\n }\n \n PJRT_Error* PJRT_Plugin_Initialize_NoOp(PJRT_Plugin_Initialize_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Plugin_Initialize_Args\", PJRT_Plugin_Initialize_Args_STRUCT_SIZE,\n       args->struct_size));\n   return nullptr;\n"
                },
                {
                    "old_start": 339,
                    "old_length": 7,
                    "new_start": 339,
                    "new_length": 7,
                    "hunk": "@@ -339,7 +339,7 @@ PJRT_Error* PJRT_Plugin_Initialize_NoOp(PJRT_Plugin_Initialize_Args* args) {\n // ---------------------------------- Client -----------------------------------\n \n PJRT_Error* PJRT_Client_Destroy(PJRT_Client_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_Destroy_Args\", PJRT_Client_Destroy_Args_STRUCT_SIZE,\n       args->struct_size));\n   delete args->client;\n"
                },
                {
                    "old_start": 347,
                    "old_length": 7,
                    "new_start": 347,
                    "new_length": 7,
                    "hunk": "@@ -347,7 +347,7 @@ PJRT_Error* PJRT_Client_Destroy(PJRT_Client_Destroy_Args* args) {\n }\n \n PJRT_Error* PJRT_Client_ProcessIndex(PJRT_Client_ProcessIndex_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CLient_ProcessIndex_Args\",\n       PJRT_Client_ProcessIndex_Args_STRUCT_SIZE, args->struct_size));\n   args->process_index = args->client->client->process_index();\n"
                },
                {
                    "old_start": 355,
                    "old_length": 7,
                    "new_start": 355,
                    "new_length": 7,
                    "hunk": "@@ -355,7 +355,7 @@ PJRT_Error* PJRT_Client_ProcessIndex(PJRT_Client_ProcessIndex_Args* args) {\n }\n \n PJRT_Error* PJRT_Client_PlatformName(PJRT_Client_PlatformName_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_PlatformName_Args\",\n       PJRT_Client_PlatformName_Args_STRUCT_SIZE, args->struct_size));\n   absl::string_view platform_name = args->client->client->platform_name();\n"
                },
                {
                    "old_start": 366,
                    "old_length": 7,
                    "new_start": 366,
                    "new_length": 7,
                    "hunk": "@@ -366,7 +366,7 @@ PJRT_Error* PJRT_Client_PlatformName(PJRT_Client_PlatformName_Args* args) {\n \n PJRT_Error* PJRT_Client_PlatformVersion(\n     PJRT_Client_PlatformVersion_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CLient_PlatformVersion_Args\",\n       PJRT_Client_PlatformVersion_Args_STRUCT_SIZE, args->struct_size));\n   absl::string_view platform_version = args->client->client->platform_version();\n"
                },
                {
                    "old_start": 376,
                    "old_length": 7,
                    "new_start": 376,
                    "new_length": 7,
                    "hunk": "@@ -376,7 +376,7 @@ PJRT_Error* PJRT_Client_PlatformVersion(\n }\n \n PJRT_Error* PJRT_Client_Devices(PJRT_Client_Devices_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_Devices_Args\", PJRT_Client_Devices_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->num_devices = args->client->devices.size();\n"
                },
                {
                    "old_start": 386,
                    "old_length": 7,
                    "new_start": 386,
                    "new_length": 7,
                    "hunk": "@@ -386,7 +386,7 @@ PJRT_Error* PJRT_Client_Devices(PJRT_Client_Devices_Args* args) {\n \n PJRT_Error* PJRT_Client_AddressableDevices(\n     PJRT_Client_AddressableDevices_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_AddressableDevices_Args\",\n       PJRT_Client_AddressableDevices_Args_STRUCT_SIZE, args->struct_size));\n   args->num_addressable_devices = args->client->addressable_devices.size();\n"
                },
                {
                    "old_start": 395,
                    "old_length": 7,
                    "new_start": 395,
                    "new_length": 7,
                    "hunk": "@@ -395,7 +395,7 @@ PJRT_Error* PJRT_Client_AddressableDevices(\n }\n \n PJRT_Error* PJRT_Client_LookupDevice(PJRT_Client_LookupDevice_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_LookupDevice_Args\",\n       PJRT_Client_LookupDevice_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(xla::PjRtDevice * device,\n"
                },
                {
                    "old_start": 406,
                    "old_length": 7,
                    "new_start": 406,
                    "new_length": 7,
                    "hunk": "@@ -406,7 +406,7 @@ PJRT_Error* PJRT_Client_LookupDevice(PJRT_Client_LookupDevice_Args* args) {\n \n PJRT_Error* PJRT_Client_LookupAddressableDevice(\n     PJRT_Client_LookupAddressableDevice_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_LookupAddressableDevice_Args\",\n       PJRT_Client_LookupAddressableDevice_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(\n"
                },
                {
                    "old_start": 418,
                    "old_length": 7,
                    "new_start": 418,
                    "new_length": 7,
                    "hunk": "@@ -418,7 +418,7 @@ PJRT_Error* PJRT_Client_LookupAddressableDevice(\n \n PJRT_Error* PJRT_LoadedExecutable_Fingerprint(\n     PJRT_LoadedExecutable_Fingerprint_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_Fingerprint_Args\",\n       PJRT_LoadedExecutable_Fingerprint_Args_STRUCT_SIZE, args->struct_size));\n   const xla::Status& status = args->executable->fingerprint.status();\n"
                },
                {
                    "old_start": 439,
                    "old_length": 7,
                    "new_start": 439,
                    "new_length": 7,
                    "hunk": "@@ -439,7 +439,7 @@ PJRT_Error* PJRT_LoadedExecutable_Fingerprint(\n \n PJRT_Error* PJRT_Client_AddressableMemories(\n     PJRT_Client_AddressableMemories_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_AddressableMemories_Args\",\n       PJRT_Client_AddressableMemories_Args_STRUCT_SIZE, args->struct_size));\n   args->num_addressable_memories = args->client->addressable_memories.size();\n"
                },
                {
                    "old_start": 553,
                    "old_length": 10,
                    "new_start": 553,
                    "new_length": 10,
                    "hunk": "@@ -553,10 +553,10 @@ const xla::XlaComputation& UnpackPjrtProgram(\n }  // namespace\n \n PJRT_Error* PJRT_Client_Compile(PJRT_Client_Compile_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_Compile_Args\", PJRT_Client_Compile_Args_STRUCT_SIZE,\n       args->struct_size));\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Program\", PJRT_Program_STRUCT_SIZE, args->program->struct_size));\n \n   PJRT_ASSIGN_OR_RETURN(\n"
                },
                {
                    "old_start": 592,
                    "old_length": 7,
                    "new_start": 592,
                    "new_length": 7,
                    "hunk": "@@ -592,7 +592,7 @@ static void PopulateDeviceAssignment(int* const device_assignment_buffer,\n \n PJRT_Error* PJRT_Client_DefaultDeviceAssignment(\n     PJRT_Client_DefaultDeviceAssignment_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_DefaultAssignment_Args\",\n       PJRT_Client_DefaultDeviceAssignment_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 618,
                    "old_length": 7,
                    "new_start": 618,
                    "new_length": 7,
                    "hunk": "@@ -618,7 +618,7 @@ PJRT_Error* PJRT_Client_DefaultDeviceAssignment(\n \n PJRT_Error* PJRT_Client_BufferFromHostBuffer(\n     PJRT_Client_BufferFromHostBuffer_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_BufferFromHostBuffer_Args\",\n       PJRT_Client_BufferFromHostBuffer_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 739,
                    "old_length": 7,
                    "new_start": 739,
                    "new_length": 7,
                    "hunk": "@@ -739,7 +739,7 @@ PJRT_Error* PJRT_Client_CreateViewOfDeviceBuffer(\n // --------------------------------- Devices -----------------------------------\n \n PJRT_Error* PJRT_DeviceDescription_Id(PJRT_DeviceDescription_Id_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_Id_Args\",\n       PJRT_DeviceDescription_Id_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 749,
                    "old_length": 7,
                    "new_start": 749,
                    "new_length": 7,
                    "hunk": "@@ -749,7 +749,7 @@ PJRT_Error* PJRT_DeviceDescription_Id(PJRT_DeviceDescription_Id_Args* args) {\n \n PJRT_Error* PJRT_DeviceDescription_ProcessIndex(\n     PJRT_DeviceDescription_ProcessIndex_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_ProcessIndex_Args\",\n       PJRT_DeviceDescription_ProcessIndex_Args_STRUCT_SIZE, args->struct_size));\n   args->process_index =\n"
                },
                {
                    "old_start": 759,
                    "old_length": 7,
                    "new_start": 759,
                    "new_length": 7,
                    "hunk": "@@ -759,7 +759,7 @@ PJRT_Error* PJRT_DeviceDescription_ProcessIndex(\n \n PJRT_Error* PJRT_DeviceDescription_Attributes(\n     PJRT_DeviceDescription_Attributes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_Attributes_Args\",\n       PJRT_DeviceDescription_Attributes_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 772,
                    "old_length": 7,
                    "new_start": 772,
                    "new_length": 7,
                    "hunk": "@@ -772,7 +772,7 @@ PJRT_Error* PJRT_DeviceDescription_Attributes(\n \n PJRT_Error* PJRT_DeviceDescription_Kind(\n     PJRT_DeviceDescription_Kind_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_Kind_Args\",\n       PJRT_DeviceDescription_Kind_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 785,
                    "old_length": 7,
                    "new_start": 785,
                    "new_length": 7,
                    "hunk": "@@ -785,7 +785,7 @@ PJRT_Error* PJRT_DeviceDescription_Kind(\n \n PJRT_Error* PJRT_DeviceDescription_DebugString(\n     PJRT_DeviceDescription_DebugString_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_DebugString_Args\",\n       PJRT_DeviceDescription_DebugString_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 798,
                    "old_length": 7,
                    "new_start": 798,
                    "new_length": 7,
                    "hunk": "@@ -798,7 +798,7 @@ PJRT_Error* PJRT_DeviceDescription_DebugString(\n \n PJRT_Error* PJRT_DeviceDescription_ToString(\n     PJRT_DeviceDescription_ToString_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_ToString_Args\",\n       PJRT_DeviceDescription_ToString_Args_STRUCT_SIZE, args->struct_size));\n   args->to_string =\n"
                },
                {
                    "old_start": 809,
                    "old_length": 7,
                    "new_start": 809,
                    "new_length": 7,
                    "hunk": "@@ -809,7 +809,7 @@ PJRT_Error* PJRT_DeviceDescription_ToString(\n }\n \n PJRT_Error* PJRT_Device_GetDescription(PJRT_Device_GetDescription_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_GetDescription_Args\",\n       PJRT_Device_GetDescription_Args_STRUCT_SIZE, args->struct_size));\n   args->device_description = &args->device->description;\n"
                },
                {
                    "old_start": 817,
                    "old_length": 7,
                    "new_start": 817,
                    "new_length": 7,
                    "hunk": "@@ -817,7 +817,7 @@ PJRT_Error* PJRT_Device_GetDescription(PJRT_Device_GetDescription_Args* args) {\n }\n \n PJRT_Error* PJRT_Device_IsAddressable(PJRT_Device_IsAddressable_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_IsAddressable_Args\",\n       PJRT_Device_IsAddressable_Args_STRUCT_SIZE, args->struct_size));\n   args->is_addressable = args->device->device->IsAddressable();\n"
                },
                {
                    "old_start": 826,
                    "old_length": 7,
                    "new_start": 826,
                    "new_length": 7,
                    "hunk": "@@ -826,7 +826,7 @@ PJRT_Error* PJRT_Device_IsAddressable(PJRT_Device_IsAddressable_Args* args) {\n \n PJRT_Error* PJRT_Device_LocalHardwareId(\n     PJRT_Device_LocalHardwareId_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_LocalHardwareId_Args\",\n       PJRT_Device_LocalHardwareId_Args_STRUCT_SIZE, args->struct_size));\n   args->local_hardware_id = args->device->device->local_hardware_id();\n"
                },
                {
                    "old_start": 835,
                    "old_length": 7,
                    "new_start": 835,
                    "new_length": 7,
                    "hunk": "@@ -835,7 +835,7 @@ PJRT_Error* PJRT_Device_LocalHardwareId(\n \n PJRT_Error* PJRT_Device_AddressableMemories(\n     PJRT_Device_AddressableMemories_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_AddressableMemories_Args\",\n       PJRT_Device_AddressableMemories_Args_STRUCT_SIZE, args->struct_size));\n   args->memories = args->device->addressable_memories.data();\n"
                },
                {
                    "old_start": 844,
                    "old_length": 7,
                    "new_start": 844,
                    "new_length": 7,
                    "hunk": "@@ -844,7 +844,7 @@ PJRT_Error* PJRT_Device_AddressableMemories(\n }\n \n PJRT_Error* PJRT_Device_DefaultMemory(PJRT_Device_DefaultMemory_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_DefaultMemory_Args\",\n       PJRT_Device_DefaultMemory_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(xla::PjRtMemorySpace * memory_space,\n"
                },
                {
                    "old_start": 854,
                    "old_length": 7,
                    "new_start": 854,
                    "new_length": 7,
                    "hunk": "@@ -854,7 +854,7 @@ PJRT_Error* PJRT_Device_DefaultMemory(PJRT_Device_DefaultMemory_Args* args) {\n }\n \n PJRT_Error* PJRT_Device_MemoryStats(PJRT_Device_MemoryStats_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_MemoryStats_Args\", PJRT_Device_MemoryStats_Args_STRUCT_SIZE,\n       args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(tsl::AllocatorStats stats,\n"
                },
                {
                    "old_start": 904,
                    "old_length": 16,
                    "new_start": 904,
                    "new_length": 16,
                    "hunk": "@@ -904,16 +904,16 @@ PJRT_Error* PJRT_Device_MemoryStats(PJRT_Device_MemoryStats_Args* args) {\n // ------------------------------- Memory --------------------------------------\n \n PJRT_Error* PJRT_Memory_Id(PJRT_Memory_Id_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\"PJRT_Memory_Id_Args\",\n-                                                PJRT_Memory_Id_Args_STRUCT_SIZE,\n-                                                args->struct_size));\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+      \"PJRT_Memory_Id_Args\", PJRT_Memory_Id_Args_STRUCT_SIZE,\n+      args->struct_size));\n \n   args->id = args->memory->memory_space->id();\n   return nullptr;\n }\n \n PJRT_Error* PJRT_Memory_Kind(PJRT_Memory_Kind_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Memory_Kind_Args\", PJRT_Memory_Kind_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->memory_kind = args->memory->memory_space->memory_space_kind().data();\n"
                },
                {
                    "old_start": 923,
                    "old_length": 7,
                    "new_start": 923,
                    "new_length": 7,
                    "hunk": "@@ -923,7 +923,7 @@ PJRT_Error* PJRT_Memory_Kind(PJRT_Memory_Kind_Args* args) {\n }\n \n PJRT_Error* PJRT_Memory_DebugString(PJRT_Memory_DebugString_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Memory_DebugString_Args\", PJRT_Memory_DebugString_Args_STRUCT_SIZE,\n       args->struct_size));\n \n"
                },
                {
                    "old_start": 933,
                    "old_length": 7,
                    "new_start": 933,
                    "new_length": 7,
                    "hunk": "@@ -933,7 +933,7 @@ PJRT_Error* PJRT_Memory_DebugString(PJRT_Memory_DebugString_Args* args) {\n }\n \n PJRT_Error* PJRT_Memory_ToString(PJRT_Memory_ToString_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Memory_ToString_Args\", PJRT_Memory_ToString_Args_STRUCT_SIZE,\n       args->struct_size));\n \n"
                },
                {
                    "old_start": 944,
                    "old_length": 7,
                    "new_start": 944,
                    "new_length": 7,
                    "hunk": "@@ -944,7 +944,7 @@ PJRT_Error* PJRT_Memory_ToString(PJRT_Memory_ToString_Args* args) {\n \n PJRT_Error* PJRT_Memory_AddressableByDevices(\n     PJRT_Memory_AddressableByDevices_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Memory_AddressableByDevices_Args\",\n       PJRT_Memory_AddressableByDevices_Args_STRUCT_SIZE, args->struct_size));\n   args->devices = args->memory->devices.data();\n"
                },
                {
                    "old_start": 955,
                    "old_length": 7,
                    "new_start": 955,
                    "new_length": 7,
                    "hunk": "@@ -955,7 +955,7 @@ PJRT_Error* PJRT_Memory_AddressableByDevices(\n // ------------------------------- Executables ---------------------------------\n \n PJRT_Error* PJRT_Executable_Destroy(PJRT_Executable_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_Destroy_Args\", PJRT_Executable_Destroy_Args_STRUCT_SIZE,\n       args->struct_size));\n   delete args->executable;\n"
                },
                {
                    "old_start": 964,
                    "old_length": 7,
                    "new_start": 964,
                    "new_length": 7,
                    "hunk": "@@ -964,7 +964,7 @@ PJRT_Error* PJRT_Executable_Destroy(PJRT_Executable_Destroy_Args* args) {\n \n PJRT_Error* PJRT_LoadedExecutable_Destroy(\n     PJRT_LoadedExecutable_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_Destroy_Args\",\n       PJRT_LoadedExecutable_Destroy_Args_STRUCT_SIZE, args->struct_size));\n   delete args->executable;\n"
                },
                {
                    "old_start": 972,
                    "old_length": 7,
                    "new_start": 972,
                    "new_length": 7,
                    "hunk": "@@ -972,7 +972,7 @@ PJRT_Error* PJRT_LoadedExecutable_Destroy(\n }\n \n PJRT_Error* PJRT_Executable_Name(PJRT_Executable_Name_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_Name_Args\", PJRT_Executable_Name_Args_STRUCT_SIZE,\n       args->struct_size));\n   absl::string_view executable_name = args->executable->get()->name();\n"
                },
                {
                    "old_start": 983,
                    "old_length": 7,
                    "new_start": 983,
                    "new_length": 7,
                    "hunk": "@@ -983,7 +983,7 @@ PJRT_Error* PJRT_Executable_Name(PJRT_Executable_Name_Args* args) {\n \n PJRT_Error* PJRT_Executable_NumReplicas(\n     PJRT_Executable_NumReplicas_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_NumReplicas_Args\",\n       PJRT_Executable_NumReplicas_Args_STRUCT_SIZE, args->struct_size));\n   args->num_replicas = args->executable->get()->num_replicas();\n"
                },
                {
                    "old_start": 992,
                    "old_length": 7,
                    "new_start": 992,
                    "new_length": 7,
                    "hunk": "@@ -992,7 +992,7 @@ PJRT_Error* PJRT_Executable_NumReplicas(\n \n PJRT_Error* PJRT_Executable_NumPartitions(\n     PJRT_Executable_NumPartitions_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_NumPartitions_Args\",\n       PJRT_Executable_NumPartitions_Args_STRUCT_SIZE, args->struct_size));\n   args->num_partitions = args->executable->get()->num_partitions();\n"
                },
                {
                    "old_start": 1001,
                    "old_length": 7,
                    "new_start": 1001,
                    "new_length": 7,
                    "hunk": "@@ -1001,7 +1001,7 @@ PJRT_Error* PJRT_Executable_NumPartitions(\n \n PJRT_Error* PJRT_LoadedExecutable_AddressableDevices(\n     PJRT_LoadedExecutable_AddressableDevices_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_AddressableDevices_Args\",\n       PJRT_LoadedExecutable_AddressableDevices_Args_STRUCT_SIZE,\n       args->struct_size));\n"
                },
                {
                    "old_start": 1012,
                    "old_length": 7,
                    "new_start": 1012,
                    "new_length": 7,
                    "hunk": "@@ -1012,7 +1012,7 @@ PJRT_Error* PJRT_LoadedExecutable_AddressableDevices(\n }\n \n PJRT_Error* PJRT_Executable_NumOutputs(PJRT_Executable_NumOutputs_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_NumOutputs_Args\",\n       PJRT_Executable_NumOutputs_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(std::vector<xla::Shape> output_shapes,\n"
                },
                {
                    "old_start": 1040,
                    "old_length": 7,
                    "new_start": 1040,
                    "new_length": 7,
                    "hunk": "@@ -1040,7 +1040,7 @@ PJRT_Error* PJRT_Executable_NumOutputs(PJRT_Executable_NumOutputs_Args* args) {\n \n PJRT_Error* PJRT_Executable_SizeOfGeneratedCodeInBytes(\n     PJRT_Executable_SizeOfGeneratedCodeInBytes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_SizeOfGeneratedCodeInBytes_Args\",\n       PJRT_Executable_SizeOfGeneratedCodeInBytes_Args_STRUCT_SIZE,\n       args->struct_size));\n"
                },
                {
                    "old_start": 1051,
                    "old_length": 10,
                    "new_start": 1051,
                    "new_length": 10,
                    "hunk": "@@ -1051,10 +1051,10 @@ PJRT_Error* PJRT_Executable_SizeOfGeneratedCodeInBytes(\n \n static xla::Status VerifyOptimizedProgramArgs(\n     PJRT_Executable_OptimizedProgram_Args* args) {\n-  TF_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  TF_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_OptimizedProgram_Args\",\n       PJRT_Executable_OptimizedProgram_Args_STRUCT_SIZE, args->struct_size));\n-  TF_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  TF_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Program\", PJRT_Program_STRUCT_SIZE, args->program->struct_size));\n   return xla::OkStatus();\n }\n"
                },
                {
                    "old_start": 1117,
                    "old_length": 7,
                    "new_start": 1117,
                    "new_length": 7,
                    "hunk": "@@ -1117,7 +1117,7 @@ PJRT_Error* PJRT_Executable_OptimizedProgram(\n \n PJRT_Error* PJRT_Executable_GetCostAnalysis(\n     PJRT_Executable_GetCostAnalysis_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_GetCostAnalysis_Args\",\n       PJRT_Executable_GetCostAnalysis_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1141,
                    "old_length": 7,
                    "new_start": 1141,
                    "new_length": 7,
                    "hunk": "@@ -1141,7 +1141,7 @@ PJRT_Error* PJRT_Executable_GetCostAnalysis(\n \n PJRT_Error* PJRT_Executable_OutputElementTypes(\n     PJRT_Executable_OutputElementTypes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_OutputElementTypes_Args\",\n       PJRT_Executable_OutputElementTypes_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1161,
                    "old_length": 7,
                    "new_start": 1161,
                    "new_length": 7,
                    "hunk": "@@ -1161,7 +1161,7 @@ PJRT_Error* PJRT_Executable_OutputElementTypes(\n \n PJRT_Error* PJRT_Executable_OutputDimensions(\n     PJRT_Executable_OutputDimensions_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_OutputDimensions_Args\",\n       PJRT_Executable_OutputDimensions_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1182,
                    "old_length": 7,
                    "new_start": 1182,
                    "new_length": 7,
                    "hunk": "@@ -1182,7 +1182,7 @@ PJRT_Error* PJRT_Executable_OutputDimensions(\n \n PJRT_Error* PJRT_Executable_OutputMemoryKinds(\n     PJRT_Executable_OutputMemoryKinds_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_OutputMemoryKinds_Args\",\n       PJRT_Executable_OutputMemoryKinds_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1203,
                    "old_length": 7,
                    "new_start": 1203,
                    "new_length": 7,
                    "hunk": "@@ -1203,7 +1203,7 @@ PJRT_Error* PJRT_Executable_OutputMemoryKinds(\n \n PJRT_Error* PJRT_LoadedExecutable_Delete(\n     PJRT_LoadedExecutable_Delete_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_Delete_Args\",\n       PJRT_LoadedExecutable_Delete_Args_STRUCT_SIZE, args->struct_size));\n   args->executable->get()->Delete();\n"
                },
                {
                    "old_start": 1212,
                    "old_length": 7,
                    "new_start": 1212,
                    "new_length": 7,
                    "hunk": "@@ -1212,7 +1212,7 @@ PJRT_Error* PJRT_LoadedExecutable_Delete(\n \n PJRT_Error* PJRT_LoadedExecutable_IsDeleted(\n     PJRT_LoadedExecutable_IsDeleted_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_IsDeleted_Args\",\n       PJRT_LoadedExecutable_IsDeleted_Args_STRUCT_SIZE, args->struct_size));\n   args->is_deleted = args->executable->get()->IsDeleted();\n"
                },
                {
                    "old_start": 1311,
                    "old_length": 12,
                    "new_start": 1311,
                    "new_length": 12,
                    "hunk": "@@ -1311,12 +1311,12 @@ static std::vector<std::vector<xla::PjRtBuffer*>> Convert2DCBuffersToCppBuffers(\n \n PJRT_Error* PJRT_LoadedExecutable_Execute(\n     PJRT_LoadedExecutable_Execute_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_Execute_Args\",\n       PJRT_LoadedExecutable_Execute_Args_STRUCT_SIZE, args->struct_size));\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\"PJRT_ExecuteOptions\",\n-                                                PJRT_ExecuteOptions_STRUCT_SIZE,\n-                                                args->options->struct_size));\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+      \"PJRT_ExecuteOptions\", PJRT_ExecuteOptions_STRUCT_SIZE,\n+      args->options->struct_size));\n   xla::ExecuteOptions options;\n   options.launch_id = args->options->launch_id;\n   options.strict_shape_checking = true;\n"
                },
                {
                    "old_start": 1438,
                    "old_length": 7,
                    "new_start": 1438,
                    "new_length": 7,
                    "hunk": "@@ -1438,7 +1438,7 @@ PJRT_Error* PJRT_LoadedExecutable_Execute(\n }\n \n PJRT_Error* PJRT_Executable_Serialize(PJRT_Executable_Serialize_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_Serialize_Args\",\n       PJRT_Executable_Serialize_Args_STRUCT_SIZE, args->struct_size));\n   std::string serialization;\n"
                },
                {
                    "old_start": 1463,
                    "old_length": 7,
                    "new_start": 1463,
                    "new_length": 7,
                    "hunk": "@@ -1463,7 +1463,7 @@ PJRT_Error* PJRT_Executable_Serialize(PJRT_Executable_Serialize_Args* args) {\n \n PJRT_Error* PJRT_Executable_DeserializeAndLoad(\n     PJRT_Executable_DeserializeAndLoad_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_DeserializeAndLoad_Args\",\n       PJRT_Executable_DeserializeAndLoad_Args_STRUCT_SIZE, args->struct_size));\n   absl::string_view serialized(args->serialized_executable,\n"
                },
                {
                    "old_start": 1480,
                    "old_length": 7,
                    "new_start": 1480,
                    "new_length": 7,
                    "hunk": "@@ -1480,7 +1480,7 @@ PJRT_Error* PJRT_Executable_DeserializeAndLoad(\n \n PJRT_Error* PJRT_LoadedExecutable_GetExecutable(\n     PJRT_LoadedExecutable_GetExecutable_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_GetExecutable_Args\",\n       PJRT_LoadedExecutable_GetExecutable_Args_STRUCT_SIZE, args->struct_size));\n   args->executable = new PJRT_Executable{args->loaded_executable->executable};\n"
                },
                {
                    "old_start": 1490,
                    "old_length": 7,
                    "new_start": 1490,
                    "new_length": 7,
                    "hunk": "@@ -1490,7 +1490,7 @@ PJRT_Error* PJRT_LoadedExecutable_GetExecutable(\n // ---------------------------------- Buffers ----------------------------------\n \n PJRT_Error* PJRT_Buffer_Destroy(PJRT_Buffer_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_Destroy_Args\", PJRT_Buffer_Destroy_Args_STRUCT_SIZE,\n       args->struct_size));\n   delete args->buffer;\n"
                },
                {
                    "old_start": 1498,
                    "old_length": 7,
                    "new_start": 1498,
                    "new_length": 7,
                    "hunk": "@@ -1498,7 +1498,7 @@ PJRT_Error* PJRT_Buffer_Destroy(PJRT_Buffer_Destroy_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_ElementType(PJRT_Buffer_ElementType_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_ElementType_Args\", PJRT_Buffer_ElementType_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->type = ConvertToPjRtBufferType(args->buffer->buffer->element_type());\n"
                },
                {
                    "old_start": 1506,
                    "old_length": 7,
                    "new_start": 1506,
                    "new_length": 7,
                    "hunk": "@@ -1506,7 +1506,7 @@ PJRT_Error* PJRT_Buffer_ElementType(PJRT_Buffer_ElementType_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_Dimensions(PJRT_Buffer_Dimensions_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_Dimensions_Args\", PJRT_Buffer_Dimensions_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->dims = args->buffer->buffer->dimensions().data();\n"
                },
                {
                    "old_start": 1516,
                    "old_length": 7,
                    "new_start": 1516,
                    "new_length": 7,
                    "hunk": "@@ -1516,7 +1516,7 @@ PJRT_Error* PJRT_Buffer_Dimensions(PJRT_Buffer_Dimensions_Args* args) {\n \n PJRT_Error* PJRT_Buffer_UnpaddedDimensions(\n     PJRT_Buffer_UnpaddedDimensions_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_UnpaddedDimensions_Args\",\n       PJRT_Buffer_UnpaddedDimensions_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1537,
                    "old_length": 7,
                    "new_start": 1537,
                    "new_length": 7,
                    "hunk": "@@ -1537,7 +1537,7 @@ PJRT_Error* PJRT_Buffer_UnpaddedDimensions(\n \n PJRT_Error* PJRT_Buffer_DynamicDimensionIndices(\n     PJRT_Buffer_DynamicDimensionIndices_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_DynamicDimensionIndices_Args\",\n       PJRT_Buffer_DynamicDimensionIndices_Args_STRUCT_SIZE, args->struct_size));\n   absl::Span<const bool> is_dyn_dim =\n"
                },
                {
                    "old_start": 1562,
                    "old_length": 7,
                    "new_start": 1562,
                    "new_length": 7,
                    "hunk": "@@ -1562,7 +1562,7 @@ PJRT_Error* PJRT_Buffer_DynamicDimensionIndices(\n \n PJRT_Error* PJRT_Buffer_GetMemoryLayout(\n     PJRT_Buffer_GetMemoryLayout_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_GetMemoryLayout_Args\",\n       PJRT_Buffer_GetMemoryLayout_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1583,
                    "old_length": 7,
                    "new_start": 1583,
                    "new_length": 7,
                    "hunk": "@@ -1583,7 +1583,7 @@ PJRT_Error* PJRT_Buffer_GetMemoryLayout(\n \n PJRT_Error* PJRT_Buffer_OnDeviceSizeInBytes(\n     PJRT_Buffer_OnDeviceSizeInBytes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_OnDeviceSizeInBytes_Args\",\n       PJRT_Buffer_OnDeviceSizeInBytes_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(args->on_device_size_in_bytes,\n"
                },
                {
                    "old_start": 1592,
                    "old_length": 7,
                    "new_start": 1592,
                    "new_length": 7,
                    "hunk": "@@ -1592,7 +1592,7 @@ PJRT_Error* PJRT_Buffer_OnDeviceSizeInBytes(\n }\n \n PJRT_Error* PJRT_Buffer_Device(PJRT_Buffer_Device_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_Device_Args\", PJRT_Buffer_Device_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->device = FindDeviceWrapper(args->buffer->buffer->device(),\n"
                },
                {
                    "old_start": 1605,
                    "old_length": 7,
                    "new_start": 1605,
                    "new_length": 7,
                    "hunk": "@@ -1605,7 +1605,7 @@ PJRT_Error* PJRT_Buffer_Device(PJRT_Buffer_Device_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_Memory(PJRT_Buffer_Memory_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_Memory_Args\", PJRT_Buffer_Memory_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->memory = FindMemoryWrapper(args->buffer->buffer->memory_space(),\n"
                },
                {
                    "old_start": 1619,
                    "old_length": 7,
                    "new_start": 1619,
                    "new_length": 7,
                    "hunk": "@@ -1619,7 +1619,7 @@ PJRT_Error* PJRT_Buffer_Memory(PJRT_Buffer_Memory_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_Delete(PJRT_Buffer_Delete_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_Delete_Args\", PJRT_Buffer_Delete_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->buffer->buffer->Delete();\n"
                },
                {
                    "old_start": 1627,
                    "old_length": 7,
                    "new_start": 1627,
                    "new_length": 7,
                    "hunk": "@@ -1627,7 +1627,7 @@ PJRT_Error* PJRT_Buffer_Delete(PJRT_Buffer_Delete_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_IsDeleted(PJRT_Buffer_IsDeleted_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_IsDeleted_Args\", PJRT_Buffer_IsDeleted_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->is_deleted = args->buffer->buffer->IsDeleted();\n"
                },
                {
                    "old_start": 1635,
                    "old_length": 7,
                    "new_start": 1635,
                    "new_length": 7,
                    "hunk": "@@ -1635,7 +1635,7 @@ PJRT_Error* PJRT_Buffer_IsDeleted(PJRT_Buffer_IsDeleted_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_CopyToDevice(PJRT_Buffer_CopyToDevice_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_CopyToDevice_Args\",\n       PJRT_Buffer_CopyToDevice_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(\n"
                },
                {
                    "old_start": 1647,
                    "old_length": 7,
                    "new_start": 1647,
                    "new_length": 7,
                    "hunk": "@@ -1647,7 +1647,7 @@ PJRT_Error* PJRT_Buffer_CopyToDevice(PJRT_Buffer_CopyToDevice_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_CopyToMemory(PJRT_Buffer_CopyToMemory_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_CopyToMemory_Args\",\n       PJRT_Buffer_CopyToMemory_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(\n"
                },
                {
                    "old_start": 1659,
                    "old_length": 7,
                    "new_start": 1659,
                    "new_length": 7,
                    "hunk": "@@ -1659,7 +1659,7 @@ PJRT_Error* PJRT_Buffer_CopyToMemory(PJRT_Buffer_CopyToMemory_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_ToHostBuffer(PJRT_Buffer_ToHostBuffer_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_ToHostBuffer_Args\",\n       PJRT_Buffer_ToHostBuffer_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1717,
                    "old_length": 7,
                    "new_start": 1717,
                    "new_length": 7,
                    "hunk": "@@ -1717,7 +1717,7 @@ PJRT_Error* PJRT_Buffer_ToHostBuffer(PJRT_Buffer_ToHostBuffer_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_IsOnCpu(PJRT_Buffer_IsOnCpu_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_IsOnCpu_Args\", PJRT_Buffer_IsOnCpu_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->is_on_cpu = args->buffer->buffer->IsOnCpu();\n"
                },
                {
                    "old_start": 1725,
                    "old_length": 7,
                    "new_start": 1725,
                    "new_length": 7,
                    "hunk": "@@ -1725,7 +1725,7 @@ PJRT_Error* PJRT_Buffer_IsOnCpu(PJRT_Buffer_IsOnCpu_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_ReadyEvent(PJRT_Buffer_ReadyEvent_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_ReadyEvent_Args\", PJRT_Buffer_ReadyEvent_Args_STRUCT_SIZE,\n       args->struct_size));\n   xla::PjRtFuture<xla::Status> wrapped_promise =\n"
                },
                {
                    "old_start": 1735,
                    "old_length": 7,
                    "new_start": 1735,
                    "new_length": 7,
                    "hunk": "@@ -1735,7 +1735,7 @@ PJRT_Error* PJRT_Buffer_ReadyEvent(PJRT_Buffer_ReadyEvent_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_UnsafePointer(PJRT_Buffer_UnsafePointer_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_UnsafePointer_Args\",\n       PJRT_Buffer_UnsafePointer_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1747,
                    "old_length": 7,
                    "new_start": 1747,
                    "new_length": 7,
                    "hunk": "@@ -1747,7 +1747,7 @@ PJRT_Error* PJRT_Buffer_UnsafePointer(PJRT_Buffer_UnsafePointer_Args* args) {\n \n PJRT_Error* PJRT_Buffer_IncreaseExternalReferenceCount(\n     PJRT_Buffer_IncreaseExternalReferenceCount_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_IncreaseExternalReferenceCount_Args\",\n       PJRT_Buffer_IncreaseExternalReferenceCount_Args_STRUCT_SIZE,\n       args->struct_size));\n"
                },
                {
                    "old_start": 1760,
                    "old_length": 7,
                    "new_start": 1760,
                    "new_length": 7,
                    "hunk": "@@ -1760,7 +1760,7 @@ PJRT_Error* PJRT_Buffer_IncreaseExternalReferenceCount(\n \n PJRT_Error* PJRT_Buffer_DecreaseExternalReferenceCount(\n     PJRT_Buffer_DecreaseExternalReferenceCount_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_DecreaseExternalReferenceCount_Args\",\n       PJRT_Buffer_DecreaseExternalReferenceCount_Args_STRUCT_SIZE,\n       args->struct_size));\n"
                },
                {
                    "old_start": 1778,
                    "old_length": 7,
                    "new_start": 1778,
                    "new_length": 7,
                    "hunk": "@@ -1778,7 +1778,7 @@ PJRT_Error* PJRT_Buffer_DecreaseExternalReferenceCount(\n \n PJRT_Error* PJRT_Buffer_OpaqueDeviceMemoryDataPointer(\n     PJRT_Buffer_OpaqueDeviceMemoryDataPointer_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_OpaqueDeviceMemoryDataPointer_Args\",\n       PJRT_Buffer_OpaqueDeviceMemoryDataPointer_Args_STRUCT_SIZE,\n       args->struct_size));\n"
                },
                {
                    "old_start": 1793,
                    "old_length": 7,
                    "new_start": 1793,
                    "new_length": 7,
                    "hunk": "@@ -1793,7 +1793,7 @@ PJRT_Error* PJRT_Buffer_OpaqueDeviceMemoryDataPointer(\n \n PJRT_Error* PJRT_CopyToDeviceStream_Destroy(\n     PJRT_CopyToDeviceStream_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CopyToDeviceStream_Destroy\",\n       PJRT_CopyToDeviceStream_Destroy_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1803,
                    "old_length": 7,
                    "new_start": 1803,
                    "new_length": 7,
                    "hunk": "@@ -1803,7 +1803,7 @@ PJRT_Error* PJRT_CopyToDeviceStream_Destroy(\n \n PJRT_Error* PJRT_CopyToDeviceStream_AddChunk(\n     PJRT_CopyToDeviceStream_AddChunk_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CopyToDeviceStream_AddChunk_Args\",\n       PJRT_CopyToDeviceStream_AddChunk_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1815,
                    "old_length": 7,
                    "new_start": 1815,
                    "new_length": 7,
                    "hunk": "@@ -1815,7 +1815,7 @@ PJRT_Error* PJRT_CopyToDeviceStream_AddChunk(\n \n PJRT_Error* PJRT_CopyToDeviceStream_TotalBytes(\n     PJRT_CopyToDeviceStream_TotalBytes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CopyToDeviceStream_TotalBytes_Args\",\n       PJRT_CopyToDeviceStream_TotalBytes_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1825,
                    "old_length": 7,
                    "new_start": 1825,
                    "new_length": 7,
                    "hunk": "@@ -1825,7 +1825,7 @@ PJRT_Error* PJRT_CopyToDeviceStream_TotalBytes(\n \n PJRT_Error* PJRT_CopyToDeviceStream_GranuleSize(\n     PJRT_CopyToDeviceStream_GranuleSize_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CopyToDeviceStream_GranuleSize_Args\",\n       PJRT_CopyToDeviceStream_GranuleSize_Args_STRUCT_SIZE, args->struct_size));\n \n"
                },
                {
                    "old_start": 1835,
                    "old_length": 7,
                    "new_start": 1835,
                    "new_length": 7,
                    "hunk": "@@ -1835,7 +1835,7 @@ PJRT_Error* PJRT_CopyToDeviceStream_GranuleSize(\n \n PJRT_Error* PJRT_CopyToDeviceStream_CurrentBytes(\n     PJRT_CopyToDeviceStream_CurrentBytes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CopyToDeviceStream_CurrentBytes_Args\",\n       PJRT_CopyToDeviceStream_CurrentBytes_Args_STRUCT_SIZE,\n       args->struct_size));\n"
                },
                {
                    "old_start": 1847,
                    "old_length": 7,
                    "new_start": 1847,
                    "new_length": 7,
                    "hunk": "@@ -1847,7 +1847,7 @@ PJRT_Error* PJRT_CopyToDeviceStream_CurrentBytes(\n // -------------------------------- Events -------------------------------------\n \n PJRT_Error* PJRT_Event_Destroy(PJRT_Event_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Event_Destroy\", PJRT_Event_Destroy_Args_STRUCT_SIZE,\n       args->struct_size));\n \n"
                },
                {
                    "old_start": 1856,
                    "old_length": 7,
                    "new_start": 1856,
                    "new_length": 7,
                    "hunk": "@@ -1856,7 +1856,7 @@ PJRT_Error* PJRT_Event_Destroy(PJRT_Event_Destroy_Args* args) {\n }\n \n PJRT_Error* PJRT_Event_IsReady(PJRT_Event_IsReady_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Event_IsReady\", PJRT_Event_IsReady_Args_STRUCT_SIZE,\n       args->struct_size));\n \n"
                },
                {
                    "old_start": 1865,
                    "old_length": 7,
                    "new_start": 1865,
                    "new_length": 7,
                    "hunk": "@@ -1865,7 +1865,7 @@ PJRT_Error* PJRT_Event_IsReady(PJRT_Event_IsReady_Args* args) {\n }\n \n PJRT_Error* PJRT_Event_Await(PJRT_Event_Await_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Event_Await\", PJRT_Event_Await_Args_STRUCT_SIZE,\n       args->struct_size));\n \n"
                },
                {
                    "old_start": 1876,
                    "old_length": 7,
                    "new_start": 1876,
                    "new_length": 7,
                    "hunk": "@@ -1876,7 +1876,7 @@ PJRT_Error* PJRT_Event_Await(PJRT_Event_Await_Args* args) {\n }\n \n PJRT_Error* PJRT_Event_Error(PJRT_Event_Error_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Event_Error\", PJRT_Event_Error_Args_STRUCT_SIZE,\n       args->struct_size));\n \n"
                },
                {
                    "old_start": 1894,
                    "old_length": 7,
                    "new_start": 1894,
                    "new_length": 7,
                    "hunk": "@@ -1894,7 +1894,7 @@ PJRT_Error* PJRT_Event_Error(PJRT_Event_Error_Args* args) {\n }\n \n PJRT_Error* PJRT_Event_OnReady(PJRT_Event_OnReady_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Event_OnReady\", PJRT_Event_OnReady_Args_STRUCT_SIZE,\n       args->struct_size));\n \n"
                },
                {
                    "old_start": 1915,
                    "old_length": 7,
                    "new_start": 1915,
                    "new_length": 7,
                    "hunk": "@@ -1915,7 +1915,7 @@ PJRT_Error* PJRT_Event_OnReady(PJRT_Event_OnReady_Args* args) {\n \n PJRT_Error* PJRT_TopologyDescription_Destroy(\n     PJRT_TopologyDescription_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_Destroy_Args\",\n       PJRT_TopologyDescription_Destroy_Args_STRUCT_SIZE, args->struct_size));\n   delete args->topology;\n"
                },
                {
                    "old_start": 1924,
                    "old_length": 7,
                    "new_start": 1924,
                    "new_length": 7,
                    "hunk": "@@ -1924,7 +1924,7 @@ PJRT_Error* PJRT_TopologyDescription_Destroy(\n \n PJRT_Error* PJRT_TopologyDescription_PlatformName(\n     PJRT_TopologyDescription_PlatformName_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_PlatformName_Args\",\n       PJRT_TopologyDescription_PlatformName_Args_STRUCT_SIZE,\n       args->struct_size));\n"
                },
                {
                    "old_start": 1936,
                    "old_length": 7,
                    "new_start": 1936,
                    "new_length": 7,
                    "hunk": "@@ -1936,7 +1936,7 @@ PJRT_Error* PJRT_TopologyDescription_PlatformName(\n \n PJRT_Error* PJRT_TopologyDescription_PlatformVersion(\n     PJRT_TopologyDescription_PlatformVersion_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_PlatformVersion_Args\",\n       PJRT_TopologyDescription_PlatformVersion_Args_STRUCT_SIZE,\n       args->struct_size));\n"
                },
                {
                    "old_start": 1949,
                    "old_length": 7,
                    "new_start": 1949,
                    "new_length": 7,
                    "hunk": "@@ -1949,7 +1949,7 @@ PJRT_Error* PJRT_TopologyDescription_PlatformVersion(\n \n PJRT_Error* PJRT_TopologyDescription_GetDeviceDescriptions(\n     PJRT_TopologyDescription_GetDeviceDescriptions_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_GetDeviceDescriptions_Args\",\n       PJRT_TopologyDescription_GetDeviceDescriptions_Args_STRUCT_SIZE,\n       args->struct_size));\n"
                },
                {
                    "old_start": 1960,
                    "old_length": 7,
                    "new_start": 1960,
                    "new_length": 7,
                    "hunk": "@@ -1960,7 +1960,7 @@ PJRT_Error* PJRT_TopologyDescription_GetDeviceDescriptions(\n \n PJRT_Error* PJRT_TopologyDescription_Serialize(\n     PJRT_TopologyDescription_Serialize_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_Serialize_Args\",\n       PJRT_TopologyDescription_Serialize_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(std::string out, args->topology->topology->Serialize());\n"
                },
                {
                    "old_start": 1977,
                    "old_length": 7,
                    "new_start": 1977,
                    "new_length": 7,
                    "hunk": "@@ -1977,7 +1977,7 @@ PJRT_Error* PJRT_TopologyDescription_Serialize(\n \n PJRT_Error* PJRT_TopologyDescription_Attributes(\n     PJRT_TopologyDescription_Attributes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_Attributes_Args\",\n       PJRT_TopologyDescription_Attributes_Args_STRUCT_SIZE, args->struct_size));\n   args->attributes = args->topology->attributes.data();\n"
                },
                {
                    "old_start": 1986,
                    "old_length": 9,
                    "new_start": 1986,
                    "new_length": 9,
                    "hunk": "@@ -1986,9 +1986,9 @@ PJRT_Error* PJRT_TopologyDescription_Attributes(\n }\n \n PJRT_Error* PJRT_Compile(PJRT_Compile_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Compile_Args\", PJRT_Compile_Args_STRUCT_SIZE, args->struct_size));\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Program\", PJRT_Program_STRUCT_SIZE, args->program->struct_size));\n \n   xla::PjRtClient* client = nullptr;"
                }
            ],
            "whole_deleted": "-  xla::Status struct_size_check = CheckMatchingStructSizes(\n-  xla::Status struct_size_check = CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\"PJRT_Memory_Id_Args\",\n-                                                PJRT_Memory_Id_Args_STRUCT_SIZE,\n-                                                args->struct_size));\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  TF_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  TF_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\"PJRT_ExecuteOptions\",\n-                                                PJRT_ExecuteOptions_STRUCT_SIZE,\n-                                                args->options->struct_size));\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n",
            "whole_added": "+  xla::Status struct_size_check = ActualStructSizeIsGreaterOrEqual(\n+  xla::Status struct_size_check = ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+      \"PJRT_Memory_Id_Args\", PJRT_Memory_Id_Args_STRUCT_SIZE,\n+      args->struct_size));\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  TF_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  TF_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+      \"PJRT_ExecuteOptions\", PJRT_ExecuteOptions_STRUCT_SIZE,\n+      args->options->struct_size));\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n",
            "whole_hunk": "@@ -285,7 +285,7 @@ xla::PjRtClient::KeyValuePutCallback ToCppKeyValuePutCallback(\n // ---------------------------------- Errors -----------------------------------\n \n void PJRT_Error_Destroy(PJRT_Error_Destroy_Args* args) {\n-  xla::Status struct_size_check = CheckMatchingStructSizes(\n+  xla::Status struct_size_check = ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Error_Destroy_Args\", PJRT_Error_Destroy_Args_STRUCT_SIZE,\n       args->struct_size);\n   if (!struct_size_check.ok()) {\n@@ -297,7 +297,7 @@ void PJRT_Error_Destroy(PJRT_Error_Destroy_Args* args) {\n }\n \n void PJRT_Error_Message(PJRT_Error_Message_Args* args) {\n-  xla::Status struct_size_check = CheckMatchingStructSizes(\n+  xla::Status struct_size_check = ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Error_Message_Args\", PJRT_Error_Message_Args_STRUCT_SIZE,\n       args->struct_size);\n   if (!struct_size_check.ok()) {\n@@ -311,7 +311,7 @@ void PJRT_Error_Message(PJRT_Error_Message_Args* args) {\n }\n \n PJRT_Error* PJRT_Error_GetCode(PJRT_Error_GetCode_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Error_GetCode_Args\", PJRT_Error_GetCode_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->code = StatusCodeToPjrtErrorCode(\n@@ -322,7 +322,7 @@ PJRT_Error* PJRT_Error_GetCode(PJRT_Error_GetCode_Args* args) {\n // ---------------------------------- Plugin -----------------------------------\n \n PJRT_Error* PJRT_Plugin_Attributes(PJRT_Plugin_Attributes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Plugin_Attributes_Args\", PJRT_Plugin_Attributes_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->num_attributes = 0;\n@@ -330,7 +330,7 @@ PJRT_Error* PJRT_Plugin_Attributes(PJRT_Plugin_Attributes_Args* args) {\n }\n \n PJRT_Error* PJRT_Plugin_Initialize_NoOp(PJRT_Plugin_Initialize_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Plugin_Initialize_Args\", PJRT_Plugin_Initialize_Args_STRUCT_SIZE,\n       args->struct_size));\n   return nullptr;\n@@ -339,7 +339,7 @@ PJRT_Error* PJRT_Plugin_Initialize_NoOp(PJRT_Plugin_Initialize_Args* args) {\n // ---------------------------------- Client -----------------------------------\n \n PJRT_Error* PJRT_Client_Destroy(PJRT_Client_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_Destroy_Args\", PJRT_Client_Destroy_Args_STRUCT_SIZE,\n       args->struct_size));\n   delete args->client;\n@@ -347,7 +347,7 @@ PJRT_Error* PJRT_Client_Destroy(PJRT_Client_Destroy_Args* args) {\n }\n \n PJRT_Error* PJRT_Client_ProcessIndex(PJRT_Client_ProcessIndex_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CLient_ProcessIndex_Args\",\n       PJRT_Client_ProcessIndex_Args_STRUCT_SIZE, args->struct_size));\n   args->process_index = args->client->client->process_index();\n@@ -355,7 +355,7 @@ PJRT_Error* PJRT_Client_ProcessIndex(PJRT_Client_ProcessIndex_Args* args) {\n }\n \n PJRT_Error* PJRT_Client_PlatformName(PJRT_Client_PlatformName_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_PlatformName_Args\",\n       PJRT_Client_PlatformName_Args_STRUCT_SIZE, args->struct_size));\n   absl::string_view platform_name = args->client->client->platform_name();\n@@ -366,7 +366,7 @@ PJRT_Error* PJRT_Client_PlatformName(PJRT_Client_PlatformName_Args* args) {\n \n PJRT_Error* PJRT_Client_PlatformVersion(\n     PJRT_Client_PlatformVersion_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CLient_PlatformVersion_Args\",\n       PJRT_Client_PlatformVersion_Args_STRUCT_SIZE, args->struct_size));\n   absl::string_view platform_version = args->client->client->platform_version();\n@@ -376,7 +376,7 @@ PJRT_Error* PJRT_Client_PlatformVersion(\n }\n \n PJRT_Error* PJRT_Client_Devices(PJRT_Client_Devices_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_Devices_Args\", PJRT_Client_Devices_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->num_devices = args->client->devices.size();\n@@ -386,7 +386,7 @@ PJRT_Error* PJRT_Client_Devices(PJRT_Client_Devices_Args* args) {\n \n PJRT_Error* PJRT_Client_AddressableDevices(\n     PJRT_Client_AddressableDevices_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_AddressableDevices_Args\",\n       PJRT_Client_AddressableDevices_Args_STRUCT_SIZE, args->struct_size));\n   args->num_addressable_devices = args->client->addressable_devices.size();\n@@ -395,7 +395,7 @@ PJRT_Error* PJRT_Client_AddressableDevices(\n }\n \n PJRT_Error* PJRT_Client_LookupDevice(PJRT_Client_LookupDevice_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_LookupDevice_Args\",\n       PJRT_Client_LookupDevice_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(xla::PjRtDevice * device,\n@@ -406,7 +406,7 @@ PJRT_Error* PJRT_Client_LookupDevice(PJRT_Client_LookupDevice_Args* args) {\n \n PJRT_Error* PJRT_Client_LookupAddressableDevice(\n     PJRT_Client_LookupAddressableDevice_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_LookupAddressableDevice_Args\",\n       PJRT_Client_LookupAddressableDevice_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(\n@@ -418,7 +418,7 @@ PJRT_Error* PJRT_Client_LookupAddressableDevice(\n \n PJRT_Error* PJRT_LoadedExecutable_Fingerprint(\n     PJRT_LoadedExecutable_Fingerprint_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_Fingerprint_Args\",\n       PJRT_LoadedExecutable_Fingerprint_Args_STRUCT_SIZE, args->struct_size));\n   const xla::Status& status = args->executable->fingerprint.status();\n@@ -439,7 +439,7 @@ PJRT_Error* PJRT_LoadedExecutable_Fingerprint(\n \n PJRT_Error* PJRT_Client_AddressableMemories(\n     PJRT_Client_AddressableMemories_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_AddressableMemories_Args\",\n       PJRT_Client_AddressableMemories_Args_STRUCT_SIZE, args->struct_size));\n   args->num_addressable_memories = args->client->addressable_memories.size();\n@@ -553,10 +553,10 @@ const xla::XlaComputation& UnpackPjrtProgram(\n }  // namespace\n \n PJRT_Error* PJRT_Client_Compile(PJRT_Client_Compile_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_Compile_Args\", PJRT_Client_Compile_Args_STRUCT_SIZE,\n       args->struct_size));\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Program\", PJRT_Program_STRUCT_SIZE, args->program->struct_size));\n \n   PJRT_ASSIGN_OR_RETURN(\n@@ -592,7 +592,7 @@ static void PopulateDeviceAssignment(int* const device_assignment_buffer,\n \n PJRT_Error* PJRT_Client_DefaultDeviceAssignment(\n     PJRT_Client_DefaultDeviceAssignment_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_DefaultAssignment_Args\",\n       PJRT_Client_DefaultDeviceAssignment_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -618,7 +618,7 @@ PJRT_Error* PJRT_Client_DefaultDeviceAssignment(\n \n PJRT_Error* PJRT_Client_BufferFromHostBuffer(\n     PJRT_Client_BufferFromHostBuffer_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Client_BufferFromHostBuffer_Args\",\n       PJRT_Client_BufferFromHostBuffer_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -739,7 +739,7 @@ PJRT_Error* PJRT_Client_CreateViewOfDeviceBuffer(\n // --------------------------------- Devices -----------------------------------\n \n PJRT_Error* PJRT_DeviceDescription_Id(PJRT_DeviceDescription_Id_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_Id_Args\",\n       PJRT_DeviceDescription_Id_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -749,7 +749,7 @@ PJRT_Error* PJRT_DeviceDescription_Id(PJRT_DeviceDescription_Id_Args* args) {\n \n PJRT_Error* PJRT_DeviceDescription_ProcessIndex(\n     PJRT_DeviceDescription_ProcessIndex_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_ProcessIndex_Args\",\n       PJRT_DeviceDescription_ProcessIndex_Args_STRUCT_SIZE, args->struct_size));\n   args->process_index =\n@@ -759,7 +759,7 @@ PJRT_Error* PJRT_DeviceDescription_ProcessIndex(\n \n PJRT_Error* PJRT_DeviceDescription_Attributes(\n     PJRT_DeviceDescription_Attributes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_Attributes_Args\",\n       PJRT_DeviceDescription_Attributes_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -772,7 +772,7 @@ PJRT_Error* PJRT_DeviceDescription_Attributes(\n \n PJRT_Error* PJRT_DeviceDescription_Kind(\n     PJRT_DeviceDescription_Kind_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_Kind_Args\",\n       PJRT_DeviceDescription_Kind_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -785,7 +785,7 @@ PJRT_Error* PJRT_DeviceDescription_Kind(\n \n PJRT_Error* PJRT_DeviceDescription_DebugString(\n     PJRT_DeviceDescription_DebugString_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_DebugString_Args\",\n       PJRT_DeviceDescription_DebugString_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -798,7 +798,7 @@ PJRT_Error* PJRT_DeviceDescription_DebugString(\n \n PJRT_Error* PJRT_DeviceDescription_ToString(\n     PJRT_DeviceDescription_ToString_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_DeviceDescription_ToString_Args\",\n       PJRT_DeviceDescription_ToString_Args_STRUCT_SIZE, args->struct_size));\n   args->to_string =\n@@ -809,7 +809,7 @@ PJRT_Error* PJRT_DeviceDescription_ToString(\n }\n \n PJRT_Error* PJRT_Device_GetDescription(PJRT_Device_GetDescription_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_GetDescription_Args\",\n       PJRT_Device_GetDescription_Args_STRUCT_SIZE, args->struct_size));\n   args->device_description = &args->device->description;\n@@ -817,7 +817,7 @@ PJRT_Error* PJRT_Device_GetDescription(PJRT_Device_GetDescription_Args* args) {\n }\n \n PJRT_Error* PJRT_Device_IsAddressable(PJRT_Device_IsAddressable_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_IsAddressable_Args\",\n       PJRT_Device_IsAddressable_Args_STRUCT_SIZE, args->struct_size));\n   args->is_addressable = args->device->device->IsAddressable();\n@@ -826,7 +826,7 @@ PJRT_Error* PJRT_Device_IsAddressable(PJRT_Device_IsAddressable_Args* args) {\n \n PJRT_Error* PJRT_Device_LocalHardwareId(\n     PJRT_Device_LocalHardwareId_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_LocalHardwareId_Args\",\n       PJRT_Device_LocalHardwareId_Args_STRUCT_SIZE, args->struct_size));\n   args->local_hardware_id = args->device->device->local_hardware_id();\n@@ -835,7 +835,7 @@ PJRT_Error* PJRT_Device_LocalHardwareId(\n \n PJRT_Error* PJRT_Device_AddressableMemories(\n     PJRT_Device_AddressableMemories_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_AddressableMemories_Args\",\n       PJRT_Device_AddressableMemories_Args_STRUCT_SIZE, args->struct_size));\n   args->memories = args->device->addressable_memories.data();\n@@ -844,7 +844,7 @@ PJRT_Error* PJRT_Device_AddressableMemories(\n }\n \n PJRT_Error* PJRT_Device_DefaultMemory(PJRT_Device_DefaultMemory_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_DefaultMemory_Args\",\n       PJRT_Device_DefaultMemory_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(xla::PjRtMemorySpace * memory_space,\n@@ -854,7 +854,7 @@ PJRT_Error* PJRT_Device_DefaultMemory(PJRT_Device_DefaultMemory_Args* args) {\n }\n \n PJRT_Error* PJRT_Device_MemoryStats(PJRT_Device_MemoryStats_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Device_MemoryStats_Args\", PJRT_Device_MemoryStats_Args_STRUCT_SIZE,\n       args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(tsl::AllocatorStats stats,\n@@ -904,16 +904,16 @@ PJRT_Error* PJRT_Device_MemoryStats(PJRT_Device_MemoryStats_Args* args) {\n // ------------------------------- Memory --------------------------------------\n \n PJRT_Error* PJRT_Memory_Id(PJRT_Memory_Id_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\"PJRT_Memory_Id_Args\",\n-                                                PJRT_Memory_Id_Args_STRUCT_SIZE,\n-                                                args->struct_size));\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+      \"PJRT_Memory_Id_Args\", PJRT_Memory_Id_Args_STRUCT_SIZE,\n+      args->struct_size));\n \n   args->id = args->memory->memory_space->id();\n   return nullptr;\n }\n \n PJRT_Error* PJRT_Memory_Kind(PJRT_Memory_Kind_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Memory_Kind_Args\", PJRT_Memory_Kind_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->memory_kind = args->memory->memory_space->memory_space_kind().data();\n@@ -923,7 +923,7 @@ PJRT_Error* PJRT_Memory_Kind(PJRT_Memory_Kind_Args* args) {\n }\n \n PJRT_Error* PJRT_Memory_DebugString(PJRT_Memory_DebugString_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Memory_DebugString_Args\", PJRT_Memory_DebugString_Args_STRUCT_SIZE,\n       args->struct_size));\n \n@@ -933,7 +933,7 @@ PJRT_Error* PJRT_Memory_DebugString(PJRT_Memory_DebugString_Args* args) {\n }\n \n PJRT_Error* PJRT_Memory_ToString(PJRT_Memory_ToString_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Memory_ToString_Args\", PJRT_Memory_ToString_Args_STRUCT_SIZE,\n       args->struct_size));\n \n@@ -944,7 +944,7 @@ PJRT_Error* PJRT_Memory_ToString(PJRT_Memory_ToString_Args* args) {\n \n PJRT_Error* PJRT_Memory_AddressableByDevices(\n     PJRT_Memory_AddressableByDevices_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Memory_AddressableByDevices_Args\",\n       PJRT_Memory_AddressableByDevices_Args_STRUCT_SIZE, args->struct_size));\n   args->devices = args->memory->devices.data();\n@@ -955,7 +955,7 @@ PJRT_Error* PJRT_Memory_AddressableByDevices(\n // ------------------------------- Executables ---------------------------------\n \n PJRT_Error* PJRT_Executable_Destroy(PJRT_Executable_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_Destroy_Args\", PJRT_Executable_Destroy_Args_STRUCT_SIZE,\n       args->struct_size));\n   delete args->executable;\n@@ -964,7 +964,7 @@ PJRT_Error* PJRT_Executable_Destroy(PJRT_Executable_Destroy_Args* args) {\n \n PJRT_Error* PJRT_LoadedExecutable_Destroy(\n     PJRT_LoadedExecutable_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_Destroy_Args\",\n       PJRT_LoadedExecutable_Destroy_Args_STRUCT_SIZE, args->struct_size));\n   delete args->executable;\n@@ -972,7 +972,7 @@ PJRT_Error* PJRT_LoadedExecutable_Destroy(\n }\n \n PJRT_Error* PJRT_Executable_Name(PJRT_Executable_Name_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_Name_Args\", PJRT_Executable_Name_Args_STRUCT_SIZE,\n       args->struct_size));\n   absl::string_view executable_name = args->executable->get()->name();\n@@ -983,7 +983,7 @@ PJRT_Error* PJRT_Executable_Name(PJRT_Executable_Name_Args* args) {\n \n PJRT_Error* PJRT_Executable_NumReplicas(\n     PJRT_Executable_NumReplicas_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_NumReplicas_Args\",\n       PJRT_Executable_NumReplicas_Args_STRUCT_SIZE, args->struct_size));\n   args->num_replicas = args->executable->get()->num_replicas();\n@@ -992,7 +992,7 @@ PJRT_Error* PJRT_Executable_NumReplicas(\n \n PJRT_Error* PJRT_Executable_NumPartitions(\n     PJRT_Executable_NumPartitions_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_NumPartitions_Args\",\n       PJRT_Executable_NumPartitions_Args_STRUCT_SIZE, args->struct_size));\n   args->num_partitions = args->executable->get()->num_partitions();\n@@ -1001,7 +1001,7 @@ PJRT_Error* PJRT_Executable_NumPartitions(\n \n PJRT_Error* PJRT_LoadedExecutable_AddressableDevices(\n     PJRT_LoadedExecutable_AddressableDevices_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_AddressableDevices_Args\",\n       PJRT_LoadedExecutable_AddressableDevices_Args_STRUCT_SIZE,\n       args->struct_size));\n@@ -1012,7 +1012,7 @@ PJRT_Error* PJRT_LoadedExecutable_AddressableDevices(\n }\n \n PJRT_Error* PJRT_Executable_NumOutputs(PJRT_Executable_NumOutputs_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_NumOutputs_Args\",\n       PJRT_Executable_NumOutputs_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(std::vector<xla::Shape> output_shapes,\n@@ -1040,7 +1040,7 @@ PJRT_Error* PJRT_Executable_NumOutputs(PJRT_Executable_NumOutputs_Args* args) {\n \n PJRT_Error* PJRT_Executable_SizeOfGeneratedCodeInBytes(\n     PJRT_Executable_SizeOfGeneratedCodeInBytes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_SizeOfGeneratedCodeInBytes_Args\",\n       PJRT_Executable_SizeOfGeneratedCodeInBytes_Args_STRUCT_SIZE,\n       args->struct_size));\n@@ -1051,10 +1051,10 @@ PJRT_Error* PJRT_Executable_SizeOfGeneratedCodeInBytes(\n \n static xla::Status VerifyOptimizedProgramArgs(\n     PJRT_Executable_OptimizedProgram_Args* args) {\n-  TF_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  TF_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_OptimizedProgram_Args\",\n       PJRT_Executable_OptimizedProgram_Args_STRUCT_SIZE, args->struct_size));\n-  TF_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  TF_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Program\", PJRT_Program_STRUCT_SIZE, args->program->struct_size));\n   return xla::OkStatus();\n }\n@@ -1117,7 +1117,7 @@ PJRT_Error* PJRT_Executable_OptimizedProgram(\n \n PJRT_Error* PJRT_Executable_GetCostAnalysis(\n     PJRT_Executable_GetCostAnalysis_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_GetCostAnalysis_Args\",\n       PJRT_Executable_GetCostAnalysis_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1141,7 +1141,7 @@ PJRT_Error* PJRT_Executable_GetCostAnalysis(\n \n PJRT_Error* PJRT_Executable_OutputElementTypes(\n     PJRT_Executable_OutputElementTypes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_OutputElementTypes_Args\",\n       PJRT_Executable_OutputElementTypes_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1161,7 +1161,7 @@ PJRT_Error* PJRT_Executable_OutputElementTypes(\n \n PJRT_Error* PJRT_Executable_OutputDimensions(\n     PJRT_Executable_OutputDimensions_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_OutputDimensions_Args\",\n       PJRT_Executable_OutputDimensions_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1182,7 +1182,7 @@ PJRT_Error* PJRT_Executable_OutputDimensions(\n \n PJRT_Error* PJRT_Executable_OutputMemoryKinds(\n     PJRT_Executable_OutputMemoryKinds_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_OutputMemoryKinds_Args\",\n       PJRT_Executable_OutputMemoryKinds_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1203,7 +1203,7 @@ PJRT_Error* PJRT_Executable_OutputMemoryKinds(\n \n PJRT_Error* PJRT_LoadedExecutable_Delete(\n     PJRT_LoadedExecutable_Delete_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_Delete_Args\",\n       PJRT_LoadedExecutable_Delete_Args_STRUCT_SIZE, args->struct_size));\n   args->executable->get()->Delete();\n@@ -1212,7 +1212,7 @@ PJRT_Error* PJRT_LoadedExecutable_Delete(\n \n PJRT_Error* PJRT_LoadedExecutable_IsDeleted(\n     PJRT_LoadedExecutable_IsDeleted_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_IsDeleted_Args\",\n       PJRT_LoadedExecutable_IsDeleted_Args_STRUCT_SIZE, args->struct_size));\n   args->is_deleted = args->executable->get()->IsDeleted();\n@@ -1311,12 +1311,12 @@ static std::vector<std::vector<xla::PjRtBuffer*>> Convert2DCBuffersToCppBuffers(\n \n PJRT_Error* PJRT_LoadedExecutable_Execute(\n     PJRT_LoadedExecutable_Execute_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_Execute_Args\",\n       PJRT_LoadedExecutable_Execute_Args_STRUCT_SIZE, args->struct_size));\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\"PJRT_ExecuteOptions\",\n-                                                PJRT_ExecuteOptions_STRUCT_SIZE,\n-                                                args->options->struct_size));\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n+      \"PJRT_ExecuteOptions\", PJRT_ExecuteOptions_STRUCT_SIZE,\n+      args->options->struct_size));\n   xla::ExecuteOptions options;\n   options.launch_id = args->options->launch_id;\n   options.strict_shape_checking = true;\n@@ -1438,7 +1438,7 @@ PJRT_Error* PJRT_LoadedExecutable_Execute(\n }\n \n PJRT_Error* PJRT_Executable_Serialize(PJRT_Executable_Serialize_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_Serialize_Args\",\n       PJRT_Executable_Serialize_Args_STRUCT_SIZE, args->struct_size));\n   std::string serialization;\n@@ -1463,7 +1463,7 @@ PJRT_Error* PJRT_Executable_Serialize(PJRT_Executable_Serialize_Args* args) {\n \n PJRT_Error* PJRT_Executable_DeserializeAndLoad(\n     PJRT_Executable_DeserializeAndLoad_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Executable_DeserializeAndLoad_Args\",\n       PJRT_Executable_DeserializeAndLoad_Args_STRUCT_SIZE, args->struct_size));\n   absl::string_view serialized(args->serialized_executable,\n@@ -1480,7 +1480,7 @@ PJRT_Error* PJRT_Executable_DeserializeAndLoad(\n \n PJRT_Error* PJRT_LoadedExecutable_GetExecutable(\n     PJRT_LoadedExecutable_GetExecutable_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_LoadedExecutable_GetExecutable_Args\",\n       PJRT_LoadedExecutable_GetExecutable_Args_STRUCT_SIZE, args->struct_size));\n   args->executable = new PJRT_Executable{args->loaded_executable->executable};\n@@ -1490,7 +1490,7 @@ PJRT_Error* PJRT_LoadedExecutable_GetExecutable(\n // ---------------------------------- Buffers ----------------------------------\n \n PJRT_Error* PJRT_Buffer_Destroy(PJRT_Buffer_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_Destroy_Args\", PJRT_Buffer_Destroy_Args_STRUCT_SIZE,\n       args->struct_size));\n   delete args->buffer;\n@@ -1498,7 +1498,7 @@ PJRT_Error* PJRT_Buffer_Destroy(PJRT_Buffer_Destroy_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_ElementType(PJRT_Buffer_ElementType_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_ElementType_Args\", PJRT_Buffer_ElementType_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->type = ConvertToPjRtBufferType(args->buffer->buffer->element_type());\n@@ -1506,7 +1506,7 @@ PJRT_Error* PJRT_Buffer_ElementType(PJRT_Buffer_ElementType_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_Dimensions(PJRT_Buffer_Dimensions_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_Dimensions_Args\", PJRT_Buffer_Dimensions_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->dims = args->buffer->buffer->dimensions().data();\n@@ -1516,7 +1516,7 @@ PJRT_Error* PJRT_Buffer_Dimensions(PJRT_Buffer_Dimensions_Args* args) {\n \n PJRT_Error* PJRT_Buffer_UnpaddedDimensions(\n     PJRT_Buffer_UnpaddedDimensions_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_UnpaddedDimensions_Args\",\n       PJRT_Buffer_UnpaddedDimensions_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1537,7 +1537,7 @@ PJRT_Error* PJRT_Buffer_UnpaddedDimensions(\n \n PJRT_Error* PJRT_Buffer_DynamicDimensionIndices(\n     PJRT_Buffer_DynamicDimensionIndices_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_DynamicDimensionIndices_Args\",\n       PJRT_Buffer_DynamicDimensionIndices_Args_STRUCT_SIZE, args->struct_size));\n   absl::Span<const bool> is_dyn_dim =\n@@ -1562,7 +1562,7 @@ PJRT_Error* PJRT_Buffer_DynamicDimensionIndices(\n \n PJRT_Error* PJRT_Buffer_GetMemoryLayout(\n     PJRT_Buffer_GetMemoryLayout_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_GetMemoryLayout_Args\",\n       PJRT_Buffer_GetMemoryLayout_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1583,7 +1583,7 @@ PJRT_Error* PJRT_Buffer_GetMemoryLayout(\n \n PJRT_Error* PJRT_Buffer_OnDeviceSizeInBytes(\n     PJRT_Buffer_OnDeviceSizeInBytes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_OnDeviceSizeInBytes_Args\",\n       PJRT_Buffer_OnDeviceSizeInBytes_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(args->on_device_size_in_bytes,\n@@ -1592,7 +1592,7 @@ PJRT_Error* PJRT_Buffer_OnDeviceSizeInBytes(\n }\n \n PJRT_Error* PJRT_Buffer_Device(PJRT_Buffer_Device_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_Device_Args\", PJRT_Buffer_Device_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->device = FindDeviceWrapper(args->buffer->buffer->device(),\n@@ -1605,7 +1605,7 @@ PJRT_Error* PJRT_Buffer_Device(PJRT_Buffer_Device_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_Memory(PJRT_Buffer_Memory_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_Memory_Args\", PJRT_Buffer_Memory_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->memory = FindMemoryWrapper(args->buffer->buffer->memory_space(),\n@@ -1619,7 +1619,7 @@ PJRT_Error* PJRT_Buffer_Memory(PJRT_Buffer_Memory_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_Delete(PJRT_Buffer_Delete_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_Delete_Args\", PJRT_Buffer_Delete_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->buffer->buffer->Delete();\n@@ -1627,7 +1627,7 @@ PJRT_Error* PJRT_Buffer_Delete(PJRT_Buffer_Delete_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_IsDeleted(PJRT_Buffer_IsDeleted_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_IsDeleted_Args\", PJRT_Buffer_IsDeleted_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->is_deleted = args->buffer->buffer->IsDeleted();\n@@ -1635,7 +1635,7 @@ PJRT_Error* PJRT_Buffer_IsDeleted(PJRT_Buffer_IsDeleted_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_CopyToDevice(PJRT_Buffer_CopyToDevice_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_CopyToDevice_Args\",\n       PJRT_Buffer_CopyToDevice_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(\n@@ -1647,7 +1647,7 @@ PJRT_Error* PJRT_Buffer_CopyToDevice(PJRT_Buffer_CopyToDevice_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_CopyToMemory(PJRT_Buffer_CopyToMemory_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_CopyToMemory_Args\",\n       PJRT_Buffer_CopyToMemory_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(\n@@ -1659,7 +1659,7 @@ PJRT_Error* PJRT_Buffer_CopyToMemory(PJRT_Buffer_CopyToMemory_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_ToHostBuffer(PJRT_Buffer_ToHostBuffer_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_ToHostBuffer_Args\",\n       PJRT_Buffer_ToHostBuffer_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1717,7 +1717,7 @@ PJRT_Error* PJRT_Buffer_ToHostBuffer(PJRT_Buffer_ToHostBuffer_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_IsOnCpu(PJRT_Buffer_IsOnCpu_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_IsOnCpu_Args\", PJRT_Buffer_IsOnCpu_Args_STRUCT_SIZE,\n       args->struct_size));\n   args->is_on_cpu = args->buffer->buffer->IsOnCpu();\n@@ -1725,7 +1725,7 @@ PJRT_Error* PJRT_Buffer_IsOnCpu(PJRT_Buffer_IsOnCpu_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_ReadyEvent(PJRT_Buffer_ReadyEvent_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_ReadyEvent_Args\", PJRT_Buffer_ReadyEvent_Args_STRUCT_SIZE,\n       args->struct_size));\n   xla::PjRtFuture<xla::Status> wrapped_promise =\n@@ -1735,7 +1735,7 @@ PJRT_Error* PJRT_Buffer_ReadyEvent(PJRT_Buffer_ReadyEvent_Args* args) {\n }\n \n PJRT_Error* PJRT_Buffer_UnsafePointer(PJRT_Buffer_UnsafePointer_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_UnsafePointer_Args\",\n       PJRT_Buffer_UnsafePointer_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1747,7 +1747,7 @@ PJRT_Error* PJRT_Buffer_UnsafePointer(PJRT_Buffer_UnsafePointer_Args* args) {\n \n PJRT_Error* PJRT_Buffer_IncreaseExternalReferenceCount(\n     PJRT_Buffer_IncreaseExternalReferenceCount_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_IncreaseExternalReferenceCount_Args\",\n       PJRT_Buffer_IncreaseExternalReferenceCount_Args_STRUCT_SIZE,\n       args->struct_size));\n@@ -1760,7 +1760,7 @@ PJRT_Error* PJRT_Buffer_IncreaseExternalReferenceCount(\n \n PJRT_Error* PJRT_Buffer_DecreaseExternalReferenceCount(\n     PJRT_Buffer_DecreaseExternalReferenceCount_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_DecreaseExternalReferenceCount_Args\",\n       PJRT_Buffer_DecreaseExternalReferenceCount_Args_STRUCT_SIZE,\n       args->struct_size));\n@@ -1778,7 +1778,7 @@ PJRT_Error* PJRT_Buffer_DecreaseExternalReferenceCount(\n \n PJRT_Error* PJRT_Buffer_OpaqueDeviceMemoryDataPointer(\n     PJRT_Buffer_OpaqueDeviceMemoryDataPointer_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Buffer_OpaqueDeviceMemoryDataPointer_Args\",\n       PJRT_Buffer_OpaqueDeviceMemoryDataPointer_Args_STRUCT_SIZE,\n       args->struct_size));\n@@ -1793,7 +1793,7 @@ PJRT_Error* PJRT_Buffer_OpaqueDeviceMemoryDataPointer(\n \n PJRT_Error* PJRT_CopyToDeviceStream_Destroy(\n     PJRT_CopyToDeviceStream_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CopyToDeviceStream_Destroy\",\n       PJRT_CopyToDeviceStream_Destroy_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1803,7 +1803,7 @@ PJRT_Error* PJRT_CopyToDeviceStream_Destroy(\n \n PJRT_Error* PJRT_CopyToDeviceStream_AddChunk(\n     PJRT_CopyToDeviceStream_AddChunk_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CopyToDeviceStream_AddChunk_Args\",\n       PJRT_CopyToDeviceStream_AddChunk_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1815,7 +1815,7 @@ PJRT_Error* PJRT_CopyToDeviceStream_AddChunk(\n \n PJRT_Error* PJRT_CopyToDeviceStream_TotalBytes(\n     PJRT_CopyToDeviceStream_TotalBytes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CopyToDeviceStream_TotalBytes_Args\",\n       PJRT_CopyToDeviceStream_TotalBytes_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1825,7 +1825,7 @@ PJRT_Error* PJRT_CopyToDeviceStream_TotalBytes(\n \n PJRT_Error* PJRT_CopyToDeviceStream_GranuleSize(\n     PJRT_CopyToDeviceStream_GranuleSize_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CopyToDeviceStream_GranuleSize_Args\",\n       PJRT_CopyToDeviceStream_GranuleSize_Args_STRUCT_SIZE, args->struct_size));\n \n@@ -1835,7 +1835,7 @@ PJRT_Error* PJRT_CopyToDeviceStream_GranuleSize(\n \n PJRT_Error* PJRT_CopyToDeviceStream_CurrentBytes(\n     PJRT_CopyToDeviceStream_CurrentBytes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_CopyToDeviceStream_CurrentBytes_Args\",\n       PJRT_CopyToDeviceStream_CurrentBytes_Args_STRUCT_SIZE,\n       args->struct_size));\n@@ -1847,7 +1847,7 @@ PJRT_Error* PJRT_CopyToDeviceStream_CurrentBytes(\n // -------------------------------- Events -------------------------------------\n \n PJRT_Error* PJRT_Event_Destroy(PJRT_Event_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Event_Destroy\", PJRT_Event_Destroy_Args_STRUCT_SIZE,\n       args->struct_size));\n \n@@ -1856,7 +1856,7 @@ PJRT_Error* PJRT_Event_Destroy(PJRT_Event_Destroy_Args* args) {\n }\n \n PJRT_Error* PJRT_Event_IsReady(PJRT_Event_IsReady_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Event_IsReady\", PJRT_Event_IsReady_Args_STRUCT_SIZE,\n       args->struct_size));\n \n@@ -1865,7 +1865,7 @@ PJRT_Error* PJRT_Event_IsReady(PJRT_Event_IsReady_Args* args) {\n }\n \n PJRT_Error* PJRT_Event_Await(PJRT_Event_Await_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Event_Await\", PJRT_Event_Await_Args_STRUCT_SIZE,\n       args->struct_size));\n \n@@ -1876,7 +1876,7 @@ PJRT_Error* PJRT_Event_Await(PJRT_Event_Await_Args* args) {\n }\n \n PJRT_Error* PJRT_Event_Error(PJRT_Event_Error_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Event_Error\", PJRT_Event_Error_Args_STRUCT_SIZE,\n       args->struct_size));\n \n@@ -1894,7 +1894,7 @@ PJRT_Error* PJRT_Event_Error(PJRT_Event_Error_Args* args) {\n }\n \n PJRT_Error* PJRT_Event_OnReady(PJRT_Event_OnReady_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Event_OnReady\", PJRT_Event_OnReady_Args_STRUCT_SIZE,\n       args->struct_size));\n \n@@ -1915,7 +1915,7 @@ PJRT_Error* PJRT_Event_OnReady(PJRT_Event_OnReady_Args* args) {\n \n PJRT_Error* PJRT_TopologyDescription_Destroy(\n     PJRT_TopologyDescription_Destroy_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_Destroy_Args\",\n       PJRT_TopologyDescription_Destroy_Args_STRUCT_SIZE, args->struct_size));\n   delete args->topology;\n@@ -1924,7 +1924,7 @@ PJRT_Error* PJRT_TopologyDescription_Destroy(\n \n PJRT_Error* PJRT_TopologyDescription_PlatformName(\n     PJRT_TopologyDescription_PlatformName_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_PlatformName_Args\",\n       PJRT_TopologyDescription_PlatformName_Args_STRUCT_SIZE,\n       args->struct_size));\n@@ -1936,7 +1936,7 @@ PJRT_Error* PJRT_TopologyDescription_PlatformName(\n \n PJRT_Error* PJRT_TopologyDescription_PlatformVersion(\n     PJRT_TopologyDescription_PlatformVersion_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_PlatformVersion_Args\",\n       PJRT_TopologyDescription_PlatformVersion_Args_STRUCT_SIZE,\n       args->struct_size));\n@@ -1949,7 +1949,7 @@ PJRT_Error* PJRT_TopologyDescription_PlatformVersion(\n \n PJRT_Error* PJRT_TopologyDescription_GetDeviceDescriptions(\n     PJRT_TopologyDescription_GetDeviceDescriptions_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_GetDeviceDescriptions_Args\",\n       PJRT_TopologyDescription_GetDeviceDescriptions_Args_STRUCT_SIZE,\n       args->struct_size));\n@@ -1960,7 +1960,7 @@ PJRT_Error* PJRT_TopologyDescription_GetDeviceDescriptions(\n \n PJRT_Error* PJRT_TopologyDescription_Serialize(\n     PJRT_TopologyDescription_Serialize_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_Serialize_Args\",\n       PJRT_TopologyDescription_Serialize_Args_STRUCT_SIZE, args->struct_size));\n   PJRT_ASSIGN_OR_RETURN(std::string out, args->topology->topology->Serialize());\n@@ -1977,7 +1977,7 @@ PJRT_Error* PJRT_TopologyDescription_Serialize(\n \n PJRT_Error* PJRT_TopologyDescription_Attributes(\n     PJRT_TopologyDescription_Attributes_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_TopologyDescription_Attributes_Args\",\n       PJRT_TopologyDescription_Attributes_Args_STRUCT_SIZE, args->struct_size));\n   args->attributes = args->topology->attributes.data();\n@@ -1986,9 +1986,9 @@ PJRT_Error* PJRT_TopologyDescription_Attributes(\n }\n \n PJRT_Error* PJRT_Compile(PJRT_Compile_Args* args) {\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Compile_Args\", PJRT_Compile_Args_STRUCT_SIZE, args->struct_size));\n-  PJRT_RETURN_IF_ERROR(CheckMatchingStructSizes(\n+  PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(\n       \"PJRT_Program\", PJRT_Program_STRUCT_SIZE, args->program->struct_size));\n \n   xla::PjRtClient* client = nullptr;"
        }
    ]
},
{
    "Id": 110,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/73a8d4db003c1b4389c841988814e9972509555b",
    "date": "2024-03-14T01:46:28-07:00",
    "message": "Reland PR #9757: GpuTimer: improve kernel execution time measurement accuracy\n\nImported from GitHub PR https://github.com/openxla/xla/pull/9757\n\nThis PR changes how `GpuTimer` measures execution time, which should improve the measurement accuracy.\nQuoting my comment in the code:\n```c++\n// When a timer is created it launches a delay kernel into the given stream and\n// queues a start event immediately afterwards. This delay kernel blocks\n// execution on the stream until GetElapsedDuration() is called, at which point\n// an end event is queued and the delay kernel exits. This allows the device\n// execution time of the tasks queued to the stream while the timer is active\n// to be measured more accurately.\n```\nthis should improve the accuracy of the measurements that are used to make auto-tuning decisions, especially for small kernels.\n\nThere are a couple of edge cases that have special treatment:\n- if `CUDA_LAUNCH_BLOCKING=1` then the delay kernel will not achieve anything, so we don't launch it\n- if any code in the timed region synchronises the device then the host will wait for the delay kernel to time out before actually launching the kernels to be measured. This will lead to a poor quality measurement, and an error message is printed. This condition can be met if one of the kernels being measured is lazily loaded, as lazy loading can trigger synchronisation. Best practice is to execute a warmup run (without timing enabled) before the timed execution.\n\nTwo parts of the autotuning code are updated to follow this best practice.\n\nThe `GpuTimer::Create*` signatures that were deprecated in https://github.com/openxla/xla/pull/9841 do not benefit from these accuracy improvements.\n\ncc: @sergachev @nouiz\nCopybara import of the project:\n\n--\n2b834f7154158ddc7294d45bb5481b8c38efdc61 by Olli Lupton <olupton@nvidia.com>:\n\nGpuTimer: use delay kernel to improve accuracy\n\nThis ensures that all the device operations to be timed are queued to\nthe relevant stream before any of them are executed, resulting in a more\naccurate measurement. This is skipped if CUDA_LAUNCH_BLOCKING=1 or\nunified addressing is not available.\n\nIn addition, make sure that auto-tuning code paths have warm-up\nexecutions of the kernels being measured. The warm-up executions should\nnot be inside GpuTimer regions.\n\nWith the example HLO:\n  parameter_0 = bf16[128,12,128]{2,1,0} parameter(0)\n  parameter_1 = bf16[12,128,128]{2,1,0} parameter(1)\n  ROOT dot.26 = bf16[12,128,128]{2,1,0} dot(parameter_0, parameter_1),\n    lhs_batch_dims={1}, lhs_contracting_dims={0}, rhs_batch_dims={0},\n    rhs_contracting_dims={2}\na cuBLAS kernel was selected with a CUPTI/NSys-measured runtime of\n3.9\u00b5s and before this change, GpuTimer measured a runtime of 17.5\u00b5s.\nWith this change, GpuTimer measures a runtime of 8.1\u00b5s.\n\nWith the example HLO:\n  p0 = bf16[128,1536]{0,1} parameter(0)\n  p1 = s8[1536,12288]{0,1} parameter(1)\n  c = bf16[1536,12288]{0,1} convert(p1)\n  ROOT d = bf16[128,12288]{1,0} dot(p0, c), lhs_contracting_dims={1},\n    rhs_contracting_dims={0}\nfive cuDNN plans were auto-tuned, with CUPTI/Nsys-measured runtimes of\n{34.9, 18.5, 30.9, 32.4, 31.5}\u00b5s. Before this change, GpuTimer measured\nruntimes of {76.6, 40.0, 52.5, 55.7, 51.8}\u00b5s. With this change, GpuTimer\nmeasures runtimes of {39.6, 23.0, 35.4, 35.1, 35.1}\u00b5s.\n\nIn summary, with this change, GpuTimer gives results that are much\ncloser to the CUPTI/Nsys measurements, with a ~uniform offset of ~4\u00b5s. A\nconstant offset doesn't matter for auto-tuning.\n\n--\n8f5ce0e7e309a46bf69c369bc4b1da8c26fd1579 by Olli Lupton <olupton@nvidia.com>:\n\nAddress CR\n\nMerging this change closes #9757\n\nPiperOrigin-RevId: 615693482",
    "label": "NO",
    "changes": [
        {
            "name": "conv_algorithm_picker.cc",
            "path": "third_party/xla/xla/service/gpu/conv_algorithm_picker.cc",
            "patches": [
                {
                    "old_start": 612,
                    "old_length": 7,
                    "new_start": 612,
                    "new_length": 6,
                    "hunk": "@@ -612,7 +612,6 @@ absl::StatusOr<AutotuneResult> GpuConvAlgorithmPicker::AutotuneOneConvRunner(\n   // Use assignment instead of brace-list to make GCC 4.9 happy.\n   RunConvOptions options;\n   options.runner_cache = runner;\n-  options.profile_result = &profile_result;\n   // The following plan timing code is based on\n   // https://github.com/NVIDIA/cudnn-frontend/blob/60496f42fdc7a4ccc059f5934e306e728a756755/include/cudnn_frontend_find_plan.h\n   float max_time = 0;\n"
                },
                {
                    "old_start": 625,
                    "old_length": 15,
                    "new_start": 624,
                    "new_length": 20,
                    "hunk": "@@ -625,15 +624,20 @@ absl::StatusOr<AutotuneResult> GpuConvAlgorithmPicker::AutotuneOneConvRunner(\n   // Dry-run to warmup the plan.\n   launch_status = RunGpuConv(config, operand_buffers, result_buffers,\n                              scratch_memory, stream, options);\n+  // It is intentional that the warm-up run does not have a profile result.\n+  // This avoids a timeout and error message if lazy module loading is enabled\n+  // by ensuring that lazy loading happens outside the GpuTimer region.\n+  options.profile_result = &profile_result;\n   constexpr int kMaxIter = 10;\n   // Iterate until the new measurement is within kThreshold of the current\n   // minimum.\n   int num_iters = 0;\n-  for (;\n-       num_iters < kMaxIter && launch_status.ok() && profile_result.is_valid();\n-       num_iters++) {\n+  for (; num_iters < kMaxIter && launch_status.ok(); ++num_iters) {\n     launch_status = RunGpuConv(config, operand_buffers, result_buffers,\n                                scratch_memory, stream, options);\n+    if (!profile_result.is_valid()) {\n+      break;\n+    }\n     float old_min_time = min_time;\n     min_time = std::min(min_time, profile_result.elapsed_time_in_ms());\n     max_time = std::max(max_time, profile_result.elapsed_time_in_ms());\n"
                }
            ],
            "whole_deleted": "-  options.profile_result = &profile_result;\n-  for (;\n-       num_iters < kMaxIter && launch_status.ok() && profile_result.is_valid();\n-       num_iters++) {\n",
            "whole_added": "+  // It is intentional that the warm-up run does not have a profile result.\n+  // This avoids a timeout and error message if lazy module loading is enabled\n+  // by ensuring that lazy loading happens outside the GpuTimer region.\n+  options.profile_result = &profile_result;\n+  for (; num_iters < kMaxIter && launch_status.ok(); ++num_iters) {\n+    if (!profile_result.is_valid()) {\n+      break;\n+    }\n",
            "whole_hunk": "@@ -612,7 +612,6 @@ absl::StatusOr<AutotuneResult> GpuConvAlgorithmPicker::AutotuneOneConvRunner(\n   // Use assignment instead of brace-list to make GCC 4.9 happy.\n   RunConvOptions options;\n   options.runner_cache = runner;\n-  options.profile_result = &profile_result;\n   // The following plan timing code is based on\n   // https://github.com/NVIDIA/cudnn-frontend/blob/60496f42fdc7a4ccc059f5934e306e728a756755/include/cudnn_frontend_find_plan.h\n   float max_time = 0;\n@@ -625,15 +624,20 @@ absl::StatusOr<AutotuneResult> GpuConvAlgorithmPicker::AutotuneOneConvRunner(\n   // Dry-run to warmup the plan.\n   launch_status = RunGpuConv(config, operand_buffers, result_buffers,\n                              scratch_memory, stream, options);\n+  // It is intentional that the warm-up run does not have a profile result.\n+  // This avoids a timeout and error message if lazy module loading is enabled\n+  // by ensuring that lazy loading happens outside the GpuTimer region.\n+  options.profile_result = &profile_result;\n   constexpr int kMaxIter = 10;\n   // Iterate until the new measurement is within kThreshold of the current\n   // minimum.\n   int num_iters = 0;\n-  for (;\n-       num_iters < kMaxIter && launch_status.ok() && profile_result.is_valid();\n-       num_iters++) {\n+  for (; num_iters < kMaxIter && launch_status.ok(); ++num_iters) {\n     launch_status = RunGpuConv(config, operand_buffers, result_buffers,\n                                scratch_memory, stream, options);\n+    if (!profile_result.is_valid()) {\n+      break;\n+    }\n     float old_min_time = min_time;\n     min_time = std::min(min_time, profile_result.elapsed_time_in_ms());\n     max_time = std::max(max_time, profile_result.elapsed_time_in_ms());\n"
        },
        {
            "name": "gemm_algorithm_picker.cc",
            "path": "third_party/xla/xla/service/gpu/gemm_algorithm_picker.cc",
            "patches": [
                {
                    "old_start": 240,
                    "old_length": 6,
                    "new_start": 240,
                    "new_length": 15,
                    "hunk": "@@ -240,6 +240,15 @@ class GemmAutotuner {\n \n     auto tuned_func = [&](const se::blas::AlgorithmType& algorithm)\n         -> absl::StatusOr<se::blas::ProfileResult> {\n+      // Do a warm-up run first, without a profile result. This avoids a timeout\n+      // and error message if lazy module loading is enabled by ensuring that\n+      // lazy loading happens outside the GpuTimer. RunGemm swallows error codes\n+      // when profile_result is passed, as it is in the measurement below, but\n+      // not otherwise. It is, therefore, consistent to ignore the error code\n+      // here.\n+      static_cast<void>(RunGemm(gemm_config, lhs_buffer_, rhs_buffer_,\n+                                output_buffer_, workspace_buffer,\n+                                deterministic_ops_, stream_, algorithm));\n       se::blas::ProfileResult profile_result;\n       // We expect GemmWithAlgorithm to fail sometimes -- in fact, it will fail\n       // for all algorithms if we're targeting < sm_50. But because we pass a\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      // Do a warm-up run first, without a profile result. This avoids a timeout\n+      // and error message if lazy module loading is enabled by ensuring that\n+      // lazy loading happens outside the GpuTimer. RunGemm swallows error codes\n+      // when profile_result is passed, as it is in the measurement below, but\n+      // not otherwise. It is, therefore, consistent to ignore the error code\n+      // here.\n+      static_cast<void>(RunGemm(gemm_config, lhs_buffer_, rhs_buffer_,\n+                                output_buffer_, workspace_buffer,\n+                                deterministic_ops_, stream_, algorithm));\n",
            "whole_hunk": "@@ -240,6 +240,15 @@ class GemmAutotuner {\n \n     auto tuned_func = [&](const se::blas::AlgorithmType& algorithm)\n         -> absl::StatusOr<se::blas::ProfileResult> {\n+      // Do a warm-up run first, without a profile result. This avoids a timeout\n+      // and error message if lazy module loading is enabled by ensuring that\n+      // lazy loading happens outside the GpuTimer. RunGemm swallows error codes\n+      // when profile_result is passed, as it is in the measurement below, but\n+      // not otherwise. It is, therefore, consistent to ignore the error code\n+      // here.\n+      static_cast<void>(RunGemm(gemm_config, lhs_buffer_, rhs_buffer_,\n+                                output_buffer_, workspace_buffer,\n+                                deterministic_ops_, stream_, algorithm));\n       se::blas::ProfileResult profile_result;\n       // We expect GemmWithAlgorithm to fail sometimes -- in fact, it will fail\n       // for all algorithms if we're targeting < sm_50. But because we pass a\n"
        },
        {
            "name": "build_defs.bzl",
            "path": "third_party/xla/xla/stream_executor/build_defs.bzl",
            "patches": [
                {
                    "old_start": 88,
                    "old_length": 3,
                    "new_start": 88,
                    "new_length": 11,
                    "hunk": "@@ -88,3 +88,11 @@ def cuda_only_cc_library(name, tags = [], **kwargs):\n         restricted_to = kwargs.get(\"restricted_to\"),\n         target_compatible_with = kwargs.get(\"target_compatible_with\"),\n     )\n+\n+# TODO(hebecker): Remove this once we've fixed our ARM build\n+def if_google_arm_build(\n+        if_true,  # @unused\n+        if_false = []):\n+    return select({\n+        \"//conditions:default\": if_false,\n+    })\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+# TODO(hebecker): Remove this once we've fixed our ARM build\n+def if_google_arm_build(\n+        if_true,  # @unused\n+        if_false = []):\n+    return select({\n+        \"//conditions:default\": if_false,\n+    })\n",
            "whole_hunk": "@@ -88,3 +88,11 @@ def cuda_only_cc_library(name, tags = [], **kwargs):\n         restricted_to = kwargs.get(\"restricted_to\"),\n         target_compatible_with = kwargs.get(\"target_compatible_with\"),\n     )\n+\n+# TODO(hebecker): Remove this once we've fixed our ARM build\n+def if_google_arm_build(\n+        if_true,  # @unused\n+        if_false = []):\n+    return select({\n+        \"//conditions:default\": if_false,\n+    })\n"
        },
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 6,
                    "new_start": 20,
                    "new_length": 7,
                    "hunk": "@@ -20,6 +20,7 @@ load(\n load(\n     \"//xla/stream_executor:build_defs.bzl\",\n     \"gpu_only_cc_library\",\n+    \"if_google_arm_build\",\n     \"if_gpu_is_configured\",\n )\n load(\n"
                },
                {
                    "old_start": 316,
                    "old_length": 11,
                    "new_start": 317,
                    "new_length": 38,
                    "hunk": "@@ -316,11 +317,38 @@ gpu_only_cc_library(\n     ],\n )\n \n+gpu_only_cc_library(\n+    name = \"gpu_timer_kernel_header\",\n+    hdrs = [\"gpu_timer_kernel.h\"],\n+)\n+\n+gpu_kernel_library(\n+    name = \"gpu_timer_kernel\",\n+    srcs = if_gpu_is_configured([\"gpu_timer_kernel.cu.cc\"]),\n+    deps = [\n+        \":gpu_timer_kernel_header\",\n+    ] + if_cuda_is_configured([\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+    ]) + if_rocm_is_configured([\n+        \"@local_config_rocm//rocm:rocm_headers\",\n+    ]),\n+)\n+\n+# TODO(hebecker): Remove this once we have fixed our ARM build\n+cc_library(\n+    name = \"gpu_timer_kernel_not_on_google_arm\",\n+    deps = if_google_arm_build(\n+        [],\n+        [\":gpu_timer_kernel\"],\n+    ),\n+)\n+\n gpu_only_cc_library(\n     name = \"gpu_timer_header\",\n     hdrs = [\"gpu_timer.h\"],\n     deps = [\n         \":gpu_executor_header\",\n+        \":gpu_timer_kernel_header\",\n         \":gpu_types_header\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/time\",\n"
                },
                {
                    "old_start": 335,
                    "old_length": 6,
                    "new_start": 363,
                    "new_length": 7,
                    "hunk": "@@ -335,6 +363,7 @@ gpu_only_cc_library(\n         \":gpu_driver_header\",\n         \":gpu_executor_header\",\n         \":gpu_stream\",\n+        \":gpu_timer_kernel_header\",\n         \":gpu_types_header\",\n         \"//xla/stream_executor\",\n         \"//xla/stream_executor:stream_executor_internal\",\n"
                },
                {
                    "old_start": 349,
                    "old_length": 7,
                    "new_start": 378,
                    "new_length": 9,
                    "hunk": "@@ -349,7 +378,9 @@ gpu_only_cc_library(\n         \"@local_tsl//tsl/platform:errors\",\n         \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n-    ] + if_cuda_is_configured([\n+    ] + if_gpu_is_configured([\n+        \":gpu_timer_kernel_not_on_google_arm\",\n+    ]) + if_cuda_is_configured([\n         \"//xla/stream_executor/cuda:cuda_driver\",\n     ]) + if_rocm_is_configured([\n         \"//xla/stream_executor/rocm:rocm_driver\",\n"
                }
            ],
            "whole_deleted": "-    ] + if_cuda_is_configured([\n",
            "whole_added": "+    \"if_google_arm_build\",\n+gpu_only_cc_library(\n+    name = \"gpu_timer_kernel_header\",\n+    hdrs = [\"gpu_timer_kernel.h\"],\n+)\n+\n+gpu_kernel_library(\n+    name = \"gpu_timer_kernel\",\n+    srcs = if_gpu_is_configured([\"gpu_timer_kernel.cu.cc\"]),\n+    deps = [\n+        \":gpu_timer_kernel_header\",\n+    ] + if_cuda_is_configured([\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+    ]) + if_rocm_is_configured([\n+        \"@local_config_rocm//rocm:rocm_headers\",\n+    ]),\n+)\n+\n+# TODO(hebecker): Remove this once we have fixed our ARM build\n+cc_library(\n+    name = \"gpu_timer_kernel_not_on_google_arm\",\n+    deps = if_google_arm_build(\n+        [],\n+        [\":gpu_timer_kernel\"],\n+    ),\n+)\n+\n+        \":gpu_timer_kernel_header\",\n+        \":gpu_timer_kernel_header\",\n+    ] + if_gpu_is_configured([\n+        \":gpu_timer_kernel_not_on_google_arm\",\n+    ]) + if_cuda_is_configured([\n",
            "whole_hunk": "@@ -20,6 +20,7 @@ load(\n load(\n     \"//xla/stream_executor:build_defs.bzl\",\n     \"gpu_only_cc_library\",\n+    \"if_google_arm_build\",\n     \"if_gpu_is_configured\",\n )\n load(\n@@ -316,11 +317,38 @@ gpu_only_cc_library(\n     ],\n )\n \n+gpu_only_cc_library(\n+    name = \"gpu_timer_kernel_header\",\n+    hdrs = [\"gpu_timer_kernel.h\"],\n+)\n+\n+gpu_kernel_library(\n+    name = \"gpu_timer_kernel\",\n+    srcs = if_gpu_is_configured([\"gpu_timer_kernel.cu.cc\"]),\n+    deps = [\n+        \":gpu_timer_kernel_header\",\n+    ] + if_cuda_is_configured([\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+    ]) + if_rocm_is_configured([\n+        \"@local_config_rocm//rocm:rocm_headers\",\n+    ]),\n+)\n+\n+# TODO(hebecker): Remove this once we have fixed our ARM build\n+cc_library(\n+    name = \"gpu_timer_kernel_not_on_google_arm\",\n+    deps = if_google_arm_build(\n+        [],\n+        [\":gpu_timer_kernel\"],\n+    ),\n+)\n+\n gpu_only_cc_library(\n     name = \"gpu_timer_header\",\n     hdrs = [\"gpu_timer.h\"],\n     deps = [\n         \":gpu_executor_header\",\n+        \":gpu_timer_kernel_header\",\n         \":gpu_types_header\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/time\",\n@@ -335,6 +363,7 @@ gpu_only_cc_library(\n         \":gpu_driver_header\",\n         \":gpu_executor_header\",\n         \":gpu_stream\",\n+        \":gpu_timer_kernel_header\",\n         \":gpu_types_header\",\n         \"//xla/stream_executor\",\n         \"//xla/stream_executor:stream_executor_internal\",\n@@ -349,7 +378,9 @@ gpu_only_cc_library(\n         \"@local_tsl//tsl/platform:errors\",\n         \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n-    ] + if_cuda_is_configured([\n+    ] + if_gpu_is_configured([\n+        \":gpu_timer_kernel_not_on_google_arm\",\n+    ]) + if_cuda_is_configured([\n         \"//xla/stream_executor/cuda:cuda_driver\",\n     ]) + if_rocm_is_configured([\n         \"//xla/stream_executor/rocm:rocm_driver\",\n"
        },
        {
            "name": "gpu_timer.cc",
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer.cc",
            "patches": [
                {
                    "old_start": 51,
                    "old_length": 10,
                    "new_start": 51,
                    "new_length": 21,
                    "hunk": "@@ -51,10 +51,21 @@ absl::Duration RandomDuration() {\n   return absl::Microseconds(distribution(rng));\n }\n \n+bool ShouldLaunchDelayKernel() {\n+  // Only launch the delay kernel if CUDA_LAUNCH_BLOCKING is not set to 1.\n+  static bool value = [] {\n+    const char* blocking = std::getenv(\"CUDA_LAUNCH_BLOCKING\");\n+    return !blocking || std::string_view{blocking} != \"1\";\n+  }();\n+  return value;\n+}\n+\n }  // namespace\n \n /*deprecated*/ /*static*/ absl::StatusOr<GpuTimer> GpuTimer::Create(\n     GpuStream* stream) {\n+  // This deprecated factory does not launch the delay kernel and may lead to\n+  // reduced measurement accuracy.\n   GpuExecutor* parent = stream->parent();\n   GpuContext* context = parent->gpu_context();\n   GpuEventHandle start_event;\n"
                },
                {
                    "old_start": 72,
                    "old_length": 6,
                    "new_start": 83,
                    "new_length": 8,
                    "hunk": "@@ -72,6 +83,8 @@ absl::Duration RandomDuration() {\n \n /*deprecated*/ /*static*/ absl::StatusOr<std::optional<GpuTimer>>\n GpuTimer::CreateIfNeeded(GpuStream* stream, bool is_needed) {\n+  // This deprecated factory does not launch the delay kernel and may lead to\n+  // reduced measurement accuracy.\n   if (is_needed) {\n     TF_ASSIGN_OR_RETURN(GpuTimer t, GpuTimer::Create(stream));\n     return {std::make_optional(std::move(t))};\n"
                },
                {
                    "old_start": 79,
                    "old_length": 16,
                    "new_start": 92,
                    "new_length": 78,
                    "hunk": "@@ -79,16 +92,78 @@ GpuTimer::CreateIfNeeded(GpuStream* stream, bool is_needed) {\n   return std::nullopt;\n }\n \n-[[deprecated(\"So it can quietly call a deprecated method\")]] /*static*/ absl::\n-    StatusOr<GpuTimer>\n-    GpuTimer::Create(Stream* stream) {\n-  return GpuTimer::Create(AsGpuStream(stream));\n+/*static*/ absl::StatusOr<GpuTimer::GpuSemaphore>\n+GpuTimer::GpuSemaphore::Create(StreamExecutor* executor) {\n+  // Allocate the value in pinned host memory that can be read from both\n+  // host and device.\n+  TF_ASSIGN_OR_RETURN(auto alloc,\n+                      executor->HostMemoryAllocate(sizeof(GpuSemaphoreState)));\n+  return GpuSemaphore{std::move(alloc)};\n }\n \n-[[deprecated(\"So it can quietly call a deprecated method\")]] /*static*/ absl::\n-    StatusOr<std::optional<GpuTimer>>\n-    GpuTimer::CreateIfNeeded(Stream* stream, bool is_needed) {\n-  return GpuTimer::CreateIfNeeded(AsGpuStream(stream), is_needed);\n+DeviceMemory<GpuSemaphoreState> GpuTimer::GpuSemaphore::device() {\n+  // This assumes unified addressing, as we do not explicitly translate the\n+  // host pointer into a device pointer.\n+  return DeviceMemory<GpuSemaphoreState>::MakeFromByteSize(\n+      ptr_->opaque(), sizeof(GpuSemaphoreState));\n+}\n+\n+/*static*/ absl::StatusOr<GpuTimer> GpuTimer::Create(Stream* real_stream) {\n+  StreamExecutor* executor = real_stream->parent();\n+  GpuStream* stream = AsGpuStream(real_stream);\n+  GpuExecutor* parent = stream->parent();\n+  GpuContext* context = parent->gpu_context();\n+  GpuEventHandle start_event;\n+  TF_RETURN_IF_ERROR(GpuDriver::InitEvent(context, &start_event,\n+                                          GpuDriver::EventFlags::kDefault));\n+  GpuEventHandle stop_event;\n+  TF_RETURN_IF_ERROR(GpuDriver::InitEvent(context, &stop_event,\n+                                          GpuDriver::EventFlags::kDefault));\n+  CHECK(start_event != nullptr && stop_event != nullptr);\n+  GpuSemaphore semaphore{};\n+  if (ShouldLaunchDelayKernel()) {\n+    // Check the assumption that this device supports unified addressing,\n+    // otherwise skip the delay kernel\n+    TF_ASSIGN_OR_RETURN(int status, GpuDriver::GetDeviceAttribute(\n+                                        CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING,\n+                                        parent->device()));\n+    if (!status) {\n+      LOG(WARNING) << \"Skipping the delay kernel because the device does not \"\n+                      \"support unified addressing\";\n+    } else {\n+      // Allocate a semaphore value that will be used to signal to the delay\n+      // kernel that it may exit.\n+      TF_ASSIGN_OR_RETURN(semaphore, GpuSemaphore::Create(executor));\n+      *semaphore = GpuSemaphoreState::Hold;\n+      // In principle the kernel could be loaded lazily and shared across\n+      // multiple GpuTimer objects.\n+      TF_ASSIGN_OR_RETURN(\n+          auto kernel,\n+          (TypedKernel<DeviceMemory<GpuSemaphoreState>,\n+                       GpuSemaphoreState>::Create(executor, \"DelayKernel\",\n+                                                  delay_kernel::kernel())));\n+      // Launch a delay kernel into this stream, which will spin until\n+      // GetElapsedDuration() is called, the timer is destroyed, or the timeout\n+      // in the kernel is reached.\n+      TF_RETURN_IF_ERROR(real_stream->ThenLaunch(\n+          ThreadDim(1, 1, 1), BlockDim(1, 1, 1), kernel, semaphore.device(),\n+          GpuSemaphoreState::Release));\n+    }\n+  }\n+  // The start event goes after the delay kernel in the stream\n+  TF_RETURN_IF_ERROR(GpuDriver::RecordEvent(parent->gpu_context(), start_event,\n+                                            stream->gpu_stream()));\n+  return absl::StatusOr<GpuTimer>{absl::in_place, parent, start_event,\n+                                  stop_event,     stream, std::move(semaphore)};\n+}\n+\n+/*static*/ absl::StatusOr<std::optional<GpuTimer>> GpuTimer::CreateIfNeeded(\n+    Stream* stream, bool is_needed) {\n+  if (is_needed) {\n+    TF_ASSIGN_OR_RETURN(GpuTimer t, GpuTimer::Create(stream));\n+    return {std::make_optional(std::move(t))};\n+  }\n+  return std::nullopt;\n }\n \n /*static*/ void GpuTimer::ReturnRandomDurationsForTesting() {\n"
                },
                {
                    "old_start": 97,
                    "old_length": 6,
                    "new_start": 172,
                    "new_length": 17,
                    "hunk": "@@ -97,6 +172,17 @@ GpuTimer::CreateIfNeeded(GpuStream* stream, bool is_needed) {\n \n GpuTimer::~GpuTimer() {\n   GpuContext* context = parent_->gpu_context();\n+  if (semaphore_ && !is_stopped_) {\n+    // Signal the delay kernel that it can exit\n+    *semaphore_ = GpuSemaphoreState::Release;\n+    // Wait for the delay kernel to exit before destroying the value that it is\n+    // watching.\n+    absl::Status status =\n+        GpuDriver::SynchronizeStream(context, stream_->gpu_stream());\n+    if (!status.ok()) {\n+      LOG(ERROR) << status;\n+    }\n+  }\n   if (start_event_ != nullptr) {\n     absl::Status status = GpuDriver::DestroyEvent(context, &start_event_);\n     if (!status.ok()) {\n"
                },
                {
                    "old_start": 117,
                    "old_length": 6,
                    "new_start": 203,
                    "new_length": 18,
                    "hunk": "@@ -117,6 +203,18 @@ absl::StatusOr<absl::Duration> GpuTimer::GetElapsedDuration() {\n   }\n   TF_RETURN_IF_ERROR(GpuDriver::RecordEvent(parent_->gpu_context(), stop_event_,\n                                             stream_->gpu_stream()));\n+  // If we launched the delay kernel then check if it already timed out.\n+  if (semaphore_) {\n+    if (*semaphore_ == GpuSemaphoreState::TimedOut) {\n+      // The delay kernel did not achieve the intended result.\n+      LOG(ERROR) << \"Delay kernel timed out: measured time has sub-optimal \"\n+                    \"accuracy. There may be a missing warmup execution, please \"\n+                    \"investigate in Nsight Systems.\";\n+    } else {\n+      // Signal that the kernel can exit\n+      *semaphore_ = GpuSemaphoreState::Release;\n+    }\n+  }\n   float elapsed_milliseconds = NAN;\n   if (!GpuDriver::GetEventElapsedTime(parent_->gpu_context(),\n                                       &elapsed_milliseconds, start_event_,\n"
                }
            ],
            "whole_deleted": "-[[deprecated(\"So it can quietly call a deprecated method\")]] /*static*/ absl::\n-    StatusOr<GpuTimer>\n-    GpuTimer::Create(Stream* stream) {\n-  return GpuTimer::Create(AsGpuStream(stream));\n-[[deprecated(\"So it can quietly call a deprecated method\")]] /*static*/ absl::\n-    StatusOr<std::optional<GpuTimer>>\n-    GpuTimer::CreateIfNeeded(Stream* stream, bool is_needed) {\n-  return GpuTimer::CreateIfNeeded(AsGpuStream(stream), is_needed);\n",
            "whole_added": "+bool ShouldLaunchDelayKernel() {\n+  // Only launch the delay kernel if CUDA_LAUNCH_BLOCKING is not set to 1.\n+  static bool value = [] {\n+    const char* blocking = std::getenv(\"CUDA_LAUNCH_BLOCKING\");\n+    return !blocking || std::string_view{blocking} != \"1\";\n+  }();\n+  return value;\n+}\n+\n+  // This deprecated factory does not launch the delay kernel and may lead to\n+  // reduced measurement accuracy.\n+  // This deprecated factory does not launch the delay kernel and may lead to\n+  // reduced measurement accuracy.\n+/*static*/ absl::StatusOr<GpuTimer::GpuSemaphore>\n+GpuTimer::GpuSemaphore::Create(StreamExecutor* executor) {\n+  // Allocate the value in pinned host memory that can be read from both\n+  // host and device.\n+  TF_ASSIGN_OR_RETURN(auto alloc,\n+                      executor->HostMemoryAllocate(sizeof(GpuSemaphoreState)));\n+  return GpuSemaphore{std::move(alloc)};\n+DeviceMemory<GpuSemaphoreState> GpuTimer::GpuSemaphore::device() {\n+  // This assumes unified addressing, as we do not explicitly translate the\n+  // host pointer into a device pointer.\n+  return DeviceMemory<GpuSemaphoreState>::MakeFromByteSize(\n+      ptr_->opaque(), sizeof(GpuSemaphoreState));\n+}\n+\n+/*static*/ absl::StatusOr<GpuTimer> GpuTimer::Create(Stream* real_stream) {\n+  StreamExecutor* executor = real_stream->parent();\n+  GpuStream* stream = AsGpuStream(real_stream);\n+  GpuExecutor* parent = stream->parent();\n+  GpuContext* context = parent->gpu_context();\n+  GpuEventHandle start_event;\n+  TF_RETURN_IF_ERROR(GpuDriver::InitEvent(context, &start_event,\n+                                          GpuDriver::EventFlags::kDefault));\n+  GpuEventHandle stop_event;\n+  TF_RETURN_IF_ERROR(GpuDriver::InitEvent(context, &stop_event,\n+                                          GpuDriver::EventFlags::kDefault));\n+  CHECK(start_event != nullptr && stop_event != nullptr);\n+  GpuSemaphore semaphore{};\n+  if (ShouldLaunchDelayKernel()) {\n+    // Check the assumption that this device supports unified addressing,\n+    // otherwise skip the delay kernel\n+    TF_ASSIGN_OR_RETURN(int status, GpuDriver::GetDeviceAttribute(\n+                                        CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING,\n+                                        parent->device()));\n+    if (!status) {\n+      LOG(WARNING) << \"Skipping the delay kernel because the device does not \"\n+                      \"support unified addressing\";\n+    } else {\n+      // Allocate a semaphore value that will be used to signal to the delay\n+      // kernel that it may exit.\n+      TF_ASSIGN_OR_RETURN(semaphore, GpuSemaphore::Create(executor));\n+      *semaphore = GpuSemaphoreState::Hold;\n+      // In principle the kernel could be loaded lazily and shared across\n+      // multiple GpuTimer objects.\n+      TF_ASSIGN_OR_RETURN(\n+          auto kernel,\n+          (TypedKernel<DeviceMemory<GpuSemaphoreState>,\n+                       GpuSemaphoreState>::Create(executor, \"DelayKernel\",\n+                                                  delay_kernel::kernel())));\n+      // Launch a delay kernel into this stream, which will spin until\n+      // GetElapsedDuration() is called, the timer is destroyed, or the timeout\n+      // in the kernel is reached.\n+      TF_RETURN_IF_ERROR(real_stream->ThenLaunch(\n+          ThreadDim(1, 1, 1), BlockDim(1, 1, 1), kernel, semaphore.device(),\n+          GpuSemaphoreState::Release));\n+    }\n+  }\n+  // The start event goes after the delay kernel in the stream\n+  TF_RETURN_IF_ERROR(GpuDriver::RecordEvent(parent->gpu_context(), start_event,\n+                                            stream->gpu_stream()));\n+  return absl::StatusOr<GpuTimer>{absl::in_place, parent, start_event,\n+                                  stop_event,     stream, std::move(semaphore)};\n+}\n+\n+/*static*/ absl::StatusOr<std::optional<GpuTimer>> GpuTimer::CreateIfNeeded(\n+    Stream* stream, bool is_needed) {\n+  if (is_needed) {\n+    TF_ASSIGN_OR_RETURN(GpuTimer t, GpuTimer::Create(stream));\n+    return {std::make_optional(std::move(t))};\n+  }\n+  return std::nullopt;\n+  if (semaphore_ && !is_stopped_) {\n+    // Signal the delay kernel that it can exit\n+    *semaphore_ = GpuSemaphoreState::Release;\n+    // Wait for the delay kernel to exit before destroying the value that it is\n+    // watching.\n+    absl::Status status =\n+        GpuDriver::SynchronizeStream(context, stream_->gpu_stream());\n+    if (!status.ok()) {\n+      LOG(ERROR) << status;\n+    }\n+  }\n+  // If we launched the delay kernel then check if it already timed out.\n+  if (semaphore_) {\n+    if (*semaphore_ == GpuSemaphoreState::TimedOut) {\n+      // The delay kernel did not achieve the intended result.\n+      LOG(ERROR) << \"Delay kernel timed out: measured time has sub-optimal \"\n+                    \"accuracy. There may be a missing warmup execution, please \"\n+                    \"investigate in Nsight Systems.\";\n+    } else {\n+      // Signal that the kernel can exit\n+      *semaphore_ = GpuSemaphoreState::Release;\n+    }\n+  }\n",
            "whole_hunk": "@@ -51,10 +51,21 @@ absl::Duration RandomDuration() {\n   return absl::Microseconds(distribution(rng));\n }\n \n+bool ShouldLaunchDelayKernel() {\n+  // Only launch the delay kernel if CUDA_LAUNCH_BLOCKING is not set to 1.\n+  static bool value = [] {\n+    const char* blocking = std::getenv(\"CUDA_LAUNCH_BLOCKING\");\n+    return !blocking || std::string_view{blocking} != \"1\";\n+  }();\n+  return value;\n+}\n+\n }  // namespace\n \n /*deprecated*/ /*static*/ absl::StatusOr<GpuTimer> GpuTimer::Create(\n     GpuStream* stream) {\n+  // This deprecated factory does not launch the delay kernel and may lead to\n+  // reduced measurement accuracy.\n   GpuExecutor* parent = stream->parent();\n   GpuContext* context = parent->gpu_context();\n   GpuEventHandle start_event;\n@@ -72,6 +83,8 @@ absl::Duration RandomDuration() {\n \n /*deprecated*/ /*static*/ absl::StatusOr<std::optional<GpuTimer>>\n GpuTimer::CreateIfNeeded(GpuStream* stream, bool is_needed) {\n+  // This deprecated factory does not launch the delay kernel and may lead to\n+  // reduced measurement accuracy.\n   if (is_needed) {\n     TF_ASSIGN_OR_RETURN(GpuTimer t, GpuTimer::Create(stream));\n     return {std::make_optional(std::move(t))};\n@@ -79,16 +92,78 @@ GpuTimer::CreateIfNeeded(GpuStream* stream, bool is_needed) {\n   return std::nullopt;\n }\n \n-[[deprecated(\"So it can quietly call a deprecated method\")]] /*static*/ absl::\n-    StatusOr<GpuTimer>\n-    GpuTimer::Create(Stream* stream) {\n-  return GpuTimer::Create(AsGpuStream(stream));\n+/*static*/ absl::StatusOr<GpuTimer::GpuSemaphore>\n+GpuTimer::GpuSemaphore::Create(StreamExecutor* executor) {\n+  // Allocate the value in pinned host memory that can be read from both\n+  // host and device.\n+  TF_ASSIGN_OR_RETURN(auto alloc,\n+                      executor->HostMemoryAllocate(sizeof(GpuSemaphoreState)));\n+  return GpuSemaphore{std::move(alloc)};\n }\n \n-[[deprecated(\"So it can quietly call a deprecated method\")]] /*static*/ absl::\n-    StatusOr<std::optional<GpuTimer>>\n-    GpuTimer::CreateIfNeeded(Stream* stream, bool is_needed) {\n-  return GpuTimer::CreateIfNeeded(AsGpuStream(stream), is_needed);\n+DeviceMemory<GpuSemaphoreState> GpuTimer::GpuSemaphore::device() {\n+  // This assumes unified addressing, as we do not explicitly translate the\n+  // host pointer into a device pointer.\n+  return DeviceMemory<GpuSemaphoreState>::MakeFromByteSize(\n+      ptr_->opaque(), sizeof(GpuSemaphoreState));\n+}\n+\n+/*static*/ absl::StatusOr<GpuTimer> GpuTimer::Create(Stream* real_stream) {\n+  StreamExecutor* executor = real_stream->parent();\n+  GpuStream* stream = AsGpuStream(real_stream);\n+  GpuExecutor* parent = stream->parent();\n+  GpuContext* context = parent->gpu_context();\n+  GpuEventHandle start_event;\n+  TF_RETURN_IF_ERROR(GpuDriver::InitEvent(context, &start_event,\n+                                          GpuDriver::EventFlags::kDefault));\n+  GpuEventHandle stop_event;\n+  TF_RETURN_IF_ERROR(GpuDriver::InitEvent(context, &stop_event,\n+                                          GpuDriver::EventFlags::kDefault));\n+  CHECK(start_event != nullptr && stop_event != nullptr);\n+  GpuSemaphore semaphore{};\n+  if (ShouldLaunchDelayKernel()) {\n+    // Check the assumption that this device supports unified addressing,\n+    // otherwise skip the delay kernel\n+    TF_ASSIGN_OR_RETURN(int status, GpuDriver::GetDeviceAttribute(\n+                                        CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING,\n+                                        parent->device()));\n+    if (!status) {\n+      LOG(WARNING) << \"Skipping the delay kernel because the device does not \"\n+                      \"support unified addressing\";\n+    } else {\n+      // Allocate a semaphore value that will be used to signal to the delay\n+      // kernel that it may exit.\n+      TF_ASSIGN_OR_RETURN(semaphore, GpuSemaphore::Create(executor));\n+      *semaphore = GpuSemaphoreState::Hold;\n+      // In principle the kernel could be loaded lazily and shared across\n+      // multiple GpuTimer objects.\n+      TF_ASSIGN_OR_RETURN(\n+          auto kernel,\n+          (TypedKernel<DeviceMemory<GpuSemaphoreState>,\n+                       GpuSemaphoreState>::Create(executor, \"DelayKernel\",\n+                                                  delay_kernel::kernel())));\n+      // Launch a delay kernel into this stream, which will spin until\n+      // GetElapsedDuration() is called, the timer is destroyed, or the timeout\n+      // in the kernel is reached.\n+      TF_RETURN_IF_ERROR(real_stream->ThenLaunch(\n+          ThreadDim(1, 1, 1), BlockDim(1, 1, 1), kernel, semaphore.device(),\n+          GpuSemaphoreState::Release));\n+    }\n+  }\n+  // The start event goes after the delay kernel in the stream\n+  TF_RETURN_IF_ERROR(GpuDriver::RecordEvent(parent->gpu_context(), start_event,\n+                                            stream->gpu_stream()));\n+  return absl::StatusOr<GpuTimer>{absl::in_place, parent, start_event,\n+                                  stop_event,     stream, std::move(semaphore)};\n+}\n+\n+/*static*/ absl::StatusOr<std::optional<GpuTimer>> GpuTimer::CreateIfNeeded(\n+    Stream* stream, bool is_needed) {\n+  if (is_needed) {\n+    TF_ASSIGN_OR_RETURN(GpuTimer t, GpuTimer::Create(stream));\n+    return {std::make_optional(std::move(t))};\n+  }\n+  return std::nullopt;\n }\n \n /*static*/ void GpuTimer::ReturnRandomDurationsForTesting() {\n@@ -97,6 +172,17 @@ GpuTimer::CreateIfNeeded(GpuStream* stream, bool is_needed) {\n \n GpuTimer::~GpuTimer() {\n   GpuContext* context = parent_->gpu_context();\n+  if (semaphore_ && !is_stopped_) {\n+    // Signal the delay kernel that it can exit\n+    *semaphore_ = GpuSemaphoreState::Release;\n+    // Wait for the delay kernel to exit before destroying the value that it is\n+    // watching.\n+    absl::Status status =\n+        GpuDriver::SynchronizeStream(context, stream_->gpu_stream());\n+    if (!status.ok()) {\n+      LOG(ERROR) << status;\n+    }\n+  }\n   if (start_event_ != nullptr) {\n     absl::Status status = GpuDriver::DestroyEvent(context, &start_event_);\n     if (!status.ok()) {\n@@ -117,6 +203,18 @@ absl::StatusOr<absl::Duration> GpuTimer::GetElapsedDuration() {\n   }\n   TF_RETURN_IF_ERROR(GpuDriver::RecordEvent(parent_->gpu_context(), stop_event_,\n                                             stream_->gpu_stream()));\n+  // If we launched the delay kernel then check if it already timed out.\n+  if (semaphore_) {\n+    if (*semaphore_ == GpuSemaphoreState::TimedOut) {\n+      // The delay kernel did not achieve the intended result.\n+      LOG(ERROR) << \"Delay kernel timed out: measured time has sub-optimal \"\n+                    \"accuracy. There may be a missing warmup execution, please \"\n+                    \"investigate in Nsight Systems.\";\n+    } else {\n+      // Signal that the kernel can exit\n+      *semaphore_ = GpuSemaphoreState::Release;\n+    }\n+  }\n   float elapsed_milliseconds = NAN;\n   if (!GpuDriver::GetEventElapsedTime(parent_->gpu_context(),\n                                       &elapsed_milliseconds, start_event_,\n"
        },
        {
            "name": "gpu_timer.h",
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer.h",
            "patches": [
                {
                    "old_start": 22,
                    "old_length": 6,
                    "new_start": 22,
                    "new_length": 7,
                    "hunk": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/time/time.h\"\n #include \"xla/stream_executor/gpu/gpu_executor.h\"\n+#include \"xla/stream_executor/gpu/gpu_timer_kernel.h\"\n #include \"xla/stream_executor/gpu/gpu_types.h\"\n \n namespace xla {\n"
                },
                {
                    "old_start": 36,
                    "old_length": 9,
                    "new_start": 37,
                    "new_length": 29,
                    "hunk": "@@ -36,9 +37,29 @@ namespace gpu {\n class GpuExecutor;\n class GpuStream;\n \n-// Timer is started once it's created, and is stopped once read.\n+// When a timer is created it launches a delay kernel into the given stream and\n+// queues a start event immediately afterwards. This delay kernel blocks\n+// execution on the stream until GetElapsedDuration() is called, at which point\n+// an end event is queued and the delay kernel exits. This allows the device\n+// execution time of the tasks queued to the stream while the timer is active\n+// to be measured more accurately.\n class GpuTimer {\n  public:\n+  class GpuSemaphore {\n+   public:\n+    GpuSemaphore() = default;\n+    static absl::StatusOr<GpuSemaphore> Create(StreamExecutor* executor);\n+    explicit operator bool() const { return bool{ptr_}; }\n+    GpuSemaphoreState& operator*() {\n+      return *static_cast<GpuSemaphoreState*>(ptr_->opaque());\n+    }\n+    DeviceMemory<GpuSemaphoreState> device();\n+\n+   private:\n+    explicit GpuSemaphore(std::unique_ptr<HostMemoryAllocation> alloc)\n+        : ptr_{std::move(alloc)} {}\n+    std::unique_ptr<HostMemoryAllocation> ptr_;\n+  };\n   static absl::StatusOr<GpuTimer> Create(Stream* stream);\n   [[deprecated(\"Pass Stream* not GpuStream*\")]] static absl::StatusOr<GpuTimer>\n   Create(GpuStream* stream);\n"
                },
                {
                    "old_start": 53,
                    "old_length": 17,
                    "new_start": 74,
                    "new_length": 20,
                    "hunk": "@@ -53,17 +74,20 @@ class GpuTimer {\n   CreateIfNeeded(GpuStream* stream, bool is_needed);\n \n   explicit GpuTimer(GpuExecutor* parent, GpuEventHandle start_event,\n-                    GpuEventHandle stop_event, GpuStream* stream)\n+                    GpuEventHandle stop_event, GpuStream* stream,\n+                    GpuSemaphore semaphore = {})\n       : parent_(parent),\n         start_event_(start_event),\n         stop_event_(stop_event),\n-        stream_(stream) {}\n+        stream_(stream),\n+        semaphore_(std::move(semaphore)) {}\n \n   GpuTimer(GpuTimer&& other)\n       : parent_(other.parent_),\n         start_event_(std::exchange(other.start_event_, nullptr)),\n         stop_event_(std::exchange(other.stop_event_, nullptr)),\n-        stream_(other.stream_) {}\n+        stream_(other.stream_),\n+        semaphore_(std::move(other.semaphore_)) {}\n \n   GpuTimer& operator=(GpuTimer&& other) {\n     if (this != &other) {\n"
                },
                {
                    "old_start": 71,
                    "old_length": 6,
                    "new_start": 95,
                    "new_length": 7,
                    "hunk": "@@ -71,6 +95,7 @@ class GpuTimer {\n       start_event_ = std::exchange(other.start_event_, nullptr);\n       stop_event_ = std::exchange(other.stop_event_, nullptr);\n       stream_ = other.stream_;\n+      semaphore_ = std::move(other.semaphore_);\n     }\n     return *this;\n   }\n"
                },
                {
                    "old_start": 86,
                    "old_length": 6,
                    "new_start": 111,
                    "new_length": 7,
                    "hunk": "@@ -86,6 +111,7 @@ class GpuTimer {\n   GpuEventHandle start_event_ = nullptr;\n   GpuEventHandle stop_event_ = nullptr;\n   GpuStream* stream_;\n+  GpuSemaphore semaphore_;\n   bool is_stopped_ = false;\n \n   GpuTimer(const GpuTimer&) = delete;\n"
                }
            ],
            "whole_deleted": "-// Timer is started once it's created, and is stopped once read.\n-                    GpuEventHandle stop_event, GpuStream* stream)\n-        stream_(stream) {}\n-        stream_(other.stream_) {}\n",
            "whole_added": "+#include \"xla/stream_executor/gpu/gpu_timer_kernel.h\"\n+// When a timer is created it launches a delay kernel into the given stream and\n+// queues a start event immediately afterwards. This delay kernel blocks\n+// execution on the stream until GetElapsedDuration() is called, at which point\n+// an end event is queued and the delay kernel exits. This allows the device\n+// execution time of the tasks queued to the stream while the timer is active\n+// to be measured more accurately.\n+  class GpuSemaphore {\n+   public:\n+    GpuSemaphore() = default;\n+    static absl::StatusOr<GpuSemaphore> Create(StreamExecutor* executor);\n+    explicit operator bool() const { return bool{ptr_}; }\n+    GpuSemaphoreState& operator*() {\n+      return *static_cast<GpuSemaphoreState*>(ptr_->opaque());\n+    }\n+    DeviceMemory<GpuSemaphoreState> device();\n+\n+   private:\n+    explicit GpuSemaphore(std::unique_ptr<HostMemoryAllocation> alloc)\n+        : ptr_{std::move(alloc)} {}\n+    std::unique_ptr<HostMemoryAllocation> ptr_;\n+  };\n+                    GpuEventHandle stop_event, GpuStream* stream,\n+                    GpuSemaphore semaphore = {})\n+        stream_(stream),\n+        semaphore_(std::move(semaphore)) {}\n+        stream_(other.stream_),\n+        semaphore_(std::move(other.semaphore_)) {}\n+      semaphore_ = std::move(other.semaphore_);\n+  GpuSemaphore semaphore_;\n",
            "whole_hunk": "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/time/time.h\"\n #include \"xla/stream_executor/gpu/gpu_executor.h\"\n+#include \"xla/stream_executor/gpu/gpu_timer_kernel.h\"\n #include \"xla/stream_executor/gpu/gpu_types.h\"\n \n namespace xla {\n@@ -36,9 +37,29 @@ namespace gpu {\n class GpuExecutor;\n class GpuStream;\n \n-// Timer is started once it's created, and is stopped once read.\n+// When a timer is created it launches a delay kernel into the given stream and\n+// queues a start event immediately afterwards. This delay kernel blocks\n+// execution on the stream until GetElapsedDuration() is called, at which point\n+// an end event is queued and the delay kernel exits. This allows the device\n+// execution time of the tasks queued to the stream while the timer is active\n+// to be measured more accurately.\n class GpuTimer {\n  public:\n+  class GpuSemaphore {\n+   public:\n+    GpuSemaphore() = default;\n+    static absl::StatusOr<GpuSemaphore> Create(StreamExecutor* executor);\n+    explicit operator bool() const { return bool{ptr_}; }\n+    GpuSemaphoreState& operator*() {\n+      return *static_cast<GpuSemaphoreState*>(ptr_->opaque());\n+    }\n+    DeviceMemory<GpuSemaphoreState> device();\n+\n+   private:\n+    explicit GpuSemaphore(std::unique_ptr<HostMemoryAllocation> alloc)\n+        : ptr_{std::move(alloc)} {}\n+    std::unique_ptr<HostMemoryAllocation> ptr_;\n+  };\n   static absl::StatusOr<GpuTimer> Create(Stream* stream);\n   [[deprecated(\"Pass Stream* not GpuStream*\")]] static absl::StatusOr<GpuTimer>\n   Create(GpuStream* stream);\n@@ -53,17 +74,20 @@ class GpuTimer {\n   CreateIfNeeded(GpuStream* stream, bool is_needed);\n \n   explicit GpuTimer(GpuExecutor* parent, GpuEventHandle start_event,\n-                    GpuEventHandle stop_event, GpuStream* stream)\n+                    GpuEventHandle stop_event, GpuStream* stream,\n+                    GpuSemaphore semaphore = {})\n       : parent_(parent),\n         start_event_(start_event),\n         stop_event_(stop_event),\n-        stream_(stream) {}\n+        stream_(stream),\n+        semaphore_(std::move(semaphore)) {}\n \n   GpuTimer(GpuTimer&& other)\n       : parent_(other.parent_),\n         start_event_(std::exchange(other.start_event_, nullptr)),\n         stop_event_(std::exchange(other.stop_event_, nullptr)),\n-        stream_(other.stream_) {}\n+        stream_(other.stream_),\n+        semaphore_(std::move(other.semaphore_)) {}\n \n   GpuTimer& operator=(GpuTimer&& other) {\n     if (this != &other) {\n@@ -71,6 +95,7 @@ class GpuTimer {\n       start_event_ = std::exchange(other.start_event_, nullptr);\n       stop_event_ = std::exchange(other.stop_event_, nullptr);\n       stream_ = other.stream_;\n+      semaphore_ = std::move(other.semaphore_);\n     }\n     return *this;\n   }\n@@ -86,6 +111,7 @@ class GpuTimer {\n   GpuEventHandle start_event_ = nullptr;\n   GpuEventHandle stop_event_ = nullptr;\n   GpuStream* stream_;\n+  GpuSemaphore semaphore_;\n   bool is_stopped_ = false;\n \n   GpuTimer(const GpuTimer&) = delete;\n"
        },
        {
            "name": "gpu_timer_kernel.cu.cc",
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer_kernel.cu.cc",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 52,
                    "hunk": "@@ -0,0 +1,52 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"xla/stream_executor/gpu/gpu_timer_kernel.h\"\n+\n+#include <cstddef>\n+\n+namespace stream_executor::gpu {\n+namespace {\n+// Wait for the value pointed to by `semaphore` to have value `target`, timing\n+// out after approximately `APPROX_TIMEOUT_SECONDS` seconds if that value is\n+// not reached. This can happen if, for example, blocking launches are enabled\n+// via CUDA_LAUNCH_BLOCKING=1. It can also happen if launching a kernel after\n+// this delay kernel causes synchronisation, e.g. because of lazy loading.\n+__global__ void DelayKernel(volatile GpuSemaphoreState* semaphore,\n+                            GpuSemaphoreState target) {\n+  constexpr int64_t WAIT_CYCLES{1024};\n+  constexpr int64_t TIMEOUT_CYCLES{200000000};  // 100ms at 2GHz\n+  const int64_t tstart{clock64()};\n+  bool target_not_reached;\n+  while ((target_not_reached = (*semaphore != target)) &&\n+         (clock64() - tstart) < TIMEOUT_CYCLES) {\n+    int64_t elapsed{};\n+    const int64_t t0{clock64()};\n+    do {\n+      elapsed = clock64() - t0;\n+    } while (elapsed < WAIT_CYCLES);\n+  }\n+  if (target_not_reached) {\n+    // We are exiting due to the timeout. Signal this back to the host so that\n+    // we can emit a warning, as it probably indicates suboptimal usage.\n+    *semaphore = GpuSemaphoreState::TimedOut;\n+  }\n+}\n+}  // namespace\n+\n+namespace delay_kernel {\n+void* kernel() { return reinterpret_cast<void*>(DelayKernel); }\n+}  // namespace delay_kernel\n+\n+}  // namespace stream_executor::gpu\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"xla/stream_executor/gpu/gpu_timer_kernel.h\"\n+\n+#include <cstddef>\n+\n+namespace stream_executor::gpu {\n+namespace {\n+// Wait for the value pointed to by `semaphore` to have value `target`, timing\n+// out after approximately `APPROX_TIMEOUT_SECONDS` seconds if that value is\n+// not reached. This can happen if, for example, blocking launches are enabled\n+// via CUDA_LAUNCH_BLOCKING=1. It can also happen if launching a kernel after\n+// this delay kernel causes synchronisation, e.g. because of lazy loading.\n+__global__ void DelayKernel(volatile GpuSemaphoreState* semaphore,\n+                            GpuSemaphoreState target) {\n+  constexpr int64_t WAIT_CYCLES{1024};\n+  constexpr int64_t TIMEOUT_CYCLES{200000000};  // 100ms at 2GHz\n+  const int64_t tstart{clock64()};\n+  bool target_not_reached;\n+  while ((target_not_reached = (*semaphore != target)) &&\n+         (clock64() - tstart) < TIMEOUT_CYCLES) {\n+    int64_t elapsed{};\n+    const int64_t t0{clock64()};\n+    do {\n+      elapsed = clock64() - t0;\n+    } while (elapsed < WAIT_CYCLES);\n+  }\n+  if (target_not_reached) {\n+    // We are exiting due to the timeout. Signal this back to the host so that\n+    // we can emit a warning, as it probably indicates suboptimal usage.\n+    *semaphore = GpuSemaphoreState::TimedOut;\n+  }\n+}\n+}  // namespace\n+\n+namespace delay_kernel {\n+void* kernel() { return reinterpret_cast<void*>(DelayKernel); }\n+}  // namespace delay_kernel\n+\n+}  // namespace stream_executor::gpu\n",
            "whole_hunk": "@@ -0,0 +1,52 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"xla/stream_executor/gpu/gpu_timer_kernel.h\"\n+\n+#include <cstddef>\n+\n+namespace stream_executor::gpu {\n+namespace {\n+// Wait for the value pointed to by `semaphore` to have value `target`, timing\n+// out after approximately `APPROX_TIMEOUT_SECONDS` seconds if that value is\n+// not reached. This can happen if, for example, blocking launches are enabled\n+// via CUDA_LAUNCH_BLOCKING=1. It can also happen if launching a kernel after\n+// this delay kernel causes synchronisation, e.g. because of lazy loading.\n+__global__ void DelayKernel(volatile GpuSemaphoreState* semaphore,\n+                            GpuSemaphoreState target) {\n+  constexpr int64_t WAIT_CYCLES{1024};\n+  constexpr int64_t TIMEOUT_CYCLES{200000000};  // 100ms at 2GHz\n+  const int64_t tstart{clock64()};\n+  bool target_not_reached;\n+  while ((target_not_reached = (*semaphore != target)) &&\n+         (clock64() - tstart) < TIMEOUT_CYCLES) {\n+    int64_t elapsed{};\n+    const int64_t t0{clock64()};\n+    do {\n+      elapsed = clock64() - t0;\n+    } while (elapsed < WAIT_CYCLES);\n+  }\n+  if (target_not_reached) {\n+    // We are exiting due to the timeout. Signal this back to the host so that\n+    // we can emit a warning, as it probably indicates suboptimal usage.\n+    *semaphore = GpuSemaphoreState::TimedOut;\n+  }\n+}\n+}  // namespace\n+\n+namespace delay_kernel {\n+void* kernel() { return reinterpret_cast<void*>(DelayKernel); }\n+}  // namespace delay_kernel\n+\n+}  // namespace stream_executor::gpu\n"
        },
        {
            "name": "gpu_timer_kernel.h",
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer_kernel.h",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 26,
                    "hunk": "@@ -0,0 +1,26 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_STREAM_EXECUTOR_GPU_GPU_TIMER_KERNEL_H_\n+#define XLA_STREAM_EXECUTOR_GPU_GPU_TIMER_KERNEL_H_\n+\n+namespace stream_executor::gpu {\n+enum struct GpuSemaphoreState { Hold, Release, TimedOut };\n+namespace delay_kernel {\n+void* kernel();  // returns a pointer to a CUDA C++ device function\n+}  // namespace delay_kernel\n+}  // namespace stream_executor::gpu\n+\n+#endif  // XLA_STREAM_EXECUTOR_GPU_GPU_TIMER_KERNEL_H_"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_STREAM_EXECUTOR_GPU_GPU_TIMER_KERNEL_H_\n+#define XLA_STREAM_EXECUTOR_GPU_GPU_TIMER_KERNEL_H_\n+\n+namespace stream_executor::gpu {\n+enum struct GpuSemaphoreState { Hold, Release, TimedOut };\n+namespace delay_kernel {\n+void* kernel();  // returns a pointer to a CUDA C++ device function\n+}  // namespace delay_kernel\n+}  // namespace stream_executor::gpu\n+\n+#endif  // XLA_STREAM_EXECUTOR_GPU_GPU_TIMER_KERNEL_H_\n",
            "whole_hunk": "@@ -0,0 +1,26 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_STREAM_EXECUTOR_GPU_GPU_TIMER_KERNEL_H_\n+#define XLA_STREAM_EXECUTOR_GPU_GPU_TIMER_KERNEL_H_\n+\n+namespace stream_executor::gpu {\n+enum struct GpuSemaphoreState { Hold, Release, TimedOut };\n+namespace delay_kernel {\n+void* kernel();  // returns a pointer to a CUDA C++ device function\n+}  // namespace delay_kernel\n+}  // namespace stream_executor::gpu\n+\n+#endif  // XLA_STREAM_EXECUTOR_GPU_GPU_TIMER_KERNEL_H_"
        }
    ]
},
{
    "Id": 82,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f28921d9bb59159eef5731e035d7640aadf8f2cb",
    "date": "2024-04-05T16:00:26-07:00",
    "message": "Pass MLIR bytecode across XLA Extension boundary for JAX when converting StableHLO<->MHLO\n\nPiperOrigin-RevId: 622294316",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/python/BUILD",
            "patches": [
                {
                    "old_start": 880,
                    "old_length": 6,
                    "new_start": 880,
                    "new_length": 7,
                    "hunk": "@@ -880,6 +880,7 @@ cc_library(\n     deps = [\n         \":refine_polymorphic_shapes\",\n         # placeholder for index annotation deps\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"//third_party/nanobind\",\n         \"//xla:status\",\n"
                },
                {
                    "old_start": 895,
                    "old_length": 6,
                    "new_start": 896,
                    "new_length": 7,
                    "hunk": "@@ -895,6 +896,7 @@ cc_library(\n         \"@local_tsl//tsl/platform:logging\",\n         \"@local_tsl//tsl/platform:statusor\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:BytecodeWriter\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FuncExtensions\",\n         \"@llvm-project//mlir:IR\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"@com_google_absl//absl/status\",\n+        \"@llvm-project//mlir:BytecodeWriter\",\n",
            "whole_hunk": "@@ -880,6 +880,7 @@ cc_library(\n     deps = [\n         \":refine_polymorphic_shapes\",\n         # placeholder for index annotation deps\n+        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"//third_party/nanobind\",\n         \"//xla:status\",\n@@ -895,6 +896,7 @@ cc_library(\n         \"@local_tsl//tsl/platform:logging\",\n         \"@local_tsl//tsl/platform:statusor\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:BytecodeWriter\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FuncExtensions\",\n         \"@llvm-project//mlir:IR\",\n"
        },
        {
            "name": "mlir.cc",
            "path": "third_party/xla/xla/python/mlir.cc",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 8,
                    "new_start": 17,
                    "new_length": 10,
                    "hunk": "@@ -17,8 +17,10 @@ limitations under the License.\n #include <string_view>\n \n #include \"mhlo/transforms/passes.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"llvm/Support/raw_ostream.h\"\n+#include \"mlir/Bytecode/BytecodeWriter.h\"  // from @llvm-project\n #include \"mlir/Conversion/ReconcileUnrealizedCasts/ReconcileUnrealizedCasts.h\"  // from @llvm-project\n #include \"mlir/Dialect/Func/Extensions/AllExtensions.h\"  // from @llvm-project\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n"
                },
                {
                    "old_start": 92,
                    "old_length": 6,
                    "new_start": 94,
                    "new_length": 16,
                    "hunk": "@@ -92,6 +94,16 @@ std::string PrintModule(mlir::ModuleOp module) {\n   return s;\n }\n \n+absl::StatusOr<std::string> SerializeUsingBytecode(mlir::ModuleOp module) {\n+  std::string bytecode;\n+  llvm::raw_string_ostream os(bytecode);\n+  mlir::BytecodeWriterConfig config;\n+  if (mlir::failed(mlir::writeBytecodeToFile(module, os, config))) {\n+    return absl::InvalidArgumentError(\"mlir::writeBytecodeToFile failed\");\n+  }\n+  return bytecode;\n+}\n+\n void EnablePrintBeforeAndAfter(mlir::PassManager& pm) {\n   auto print_before = [](mlir::Pass*, mlir::Operation*) { return true; };\n   auto print_after = [](mlir::Pass*, mlir::Operation*) { return true; };\n"
                },
                {
                    "old_start": 138,
                    "old_length": 7,
                    "new_start": 150,
                    "new_length": 7,
                    "hunk": "@@ -138,7 +150,7 @@ absl::StatusOr<XlaComputation> PyMlirModuleToXlaComputation(\n   return computation;\n }\n \n-absl::StatusOr<std::string> PyMhloToStablehlo(std::string_view mlir_module) {\n+absl::StatusOr<nb::bytes> PyMhloToStablehlo(std::string_view mlir_module) {\n   mlir::MLIRContext context;\n   if (VLOG_IS_ON(3)) context.disableMultithreading();\n   // JAX can be customized in a way that involves operations from custom\n"
                },
                {
                    "old_start": 156,
                    "old_length": 10,
                    "new_start": 168,
                    "new_length": 13,
                    "hunk": "@@ -156,10 +168,13 @@ absl::StatusOr<std::string> PyMhloToStablehlo(std::string_view mlir_module) {\n   if (!mlir::succeeded(pm.run(*module))) {\n     return tsl::errors::InvalidArgument(\"MHLO => StableHLO failed\");\n   }\n-  return PrintModule(*module);\n+  // Use bytecode, passing unregistered dialects with properties causes issues\n+  // when using textual assembly.\n+  TF_ASSIGN_OR_RETURN(std::string bytecode, SerializeUsingBytecode(*module));\n+  return nb::bytes(bytecode.data(), bytecode.size());\n }\n \n-absl::StatusOr<std::string> PyStablehloToMhlo(const nb::bytes& mlir_module) {\n+absl::StatusOr<nb::bytes> PyStablehloToMhlo(const nb::bytes& mlir_module) {\n   mlir::MLIRContext context;\n   if (VLOG_IS_ON(3)) context.disableMultithreading();\n   // See PyMhloToStablehlo for an explanation of why we're allowing unregistered\n"
                },
                {
                    "old_start": 175,
                    "old_length": 7,
                    "new_start": 190,
                    "new_length": 11,
                    "hunk": "@@ -175,7 +190,11 @@ absl::StatusOr<std::string> PyStablehloToMhlo(const nb::bytes& mlir_module) {\n   if (!mlir::succeeded(pm.run(*module))) {\n     return tsl::errors::InvalidArgument(\"StableHLO => MHLO failed\");\n   }\n-  return PrintModule(*module);\n+\n+  // Use bytecode, passing unregistered dialects with properties causes issues\n+  // when using textual assembly.\n+  TF_ASSIGN_OR_RETURN(std::string bytecode, SerializeUsingBytecode(*module));\n+  return nb::bytes(bytecode.data(), bytecode.size());\n }\n \n absl::StatusOr<nb::bytes> PySerializePortableArtifact(\n"
                }
            ],
            "whole_deleted": "-absl::StatusOr<std::string> PyMhloToStablehlo(std::string_view mlir_module) {\n-  return PrintModule(*module);\n-absl::StatusOr<std::string> PyStablehloToMhlo(const nb::bytes& mlir_module) {\n-  return PrintModule(*module);\n",
            "whole_added": "+#include \"absl/status/status.h\"\n+#include \"mlir/Bytecode/BytecodeWriter.h\"  // from @llvm-project\n+absl::StatusOr<std::string> SerializeUsingBytecode(mlir::ModuleOp module) {\n+  std::string bytecode;\n+  llvm::raw_string_ostream os(bytecode);\n+  mlir::BytecodeWriterConfig config;\n+  if (mlir::failed(mlir::writeBytecodeToFile(module, os, config))) {\n+    return absl::InvalidArgumentError(\"mlir::writeBytecodeToFile failed\");\n+  }\n+  return bytecode;\n+}\n+\n+absl::StatusOr<nb::bytes> PyMhloToStablehlo(std::string_view mlir_module) {\n+  // Use bytecode, passing unregistered dialects with properties causes issues\n+  // when using textual assembly.\n+  TF_ASSIGN_OR_RETURN(std::string bytecode, SerializeUsingBytecode(*module));\n+  return nb::bytes(bytecode.data(), bytecode.size());\n+absl::StatusOr<nb::bytes> PyStablehloToMhlo(const nb::bytes& mlir_module) {\n+\n+  // Use bytecode, passing unregistered dialects with properties causes issues\n+  // when using textual assembly.\n+  TF_ASSIGN_OR_RETURN(std::string bytecode, SerializeUsingBytecode(*module));\n+  return nb::bytes(bytecode.data(), bytecode.size());\n",
            "whole_hunk": "@@ -17,8 +17,10 @@ limitations under the License.\n #include <string_view>\n \n #include \"mhlo/transforms/passes.h\"\n+#include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"llvm/Support/raw_ostream.h\"\n+#include \"mlir/Bytecode/BytecodeWriter.h\"  // from @llvm-project\n #include \"mlir/Conversion/ReconcileUnrealizedCasts/ReconcileUnrealizedCasts.h\"  // from @llvm-project\n #include \"mlir/Dialect/Func/Extensions/AllExtensions.h\"  // from @llvm-project\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n@@ -92,6 +94,16 @@ std::string PrintModule(mlir::ModuleOp module) {\n   return s;\n }\n \n+absl::StatusOr<std::string> SerializeUsingBytecode(mlir::ModuleOp module) {\n+  std::string bytecode;\n+  llvm::raw_string_ostream os(bytecode);\n+  mlir::BytecodeWriterConfig config;\n+  if (mlir::failed(mlir::writeBytecodeToFile(module, os, config))) {\n+    return absl::InvalidArgumentError(\"mlir::writeBytecodeToFile failed\");\n+  }\n+  return bytecode;\n+}\n+\n void EnablePrintBeforeAndAfter(mlir::PassManager& pm) {\n   auto print_before = [](mlir::Pass*, mlir::Operation*) { return true; };\n   auto print_after = [](mlir::Pass*, mlir::Operation*) { return true; };\n@@ -138,7 +150,7 @@ absl::StatusOr<XlaComputation> PyMlirModuleToXlaComputation(\n   return computation;\n }\n \n-absl::StatusOr<std::string> PyMhloToStablehlo(std::string_view mlir_module) {\n+absl::StatusOr<nb::bytes> PyMhloToStablehlo(std::string_view mlir_module) {\n   mlir::MLIRContext context;\n   if (VLOG_IS_ON(3)) context.disableMultithreading();\n   // JAX can be customized in a way that involves operations from custom\n@@ -156,10 +168,13 @@ absl::StatusOr<std::string> PyMhloToStablehlo(std::string_view mlir_module) {\n   if (!mlir::succeeded(pm.run(*module))) {\n     return tsl::errors::InvalidArgument(\"MHLO => StableHLO failed\");\n   }\n-  return PrintModule(*module);\n+  // Use bytecode, passing unregistered dialects with properties causes issues\n+  // when using textual assembly.\n+  TF_ASSIGN_OR_RETURN(std::string bytecode, SerializeUsingBytecode(*module));\n+  return nb::bytes(bytecode.data(), bytecode.size());\n }\n \n-absl::StatusOr<std::string> PyStablehloToMhlo(const nb::bytes& mlir_module) {\n+absl::StatusOr<nb::bytes> PyStablehloToMhlo(const nb::bytes& mlir_module) {\n   mlir::MLIRContext context;\n   if (VLOG_IS_ON(3)) context.disableMultithreading();\n   // See PyMhloToStablehlo for an explanation of why we're allowing unregistered\n@@ -175,7 +190,11 @@ absl::StatusOr<std::string> PyStablehloToMhlo(const nb::bytes& mlir_module) {\n   if (!mlir::succeeded(pm.run(*module))) {\n     return tsl::errors::InvalidArgument(\"StableHLO => MHLO failed\");\n   }\n-  return PrintModule(*module);\n+\n+  // Use bytecode, passing unregistered dialects with properties causes issues\n+  // when using textual assembly.\n+  TF_ASSIGN_OR_RETURN(std::string bytecode, SerializeUsingBytecode(*module));\n+  return nb::bytes(bytecode.data(), bytecode.size());\n }\n \n absl::StatusOr<nb::bytes> PySerializePortableArtifact(\n"
        },
        {
            "name": "xla_client.py",
            "path": "third_party/xla/xla/python/xla_client.py",
            "patches": [
                {
                    "old_start": 52,
                    "old_length": 7,
                    "new_start": 52,
                    "new_length": 7,
                    "hunk": "@@ -52,7 +52,7 @@ profiler = _xla.profiler\n _version = 254\n \n # Version number for MLIR:Python components.\n-mlir_api_version = 55\n+mlir_api_version = 56\n \n xla_platform_names = {\n     'cpu': 'Host',\n"
                }
            ],
            "whole_deleted": "-mlir_api_version = 55\n",
            "whole_added": "+mlir_api_version = 56\n",
            "whole_hunk": "@@ -52,7 +52,7 @@ profiler = _xla.profiler\n _version = 254\n \n # Version number for MLIR:Python components.\n-mlir_api_version = 55\n+mlir_api_version = 56\n \n xla_platform_names = {\n     'cpu': 'Host',\n"
        },
        {
            "name": "mlir.pyi",
            "path": "third_party/xla/xla/python/xla_extension/mlir.pyi",
            "patches": [
                {
                    "old_start": 24,
                    "old_length": 8,
                    "new_start": 24,
                    "new_length": 8,
                    "hunk": "@@ -24,8 +24,8 @@ def mlir_module_to_xla_computation(\n     use_tuple_args: bool = ...,\n     return_tuple: bool = ...,\n ) -> XlaComputation: ...\n-def mhlo_to_stablehlo(mlir_module: Union[bytes, str]) -> str: ...\n-def stablehlo_to_mhlo(mlir_module: Union[bytes, str]) -> str: ...\n+def mhlo_to_stablehlo(mlir_module: Union[bytes, str]) -> bytes: ...\n+def stablehlo_to_mhlo(mlir_module: Union[bytes, str]) -> bytes: ...\n def serialize_portable_artifact(mlir_module: str, target: str) -> bytes: ...\n def deserialize_portable_artifact(mlir_module: bytes) -> str: ...\n def refine_polymorphic_shapes("
                }
            ],
            "whole_deleted": "-def mhlo_to_stablehlo(mlir_module: Union[bytes, str]) -> str: ...\n-def stablehlo_to_mhlo(mlir_module: Union[bytes, str]) -> str: ...\n",
            "whole_added": "+def mhlo_to_stablehlo(mlir_module: Union[bytes, str]) -> bytes: ...\n+def stablehlo_to_mhlo(mlir_module: Union[bytes, str]) -> bytes: ...\n",
            "whole_hunk": "@@ -24,8 +24,8 @@ def mlir_module_to_xla_computation(\n     use_tuple_args: bool = ...,\n     return_tuple: bool = ...,\n ) -> XlaComputation: ...\n-def mhlo_to_stablehlo(mlir_module: Union[bytes, str]) -> str: ...\n-def stablehlo_to_mhlo(mlir_module: Union[bytes, str]) -> str: ...\n+def mhlo_to_stablehlo(mlir_module: Union[bytes, str]) -> bytes: ...\n+def stablehlo_to_mhlo(mlir_module: Union[bytes, str]) -> bytes: ...\n def serialize_portable_artifact(mlir_module: str, target: str) -> bytes: ...\n def deserialize_portable_artifact(mlir_module: bytes) -> str: ...\n def refine_polymorphic_shapes("
        }
    ]
},
{
    "Id": 51,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ba549a246d9d466de305a42640c878e43cbaf796",
    "date": "2024-05-09T15:12:29-07:00",
    "message": "Adds the necessary logic for the creation, deletion and for checking the readiness of a `BasicStringArray`, a simple `ifrt::Array` implementation that wraps a local (or host) string buffer.\n\nPiperOrigin-RevId: 632270272",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/python/pjrt_ifrt/BUILD",
            "patches": [
                {
                    "old_start": 240,
                    "old_length": 6,
                    "new_start": 240,
                    "new_length": 7,
                    "hunk": "@@ -240,6 +240,7 @@ cc_library(\n     ],\n     compatible_with = get_compatible_with_portable(),\n     deps = [\n+        \":basic_string_array\",\n         \":xla_ifrt\",\n         \"//xla:literal\",\n         \"//xla:shape_util\",\n"
                },
                {
                    "old_start": 276,
                    "old_length": 6,
                    "new_start": 277,
                    "new_length": 7,
                    "hunk": "@@ -276,6 +277,7 @@ cc_library(\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:IR\",\n+        \"@local_tsl//tsl/platform:casts\",\n         \"@local_tsl//tsl/platform:errors\",\n         \"@local_tsl//tsl/platform:logging\",\n         \"@local_tsl//tsl/platform:statusor\",\n"
                },
                {
                    "old_start": 298,
                    "old_length": 10,
                    "new_start": 300,
                    "new_length": 11,
                    "hunk": "@@ -298,10 +300,11 @@ cc_library(\n     name = \"basic_string_array\",\n     srcs = [\"basic_string_array.cc\"],\n     hdrs = [\"basic_string_array.h\"],\n+    compatible_with = get_compatible_with_portable(),\n     deps = [\n-        \":pjrt_ifrt\",\n         \"//xla:status\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/pjrt:pjrt_layout\",\n         \"//xla/python/ifrt\",\n         \"//xla/tsl/concurrency:ref_count\",\n         \"@com_google_absl//absl/base:core_headers\",\n"
                },
                {
                    "old_start": 310,
                    "old_length": 11,
                    "new_start": 313,
                    "new_length": 37,
                    "hunk": "@@ -310,11 +313,37 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n     ],\n )\n \n+xla_cc_test(\n+    name = \"basic_string_array_test\",\n+    srcs = [\"basic_string_array_test.cc\"],\n+    deps = [\n+        \":basic_string_array\",\n+        \":tfrt_cpu_client_test_lib\",\n+        \"//xla/pjrt:pjrt_future\",\n+        \"//xla/python/ifrt\",\n+        \"//xla/python/ifrt:test_util\",\n+        \"//xla/tsl/concurrency:ref_count\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@local_tsl//tsl/lib/core:status_test_util\",\n+        \"@local_tsl//tsl/platform:env\",\n+        \"@local_tsl//tsl/platform:statusor\",\n+        \"@local_tsl//tsl/platform:test\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"pjrt_array_impl_test_tfrt_cpu\",\n     size = \"small\",\n"
                }
            ],
            "whole_deleted": "-        \":pjrt_ifrt\",\n",
            "whole_added": "+        \":basic_string_array\",\n+        \"@local_tsl//tsl/platform:casts\",\n+    compatible_with = get_compatible_with_portable(),\n+        \"//xla/pjrt:pjrt_layout\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+xla_cc_test(\n+    name = \"basic_string_array_test\",\n+    srcs = [\"basic_string_array_test.cc\"],\n+    deps = [\n+        \":basic_string_array\",\n+        \":tfrt_cpu_client_test_lib\",\n+        \"//xla/pjrt:pjrt_future\",\n+        \"//xla/python/ifrt\",\n+        \"//xla/python/ifrt:test_util\",\n+        \"//xla/tsl/concurrency:ref_count\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@local_tsl//tsl/lib/core:status_test_util\",\n+        \"@local_tsl//tsl/platform:env\",\n+        \"@local_tsl//tsl/platform:statusor\",\n+        \"@local_tsl//tsl/platform:test\",\n+    ],\n+)\n+\n",
            "whole_hunk": "@@ -240,6 +240,7 @@ cc_library(\n     ],\n     compatible_with = get_compatible_with_portable(),\n     deps = [\n+        \":basic_string_array\",\n         \":xla_ifrt\",\n         \"//xla:literal\",\n         \"//xla:shape_util\",\n@@ -276,6 +277,7 @@ cc_library(\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:IR\",\n+        \"@local_tsl//tsl/platform:casts\",\n         \"@local_tsl//tsl/platform:errors\",\n         \"@local_tsl//tsl/platform:logging\",\n         \"@local_tsl//tsl/platform:statusor\",\n@@ -298,10 +300,11 @@ cc_library(\n     name = \"basic_string_array\",\n     srcs = [\"basic_string_array.cc\"],\n     hdrs = [\"basic_string_array.h\"],\n+    compatible_with = get_compatible_with_portable(),\n     deps = [\n-        \":pjrt_ifrt\",\n         \"//xla:status\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/pjrt:pjrt_layout\",\n         \"//xla/python/ifrt\",\n         \"//xla/tsl/concurrency:ref_count\",\n         \"@com_google_absl//absl/base:core_headers\",\n@@ -310,11 +313,37 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n     ],\n )\n \n+xla_cc_test(\n+    name = \"basic_string_array_test\",\n+    srcs = [\"basic_string_array_test.cc\"],\n+    deps = [\n+        \":basic_string_array\",\n+        \":tfrt_cpu_client_test_lib\",\n+        \"//xla/pjrt:pjrt_future\",\n+        \"//xla/python/ifrt\",\n+        \"//xla/python/ifrt:test_util\",\n+        \"//xla/tsl/concurrency:ref_count\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@local_tsl//tsl/lib/core:status_test_util\",\n+        \"@local_tsl//tsl/platform:env\",\n+        \"@local_tsl//tsl/platform:statusor\",\n+        \"@local_tsl//tsl/platform:test\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"pjrt_array_impl_test_tfrt_cpu\",\n     size = \"small\",\n"
        },
        {
            "name": "basic_string_array.cc",
            "path": "third_party/xla/xla/python/pjrt_ifrt/basic_string_array.cc",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 7,
                    "new_start": 26,
                    "new_length": 9,
                    "hunk": "@@ -26,7 +26,9 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/pjrt/pjrt_layout.h\"\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/shape.h\"\n"
                },
                {
                    "old_start": 43,
                    "old_length": 17,
                    "new_start": 45,
                    "new_length": 14,
                    "hunk": "@@ -43,17 +45,14 @@ char BasicStringArray::ID = 0;\n absl::StatusOr<tsl::RCReference<BasicStringArray>> BasicStringArray::Create(\n     Client* client, Shape shape, std::shared_ptr<const Sharding> sharding,\n     Future<Buffers> buffers, OnDoneWithBuffer on_done_with_buffer) {\n+  if (!buffers.IsValid()) {\n+    return absl::InvalidArgumentError(\"Got buffers_ future is invalid\");\n+  }\n   return tsl::MakeRef<BasicStringArray>(client, std::move(shape),\n                                         std::move(sharding), std::move(buffers),\n                                         std::move(on_done_with_buffer));\n }\n \n-absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::FullyReplicatedShard(\n-    ArrayCopySemantics semantics) {\n-  // Make a single sharded BasicStringArray from the first shard.\n-  return absl::UnimplementedError(\"Not implemented\");\n-}\n-\n BasicStringArray::BasicStringArray(Client* client, Shape shape,\n                                    std::shared_ptr<const Sharding> sharding,\n                                    Future<Buffers> buffers,\n"
                },
                {
                    "old_start": 64,
                    "old_length": 6,
                    "new_start": 63,
                    "new_length": 53,
                    "hunk": "@@ -64,6 +63,53 @@ BasicStringArray::BasicStringArray(Client* client, Shape shape,\n       buffers_(std::move(buffers)),\n       on_done_with_buffer_(std::move(on_done_with_buffer)) {}\n \n+BasicStringArray::~BasicStringArray() { DeleteInternal(); }\n+\n+Future<> BasicStringArray::Delete() {\n+  DeleteInternal();\n+  return Future<>(absl::OkStatus());\n+}\n+\n+bool BasicStringArray::IsDeleted() const {\n+  absl::MutexLock lock(&mu_);\n+  return is_deleted_;\n+}\n+\n+void BasicStringArray::DeleteInternal() {\n+  absl::MutexLock lock(&mu_);\n+  if (is_deleted_) {\n+    return;\n+  }\n+  if (on_done_with_buffer_) {\n+    std::move(on_done_with_buffer_)();\n+  }\n+  is_deleted_ = true;\n+}\n+\n+Future<> BasicStringArray::GetReadyFuture() const {\n+  DCHECK(this);\n+  absl::MutexLock lock(&mu_);\n+  if (is_deleted_) {\n+    return Future<>(\n+        absl::FailedPreconditionError(\"Array has already been deleted\"));\n+  }\n+  if (ready_future_.IsValid()) {\n+    return ready_future_;\n+  }\n+\n+  // TODO(b/337922817) The ready future returned should capture the status\n+  // of consistency checks across the buffers, shape and sharding. These checks\n+  // will run when the buffers become available - i.e., when the `buffers_`\n+  // future becomes ready.\n+  auto promise = Future<>::CreatePromise();\n+  ready_future_ = Future<>(promise);\n+  buffers_.OnReady(\n+      [promise = std::move(promise)](absl::StatusOr<Buffers> buffers) mutable {\n+        promise.Set(buffers.status());\n+      });\n+  return ready_future_;\n+}\n+\n absl::StatusOr<std::vector<tsl::RCReference<Array>>>\n BasicStringArray::DisassembleIntoSingleDeviceArrays(\n     ArrayCopySemantics semantics) {\n"
                },
                {
                    "old_start": 85,
                    "old_length": 19,
                    "new_start": 131,
                    "new_length": 14,
                    "hunk": "@@ -85,19 +131,14 @@ absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::Reshard(\n   return absl::UnimplementedError(\"Not implemented\");\n }\n \n-Future<> BasicStringArray::GetReadyFuture() const {\n-  DCHECK(this);\n-  return Future<>(absl::UnimplementedError(\"Not implemented\"));\n-}\n-\n-Future<> BasicStringArray::Delete() {\n-  DCHECK(this);\n-  return Future<>(absl::UnimplementedError(\"Not implemented\"));\n+absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::FullyReplicatedShard(\n+    ArrayCopySemantics semantics) {\n+  // Make a single sharded BasicStringArray from the first shard.\n+  return absl::UnimplementedError(\"Not implemented\");\n }\n \n-bool BasicStringArray::IsDeleted() const {\n-  DCHECK(this);\n-  return false;\n+absl::StatusOr<std::unique_ptr<PjRtLayout>> BasicStringArray::layout() const {\n+  return absl::UnimplementedError(\"Not implemented\");\n }\n \n std::string BasicStringArray::DebugString() const {\n"
                }
            ],
            "whole_deleted": "-absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::FullyReplicatedShard(\n-    ArrayCopySemantics semantics) {\n-  // Make a single sharded BasicStringArray from the first shard.\n-  return absl::UnimplementedError(\"Not implemented\");\n-}\n-\n-Future<> BasicStringArray::GetReadyFuture() const {\n-  DCHECK(this);\n-  return Future<>(absl::UnimplementedError(\"Not implemented\"));\n-}\n-\n-Future<> BasicStringArray::Delete() {\n-  DCHECK(this);\n-  return Future<>(absl::UnimplementedError(\"Not implemented\"));\n-bool BasicStringArray::IsDeleted() const {\n-  DCHECK(this);\n-  return false;\n",
            "whole_added": "+#include \"absl/synchronization/mutex.h\"\n+#include \"xla/pjrt/pjrt_layout.h\"\n+  if (!buffers.IsValid()) {\n+    return absl::InvalidArgumentError(\"Got buffers_ future is invalid\");\n+  }\n+BasicStringArray::~BasicStringArray() { DeleteInternal(); }\n+\n+Future<> BasicStringArray::Delete() {\n+  DeleteInternal();\n+  return Future<>(absl::OkStatus());\n+}\n+\n+bool BasicStringArray::IsDeleted() const {\n+  absl::MutexLock lock(&mu_);\n+  return is_deleted_;\n+}\n+\n+void BasicStringArray::DeleteInternal() {\n+  absl::MutexLock lock(&mu_);\n+  if (is_deleted_) {\n+    return;\n+  }\n+  if (on_done_with_buffer_) {\n+    std::move(on_done_with_buffer_)();\n+  }\n+  is_deleted_ = true;\n+}\n+\n+Future<> BasicStringArray::GetReadyFuture() const {\n+  DCHECK(this);\n+  absl::MutexLock lock(&mu_);\n+  if (is_deleted_) {\n+    return Future<>(\n+        absl::FailedPreconditionError(\"Array has already been deleted\"));\n+  }\n+  if (ready_future_.IsValid()) {\n+    return ready_future_;\n+  }\n+\n+  // TODO(b/337922817) The ready future returned should capture the status\n+  // of consistency checks across the buffers, shape and sharding. These checks\n+  // will run when the buffers become available - i.e., when the `buffers_`\n+  // future becomes ready.\n+  auto promise = Future<>::CreatePromise();\n+  ready_future_ = Future<>(promise);\n+  buffers_.OnReady(\n+      [promise = std::move(promise)](absl::StatusOr<Buffers> buffers) mutable {\n+        promise.Set(buffers.status());\n+      });\n+  return ready_future_;\n+}\n+\n+absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::FullyReplicatedShard(\n+    ArrayCopySemantics semantics) {\n+  // Make a single sharded BasicStringArray from the first shard.\n+  return absl::UnimplementedError(\"Not implemented\");\n+absl::StatusOr<std::unique_ptr<PjRtLayout>> BasicStringArray::layout() const {\n+  return absl::UnimplementedError(\"Not implemented\");\n",
            "whole_hunk": "@@ -26,7 +26,9 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n+#include \"xla/pjrt/pjrt_layout.h\"\n #include \"xla/python/ifrt/array.h\"\n #include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/shape.h\"\n@@ -43,17 +45,14 @@ char BasicStringArray::ID = 0;\n absl::StatusOr<tsl::RCReference<BasicStringArray>> BasicStringArray::Create(\n     Client* client, Shape shape, std::shared_ptr<const Sharding> sharding,\n     Future<Buffers> buffers, OnDoneWithBuffer on_done_with_buffer) {\n+  if (!buffers.IsValid()) {\n+    return absl::InvalidArgumentError(\"Got buffers_ future is invalid\");\n+  }\n   return tsl::MakeRef<BasicStringArray>(client, std::move(shape),\n                                         std::move(sharding), std::move(buffers),\n                                         std::move(on_done_with_buffer));\n }\n \n-absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::FullyReplicatedShard(\n-    ArrayCopySemantics semantics) {\n-  // Make a single sharded BasicStringArray from the first shard.\n-  return absl::UnimplementedError(\"Not implemented\");\n-}\n-\n BasicStringArray::BasicStringArray(Client* client, Shape shape,\n                                    std::shared_ptr<const Sharding> sharding,\n                                    Future<Buffers> buffers,\n@@ -64,6 +63,53 @@ BasicStringArray::BasicStringArray(Client* client, Shape shape,\n       buffers_(std::move(buffers)),\n       on_done_with_buffer_(std::move(on_done_with_buffer)) {}\n \n+BasicStringArray::~BasicStringArray() { DeleteInternal(); }\n+\n+Future<> BasicStringArray::Delete() {\n+  DeleteInternal();\n+  return Future<>(absl::OkStatus());\n+}\n+\n+bool BasicStringArray::IsDeleted() const {\n+  absl::MutexLock lock(&mu_);\n+  return is_deleted_;\n+}\n+\n+void BasicStringArray::DeleteInternal() {\n+  absl::MutexLock lock(&mu_);\n+  if (is_deleted_) {\n+    return;\n+  }\n+  if (on_done_with_buffer_) {\n+    std::move(on_done_with_buffer_)();\n+  }\n+  is_deleted_ = true;\n+}\n+\n+Future<> BasicStringArray::GetReadyFuture() const {\n+  DCHECK(this);\n+  absl::MutexLock lock(&mu_);\n+  if (is_deleted_) {\n+    return Future<>(\n+        absl::FailedPreconditionError(\"Array has already been deleted\"));\n+  }\n+  if (ready_future_.IsValid()) {\n+    return ready_future_;\n+  }\n+\n+  // TODO(b/337922817) The ready future returned should capture the status\n+  // of consistency checks across the buffers, shape and sharding. These checks\n+  // will run when the buffers become available - i.e., when the `buffers_`\n+  // future becomes ready.\n+  auto promise = Future<>::CreatePromise();\n+  ready_future_ = Future<>(promise);\n+  buffers_.OnReady(\n+      [promise = std::move(promise)](absl::StatusOr<Buffers> buffers) mutable {\n+        promise.Set(buffers.status());\n+      });\n+  return ready_future_;\n+}\n+\n absl::StatusOr<std::vector<tsl::RCReference<Array>>>\n BasicStringArray::DisassembleIntoSingleDeviceArrays(\n     ArrayCopySemantics semantics) {\n@@ -85,19 +131,14 @@ absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::Reshard(\n   return absl::UnimplementedError(\"Not implemented\");\n }\n \n-Future<> BasicStringArray::GetReadyFuture() const {\n-  DCHECK(this);\n-  return Future<>(absl::UnimplementedError(\"Not implemented\"));\n-}\n-\n-Future<> BasicStringArray::Delete() {\n-  DCHECK(this);\n-  return Future<>(absl::UnimplementedError(\"Not implemented\"));\n+absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::FullyReplicatedShard(\n+    ArrayCopySemantics semantics) {\n+  // Make a single sharded BasicStringArray from the first shard.\n+  return absl::UnimplementedError(\"Not implemented\");\n }\n \n-bool BasicStringArray::IsDeleted() const {\n-  DCHECK(this);\n-  return false;\n+absl::StatusOr<std::unique_ptr<PjRtLayout>> BasicStringArray::layout() const {\n+  return absl::UnimplementedError(\"Not implemented\");\n }\n \n std::string BasicStringArray::DebugString() const {\n"
        },
        {
            "name": "basic_string_array.h",
            "path": "third_party/xla/xla/python/pjrt_ifrt/basic_string_array.h",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 20,
                    "new_start": 17,
                    "new_length": 26,
                    "hunk": "@@ -17,20 +17,26 @@ limitations under the License.\n #define XLA_PYTHON_PJRT_IFRT_BASIC_STRING_ARRAY_H_\n \n #include <cstdint>\n+#include <functional>\n #include <memory>\n #include <optional>\n #include <string>\n-#include <variant>\n #include <vector>\n \n #include \"absl/base/attributes.h\"\n+#include \"absl/base/thread_annotations.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/types/span.h\"\n #include \"llvm/Support/ExtensibleRTTI.h\"\n+#include \"xla/pjrt/pjrt_layout.h\"\n #include \"xla/python/ifrt/array.h\"\n+#include \"xla/python/ifrt/dtype.h\"\n #include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/shape.h\"\n-#include \"xla/python/pjrt_ifrt/pjrt_client.h\"\n+#include \"xla/python/ifrt/sharding.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n \n namespace xla {\n"
                },
                {
                    "old_start": 53,
                    "old_length": 23,
                    "new_start": 59,
                    "new_length": 23,
                    "hunk": "@@ -53,23 +59,23 @@ class BasicStringArray final\n \n   // Called when this object is done with the string buffer provided at the\n   // construction time.\n-  using OnDoneWithBuffer = absl::AnyInvocable<void() &&>;\n+  using OnDoneWithBuffer = std::function<void()>;\n \n-  // General array construction (with static shape). The `buffers` and their\n-  // elements (absl::string_views) must live until the `on_done_with_buffer` is\n-  // called. The number and order of buffers must match the number and order\n-  // of devices in `sharding`.\n+  // General array construction. The `buffers` and their elements\n+  // (absl::string_views) must live until the `on_done_with_buffer` is called.\n+  // The number and order of buffers must match the number and order of devices\n+  // in `sharding`.\n   static absl::StatusOr<tsl::RCReference<BasicStringArray>> Create(\n       Client* client, Shape shape, std::shared_ptr<const Sharding> sharding,\n       Future<Buffers> buffers, OnDoneWithBuffer on_done_with_buffer);\n \n+  ~BasicStringArray() override;\n+\n   absl::StatusOr<tsl::RCReference<Array>> FullyReplicatedShard(\n       ArrayCopySemantics semantics) override;\n \n   // ifrt::Array API\n \n-  ~BasicStringArray() override = default;\n-\n   Client* client() const override {\n     DCHECK(this);\n     return client_;\n"
                },
                {
                    "old_start": 80,
                    "old_length": 7,
                    "new_start": 86,
                    "new_length": 10,
                    "hunk": "@@ -80,7 +86,10 @@ class BasicStringArray final\n     return DType(DType::kString);\n   }\n \n-  const Shape& shape() const override { return shape_; }\n+  const Shape& shape() const override {\n+    DCHECK(this);\n+    return shape_;\n+  }\n \n   const Sharding& sharding() const override {\n     DCHECK(this);\n"
                },
                {
                    "old_start": 132,
                    "old_length": 12,
                    "new_start": 141,
                    "new_length": 22,
                    "hunk": "@@ -132,12 +141,22 @@ class BasicStringArray final\n                    Future<Buffers> buffers,\n                    OnDoneWithBuffer on_done_with_buffer);\n \n+  // Internal implementation of delete.\n+  void DeleteInternal() ABSL_LOCKS_EXCLUDED(mu_);\n+\n   Client* client_;\n   Shape shape_;\n   std::shared_ptr<const Sharding> sharding_;\n-\n   Future<Buffers> buffers_;\n-  OnDoneWithBuffer on_done_with_buffer_;\n+\n+  // TODO(b/337922817): Consider checking the buffers when they become available\n+  // (i.e., the future above becomes ready) to ensure that they are consistent\n+  // with the Shape and Sharding provided at the construction time.\n+\n+  mutable absl::Mutex mu_;\n+  OnDoneWithBuffer on_done_with_buffer_ ABSL_GUARDED_BY(mu_);\n+  bool is_deleted_ ABSL_GUARDED_BY(mu_) = false;\n+  mutable Future<> ready_future_ ABSL_GUARDED_BY(mu_);\n };\n \n }  // namespace ifrt\n"
                }
            ],
            "whole_deleted": "-#include <variant>\n-#include \"xla/python/pjrt_ifrt/pjrt_client.h\"\n-  using OnDoneWithBuffer = absl::AnyInvocable<void() &&>;\n-  // General array construction (with static shape). The `buffers` and their\n-  // elements (absl::string_views) must live until the `on_done_with_buffer` is\n-  // called. The number and order of buffers must match the number and order\n-  // of devices in `sharding`.\n-  ~BasicStringArray() override = default;\n-\n-  const Shape& shape() const override { return shape_; }\n-\n-  OnDoneWithBuffer on_done_with_buffer_;\n",
            "whole_added": "+#include <functional>\n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/pjrt/pjrt_layout.h\"\n+#include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/sharding.h\"\n+  using OnDoneWithBuffer = std::function<void()>;\n+  // General array construction. The `buffers` and their elements\n+  // (absl::string_views) must live until the `on_done_with_buffer` is called.\n+  // The number and order of buffers must match the number and order of devices\n+  // in `sharding`.\n+  ~BasicStringArray() override;\n+\n+  const Shape& shape() const override {\n+    DCHECK(this);\n+    return shape_;\n+  }\n+  // Internal implementation of delete.\n+  void DeleteInternal() ABSL_LOCKS_EXCLUDED(mu_);\n+\n+\n+  // TODO(b/337922817): Consider checking the buffers when they become available\n+  // (i.e., the future above becomes ready) to ensure that they are consistent\n+  // with the Shape and Sharding provided at the construction time.\n+\n+  mutable absl::Mutex mu_;\n+  OnDoneWithBuffer on_done_with_buffer_ ABSL_GUARDED_BY(mu_);\n+  bool is_deleted_ ABSL_GUARDED_BY(mu_) = false;\n+  mutable Future<> ready_future_ ABSL_GUARDED_BY(mu_);\n",
            "whole_hunk": "@@ -17,20 +17,26 @@ limitations under the License.\n #define XLA_PYTHON_PJRT_IFRT_BASIC_STRING_ARRAY_H_\n \n #include <cstdint>\n+#include <functional>\n #include <memory>\n #include <optional>\n #include <string>\n-#include <variant>\n #include <vector>\n \n #include \"absl/base/attributes.h\"\n+#include \"absl/base/thread_annotations.h\"\n #include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/types/span.h\"\n #include \"llvm/Support/ExtensibleRTTI.h\"\n+#include \"xla/pjrt/pjrt_layout.h\"\n #include \"xla/python/ifrt/array.h\"\n+#include \"xla/python/ifrt/dtype.h\"\n #include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/shape.h\"\n-#include \"xla/python/pjrt_ifrt/pjrt_client.h\"\n+#include \"xla/python/ifrt/sharding.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n \n namespace xla {\n@@ -53,23 +59,23 @@ class BasicStringArray final\n \n   // Called when this object is done with the string buffer provided at the\n   // construction time.\n-  using OnDoneWithBuffer = absl::AnyInvocable<void() &&>;\n+  using OnDoneWithBuffer = std::function<void()>;\n \n-  // General array construction (with static shape). The `buffers` and their\n-  // elements (absl::string_views) must live until the `on_done_with_buffer` is\n-  // called. The number and order of buffers must match the number and order\n-  // of devices in `sharding`.\n+  // General array construction. The `buffers` and their elements\n+  // (absl::string_views) must live until the `on_done_with_buffer` is called.\n+  // The number and order of buffers must match the number and order of devices\n+  // in `sharding`.\n   static absl::StatusOr<tsl::RCReference<BasicStringArray>> Create(\n       Client* client, Shape shape, std::shared_ptr<const Sharding> sharding,\n       Future<Buffers> buffers, OnDoneWithBuffer on_done_with_buffer);\n \n+  ~BasicStringArray() override;\n+\n   absl::StatusOr<tsl::RCReference<Array>> FullyReplicatedShard(\n       ArrayCopySemantics semantics) override;\n \n   // ifrt::Array API\n \n-  ~BasicStringArray() override = default;\n-\n   Client* client() const override {\n     DCHECK(this);\n     return client_;\n@@ -80,7 +86,10 @@ class BasicStringArray final\n     return DType(DType::kString);\n   }\n \n-  const Shape& shape() const override { return shape_; }\n+  const Shape& shape() const override {\n+    DCHECK(this);\n+    return shape_;\n+  }\n \n   const Sharding& sharding() const override {\n     DCHECK(this);\n@@ -132,12 +141,22 @@ class BasicStringArray final\n                    Future<Buffers> buffers,\n                    OnDoneWithBuffer on_done_with_buffer);\n \n+  // Internal implementation of delete.\n+  void DeleteInternal() ABSL_LOCKS_EXCLUDED(mu_);\n+\n   Client* client_;\n   Shape shape_;\n   std::shared_ptr<const Sharding> sharding_;\n-\n   Future<Buffers> buffers_;\n-  OnDoneWithBuffer on_done_with_buffer_;\n+\n+  // TODO(b/337922817): Consider checking the buffers when they become available\n+  // (i.e., the future above becomes ready) to ensure that they are consistent\n+  // with the Shape and Sharding provided at the construction time.\n+\n+  mutable absl::Mutex mu_;\n+  OnDoneWithBuffer on_done_with_buffer_ ABSL_GUARDED_BY(mu_);\n+  bool is_deleted_ ABSL_GUARDED_BY(mu_) = false;\n+  mutable Future<> ready_future_ ABSL_GUARDED_BY(mu_);\n };\n \n }  // namespace ifrt\n"
        },
        {
            "name": "basic_string_array_test.cc",
            "path": "third_party/xla/xla/python/pjrt_ifrt/basic_string_array_test.cc",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 251,
                    "hunk": "@@ -0,0 +1,251 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/python/pjrt_ifrt/basic_string_array.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/notification.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/pjrt/pjrt_future.h\"\n+#include \"xla/python/ifrt/array.h\"\n+#include \"xla/python/ifrt/device.h\"\n+#include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/future.h\"\n+#include \"xla/python/ifrt/memory.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding.h\"\n+#include \"xla/python/ifrt/test_util.h\"\n+#include \"xla/tsl/concurrency/ref_count.h\"\n+#include \"tsl/lib/core/status_test_util.h\"\n+#include \"tsl/platform/env.h\"\n+#include \"tsl/platform/statusor.h\"\n+#include \"tsl/platform/test.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+namespace {\n+\n+using ::tsl::testing::StatusIs;\n+\n+// Makes a simple single device sharded string array by means of\n+// `BasicStringArray::Create` factory method.\n+absl::StatusOr<tsl::RCReference<BasicStringArray>> CreateTestArray(\n+    Client* client, Future<BasicStringArray::Buffers> buffers,\n+    BasicStringArray::OnDoneWithBuffer on_done_with_buffer) {\n+  Shape shape({1});\n+  Device* device = client->addressable_devices().at(0);\n+  std::shared_ptr<const Sharding> sharding =\n+      SingleDeviceSharding::Create(device, MemoryKind());\n+\n+  return BasicStringArray::Create(client, shape, sharding, std::move(buffers),\n+                                  std::move(on_done_with_buffer));\n+}\n+\n+TEST(BasicStringArrayTest, CreateSuccess) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+\n+  // This test implicitly tests that the on_done_with_buffer can be a nullptr,\n+  // and that the destruction of the BasicStringArray object completes\n+  // successfully (even when the callback is a nullptr).\n+  TF_EXPECT_OK(CreateTestArray(client.get(),\n+                               Future<BasicStringArray::Buffers>(buffers),\n+                               /*on_done_with_buffer=*/nullptr));\n+}\n+\n+TEST(BasicStringArrayTest, CreateFailure) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  // Create fails if with invalid future.\n+  EXPECT_THAT(CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(),\n+                              /*on_done_with_buffer=*/nullptr),\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n+}\n+\n+TEST(BasicStringArrayTest, Destruction) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+\n+  absl::Notification on_done_with_buffer_called;\n+  BasicStringArray::OnDoneWithBuffer on_done_with_buffer =\n+      [&on_done_with_buffer_called]() { on_done_with_buffer_called.Notify(); };\n+\n+  auto array_creation_status_promise = PjRtFuture<>::CreatePromise();\n+\n+  tsl::Env::Default()->SchedClosure(([&]() {\n+    auto array = CreateTestArray(client.get(),\n+                                 Future<BasicStringArray::Buffers>(buffers),\n+                                 std::move(on_done_with_buffer));\n+\n+    array_creation_status_promise.Set(array.status());\n+    // `array` goes out of scope and gets destroyed.\n+  }));\n+\n+  // Make sure that the array has been created successfully.\n+  TF_ASSERT_OK(Future<>(array_creation_status_promise).Await());\n+\n+  // Destruction must release the buffer. That is, the `on_done_with_buffer`\n+  // callback must be called.\n+  on_done_with_buffer_called.WaitForNotification();\n+}\n+\n+TEST(BasicStringArrayTest, GetReadyFutureSuccess) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  // Make a BasicStringArray with a future that is not ready.\n+  auto promise = Future<BasicStringArray::Buffers>::CreatePromise();\n+  auto buffers_future = Future<BasicStringArray::Buffers>(promise);\n+  TF_ASSERT_OK_AND_ASSIGN(auto array,\n+                          CreateTestArray(client.get(), buffers_future,\n+                                          /*on_done_with_buffer=*/nullptr));\n+\n+  // Array should not be ready since the buffers future is not ready.\n+  auto ready_future = array->GetReadyFuture();\n+  EXPECT_FALSE(ready_future.IsKnownReady());\n+\n+  // Make the buffers future ready asynchronously.\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+  tsl::Env::Default()->SchedClosure([&]() { promise.Set(buffers); });\n+  TF_EXPECT_OK(ready_future.Await());\n+}\n+\n+TEST(BasicStringArrayTest, GetReadyFutureFailure) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  // Make a BasicStringArray with a future that is not ready.\n+  auto promise = Future<BasicStringArray::Buffers>::CreatePromise();\n+  auto buffers_future = Future<BasicStringArray::Buffers>(promise);\n+  TF_ASSERT_OK_AND_ASSIGN(auto array,\n+                          CreateTestArray(client.get(), buffers_future,\n+                                          /*on_done_with_buffer=*/nullptr));\n+\n+  // Array should not be ready since the buffers future is not ready.\n+  auto ready_future = array->GetReadyFuture();\n+  EXPECT_FALSE(ready_future.IsKnownReady());\n+\n+  // Make the buffers future ready with an error asynchronously\n+  tsl::Env::Default()->SchedClosure(\n+      [&]() { promise.Set(absl::InternalError(\"injected error\")); });\n+\n+  EXPECT_THAT(ready_future.Await(), StatusIs(absl::StatusCode::kInternal));\n+}\n+\n+TEST(BasicStringArrayTest, Delete) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+  absl::Notification on_done_with_buffer_called;\n+  BasicStringArray::OnDoneWithBuffer on_done_with_buffer =\n+      [&on_done_with_buffer_called]() { on_done_with_buffer_called.Notify(); };\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto array,\n+      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+                      std::move(on_done_with_buffer)));\n+\n+  tsl::Env::Default()->SchedClosure([&]() { array->Delete(); });\n+\n+  // Delete must have released the buffer by calling `on_done_with_buffer`.\n+  on_done_with_buffer_called.WaitForNotification();\n+\n+  // IsDeleted should return true.\n+  EXPECT_TRUE(array->IsDeleted());\n+}\n+\n+TEST(BasicStringArrayTest, MakeArrayFromHostBufferSuccess) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  Shape shape({1});\n+  Device* device = client->addressable_devices().at(0);\n+  std::shared_ptr<const Sharding> sharding =\n+      SingleDeviceSharding::Create(device, MemoryKind());\n+\n+  auto string_views = std::make_shared<std::vector<absl::string_view>>();\n+  string_views->push_back(\"abc\");\n+  string_views->push_back(\"def\");\n+  const void* data = string_views->data();\n+  auto on_done_with_host_buffer = [string_views = std::move(string_views)]() {};\n+\n+  TF_ASSERT_OK(client->MakeArrayFromHostBuffer(\n+      data, DType(DType::kString), shape,\n+      /*byte_strides=*/std::nullopt, std::move(sharding),\n+      Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+      std::move(on_done_with_host_buffer)));\n+}\n+\n+TEST(BasicStringArrayTest, MakeArrayFromHostBufferErrorHandling) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  Shape shape({1});\n+  Device* device = client->addressable_devices().at(0);\n+  std::shared_ptr<const Sharding> single_device_sharding =\n+      SingleDeviceSharding::Create(device, MemoryKind());\n+  auto string_views = std::make_shared<std::vector<absl::string_view>>();\n+  string_views->push_back(\"abc\");\n+  string_views->push_back(\"def\");\n+  const void* data = string_views->data();\n+  auto on_done_with_host_buffer = [string_views = std::move(string_views)]() {};\n+\n+  // MakeArrayFromHostBuffer should check and fail if `byte_strides` in not\n+  // nullopt.\n+  EXPECT_THAT(\n+      client->MakeArrayFromHostBuffer(\n+          data, DType(DType::kString), shape,\n+          /*byte_strides=*/std::optional<absl::Span<const int64_t>>({8}),\n+          single_device_sharding,\n+          Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+          on_done_with_host_buffer),\n+      StatusIs(absl::StatusCode::kInvalidArgument));\n+\n+  // MakeArrayFromHostBuffer should check and fail if the sharding is not a\n+  // SingleDeviceSharding.\n+  std::shared_ptr<const Sharding> opaque_sharding =\n+      OpaqueSharding::Create(DeviceList({device}), MemoryKind());\n+  EXPECT_THAT(client->MakeArrayFromHostBuffer(\n+                  data, DType(DType::kString), shape,\n+                  /*byte_strides=*/std::nullopt, opaque_sharding,\n+                  Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+                  on_done_with_host_buffer),\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n+\n+  // MakeArrayFromHostBuffer should check and fail if the requested\n+  // HostBufferSemantics is not supported.\n+  for (Client::HostBufferSemantics host_buffer_semantics :\n+       {Client::HostBufferSemantics::kImmutableUntilTransferCompletes,\n+        Client::HostBufferSemantics::kImmutableZeroCopy,\n+        Client::HostBufferSemantics::kMutableZeroCopy}) {\n+    SCOPED_TRACE(\n+        absl::StrCat(\"host_buffer_semantics: \", host_buffer_semantics));\n+    EXPECT_THAT(client->MakeArrayFromHostBuffer(\n+                    data, DType(DType::kString), shape,\n+                    /*byte_strides=*/std::nullopt, single_device_sharding,\n+                    host_buffer_semantics, on_done_with_host_buffer),\n+                StatusIs(absl::StatusCode::kInvalidArgument));\n+  }\n+}\n+\n+}  // namespace\n+}  // namespace ifrt\n+}  // namespace xla\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/python/pjrt_ifrt/basic_string_array.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/notification.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/pjrt/pjrt_future.h\"\n+#include \"xla/python/ifrt/array.h\"\n+#include \"xla/python/ifrt/device.h\"\n+#include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/future.h\"\n+#include \"xla/python/ifrt/memory.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding.h\"\n+#include \"xla/python/ifrt/test_util.h\"\n+#include \"xla/tsl/concurrency/ref_count.h\"\n+#include \"tsl/lib/core/status_test_util.h\"\n+#include \"tsl/platform/env.h\"\n+#include \"tsl/platform/statusor.h\"\n+#include \"tsl/platform/test.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+namespace {\n+\n+using ::tsl::testing::StatusIs;\n+\n+// Makes a simple single device sharded string array by means of\n+// `BasicStringArray::Create` factory method.\n+absl::StatusOr<tsl::RCReference<BasicStringArray>> CreateTestArray(\n+    Client* client, Future<BasicStringArray::Buffers> buffers,\n+    BasicStringArray::OnDoneWithBuffer on_done_with_buffer) {\n+  Shape shape({1});\n+  Device* device = client->addressable_devices().at(0);\n+  std::shared_ptr<const Sharding> sharding =\n+      SingleDeviceSharding::Create(device, MemoryKind());\n+\n+  return BasicStringArray::Create(client, shape, sharding, std::move(buffers),\n+                                  std::move(on_done_with_buffer));\n+}\n+\n+TEST(BasicStringArrayTest, CreateSuccess) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+\n+  // This test implicitly tests that the on_done_with_buffer can be a nullptr,\n+  // and that the destruction of the BasicStringArray object completes\n+  // successfully (even when the callback is a nullptr).\n+  TF_EXPECT_OK(CreateTestArray(client.get(),\n+                               Future<BasicStringArray::Buffers>(buffers),\n+                               /*on_done_with_buffer=*/nullptr));\n+}\n+\n+TEST(BasicStringArrayTest, CreateFailure) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  // Create fails if with invalid future.\n+  EXPECT_THAT(CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(),\n+                              /*on_done_with_buffer=*/nullptr),\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n+}\n+\n+TEST(BasicStringArrayTest, Destruction) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+\n+  absl::Notification on_done_with_buffer_called;\n+  BasicStringArray::OnDoneWithBuffer on_done_with_buffer =\n+      [&on_done_with_buffer_called]() { on_done_with_buffer_called.Notify(); };\n+\n+  auto array_creation_status_promise = PjRtFuture<>::CreatePromise();\n+\n+  tsl::Env::Default()->SchedClosure(([&]() {\n+    auto array = CreateTestArray(client.get(),\n+                                 Future<BasicStringArray::Buffers>(buffers),\n+                                 std::move(on_done_with_buffer));\n+\n+    array_creation_status_promise.Set(array.status());\n+    // `array` goes out of scope and gets destroyed.\n+  }));\n+\n+  // Make sure that the array has been created successfully.\n+  TF_ASSERT_OK(Future<>(array_creation_status_promise).Await());\n+\n+  // Destruction must release the buffer. That is, the `on_done_with_buffer`\n+  // callback must be called.\n+  on_done_with_buffer_called.WaitForNotification();\n+}\n+\n+TEST(BasicStringArrayTest, GetReadyFutureSuccess) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  // Make a BasicStringArray with a future that is not ready.\n+  auto promise = Future<BasicStringArray::Buffers>::CreatePromise();\n+  auto buffers_future = Future<BasicStringArray::Buffers>(promise);\n+  TF_ASSERT_OK_AND_ASSIGN(auto array,\n+                          CreateTestArray(client.get(), buffers_future,\n+                                          /*on_done_with_buffer=*/nullptr));\n+\n+  // Array should not be ready since the buffers future is not ready.\n+  auto ready_future = array->GetReadyFuture();\n+  EXPECT_FALSE(ready_future.IsKnownReady());\n+\n+  // Make the buffers future ready asynchronously.\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+  tsl::Env::Default()->SchedClosure([&]() { promise.Set(buffers); });\n+  TF_EXPECT_OK(ready_future.Await());\n+}\n+\n+TEST(BasicStringArrayTest, GetReadyFutureFailure) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  // Make a BasicStringArray with a future that is not ready.\n+  auto promise = Future<BasicStringArray::Buffers>::CreatePromise();\n+  auto buffers_future = Future<BasicStringArray::Buffers>(promise);\n+  TF_ASSERT_OK_AND_ASSIGN(auto array,\n+                          CreateTestArray(client.get(), buffers_future,\n+                                          /*on_done_with_buffer=*/nullptr));\n+\n+  // Array should not be ready since the buffers future is not ready.\n+  auto ready_future = array->GetReadyFuture();\n+  EXPECT_FALSE(ready_future.IsKnownReady());\n+\n+  // Make the buffers future ready with an error asynchronously\n+  tsl::Env::Default()->SchedClosure(\n+      [&]() { promise.Set(absl::InternalError(\"injected error\")); });\n+\n+  EXPECT_THAT(ready_future.Await(), StatusIs(absl::StatusCode::kInternal));\n+}\n+\n+TEST(BasicStringArrayTest, Delete) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+  absl::Notification on_done_with_buffer_called;\n+  BasicStringArray::OnDoneWithBuffer on_done_with_buffer =\n+      [&on_done_with_buffer_called]() { on_done_with_buffer_called.Notify(); };\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto array,\n+      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+                      std::move(on_done_with_buffer)));\n+\n+  tsl::Env::Default()->SchedClosure([&]() { array->Delete(); });\n+\n+  // Delete must have released the buffer by calling `on_done_with_buffer`.\n+  on_done_with_buffer_called.WaitForNotification();\n+\n+  // IsDeleted should return true.\n+  EXPECT_TRUE(array->IsDeleted());\n+}\n+\n+TEST(BasicStringArrayTest, MakeArrayFromHostBufferSuccess) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  Shape shape({1});\n+  Device* device = client->addressable_devices().at(0);\n+  std::shared_ptr<const Sharding> sharding =\n+      SingleDeviceSharding::Create(device, MemoryKind());\n+\n+  auto string_views = std::make_shared<std::vector<absl::string_view>>();\n+  string_views->push_back(\"abc\");\n+  string_views->push_back(\"def\");\n+  const void* data = string_views->data();\n+  auto on_done_with_host_buffer = [string_views = std::move(string_views)]() {};\n+\n+  TF_ASSERT_OK(client->MakeArrayFromHostBuffer(\n+      data, DType(DType::kString), shape,\n+      /*byte_strides=*/std::nullopt, std::move(sharding),\n+      Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+      std::move(on_done_with_host_buffer)));\n+}\n+\n+TEST(BasicStringArrayTest, MakeArrayFromHostBufferErrorHandling) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  Shape shape({1});\n+  Device* device = client->addressable_devices().at(0);\n+  std::shared_ptr<const Sharding> single_device_sharding =\n+      SingleDeviceSharding::Create(device, MemoryKind());\n+  auto string_views = std::make_shared<std::vector<absl::string_view>>();\n+  string_views->push_back(\"abc\");\n+  string_views->push_back(\"def\");\n+  const void* data = string_views->data();\n+  auto on_done_with_host_buffer = [string_views = std::move(string_views)]() {};\n+\n+  // MakeArrayFromHostBuffer should check and fail if `byte_strides` in not\n+  // nullopt.\n+  EXPECT_THAT(\n+      client->MakeArrayFromHostBuffer(\n+          data, DType(DType::kString), shape,\n+          /*byte_strides=*/std::optional<absl::Span<const int64_t>>({8}),\n+          single_device_sharding,\n+          Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+          on_done_with_host_buffer),\n+      StatusIs(absl::StatusCode::kInvalidArgument));\n+\n+  // MakeArrayFromHostBuffer should check and fail if the sharding is not a\n+  // SingleDeviceSharding.\n+  std::shared_ptr<const Sharding> opaque_sharding =\n+      OpaqueSharding::Create(DeviceList({device}), MemoryKind());\n+  EXPECT_THAT(client->MakeArrayFromHostBuffer(\n+                  data, DType(DType::kString), shape,\n+                  /*byte_strides=*/std::nullopt, opaque_sharding,\n+                  Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+                  on_done_with_host_buffer),\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n+\n+  // MakeArrayFromHostBuffer should check and fail if the requested\n+  // HostBufferSemantics is not supported.\n+  for (Client::HostBufferSemantics host_buffer_semantics :\n+       {Client::HostBufferSemantics::kImmutableUntilTransferCompletes,\n+        Client::HostBufferSemantics::kImmutableZeroCopy,\n+        Client::HostBufferSemantics::kMutableZeroCopy}) {\n+    SCOPED_TRACE(\n+        absl::StrCat(\"host_buffer_semantics: \", host_buffer_semantics));\n+    EXPECT_THAT(client->MakeArrayFromHostBuffer(\n+                    data, DType(DType::kString), shape,\n+                    /*byte_strides=*/std::nullopt, single_device_sharding,\n+                    host_buffer_semantics, on_done_with_host_buffer),\n+                StatusIs(absl::StatusCode::kInvalidArgument));\n+  }\n+}\n+\n+}  // namespace\n+}  // namespace ifrt\n+}  // namespace xla\n",
            "whole_hunk": "@@ -0,0 +1,251 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/python/pjrt_ifrt/basic_string_array.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <optional>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/notification.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/pjrt/pjrt_future.h\"\n+#include \"xla/python/ifrt/array.h\"\n+#include \"xla/python/ifrt/device.h\"\n+#include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/future.h\"\n+#include \"xla/python/ifrt/memory.h\"\n+#include \"xla/python/ifrt/shape.h\"\n+#include \"xla/python/ifrt/sharding.h\"\n+#include \"xla/python/ifrt/test_util.h\"\n+#include \"xla/tsl/concurrency/ref_count.h\"\n+#include \"tsl/lib/core/status_test_util.h\"\n+#include \"tsl/platform/env.h\"\n+#include \"tsl/platform/statusor.h\"\n+#include \"tsl/platform/test.h\"\n+\n+namespace xla {\n+namespace ifrt {\n+namespace {\n+\n+using ::tsl::testing::StatusIs;\n+\n+// Makes a simple single device sharded string array by means of\n+// `BasicStringArray::Create` factory method.\n+absl::StatusOr<tsl::RCReference<BasicStringArray>> CreateTestArray(\n+    Client* client, Future<BasicStringArray::Buffers> buffers,\n+    BasicStringArray::OnDoneWithBuffer on_done_with_buffer) {\n+  Shape shape({1});\n+  Device* device = client->addressable_devices().at(0);\n+  std::shared_ptr<const Sharding> sharding =\n+      SingleDeviceSharding::Create(device, MemoryKind());\n+\n+  return BasicStringArray::Create(client, shape, sharding, std::move(buffers),\n+                                  std::move(on_done_with_buffer));\n+}\n+\n+TEST(BasicStringArrayTest, CreateSuccess) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+\n+  // This test implicitly tests that the on_done_with_buffer can be a nullptr,\n+  // and that the destruction of the BasicStringArray object completes\n+  // successfully (even when the callback is a nullptr).\n+  TF_EXPECT_OK(CreateTestArray(client.get(),\n+                               Future<BasicStringArray::Buffers>(buffers),\n+                               /*on_done_with_buffer=*/nullptr));\n+}\n+\n+TEST(BasicStringArrayTest, CreateFailure) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  // Create fails if with invalid future.\n+  EXPECT_THAT(CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(),\n+                              /*on_done_with_buffer=*/nullptr),\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n+}\n+\n+TEST(BasicStringArrayTest, Destruction) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+\n+  absl::Notification on_done_with_buffer_called;\n+  BasicStringArray::OnDoneWithBuffer on_done_with_buffer =\n+      [&on_done_with_buffer_called]() { on_done_with_buffer_called.Notify(); };\n+\n+  auto array_creation_status_promise = PjRtFuture<>::CreatePromise();\n+\n+  tsl::Env::Default()->SchedClosure(([&]() {\n+    auto array = CreateTestArray(client.get(),\n+                                 Future<BasicStringArray::Buffers>(buffers),\n+                                 std::move(on_done_with_buffer));\n+\n+    array_creation_status_promise.Set(array.status());\n+    // `array` goes out of scope and gets destroyed.\n+  }));\n+\n+  // Make sure that the array has been created successfully.\n+  TF_ASSERT_OK(Future<>(array_creation_status_promise).Await());\n+\n+  // Destruction must release the buffer. That is, the `on_done_with_buffer`\n+  // callback must be called.\n+  on_done_with_buffer_called.WaitForNotification();\n+}\n+\n+TEST(BasicStringArrayTest, GetReadyFutureSuccess) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  // Make a BasicStringArray with a future that is not ready.\n+  auto promise = Future<BasicStringArray::Buffers>::CreatePromise();\n+  auto buffers_future = Future<BasicStringArray::Buffers>(promise);\n+  TF_ASSERT_OK_AND_ASSIGN(auto array,\n+                          CreateTestArray(client.get(), buffers_future,\n+                                          /*on_done_with_buffer=*/nullptr));\n+\n+  // Array should not be ready since the buffers future is not ready.\n+  auto ready_future = array->GetReadyFuture();\n+  EXPECT_FALSE(ready_future.IsKnownReady());\n+\n+  // Make the buffers future ready asynchronously.\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+  tsl::Env::Default()->SchedClosure([&]() { promise.Set(buffers); });\n+  TF_EXPECT_OK(ready_future.Await());\n+}\n+\n+TEST(BasicStringArrayTest, GetReadyFutureFailure) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  // Make a BasicStringArray with a future that is not ready.\n+  auto promise = Future<BasicStringArray::Buffers>::CreatePromise();\n+  auto buffers_future = Future<BasicStringArray::Buffers>(promise);\n+  TF_ASSERT_OK_AND_ASSIGN(auto array,\n+                          CreateTestArray(client.get(), buffers_future,\n+                                          /*on_done_with_buffer=*/nullptr));\n+\n+  // Array should not be ready since the buffers future is not ready.\n+  auto ready_future = array->GetReadyFuture();\n+  EXPECT_FALSE(ready_future.IsKnownReady());\n+\n+  // Make the buffers future ready with an error asynchronously\n+  tsl::Env::Default()->SchedClosure(\n+      [&]() { promise.Set(absl::InternalError(\"injected error\")); });\n+\n+  EXPECT_THAT(ready_future.Await(), StatusIs(absl::StatusCode::kInternal));\n+}\n+\n+TEST(BasicStringArrayTest, Delete) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back({\"abc\", \"def\"});\n+  absl::Notification on_done_with_buffer_called;\n+  BasicStringArray::OnDoneWithBuffer on_done_with_buffer =\n+      [&on_done_with_buffer_called]() { on_done_with_buffer_called.Notify(); };\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto array,\n+      CreateTestArray(client.get(), Future<BasicStringArray::Buffers>(buffers),\n+                      std::move(on_done_with_buffer)));\n+\n+  tsl::Env::Default()->SchedClosure([&]() { array->Delete(); });\n+\n+  // Delete must have released the buffer by calling `on_done_with_buffer`.\n+  on_done_with_buffer_called.WaitForNotification();\n+\n+  // IsDeleted should return true.\n+  EXPECT_TRUE(array->IsDeleted());\n+}\n+\n+TEST(BasicStringArrayTest, MakeArrayFromHostBufferSuccess) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  Shape shape({1});\n+  Device* device = client->addressable_devices().at(0);\n+  std::shared_ptr<const Sharding> sharding =\n+      SingleDeviceSharding::Create(device, MemoryKind());\n+\n+  auto string_views = std::make_shared<std::vector<absl::string_view>>();\n+  string_views->push_back(\"abc\");\n+  string_views->push_back(\"def\");\n+  const void* data = string_views->data();\n+  auto on_done_with_host_buffer = [string_views = std::move(string_views)]() {};\n+\n+  TF_ASSERT_OK(client->MakeArrayFromHostBuffer(\n+      data, DType(DType::kString), shape,\n+      /*byte_strides=*/std::nullopt, std::move(sharding),\n+      Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+      std::move(on_done_with_host_buffer)));\n+}\n+\n+TEST(BasicStringArrayTest, MakeArrayFromHostBufferErrorHandling) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto client, test_util::GetClient());\n+  Shape shape({1});\n+  Device* device = client->addressable_devices().at(0);\n+  std::shared_ptr<const Sharding> single_device_sharding =\n+      SingleDeviceSharding::Create(device, MemoryKind());\n+  auto string_views = std::make_shared<std::vector<absl::string_view>>();\n+  string_views->push_back(\"abc\");\n+  string_views->push_back(\"def\");\n+  const void* data = string_views->data();\n+  auto on_done_with_host_buffer = [string_views = std::move(string_views)]() {};\n+\n+  // MakeArrayFromHostBuffer should check and fail if `byte_strides` in not\n+  // nullopt.\n+  EXPECT_THAT(\n+      client->MakeArrayFromHostBuffer(\n+          data, DType(DType::kString), shape,\n+          /*byte_strides=*/std::optional<absl::Span<const int64_t>>({8}),\n+          single_device_sharding,\n+          Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+          on_done_with_host_buffer),\n+      StatusIs(absl::StatusCode::kInvalidArgument));\n+\n+  // MakeArrayFromHostBuffer should check and fail if the sharding is not a\n+  // SingleDeviceSharding.\n+  std::shared_ptr<const Sharding> opaque_sharding =\n+      OpaqueSharding::Create(DeviceList({device}), MemoryKind());\n+  EXPECT_THAT(client->MakeArrayFromHostBuffer(\n+                  data, DType(DType::kString), shape,\n+                  /*byte_strides=*/std::nullopt, opaque_sharding,\n+                  Client::HostBufferSemantics::kImmutableOnlyDuringCall,\n+                  on_done_with_host_buffer),\n+              StatusIs(absl::StatusCode::kInvalidArgument));\n+\n+  // MakeArrayFromHostBuffer should check and fail if the requested\n+  // HostBufferSemantics is not supported.\n+  for (Client::HostBufferSemantics host_buffer_semantics :\n+       {Client::HostBufferSemantics::kImmutableUntilTransferCompletes,\n+        Client::HostBufferSemantics::kImmutableZeroCopy,\n+        Client::HostBufferSemantics::kMutableZeroCopy}) {\n+    SCOPED_TRACE(\n+        absl::StrCat(\"host_buffer_semantics: \", host_buffer_semantics));\n+    EXPECT_THAT(client->MakeArrayFromHostBuffer(\n+                    data, DType(DType::kString), shape,\n+                    /*byte_strides=*/std::nullopt, single_device_sharding,\n+                    host_buffer_semantics, on_done_with_host_buffer),\n+                StatusIs(absl::StatusCode::kInvalidArgument));\n+  }\n+}\n+\n+}  // namespace\n+}  // namespace ifrt\n+}  // namespace xla\n"
        },
        {
            "name": "pjrt_client.cc",
            "path": "third_party/xla/xla/python/pjrt_ifrt/pjrt_client.cc",
            "patches": [
                {
                    "old_start": 32,
                    "old_length": 6,
                    "new_start": 32,
                    "new_length": 7,
                    "hunk": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/Support/Casting.h\"\n #include \"xla/layout.h\"\n"
                },
                {
                    "old_start": 43,
                    "old_length": 12,
                    "new_start": 44,
                    "new_length": 14,
                    "hunk": "@@ -43,12 +44,14 @@ limitations under the License.\n #include \"xla/python/ifrt/client.h\"\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/remap_plan.h\"\n #include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/ifrt/tuple.h\"\n #include \"xla/python/ifrt/value.h\"\n+#include \"xla/python/pjrt_ifrt/basic_string_array.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_array.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_device.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_memory.h\"\n"
                },
                {
                    "old_start": 57,
                    "old_length": 6,
                    "new_start": 60,
                    "new_length": 8,
                    "hunk": "@@ -57,6 +60,8 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/xla_sharding.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/util.h\"\n+#include \"tsl/platform/casts.h\"\n+#include \"tsl/platform/errors.h\"\n #include \"tsl/platform/logging.h\"\n #include \"tsl/platform/statusor.h\"\n \n"
                },
                {
                    "old_start": 71,
                    "old_length": 6,
                    "new_start": 76,
                    "new_length": 57,
                    "hunk": "@@ -71,6 +76,57 @@ absl::AnyInvocable<void() &&> FromStdFunction(std::function<void()>&& f) {\n   return f ? std::move(f) : absl::AnyInvocable<void() &&>();\n }\n \n+absl::StatusOr<tsl::RCReference<Array>> MakeStringArrayFromHostBuffer(\n+    Client* client, const void* data, DType dtype, Shape shape,\n+    std::optional<absl::Span<const int64_t>> byte_strides,\n+    std::shared_ptr<const Sharding> sharding,\n+    Client::HostBufferSemantics semantics,\n+    std::function<void()> on_done_with_host_buffer) {\n+  auto param_validation = [&]() -> absl::Status {\n+    if (byte_strides.has_value()) {\n+      return absl::InvalidArgumentError(\n+          \"byte_strides is not currently supported for making \"\n+          \"BasicStringArrays.\");\n+    }\n+    if (semantics != Client::HostBufferSemantics::kImmutableOnlyDuringCall) {\n+      return absl::InvalidArgumentError(\n+          \"HostBufferSemantics other than kImmutableOnlyDuringCall are not \"\n+          \"currently supported for making BasicStringArrays.\");\n+    }\n+    if (!llvm::isa<const SingleDeviceSharding>(sharding.get())) {\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(\"Only SingleDeviceSharding is supported for making \"\n+                       \"BasicStringArrays: got: \",\n+                       sharding->DebugString()));\n+    }\n+    return absl::OkStatus();\n+  }();\n+\n+  TF_RETURN_IF_ERROR(param_validation);\n+\n+  auto num_elements = shape.num_elements();\n+  auto strings = std::make_shared<std::vector<std::string>>();\n+  strings->reserve(num_elements);\n+  auto string_views = std::make_shared<std::vector<absl::string_view>>();\n+  string_views->reserve(num_elements);\n+  auto element = static_cast<const absl::string_view*>(data);\n+  for (int i = 0; i < num_elements; ++i, ++element) {\n+    strings->push_back(std::string(*element));\n+    string_views->push_back(absl::string_view(strings->back()));\n+  }\n+  std::move(on_done_with_host_buffer)();\n+\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back(*string_views);\n+  auto buffer_releaser = [strings = std::move(strings),\n+                          string_views = std::move(string_views)]() {};\n+\n+  return BasicStringArray::Create(\n+      client, std::move(shape), std::move(sharding),\n+      Future<BasicStringArray::Buffers>(std::move(buffers)),\n+      std::move(buffer_releaser));\n+}\n+\n }  // namespace\n \n char PjRtCompatibleClient::ID = 0;\n"
                },
                {
                    "old_start": 209,
                    "old_length": 6,
                    "new_start": 265,
                    "new_length": 11,
                    "hunk": "@@ -209,6 +265,11 @@ absl::StatusOr<tsl::RCReference<Array>> PjRtClient::MakeArrayFromHostBuffer(\n     Client::HostBufferSemantics semantics,\n     std::function<void()> on_done_with_host_buffer) {\n   DCHECK(this);\n+  if (dtype.kind() == DType::kString) {\n+    return MakeStringArrayFromHostBuffer(this, data, dtype, shape, byte_strides,\n+                                         sharding, semantics,\n+                                         on_done_with_host_buffer);\n+  }\n   if (!llvm::isa<const SingleDeviceSharding>(sharding.get())) {\n     return InvalidArgument(\n         \"Only SingleDeviceSharding is supported: sharding=%s\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"absl/strings/string_view.h\"\n+#include \"xla/python/ifrt/future.h\"\n+#include \"xla/python/pjrt_ifrt/basic_string_array.h\"\n+#include \"tsl/platform/casts.h\"\n+#include \"tsl/platform/errors.h\"\n+absl::StatusOr<tsl::RCReference<Array>> MakeStringArrayFromHostBuffer(\n+    Client* client, const void* data, DType dtype, Shape shape,\n+    std::optional<absl::Span<const int64_t>> byte_strides,\n+    std::shared_ptr<const Sharding> sharding,\n+    Client::HostBufferSemantics semantics,\n+    std::function<void()> on_done_with_host_buffer) {\n+  auto param_validation = [&]() -> absl::Status {\n+    if (byte_strides.has_value()) {\n+      return absl::InvalidArgumentError(\n+          \"byte_strides is not currently supported for making \"\n+          \"BasicStringArrays.\");\n+    }\n+    if (semantics != Client::HostBufferSemantics::kImmutableOnlyDuringCall) {\n+      return absl::InvalidArgumentError(\n+          \"HostBufferSemantics other than kImmutableOnlyDuringCall are not \"\n+          \"currently supported for making BasicStringArrays.\");\n+    }\n+    if (!llvm::isa<const SingleDeviceSharding>(sharding.get())) {\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(\"Only SingleDeviceSharding is supported for making \"\n+                       \"BasicStringArrays: got: \",\n+                       sharding->DebugString()));\n+    }\n+    return absl::OkStatus();\n+  }();\n+\n+  TF_RETURN_IF_ERROR(param_validation);\n+\n+  auto num_elements = shape.num_elements();\n+  auto strings = std::make_shared<std::vector<std::string>>();\n+  strings->reserve(num_elements);\n+  auto string_views = std::make_shared<std::vector<absl::string_view>>();\n+  string_views->reserve(num_elements);\n+  auto element = static_cast<const absl::string_view*>(data);\n+  for (int i = 0; i < num_elements; ++i, ++element) {\n+    strings->push_back(std::string(*element));\n+    string_views->push_back(absl::string_view(strings->back()));\n+  }\n+  std::move(on_done_with_host_buffer)();\n+\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back(*string_views);\n+  auto buffer_releaser = [strings = std::move(strings),\n+                          string_views = std::move(string_views)]() {};\n+\n+  return BasicStringArray::Create(\n+      client, std::move(shape), std::move(sharding),\n+      Future<BasicStringArray::Buffers>(std::move(buffers)),\n+      std::move(buffer_releaser));\n+}\n+\n+  if (dtype.kind() == DType::kString) {\n+    return MakeStringArrayFromHostBuffer(this, data, dtype, shape, byte_strides,\n+                                         sharding, semantics,\n+                                         on_done_with_host_buffer);\n+  }\n",
            "whole_hunk": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_join.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/Support/Casting.h\"\n #include \"xla/layout.h\"\n@@ -43,12 +44,14 @@ limitations under the License.\n #include \"xla/python/ifrt/client.h\"\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/dtype.h\"\n+#include \"xla/python/ifrt/future.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/ifrt/remap_plan.h\"\n #include \"xla/python/ifrt/shape.h\"\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/ifrt/tuple.h\"\n #include \"xla/python/ifrt/value.h\"\n+#include \"xla/python/pjrt_ifrt/basic_string_array.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_array.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_device.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_memory.h\"\n@@ -57,6 +60,8 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/xla_sharding.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/util.h\"\n+#include \"tsl/platform/casts.h\"\n+#include \"tsl/platform/errors.h\"\n #include \"tsl/platform/logging.h\"\n #include \"tsl/platform/statusor.h\"\n \n@@ -71,6 +76,57 @@ absl::AnyInvocable<void() &&> FromStdFunction(std::function<void()>&& f) {\n   return f ? std::move(f) : absl::AnyInvocable<void() &&>();\n }\n \n+absl::StatusOr<tsl::RCReference<Array>> MakeStringArrayFromHostBuffer(\n+    Client* client, const void* data, DType dtype, Shape shape,\n+    std::optional<absl::Span<const int64_t>> byte_strides,\n+    std::shared_ptr<const Sharding> sharding,\n+    Client::HostBufferSemantics semantics,\n+    std::function<void()> on_done_with_host_buffer) {\n+  auto param_validation = [&]() -> absl::Status {\n+    if (byte_strides.has_value()) {\n+      return absl::InvalidArgumentError(\n+          \"byte_strides is not currently supported for making \"\n+          \"BasicStringArrays.\");\n+    }\n+    if (semantics != Client::HostBufferSemantics::kImmutableOnlyDuringCall) {\n+      return absl::InvalidArgumentError(\n+          \"HostBufferSemantics other than kImmutableOnlyDuringCall are not \"\n+          \"currently supported for making BasicStringArrays.\");\n+    }\n+    if (!llvm::isa<const SingleDeviceSharding>(sharding.get())) {\n+      return absl::InvalidArgumentError(\n+          absl::StrCat(\"Only SingleDeviceSharding is supported for making \"\n+                       \"BasicStringArrays: got: \",\n+                       sharding->DebugString()));\n+    }\n+    return absl::OkStatus();\n+  }();\n+\n+  TF_RETURN_IF_ERROR(param_validation);\n+\n+  auto num_elements = shape.num_elements();\n+  auto strings = std::make_shared<std::vector<std::string>>();\n+  strings->reserve(num_elements);\n+  auto string_views = std::make_shared<std::vector<absl::string_view>>();\n+  string_views->reserve(num_elements);\n+  auto element = static_cast<const absl::string_view*>(data);\n+  for (int i = 0; i < num_elements; ++i, ++element) {\n+    strings->push_back(std::string(*element));\n+    string_views->push_back(absl::string_view(strings->back()));\n+  }\n+  std::move(on_done_with_host_buffer)();\n+\n+  BasicStringArray::Buffers buffers;\n+  buffers.push_back(*string_views);\n+  auto buffer_releaser = [strings = std::move(strings),\n+                          string_views = std::move(string_views)]() {};\n+\n+  return BasicStringArray::Create(\n+      client, std::move(shape), std::move(sharding),\n+      Future<BasicStringArray::Buffers>(std::move(buffers)),\n+      std::move(buffer_releaser));\n+}\n+\n }  // namespace\n \n char PjRtCompatibleClient::ID = 0;\n@@ -209,6 +265,11 @@ absl::StatusOr<tsl::RCReference<Array>> PjRtClient::MakeArrayFromHostBuffer(\n     Client::HostBufferSemantics semantics,\n     std::function<void()> on_done_with_host_buffer) {\n   DCHECK(this);\n+  if (dtype.kind() == DType::kString) {\n+    return MakeStringArrayFromHostBuffer(this, data, dtype, shape, byte_strides,\n+                                         sharding, semantics,\n+                                         on_done_with_host_buffer);\n+  }\n   if (!llvm::isa<const SingleDeviceSharding>(sharding.get())) {\n     return InvalidArgument(\n         \"Only SingleDeviceSharding is supported: sharding=%s\",\n"
        },
        {
            "name": "pjrt_client.h",
            "path": "third_party/xla/xla/python/pjrt_ifrt/pjrt_client.h",
            "patches": [
                {
                    "old_start": 105,
                    "old_length": 6,
                    "new_start": 105,
                    "new_length": 12,
                    "hunk": "@@ -105,6 +105,12 @@ class PjRtClient final\n \n   ~PjRtClient() override;\n \n+  // For making Arrays with `dtype` as kString:\n+  //   (1) the `data` argument should point to an array of `absl::string_view`\n+  //   in major-to-minor order,\n+  //   (2) `byte_strides` are not supported, and non-`nullopt` values cause this\n+  //   function to fail.\n+  //   (3) only the `kImmutableDuringCall` semantics is supported currently.\n   absl::StatusOr<tsl::RCReference<Array>> MakeArrayFromHostBuffer(\n       const void* data, DType dtype, Shape shape,\n       std::optional<absl::Span<const int64_t>> byte_strides,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // For making Arrays with `dtype` as kString:\n+  //   (1) the `data` argument should point to an array of `absl::string_view`\n+  //   in major-to-minor order,\n+  //   (2) `byte_strides` are not supported, and non-`nullopt` values cause this\n+  //   function to fail.\n+  //   (3) only the `kImmutableDuringCall` semantics is supported currently.\n",
            "whole_hunk": "@@ -105,6 +105,12 @@ class PjRtClient final\n \n   ~PjRtClient() override;\n \n+  // For making Arrays with `dtype` as kString:\n+  //   (1) the `data` argument should point to an array of `absl::string_view`\n+  //   in major-to-minor order,\n+  //   (2) `byte_strides` are not supported, and non-`nullopt` values cause this\n+  //   function to fail.\n+  //   (3) only the `kImmutableDuringCall` semantics is supported currently.\n   absl::StatusOr<tsl::RCReference<Array>> MakeArrayFromHostBuffer(\n       const void* data, DType dtype, Shape shape,\n       std::optional<absl::Span<const int64_t>> byte_strides,"
        }
    ]
},
{
    "Id": 227,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b59abe629e614d9cacf1169c0cbdc9070f23dd48",
    "date": "2023-12-06T19:11:12-08:00",
    "message": "Fix a bug in the mhlo quant unpacking pass\n\nThe bug occurs when output has dynamic shape but input doesn't. The fix checks output shape to determine whether to use dynamic broadcast op.\n\nPiperOrigin-RevId: 588612987",
    "label": "YES",
    "changes": [
        {
            "name": "convert_mhlo_quant_to_int.cc",
            "path": "tensorflow/compiler/mlir/quantization/stablehlo/passes/bridge/convert_mhlo_quant_to_int.cc",
            "patches": [
                {
                    "old_start": 744,
                    "old_length": 9,
                    "new_start": 744,
                    "new_length": 9,
                    "hunk": "@@ -744,9 +744,9 @@ Value BroadcastZpContribution(OpBuilder &builder, Location loc,\n       broadcast_dims[idx] = result_batching_idx++;\n     }\n   }\n-  // Use broadcast_in_dim or dyanmic_broadcast_in_dim based on input shape\n+  // Use broadcast_in_dim or dyanmic_broadcast_in_dim based on output shape\n   // dynamism.\n-  if (zp_contribution.getType().cast<ShapedType>().hasStaticShape()) {\n+  if (output_tensor_type.cast<ShapedType>().hasStaticShape()) {\n     zp_contribution = builder.create<mhlo::BroadcastInDimOp>(\n         loc, output_tensor_type, zp_contribution,\n         DenseIntElementsAttr::get(\n"
                }
            ],
            "whole_deleted": "-  // Use broadcast_in_dim or dyanmic_broadcast_in_dim based on input shape\n-  if (zp_contribution.getType().cast<ShapedType>().hasStaticShape()) {\n",
            "whole_added": "+  // Use broadcast_in_dim or dyanmic_broadcast_in_dim based on output shape\n+  if (output_tensor_type.cast<ShapedType>().hasStaticShape()) {\n",
            "whole_hunk": "@@ -744,9 +744,9 @@ Value BroadcastZpContribution(OpBuilder &builder, Location loc,\n       broadcast_dims[idx] = result_batching_idx++;\n     }\n   }\n-  // Use broadcast_in_dim or dyanmic_broadcast_in_dim based on input shape\n+  // Use broadcast_in_dim or dyanmic_broadcast_in_dim based on output shape\n   // dynamism.\n-  if (zp_contribution.getType().cast<ShapedType>().hasStaticShape()) {\n+  if (output_tensor_type.cast<ShapedType>().hasStaticShape()) {\n     zp_contribution = builder.create<mhlo::BroadcastInDimOp>(\n         loc, output_tensor_type, zp_contribution,\n         DenseIntElementsAttr::get(\n"
        },
        {
            "name": "convert-mhlo-quant-to-int.mlir",
            "path": "tensorflow/compiler/mlir/quantization/stablehlo/tests/bridge/convert-mhlo-quant-to-int.mlir",
            "patches": [
                {
                    "old_start": 591,
                    "old_length": 6,
                    "new_start": 590,
                    "new_length": 39,
                    "hunk": "@@ -591,6 +590,39 @@ func.func @dot_dynamic_result_dim(\n \n // -----\n \n+// CHECK-LABEL: func @dot_dynamic_batch_dim\n+func.func @dot_dynamic_batch_dim(\n+    %arg0: tensor<?x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n+    %arg1: tensor<2x2x!quant.uniform<i8:f32, 1.000000e+00:3>>\n+  ) -> tensor<?x2x!quant.uniform<i32:f32, 1.000000e+00:3>> {\n+  // CHECK: \"mhlo.dot_general\"\n+  // CHECK-SAME: lhs_contracting_dimensions = [1]\n+  // CHECK-SAME: rhs_contracting_dimensions = [0]\n+  // CHECK-SAME: (tensor<?x2xi8>, tensor<2x2xi8>) -> tensor<?x2xi32>\n+\n+  // CHECK: mhlo.reduce\n+  // CHECK-SAME: applies mhlo.add across dimensions = [1]\n+  // CHECK-SAME: (tensor<?x2xi32>, tensor<i32>) -> tensor<?xi32>\n+  // CHECK: mhlo.dynamic_broadcast_in_dim\n+  // CHECK-SAME: broadcast_dimensions = dense<0>\n+  // CHECK-SAME: (tensor<?xi32>, tensor<2xi64>) -> tensor<?x2xi32>\n+\n+  // CHECK: mhlo.reduce\n+  // CHECK-SAME: applies mhlo.add across dimensions = [0]\n+  // CHECK-SAME: (tensor<2x2xi32>, tensor<i32>) -> tensor<2xi32>\n+  // CHECK: mhlo.dynamic_broadcast_in_dim\n+  // CHECK-SAME: broadcast_dimensions = dense<1>\n+  // CHECK-SAME: (tensor<2xi32>, tensor<2xi64>) -> tensor<?x2xi32>\n+\n+  %0 = \"mhlo.dot\" (%arg0, %arg1) : (\n+      tensor<?x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n+      tensor<2x2x!quant.uniform<i8:f32, 1.000000e+00:3>>\n+    ) -> tensor<?x2x!quant.uniform<i32:f32, 1.000000e+00:3>>\n+  return %0 : tensor<?x2x!quant.uniform<i32:f32, 1.000000e+00:3>>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @dot_general\n func.func @dot_general(\n     %arg0: tensor<2x5x6x!quant.uniform<i8:f32, 2.000000e+00:3>>,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// CHECK-LABEL: func @dot_dynamic_batch_dim\n+func.func @dot_dynamic_batch_dim(\n+    %arg0: tensor<?x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n+    %arg1: tensor<2x2x!quant.uniform<i8:f32, 1.000000e+00:3>>\n+  ) -> tensor<?x2x!quant.uniform<i32:f32, 1.000000e+00:3>> {\n+  // CHECK: \"mhlo.dot_general\"\n+  // CHECK-SAME: lhs_contracting_dimensions = [1]\n+  // CHECK-SAME: rhs_contracting_dimensions = [0]\n+  // CHECK-SAME: (tensor<?x2xi8>, tensor<2x2xi8>) -> tensor<?x2xi32>\n+\n+  // CHECK: mhlo.reduce\n+  // CHECK-SAME: applies mhlo.add across dimensions = [1]\n+  // CHECK-SAME: (tensor<?x2xi32>, tensor<i32>) -> tensor<?xi32>\n+  // CHECK: mhlo.dynamic_broadcast_in_dim\n+  // CHECK-SAME: broadcast_dimensions = dense<0>\n+  // CHECK-SAME: (tensor<?xi32>, tensor<2xi64>) -> tensor<?x2xi32>\n+\n+  // CHECK: mhlo.reduce\n+  // CHECK-SAME: applies mhlo.add across dimensions = [0]\n+  // CHECK-SAME: (tensor<2x2xi32>, tensor<i32>) -> tensor<2xi32>\n+  // CHECK: mhlo.dynamic_broadcast_in_dim\n+  // CHECK-SAME: broadcast_dimensions = dense<1>\n+  // CHECK-SAME: (tensor<2xi32>, tensor<2xi64>) -> tensor<?x2xi32>\n+\n+  %0 = \"mhlo.dot\" (%arg0, %arg1) : (\n+      tensor<?x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n+      tensor<2x2x!quant.uniform<i8:f32, 1.000000e+00:3>>\n+    ) -> tensor<?x2x!quant.uniform<i32:f32, 1.000000e+00:3>>\n+  return %0 : tensor<?x2x!quant.uniform<i32:f32, 1.000000e+00:3>>\n+}\n+\n+// -----\n+\n",
            "whole_hunk": "@@ -591,6 +590,39 @@ func.func @dot_dynamic_result_dim(\n \n // -----\n \n+// CHECK-LABEL: func @dot_dynamic_batch_dim\n+func.func @dot_dynamic_batch_dim(\n+    %arg0: tensor<?x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n+    %arg1: tensor<2x2x!quant.uniform<i8:f32, 1.000000e+00:3>>\n+  ) -> tensor<?x2x!quant.uniform<i32:f32, 1.000000e+00:3>> {\n+  // CHECK: \"mhlo.dot_general\"\n+  // CHECK-SAME: lhs_contracting_dimensions = [1]\n+  // CHECK-SAME: rhs_contracting_dimensions = [0]\n+  // CHECK-SAME: (tensor<?x2xi8>, tensor<2x2xi8>) -> tensor<?x2xi32>\n+\n+  // CHECK: mhlo.reduce\n+  // CHECK-SAME: applies mhlo.add across dimensions = [1]\n+  // CHECK-SAME: (tensor<?x2xi32>, tensor<i32>) -> tensor<?xi32>\n+  // CHECK: mhlo.dynamic_broadcast_in_dim\n+  // CHECK-SAME: broadcast_dimensions = dense<0>\n+  // CHECK-SAME: (tensor<?xi32>, tensor<2xi64>) -> tensor<?x2xi32>\n+\n+  // CHECK: mhlo.reduce\n+  // CHECK-SAME: applies mhlo.add across dimensions = [0]\n+  // CHECK-SAME: (tensor<2x2xi32>, tensor<i32>) -> tensor<2xi32>\n+  // CHECK: mhlo.dynamic_broadcast_in_dim\n+  // CHECK-SAME: broadcast_dimensions = dense<1>\n+  // CHECK-SAME: (tensor<2xi32>, tensor<2xi64>) -> tensor<?x2xi32>\n+\n+  %0 = \"mhlo.dot\" (%arg0, %arg1) : (\n+      tensor<?x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n+      tensor<2x2x!quant.uniform<i8:f32, 1.000000e+00:3>>\n+    ) -> tensor<?x2x!quant.uniform<i32:f32, 1.000000e+00:3>>\n+  return %0 : tensor<?x2x!quant.uniform<i32:f32, 1.000000e+00:3>>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @dot_general\n func.func @dot_general(\n     %arg0: tensor<2x5x6x!quant.uniform<i8:f32, 2.000000e+00:3>>,"
        }
    ]
},
{
    "Id": 576,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/de03551e0b32eab46372ae6e00a47b5b6b612b29",
    "date": "2023-01-20T17:33:07-08:00",
    "message": "Fix logic for finding first non-zero and slicing information for\nhttps://github.com/google/jax/issues/13824.\n\nThe problem is that the code to scan the literal for non-zeros was completely broken. This made it find negative bounds when scanning (because it never would check the final dimension for -1's and simply scan memory arbitrarily until it found a non-zero).\nI also fixed how the non-zero checker works for an entirely zero weight matrix. In this case, the bounds would all loop (which is wrong).\nAlso, when checking the slicing to see if only the feature dim was sliced, the output shape was used instead of the input shape making the check trivially true (slice-dims always equal the input-dims).\n\nPiperOrigin-RevId: 503555714",
    "label": "YES",
    "changes": [
        {
            "name": "cudnn_simplify_padding.cc",
            "path": "tensorflow/compiler/xla/service/gpu/cudnn_simplify_padding.cc",
            "patches": [
                {
                    "old_start": 164,
                    "old_length": 36,
                    "new_start": 164,
                    "new_length": 43,
                    "hunk": "@@ -164,36 +164,43 @@ std::optional<int64_t> NumTrailingZeroOutputFeatures(HloInstruction* conv) {\n     for (int64_t dim : dims) {\n       multi_index.push_back(dim - 1);\n     }\n-    while (true) {\n-      if (!lit.IsZero(multi_index)) {\n-        break;\n-      }\n-      multi_index[multi_index.size() - 1]--;\n-      for (int i = multi_index.size() - 2; i > 0; i--) {\n-        if (multi_index[i] == -1) {\n-          multi_index[i] = dims[i] - 1;\n-          multi_index[i - 1]--;\n-        } else {\n-          break;\n+    // This iterates through the literal with feature_dim as the most\n+    // major dimension looking for the final non-zero feature.\n+    auto decrement_multi_index = [&] {\n+      for (int i = 0; i < multi_index.size(); ++i) {\n+        if (i != feature_dim) {\n+          int64_t& idx = multi_index[i];\n+          --idx;\n+          if (idx == -1) {\n+            idx = dims[i] - 1;\n+          } else {\n+            return true;\n+          }\n         }\n       }\n-      if (multi_index[0] == -1) {\n+      int64_t& idx = multi_index[feature_dim];\n+      --idx;\n+      return idx != -1;\n+    };\n+    do {\n+      if (!lit.IsZero(multi_index)) {\n         break;\n       }\n-    }\n+    } while (decrement_multi_index());\n \n-    VLOG(2) << \"First nonzero index in weights constant is \"\n-            << absl::StrJoin(multi_index, \",\");\n-    int64_t first_nonzero_feature = multi_index[feature_dim];\n-    // \"round up\" the first nonzero feature index if it's not *all* zeros.\n-    for (int i = 0; i < multi_index.size(); i++) {\n-      if (i != feature_dim && multi_index[i] != 0) {\n-        first_nonzero_feature++;\n-        break;\n-      }\n+    // The iteration stops if a feature has a non-zero value (or -1), but we\n+    // want the first zero feature which is always the next one (or 0 if -1).\n+    int64_t first_trailing_zero_feature = multi_index[feature_dim] + 1;\n+\n+    if (first_trailing_zero_feature == 0) {\n+      VLOG(2) << \"Weights constant is entirely zero.\";\n+    } else {\n+      VLOG(2) << \"First nonzero index in weights constant is \"\n+              << absl::StrJoin(multi_index, \",\");\n     }\n-    int64_t ret = std::max<int64_t>(\n-        0, weights->shape().dimensions(feature_dim) - first_nonzero_feature);\n+    int64_t ret =\n+        std::max<int64_t>(0, weights->shape().dimensions(feature_dim) -\n+                                 first_trailing_zero_feature);\n     VLOG(2) << \"Success: weights is a constant; num zero trailing output \"\n                \"features is \"\n             << ret;\n"
                },
                {
                    "old_start": 333,
                    "old_length": 8,
                    "new_start": 340,
                    "new_length": 10,
                    "hunk": "@@ -333,8 +340,10 @@ StatusOr<bool> TrySimplifyPadding(HloInstruction* instr) {\n \n   // We're only allowed to slice the feature dim.\n   for (int64_t dim = 0; dim < slice->slice_limits().size(); dim++) {\n-    if (dim != output_feature_dim &&\n-        slice->slice_limits(dim) != slice->shape().dimensions(dim)) {\n+    if (slice->slice_starts(dim) != 0 || slice->slice_strides(dim) != 1 ||\n+        (dim != output_feature_dim &&\n+         slice->slice_limits(dim) !=\n+             slice->operand(0)->shape().dimensions(dim))) {\n       VLOG(2) << \"fail: Slice removes something other than the features dim.\";\n       return false;\n     }\n"
                }
            ],
            "whole_deleted": "-    while (true) {\n-      if (!lit.IsZero(multi_index)) {\n-        break;\n-      }\n-      multi_index[multi_index.size() - 1]--;\n-      for (int i = multi_index.size() - 2; i > 0; i--) {\n-        if (multi_index[i] == -1) {\n-          multi_index[i] = dims[i] - 1;\n-          multi_index[i - 1]--;\n-        } else {\n-          break;\n-      if (multi_index[0] == -1) {\n-    }\n-    VLOG(2) << \"First nonzero index in weights constant is \"\n-            << absl::StrJoin(multi_index, \",\");\n-    int64_t first_nonzero_feature = multi_index[feature_dim];\n-    // \"round up\" the first nonzero feature index if it's not *all* zeros.\n-    for (int i = 0; i < multi_index.size(); i++) {\n-      if (i != feature_dim && multi_index[i] != 0) {\n-        first_nonzero_feature++;\n-        break;\n-      }\n-    int64_t ret = std::max<int64_t>(\n-        0, weights->shape().dimensions(feature_dim) - first_nonzero_feature);\n-    if (dim != output_feature_dim &&\n-        slice->slice_limits(dim) != slice->shape().dimensions(dim)) {\n",
            "whole_added": "+    // This iterates through the literal with feature_dim as the most\n+    // major dimension looking for the final non-zero feature.\n+    auto decrement_multi_index = [&] {\n+      for (int i = 0; i < multi_index.size(); ++i) {\n+        if (i != feature_dim) {\n+          int64_t& idx = multi_index[i];\n+          --idx;\n+          if (idx == -1) {\n+            idx = dims[i] - 1;\n+          } else {\n+            return true;\n+          }\n+      int64_t& idx = multi_index[feature_dim];\n+      --idx;\n+      return idx != -1;\n+    };\n+    do {\n+      if (!lit.IsZero(multi_index)) {\n+    } while (decrement_multi_index());\n+    // The iteration stops if a feature has a non-zero value (or -1), but we\n+    // want the first zero feature which is always the next one (or 0 if -1).\n+    int64_t first_trailing_zero_feature = multi_index[feature_dim] + 1;\n+\n+    if (first_trailing_zero_feature == 0) {\n+      VLOG(2) << \"Weights constant is entirely zero.\";\n+    } else {\n+      VLOG(2) << \"First nonzero index in weights constant is \"\n+              << absl::StrJoin(multi_index, \",\");\n+    int64_t ret =\n+        std::max<int64_t>(0, weights->shape().dimensions(feature_dim) -\n+                                 first_trailing_zero_feature);\n+    if (slice->slice_starts(dim) != 0 || slice->slice_strides(dim) != 1 ||\n+        (dim != output_feature_dim &&\n+         slice->slice_limits(dim) !=\n+             slice->operand(0)->shape().dimensions(dim))) {\n",
            "whole_hunk": "@@ -164,36 +164,43 @@ std::optional<int64_t> NumTrailingZeroOutputFeatures(HloInstruction* conv) {\n     for (int64_t dim : dims) {\n       multi_index.push_back(dim - 1);\n     }\n-    while (true) {\n-      if (!lit.IsZero(multi_index)) {\n-        break;\n-      }\n-      multi_index[multi_index.size() - 1]--;\n-      for (int i = multi_index.size() - 2; i > 0; i--) {\n-        if (multi_index[i] == -1) {\n-          multi_index[i] = dims[i] - 1;\n-          multi_index[i - 1]--;\n-        } else {\n-          break;\n+    // This iterates through the literal with feature_dim as the most\n+    // major dimension looking for the final non-zero feature.\n+    auto decrement_multi_index = [&] {\n+      for (int i = 0; i < multi_index.size(); ++i) {\n+        if (i != feature_dim) {\n+          int64_t& idx = multi_index[i];\n+          --idx;\n+          if (idx == -1) {\n+            idx = dims[i] - 1;\n+          } else {\n+            return true;\n+          }\n         }\n       }\n-      if (multi_index[0] == -1) {\n+      int64_t& idx = multi_index[feature_dim];\n+      --idx;\n+      return idx != -1;\n+    };\n+    do {\n+      if (!lit.IsZero(multi_index)) {\n         break;\n       }\n-    }\n+    } while (decrement_multi_index());\n \n-    VLOG(2) << \"First nonzero index in weights constant is \"\n-            << absl::StrJoin(multi_index, \",\");\n-    int64_t first_nonzero_feature = multi_index[feature_dim];\n-    // \"round up\" the first nonzero feature index if it's not *all* zeros.\n-    for (int i = 0; i < multi_index.size(); i++) {\n-      if (i != feature_dim && multi_index[i] != 0) {\n-        first_nonzero_feature++;\n-        break;\n-      }\n+    // The iteration stops if a feature has a non-zero value (or -1), but we\n+    // want the first zero feature which is always the next one (or 0 if -1).\n+    int64_t first_trailing_zero_feature = multi_index[feature_dim] + 1;\n+\n+    if (first_trailing_zero_feature == 0) {\n+      VLOG(2) << \"Weights constant is entirely zero.\";\n+    } else {\n+      VLOG(2) << \"First nonzero index in weights constant is \"\n+              << absl::StrJoin(multi_index, \",\");\n     }\n-    int64_t ret = std::max<int64_t>(\n-        0, weights->shape().dimensions(feature_dim) - first_nonzero_feature);\n+    int64_t ret =\n+        std::max<int64_t>(0, weights->shape().dimensions(feature_dim) -\n+                                 first_trailing_zero_feature);\n     VLOG(2) << \"Success: weights is a constant; num zero trailing output \"\n                \"features is \"\n             << ret;\n@@ -333,8 +340,10 @@ StatusOr<bool> TrySimplifyPadding(HloInstruction* instr) {\n \n   // We're only allowed to slice the feature dim.\n   for (int64_t dim = 0; dim < slice->slice_limits().size(); dim++) {\n-    if (dim != output_feature_dim &&\n-        slice->slice_limits(dim) != slice->shape().dimensions(dim)) {\n+    if (slice->slice_starts(dim) != 0 || slice->slice_strides(dim) != 1 ||\n+        (dim != output_feature_dim &&\n+         slice->slice_limits(dim) !=\n+             slice->operand(0)->shape().dimensions(dim))) {\n       VLOG(2) << \"fail: Slice removes something other than the features dim.\";\n       return false;\n     }\n"
        },
        {
            "name": "cudnn_simplify_padding_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/cudnn_simplify_padding_test.cc",
            "patches": [
                {
                    "old_start": 590,
                    "old_length": 5,
                    "new_start": 590,
                    "new_length": 98,
                    "hunk": "@@ -590,5 +590,98 @@ TEST_F(CudnnSimplifyPaddingTest, SliceMoreElementsThanPad) {\n   }\n }\n \n+TEST_F(CudnnSimplifyPaddingTest, NoChangeOnNonTrivialConstants) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule jit_outer\n+\n+ENTRY main.26 {\n+  reshape.2 = f32[1,3,3,12]{3,2,1,0} parameter(0)\n+  constant.1 = f32[3,3,1,12]{3,2,1,0} constant({ {\n+    { /*i1=0*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } } } })\n+  cudnn-conv = (f32[1,5,5,12]{3,2,1,0}, u8[0]{0}) custom-call(reshape.2, constant.1), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, feature_group_count=12, custom_call_target=\"__cudnn$convForward\"\n+  get-tuple-element = f32[1,5,5,12]{3,2,1,0} get-tuple-element(cudnn-conv), index=0\n+  slice.2 = f32[1,5,1,12]{3,2,1,0} slice(get-tuple-element), slice={[0:1], [0:5], [0:1], [0:12]}\n+  constant.0 = f32[] constant(0)\n+  ROOT pad.1 = f32[1,5,3,12]{3,2,1,0} pad(slice.2, constant.0), padding=0_0x0_0x2_0x0_0\n+}\n+  )\")\n+                    .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunJustThisPass(module.get()));\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(CudnnSimplifyPaddingTest, NoChangeOnComplexSlices) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule jit_outer\n+\n+ENTRY main.26 {\n+  reshape.2 = f32[1,3,3,12]{3,2,1,0} parameter(0)\n+  constant.1 = f32[3,3,1,12]{3,2,1,0} constant({ {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } } } })\n+  cudnn-conv = (f32[1,5,5,12]{3,2,1,0}, u8[0]{0}) custom-call(reshape.2, constant.1), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, feature_group_count=12, custom_call_target=\"__cudnn$convForward\"\n+  get-tuple-element = f32[1,5,5,12]{3,2,1,0} get-tuple-element(cudnn-conv), index=0\n+  slice.2 = f32[1,5,5,4]{3,2,1,0} slice(get-tuple-element), slice={[0:1], [0:5], [0:5], [2:6]}\n+  constant.0 = f32[] constant(0)\n+  ROOT pad.1 = f32[1,5,5,12]{3,2,1,0} pad(slice.2, constant.0), padding=0_0x0_0x0_0x0_8\n+}\n+  )\")\n+                    .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunJustThisPass(module.get()));\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(CudnnSimplifyPaddingTest, ScanOrderFeatureDimLast) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule jit_outer\n+\n+ENTRY main.26 {\n+  reshape.2 = f32[1,3,3,12]{3,2,1,0} parameter(0)\n+  constant.1 = f32[3,3,1,12]{3,2,1,0} constant({ {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } } } })\n+  cudnn-conv = (f32[1,5,5,12]{3,2,1,0}, u8[0]{0}) custom-call(reshape.2, constant.1), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, feature_group_count=12, custom_call_target=\"__cudnn$convForward\"\n+  get-tuple-element = f32[1,5,5,12]{3,2,1,0} get-tuple-element(cudnn-conv), index=0\n+  slice.2 = f32[1,5,5,6]{3,2,1,0} slice(get-tuple-element), slice={[0:1], [0:5], [0:5], [0:6]}\n+  constant.0 = f32[] constant(0)\n+  ROOT pad.1 = f32[1,5,5,12]{3,2,1,0} pad(slice.2, constant.0), padding=0_0x0_0x0_0x0_6\n+}\n+  )\")\n+                    .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunJustThisPass(module.get()));\n+  EXPECT_FALSE(changed);\n+}\n+\n }  // anonymous namespace\n }  // namespace xla::gpu"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(CudnnSimplifyPaddingTest, NoChangeOnNonTrivialConstants) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule jit_outer\n+\n+ENTRY main.26 {\n+  reshape.2 = f32[1,3,3,12]{3,2,1,0} parameter(0)\n+  constant.1 = f32[3,3,1,12]{3,2,1,0} constant({ {\n+    { /*i1=0*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } } } })\n+  cudnn-conv = (f32[1,5,5,12]{3,2,1,0}, u8[0]{0}) custom-call(reshape.2, constant.1), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, feature_group_count=12, custom_call_target=\"__cudnn$convForward\"\n+  get-tuple-element = f32[1,5,5,12]{3,2,1,0} get-tuple-element(cudnn-conv), index=0\n+  slice.2 = f32[1,5,1,12]{3,2,1,0} slice(get-tuple-element), slice={[0:1], [0:5], [0:1], [0:12]}\n+  constant.0 = f32[] constant(0)\n+  ROOT pad.1 = f32[1,5,3,12]{3,2,1,0} pad(slice.2, constant.0), padding=0_0x0_0x2_0x0_0\n+}\n+  )\")\n+                    .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunJustThisPass(module.get()));\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(CudnnSimplifyPaddingTest, NoChangeOnComplexSlices) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule jit_outer\n+\n+ENTRY main.26 {\n+  reshape.2 = f32[1,3,3,12]{3,2,1,0} parameter(0)\n+  constant.1 = f32[3,3,1,12]{3,2,1,0} constant({ {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } } } })\n+  cudnn-conv = (f32[1,5,5,12]{3,2,1,0}, u8[0]{0}) custom-call(reshape.2, constant.1), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, feature_group_count=12, custom_call_target=\"__cudnn$convForward\"\n+  get-tuple-element = f32[1,5,5,12]{3,2,1,0} get-tuple-element(cudnn-conv), index=0\n+  slice.2 = f32[1,5,5,4]{3,2,1,0} slice(get-tuple-element), slice={[0:1], [0:5], [0:5], [2:6]}\n+  constant.0 = f32[] constant(0)\n+  ROOT pad.1 = f32[1,5,5,12]{3,2,1,0} pad(slice.2, constant.0), padding=0_0x0_0x0_0x0_8\n+}\n+  )\")\n+                    .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunJustThisPass(module.get()));\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(CudnnSimplifyPaddingTest, ScanOrderFeatureDimLast) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule jit_outer\n+\n+ENTRY main.26 {\n+  reshape.2 = f32[1,3,3,12]{3,2,1,0} parameter(0)\n+  constant.1 = f32[3,3,1,12]{3,2,1,0} constant({ {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } } } })\n+  cudnn-conv = (f32[1,5,5,12]{3,2,1,0}, u8[0]{0}) custom-call(reshape.2, constant.1), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, feature_group_count=12, custom_call_target=\"__cudnn$convForward\"\n+  get-tuple-element = f32[1,5,5,12]{3,2,1,0} get-tuple-element(cudnn-conv), index=0\n+  slice.2 = f32[1,5,5,6]{3,2,1,0} slice(get-tuple-element), slice={[0:1], [0:5], [0:5], [0:6]}\n+  constant.0 = f32[] constant(0)\n+  ROOT pad.1 = f32[1,5,5,12]{3,2,1,0} pad(slice.2, constant.0), padding=0_0x0_0x0_0x0_6\n+}\n+  )\")\n+                    .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunJustThisPass(module.get()));\n+  EXPECT_FALSE(changed);\n+}\n+\n",
            "whole_hunk": "@@ -590,5 +590,98 @@ TEST_F(CudnnSimplifyPaddingTest, SliceMoreElementsThanPad) {\n   }\n }\n \n+TEST_F(CudnnSimplifyPaddingTest, NoChangeOnNonTrivialConstants) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule jit_outer\n+\n+ENTRY main.26 {\n+  reshape.2 = f32[1,3,3,12]{3,2,1,0} parameter(0)\n+  constant.1 = f32[3,3,1,12]{3,2,1,0} constant({ {\n+    { /*i1=0*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } } } })\n+  cudnn-conv = (f32[1,5,5,12]{3,2,1,0}, u8[0]{0}) custom-call(reshape.2, constant.1), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, feature_group_count=12, custom_call_target=\"__cudnn$convForward\"\n+  get-tuple-element = f32[1,5,5,12]{3,2,1,0} get-tuple-element(cudnn-conv), index=0\n+  slice.2 = f32[1,5,1,12]{3,2,1,0} slice(get-tuple-element), slice={[0:1], [0:5], [0:1], [0:12]}\n+  constant.0 = f32[] constant(0)\n+  ROOT pad.1 = f32[1,5,3,12]{3,2,1,0} pad(slice.2, constant.0), padding=0_0x0_0x2_0x0_0\n+}\n+  )\")\n+                    .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunJustThisPass(module.get()));\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(CudnnSimplifyPaddingTest, NoChangeOnComplexSlices) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule jit_outer\n+\n+ENTRY main.26 {\n+  reshape.2 = f32[1,3,3,12]{3,2,1,0} parameter(0)\n+  constant.1 = f32[3,3,1,12]{3,2,1,0} constant({ {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } } } })\n+  cudnn-conv = (f32[1,5,5,12]{3,2,1,0}, u8[0]{0}) custom-call(reshape.2, constant.1), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, feature_group_count=12, custom_call_target=\"__cudnn$convForward\"\n+  get-tuple-element = f32[1,5,5,12]{3,2,1,0} get-tuple-element(cudnn-conv), index=0\n+  slice.2 = f32[1,5,5,4]{3,2,1,0} slice(get-tuple-element), slice={[0:1], [0:5], [0:5], [2:6]}\n+  constant.0 = f32[] constant(0)\n+  ROOT pad.1 = f32[1,5,5,12]{3,2,1,0} pad(slice.2, constant.0), padding=0_0x0_0x0_0x0_8\n+}\n+  )\")\n+                    .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunJustThisPass(module.get()));\n+  EXPECT_FALSE(changed);\n+}\n+\n+TEST_F(CudnnSimplifyPaddingTest, ScanOrderFeatureDimLast) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule jit_outer\n+\n+ENTRY main.26 {\n+  reshape.2 = f32[1,3,3,12]{3,2,1,0} parameter(0)\n+  constant.1 = f32[3,3,1,12]{3,2,1,0} constant({ {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } }\n+  }, {\n+    { /*i1=0*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=1*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } },\n+    { /*i1=2*/ { 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 } } } })\n+  cudnn-conv = (f32[1,5,5,12]{3,2,1,0}, u8[0]{0}) custom-call(reshape.2, constant.1), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, feature_group_count=12, custom_call_target=\"__cudnn$convForward\"\n+  get-tuple-element = f32[1,5,5,12]{3,2,1,0} get-tuple-element(cudnn-conv), index=0\n+  slice.2 = f32[1,5,5,6]{3,2,1,0} slice(get-tuple-element), slice={[0:1], [0:5], [0:5], [0:6]}\n+  constant.0 = f32[] constant(0)\n+  ROOT pad.1 = f32[1,5,5,12]{3,2,1,0} pad(slice.2, constant.0), padding=0_0x0_0x0_0x0_6\n+}\n+  )\")\n+                    .value();\n+\n+  TF_ASSERT_OK_AND_ASSIGN(bool changed, RunJustThisPass(module.get()));\n+  EXPECT_FALSE(changed);\n+}\n+\n }  // anonymous namespace\n }  // namespace xla::gpu"
        }
    ]
},
{
    "Id": 676,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f5381e0e10b5a61344109c1b7c174c68110f7629",
    "date": "2022-09-30T13:39:28-07:00",
    "message": "Fix OOB error when op input sizes do not match.\n\nIn cases where op input sizes are specified as in\n```\nREGISTER_OP(\"DynamicStitch\")\n    .Input(\"indices: N * int32\")\n    .Input(\"data: N * T\")\n    .Output(\"merged: T\")\n    .Attr(\"N : int >= 1\")\n    .Attr(\"T : type\")\n    .SetShapeFn(DynamicStitchShapeFunction);\n```\nif differing number of inputs are provided (e.g. 3 for `indices` and 4 for `data`)\nwe can get a crash in the executor when parsing the inputs, even before the kernel\ncalled.  Here we avoid this by checking the return code for the argument id and\nexit early.\n\nPiperOrigin-RevId: 478068540",
    "label": "YES",
    "changes": [
        {
            "name": "execute.cc",
            "path": "tensorflow/core/common_runtime/eager/execute.cc",
            "patches": [
                {
                    "old_start": 313,
                    "old_length": 6,
                    "new_start": 313,
                    "new_length": 10,
                    "hunk": "@@ -313,6 +313,10 @@ bool IsHostMemoryArg(const EagerOperation& op, const NodeDef* node_def,\n   const auto& host_memory_args = kernel_def->host_memory_arg();\n   const OpDef& op_def = OpRegistry::Global()->LookUp(op.Name())->op_def;\n   const int arg_id = OpPortIdToArgId(*node_def, op_def.input_arg(), port_id);\n+  // Fail if argument ID not found.\n+  if (arg_id < 0) {\n+    return false;\n+  }\n   return std::find(host_memory_args.begin(), host_memory_args.end(),\n                    op_def.input_arg(arg_id).name()) != host_memory_args.end();\n }\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  // Fail if argument ID not found.\n+  if (arg_id < 0) {\n+    return false;\n+  }\n",
            "whole_hunk": "@@ -313,6 +313,10 @@ bool IsHostMemoryArg(const EagerOperation& op, const NodeDef* node_def,\n   const auto& host_memory_args = kernel_def->host_memory_arg();\n   const OpDef& op_def = OpRegistry::Global()->LookUp(op.Name())->op_def;\n   const int arg_id = OpPortIdToArgId(*node_def, op_def.input_arg(), port_id);\n+  // Fail if argument ID not found.\n+  if (arg_id < 0) {\n+    return false;\n+  }\n   return std::find(host_memory_args.begin(), host_memory_args.end(),\n                    op_def.input_arg(arg_id).name()) != host_memory_args.end();\n }\n"
        },
        {
            "name": "dynamic_stitch_op_test.py",
            "path": "tensorflow/python/kernel_tests/data_structures/dynamic_stitch_op_test.py",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 6,
                    "new_start": 18,
                    "new_length": 7,
                    "hunk": "@@ -18,6 +18,7 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import data_flow_ops\n"
                },
                {
                    "old_start": 308,
                    "old_length": 6,
                    "new_start": 309,
                    "new_length": 29,
                    "hunk": "@@ -308,6 +309,29 @@ class ParallelDynamicStitchTest(DynamicStitchTestBase, test.TestCase):\n     for datum, grad in zip(data, self.evaluate(grads[3:])):\n       self.assertAllEqual(7.0 * self.evaluate(datum), grad)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testMismatchedDataAndIndexListSizes(self):\n+    indices = [\n+        constant_op.constant([2]),\n+        constant_op.constant([1]),\n+        constant_op.constant([0]),\n+        constant_op.constant([3]),\n+    ]\n+    data = [\n+        constant_op.constant([1.0]),\n+        constant_op.constant([2.0]),\n+        constant_op.constant([3.0]),\n+        constant_op.constant([4.0])\n+    ]\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"expected inputs .* do not match|List argument .* must match\"):\n+      self.evaluate(data_flow_ops.dynamic_stitch(indices[0:2], data))\n+\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"expected inputs .* do not match|List argument .* must match\"):\n+      self.evaluate(data_flow_ops.dynamic_stitch(indices, data[0:2]))\n \n if __name__ == \"__main__\":\n   test.main()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from tensorflow.python.framework import errors\n+  @test_util.run_in_graph_and_eager_modes\n+  def testMismatchedDataAndIndexListSizes(self):\n+    indices = [\n+        constant_op.constant([2]),\n+        constant_op.constant([1]),\n+        constant_op.constant([0]),\n+        constant_op.constant([3]),\n+    ]\n+    data = [\n+        constant_op.constant([1.0]),\n+        constant_op.constant([2.0]),\n+        constant_op.constant([3.0]),\n+        constant_op.constant([4.0])\n+    ]\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"expected inputs .* do not match|List argument .* must match\"):\n+      self.evaluate(data_flow_ops.dynamic_stitch(indices[0:2], data))\n+\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"expected inputs .* do not match|List argument .* must match\"):\n+      self.evaluate(data_flow_ops.dynamic_stitch(indices, data[0:2]))\n",
            "whole_hunk": "@@ -18,6 +18,7 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import data_flow_ops\n@@ -308,6 +309,29 @@ class ParallelDynamicStitchTest(DynamicStitchTestBase, test.TestCase):\n     for datum, grad in zip(data, self.evaluate(grads[3:])):\n       self.assertAllEqual(7.0 * self.evaluate(datum), grad)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testMismatchedDataAndIndexListSizes(self):\n+    indices = [\n+        constant_op.constant([2]),\n+        constant_op.constant([1]),\n+        constant_op.constant([0]),\n+        constant_op.constant([3]),\n+    ]\n+    data = [\n+        constant_op.constant([1.0]),\n+        constant_op.constant([2.0]),\n+        constant_op.constant([3.0]),\n+        constant_op.constant([4.0])\n+    ]\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"expected inputs .* do not match|List argument .* must match\"):\n+      self.evaluate(data_flow_ops.dynamic_stitch(indices[0:2], data))\n+\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"expected inputs .* do not match|List argument .* must match\"):\n+      self.evaluate(data_flow_ops.dynamic_stitch(indices, data[0:2]))\n \n if __name__ == \"__main__\":\n   test.main()"
        }
    ]
},
{
    "Id": 537,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/24b8ded9e921e78f91ed3fec50d7623803ac4bd4",
    "date": "2023-02-10T17:43:14-08:00",
    "message": "Replace instance checks for `ops.EagerTensor` with `core.Value`.\n\nPiperOrigin-RevId: 508793367",
    "label": "NO",
    "changes": [
        {
            "name": "tensor_util.py",
            "path": "tensorflow/python/framework/tensor_util.py",
            "patches": [
                {
                    "old_start": 398,
                    "old_length": 7,
                    "new_start": 398,
                    "new_length": 7,
                    "hunk": "@@ -398,7 +398,7 @@ def _AssertCompatible(values, dtype):\n \n def _is_array_like(obj):  # pylint: disable=invalid-name\n   \"\"\"Check if a given object is array-like.\"\"\"\n-  if isinstance(obj, ops.Tensor) and not isinstance(obj, ops._EagerTensorBase):  # pylint: disable=protected-access\n+  if isinstance(obj, ops.Tensor) and not isinstance(obj, core.Value):  # pylint: disable=protected-access\n     # Tensor implements __array__ only so it can inform the user that it is not\n     # a valid array.\n     return False\n"
                },
                {
                    "old_start": 934,
                    "old_length": 7,
                    "new_start": 934,
                    "new_length": 7,
                    "hunk": "@@ -934,7 +934,7 @@ def constant_value(tensor, partial=False):  # pylint: disable=invalid-name\n   Raises:\n     TypeError: if tensor is not an ops.Tensor.\n   \"\"\"\n-  if isinstance(tensor, ops.EagerTensor):\n+  if isinstance(tensor, core.Value):\n     try:\n       return tensor.numpy()\n     except errors_impl.UnimplementedError:\n"
                },
                {
                    "old_start": 972,
                    "old_length": 7,
                    "new_start": 972,
                    "new_length": 7,
                    "hunk": "@@ -972,7 +972,7 @@ def constant_value_as_shape(tensor):  # pylint: disable=invalid-name\n   Raises:\n     ValueError: If the shape is rank-0 and is not statically known to be -1.\n   \"\"\"\n-  if isinstance(tensor, ops.EagerTensor):\n+  if isinstance(tensor, core.Value):\n     return tensor_shape.TensorShape(\n         [dim if dim != -1 else None for dim in tensor.numpy()])\n "
                }
            ],
            "whole_deleted": "-  if isinstance(obj, ops.Tensor) and not isinstance(obj, ops._EagerTensorBase):  # pylint: disable=protected-access\n-  if isinstance(tensor, ops.EagerTensor):\n-  if isinstance(tensor, ops.EagerTensor):\n",
            "whole_added": "+  if isinstance(obj, ops.Tensor) and not isinstance(obj, core.Value):  # pylint: disable=protected-access\n+  if isinstance(tensor, core.Value):\n+  if isinstance(tensor, core.Value):\n",
            "whole_hunk": "@@ -398,7 +398,7 @@ def _AssertCompatible(values, dtype):\n \n def _is_array_like(obj):  # pylint: disable=invalid-name\n   \"\"\"Check if a given object is array-like.\"\"\"\n-  if isinstance(obj, ops.Tensor) and not isinstance(obj, ops._EagerTensorBase):  # pylint: disable=protected-access\n+  if isinstance(obj, ops.Tensor) and not isinstance(obj, core.Value):  # pylint: disable=protected-access\n     # Tensor implements __array__ only so it can inform the user that it is not\n     # a valid array.\n     return False\n@@ -934,7 +934,7 @@ def constant_value(tensor, partial=False):  # pylint: disable=invalid-name\n   Raises:\n     TypeError: if tensor is not an ops.Tensor.\n   \"\"\"\n-  if isinstance(tensor, ops.EagerTensor):\n+  if isinstance(tensor, core.Value):\n     try:\n       return tensor.numpy()\n     except errors_impl.UnimplementedError:\n@@ -972,7 +972,7 @@ def constant_value_as_shape(tensor):  # pylint: disable=invalid-name\n   Raises:\n     ValueError: If the shape is rank-0 and is not statically known to be -1.\n   \"\"\"\n-  if isinstance(tensor, ops.EagerTensor):\n+  if isinstance(tensor, core.Value):\n     return tensor_shape.TensorShape(\n         [dim if dim != -1 else None for dim in tensor.numpy()])\n "
        }
    ]
},
{
    "Id": 353,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/5006a30cce50b6f613317a2ba5db16edd34cf02e",
    "date": "2023-07-14T16:03:18-07:00",
    "message": "Improve ragged_cross_op input ragged splits check and fix flaky ragged_cross_op_tests.\n\nPiperOrigin-RevId: 548243985",
    "label": "NO",
    "changes": [
        {
            "name": "ragged_cross_op.cc",
            "path": "tensorflow/core/kernels/ragged_cross_op.cc",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 6,
                    "new_start": 17,
                    "new_length": 7,
                    "hunk": "@@ -17,6 +17,7 @@ limitations under the License.\n #include <string>\n #include <vector>\n \n+#include \"absl/status/status.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n"
                },
                {
                    "old_start": 386,
                    "old_length": 13,
                    "new_start": 387,
                    "new_length": 32,
                    "hunk": "@@ -386,13 +387,32 @@ class RaggedCrossOp : public OpKernel {\n \n     // Validate tensor shapes.\n     for (int i = 0; i < num_ragged; ++i) {\n-      if (!TensorShapeUtils::IsVector(ragged_values_list[i].shape())) {\n-        return errors::InvalidArgument(\n+      if (!TensorShapeUtils::IsVector(ragged_values_list[i].shape()) ||\n+          !TensorShapeUtils::IsVector(ragged_splits_list[i].shape())) {\n+        return absl::InvalidArgumentError(\n             \"tf.ragged.cross only supports inputs with rank=2.\");\n       }\n-      if (!TensorShapeUtils::IsVector(ragged_splits_list[i].shape()) ||\n-          (ragged_splits_list[i].NumElements() == 0)) {\n-        return errors::InvalidArgument(\"Invalid RaggedTensor\");\n+      if (ragged_splits_list[i].NumElements() == 0) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid RaggedTensor: Ragged splits must be non-empty.\");\n+      }\n+      auto flat_row_splits = ragged_splits_list[i].flat<SplitsType>();\n+      if (flat_row_splits(0) != 0) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid RaggedTensor: Ragged splits must start from 0.\");\n+      }\n+      int64_t num_values = ragged_values_list[i].NumElements();\n+      if (flat_row_splits(flat_row_splits.size() - 1) != num_values) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid RaggedTensor: \"\n+            \"Ragged splits must end with the number of values.\");\n+      }\n+      for (int i = 1; i < flat_row_splits.size(); ++i) {\n+        if (flat_row_splits(i - 1) > flat_row_splits(i)) {\n+          return absl::InvalidArgumentError(\n+              \"Invalid RaggedTensor: \"\n+              \"Ragged splits must be sorted in ascending order.\");\n+        }\n       }\n     }\n     for (int i = 0; i < num_sparse; ++i) {\n"
                }
            ],
            "whole_deleted": "-      if (!TensorShapeUtils::IsVector(ragged_values_list[i].shape())) {\n-        return errors::InvalidArgument(\n-      if (!TensorShapeUtils::IsVector(ragged_splits_list[i].shape()) ||\n-          (ragged_splits_list[i].NumElements() == 0)) {\n-        return errors::InvalidArgument(\"Invalid RaggedTensor\");\n",
            "whole_added": "+#include \"absl/status/status.h\"\n+      if (!TensorShapeUtils::IsVector(ragged_values_list[i].shape()) ||\n+          !TensorShapeUtils::IsVector(ragged_splits_list[i].shape())) {\n+        return absl::InvalidArgumentError(\n+      if (ragged_splits_list[i].NumElements() == 0) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid RaggedTensor: Ragged splits must be non-empty.\");\n+      }\n+      auto flat_row_splits = ragged_splits_list[i].flat<SplitsType>();\n+      if (flat_row_splits(0) != 0) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid RaggedTensor: Ragged splits must start from 0.\");\n+      }\n+      int64_t num_values = ragged_values_list[i].NumElements();\n+      if (flat_row_splits(flat_row_splits.size() - 1) != num_values) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid RaggedTensor: \"\n+            \"Ragged splits must end with the number of values.\");\n+      }\n+      for (int i = 1; i < flat_row_splits.size(); ++i) {\n+        if (flat_row_splits(i - 1) > flat_row_splits(i)) {\n+          return absl::InvalidArgumentError(\n+              \"Invalid RaggedTensor: \"\n+              \"Ragged splits must be sorted in ascending order.\");\n+        }\n",
            "whole_hunk": "@@ -17,6 +17,7 @@ limitations under the License.\n #include <string>\n #include <vector>\n \n+#include \"absl/status/status.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n@@ -386,13 +387,32 @@ class RaggedCrossOp : public OpKernel {\n \n     // Validate tensor shapes.\n     for (int i = 0; i < num_ragged; ++i) {\n-      if (!TensorShapeUtils::IsVector(ragged_values_list[i].shape())) {\n-        return errors::InvalidArgument(\n+      if (!TensorShapeUtils::IsVector(ragged_values_list[i].shape()) ||\n+          !TensorShapeUtils::IsVector(ragged_splits_list[i].shape())) {\n+        return absl::InvalidArgumentError(\n             \"tf.ragged.cross only supports inputs with rank=2.\");\n       }\n-      if (!TensorShapeUtils::IsVector(ragged_splits_list[i].shape()) ||\n-          (ragged_splits_list[i].NumElements() == 0)) {\n-        return errors::InvalidArgument(\"Invalid RaggedTensor\");\n+      if (ragged_splits_list[i].NumElements() == 0) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid RaggedTensor: Ragged splits must be non-empty.\");\n+      }\n+      auto flat_row_splits = ragged_splits_list[i].flat<SplitsType>();\n+      if (flat_row_splits(0) != 0) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid RaggedTensor: Ragged splits must start from 0.\");\n+      }\n+      int64_t num_values = ragged_values_list[i].NumElements();\n+      if (flat_row_splits(flat_row_splits.size() - 1) != num_values) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid RaggedTensor: \"\n+            \"Ragged splits must end with the number of values.\");\n+      }\n+      for (int i = 1; i < flat_row_splits.size(); ++i) {\n+        if (flat_row_splits(i - 1) > flat_row_splits(i)) {\n+          return absl::InvalidArgumentError(\n+              \"Invalid RaggedTensor: \"\n+              \"Ragged splits must be sorted in ascending order.\");\n+        }\n       }\n     }\n     for (int i = 0; i < num_sparse; ++i) {\n"
        },
        {
            "name": "ragged_cross_op_test.py",
            "path": "tensorflow/python/ops/ragged/ragged_cross_op_test.py",
            "patches": [
                {
                    "old_start": 27,
                    "old_length": 7,
                    "new_start": 27,
                    "new_length": 6,
                    "hunk": "@@ -27,7 +27,6 @@ from tensorflow.python.framework import tensor_spec\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gen_ragged_array_ops\n-from tensorflow.python.ops import random_ops\n from tensorflow.python.ops import sparse_ops\n from tensorflow.python.ops.ragged import ragged_array_ops\n from tensorflow.python.ops.ragged import ragged_factory_ops\n"
                },
                {
                    "old_start": 350,
                    "old_length": 29,
                    "new_start": 349,
                    "new_length": 32,
                    "hunk": "@@ -350,29 +349,32 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       dict(\n           testcase_name='BadDType',\n           inputs=[ragged_const([[1.1], [2.2, 3.3]])],\n-          message=r'Unexpected dtype for inputs\\[0\\]'),\n+          message=r'Unexpected dtype for inputs\\[0\\]',\n+      ),\n       dict(\n           testcase_name='StaticBatchSizeMismatch1',\n-          inputs=[ragged_const([[1]]),\n-                  ragged_const([[2], [3]])],\n+          inputs=[ragged_const([[1]]), ragged_const([[2], [3]])],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='inputs must all have the same batch dimension size'),\n+          message='inputs must all have the same batch dimension size',\n+      ),\n       dict(\n           testcase_name='StaticBatchSizeMismatch2',\n-          inputs=[ragged_const([[1]]),\n-                  dense_const([[2], [3]])],\n+          inputs=[ragged_const([[1]]), dense_const([[2], [3]])],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='inputs must all have the same batch dimension size'),\n+          message='inputs must all have the same batch dimension size',\n+      ),\n       dict(\n           testcase_name='3DDenseTensor',\n           inputs=[dense_const([[[1]]])],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n       dict(\n           testcase_name='0DDenseTensor',\n           inputs=[dense_const(1)],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n   ])\n   def testStaticError(self, inputs, exception=ValueError, message=None):\n     with self.assertRaisesRegex(exception, message):\n"
                },
                {
                    "old_start": 382,
                    "old_length": 25,
                    "new_start": 384,
                    "new_length": 29,
                    "hunk": "@@ -382,25 +384,29 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       dict(\n           testcase_name='3DRaggedTensor',\n           inputs=[ragged_const([[[1]]], ragged_rank=1)],\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n       dict(\n           testcase_name='0DDenseTensor',\n           inputs=[dense_const(1)],\n           signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n       dict(\n           testcase_name='1DDenseTensor',\n           inputs=[dense_const([1])],\n           signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n       dict(\n           testcase_name='3DDenseTensor',\n           inputs=[dense_const([[[1]]])],\n           signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n   ])\n   def testRuntimeError(self,\n                        inputs,\n"
                },
                {
                    "old_start": 458,
                    "old_length": 7,
                    "new_start": 464,
                    "new_length": 15,
                    "hunk": "@@ -458,7 +464,15 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n           out_values_type=dtypes.string,\n           out_row_splits_type=dtypes.int64))\n \n-  def testRaggedCrossInvalidValue(self):\n+  @parameterized.named_parameters([\n+      dict(testcase_name='EmptySplits', ragged_splits=[]),\n+      dict(\n+          testcase_name='NegativeSplits', ragged_splits=[-216, -114, -58, -54]\n+      ),\n+      dict(testcase_name='TooLargeValueSplits', ragged_splits=[0, 1, 2, 10]),\n+      dict(testcase_name='UnsortedSplits', ragged_splits=[0, 2, 2, 1]),\n+  ])\n+  def testRaggedCrossInvalidRaggedSplits(self, ragged_splits):\n     # Test case in GitHub isseu 59114.\n     with self.assertRaisesRegex(\n         (ValueError, errors.InvalidArgumentError), 'Invalid RaggedTensor'\n"
                },
                {
                    "old_start": 468,
                    "old_length": 8,
                    "new_start": 482,
                    "new_length": 8,
                    "hunk": "@@ -468,8 +482,8 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       ragged_values = [\n           ragged_values_0,\n       ]\n-      ragged_row_splits_0_tensor = random_ops.random_uniform(\n-          [4], minval=-256, maxval=257, dtype=dtypes.int64\n+      ragged_row_splits_0_tensor = ragged_const(\n+          ragged_splits, dtype=dtypes.int64\n       )\n       ragged_row_splits_0 = array_ops.identity(ragged_row_splits_0_tensor)\n       ragged_row_splits = ["
                }
            ],
            "whole_deleted": "-from tensorflow.python.ops import random_ops\n-          message=r'Unexpected dtype for inputs\\[0\\]'),\n-          inputs=[ragged_const([[1]]),\n-                  ragged_const([[2], [3]])],\n-          message='inputs must all have the same batch dimension size'),\n-          inputs=[ragged_const([[1]]),\n-                  dense_const([[2], [3]])],\n-          message='inputs must all have the same batch dimension size'),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n-  def testRaggedCrossInvalidValue(self):\n-      ragged_row_splits_0_tensor = random_ops.random_uniform(\n-          [4], minval=-256, maxval=257, dtype=dtypes.int64\n",
            "whole_added": "+          message=r'Unexpected dtype for inputs\\[0\\]',\n+      ),\n+          inputs=[ragged_const([[1]]), ragged_const([[2], [3]])],\n+          message='inputs must all have the same batch dimension size',\n+      ),\n+          inputs=[ragged_const([[1]]), dense_const([[2], [3]])],\n+          message='inputs must all have the same batch dimension size',\n+      ),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n+  @parameterized.named_parameters([\n+      dict(testcase_name='EmptySplits', ragged_splits=[]),\n+      dict(\n+          testcase_name='NegativeSplits', ragged_splits=[-216, -114, -58, -54]\n+      ),\n+      dict(testcase_name='TooLargeValueSplits', ragged_splits=[0, 1, 2, 10]),\n+      dict(testcase_name='UnsortedSplits', ragged_splits=[0, 2, 2, 1]),\n+  ])\n+  def testRaggedCrossInvalidRaggedSplits(self, ragged_splits):\n+      ragged_row_splits_0_tensor = ragged_const(\n+          ragged_splits, dtype=dtypes.int64\n",
            "whole_hunk": "@@ -27,7 +27,6 @@ from tensorflow.python.framework import tensor_spec\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gen_ragged_array_ops\n-from tensorflow.python.ops import random_ops\n from tensorflow.python.ops import sparse_ops\n from tensorflow.python.ops.ragged import ragged_array_ops\n from tensorflow.python.ops.ragged import ragged_factory_ops\n@@ -350,29 +349,32 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       dict(\n           testcase_name='BadDType',\n           inputs=[ragged_const([[1.1], [2.2, 3.3]])],\n-          message=r'Unexpected dtype for inputs\\[0\\]'),\n+          message=r'Unexpected dtype for inputs\\[0\\]',\n+      ),\n       dict(\n           testcase_name='StaticBatchSizeMismatch1',\n-          inputs=[ragged_const([[1]]),\n-                  ragged_const([[2], [3]])],\n+          inputs=[ragged_const([[1]]), ragged_const([[2], [3]])],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='inputs must all have the same batch dimension size'),\n+          message='inputs must all have the same batch dimension size',\n+      ),\n       dict(\n           testcase_name='StaticBatchSizeMismatch2',\n-          inputs=[ragged_const([[1]]),\n-                  dense_const([[2], [3]])],\n+          inputs=[ragged_const([[1]]), dense_const([[2], [3]])],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='inputs must all have the same batch dimension size'),\n+          message='inputs must all have the same batch dimension size',\n+      ),\n       dict(\n           testcase_name='3DDenseTensor',\n           inputs=[dense_const([[[1]]])],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n       dict(\n           testcase_name='0DDenseTensor',\n           inputs=[dense_const(1)],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n   ])\n   def testStaticError(self, inputs, exception=ValueError, message=None):\n     with self.assertRaisesRegex(exception, message):\n@@ -382,25 +384,29 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       dict(\n           testcase_name='3DRaggedTensor',\n           inputs=[ragged_const([[[1]]], ragged_rank=1)],\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n       dict(\n           testcase_name='0DDenseTensor',\n           inputs=[dense_const(1)],\n           signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n       dict(\n           testcase_name='1DDenseTensor',\n           inputs=[dense_const([1])],\n           signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n       dict(\n           testcase_name='3DDenseTensor',\n           inputs=[dense_const([[[1]]])],\n           signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n           exception=(ValueError, errors.InvalidArgumentError),\n-          message='tf.ragged.cross only supports inputs with rank=2'),\n+          message='tf.ragged.cross only supports inputs with rank=2',\n+      ),\n   ])\n   def testRuntimeError(self,\n                        inputs,\n@@ -458,7 +464,15 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n           out_values_type=dtypes.string,\n           out_row_splits_type=dtypes.int64))\n \n-  def testRaggedCrossInvalidValue(self):\n+  @parameterized.named_parameters([\n+      dict(testcase_name='EmptySplits', ragged_splits=[]),\n+      dict(\n+          testcase_name='NegativeSplits', ragged_splits=[-216, -114, -58, -54]\n+      ),\n+      dict(testcase_name='TooLargeValueSplits', ragged_splits=[0, 1, 2, 10]),\n+      dict(testcase_name='UnsortedSplits', ragged_splits=[0, 2, 2, 1]),\n+  ])\n+  def testRaggedCrossInvalidRaggedSplits(self, ragged_splits):\n     # Test case in GitHub isseu 59114.\n     with self.assertRaisesRegex(\n         (ValueError, errors.InvalidArgumentError), 'Invalid RaggedTensor'\n@@ -468,8 +482,8 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       ragged_values = [\n           ragged_values_0,\n       ]\n-      ragged_row_splits_0_tensor = random_ops.random_uniform(\n-          [4], minval=-256, maxval=257, dtype=dtypes.int64\n+      ragged_row_splits_0_tensor = ragged_const(\n+          ragged_splits, dtype=dtypes.int64\n       )\n       ragged_row_splits_0 = array_ops.identity(ragged_row_splits_0_tensor)\n       ragged_row_splits = ["
        }
    ]
},
{
    "Id": 577,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f52a6ffe40e525254b54ac371e6a8e3531489655",
    "date": "2023-01-20T14:44:24-08:00",
    "message": "[MacOS][Pluggable Device] Fixes for macos pluggable device.\n\n1. Disable the Eigen max align check as we use custom device memory\n   allocator in Metal plugin\n2. Disable list kernels on Pluggable devices as Device memory on MacOS\n   pluggable device doesn't allow us to index at a offset from the base\n   MTLBuufer location.\n3. Disable XLA compilation path as its not supported in pluggable\n   architecture",
    "label": "YES",
    "changes": [
        {
            "name": "execute.cc",
            "path": "tensorflow/core/common_runtime/eager/execute.cc",
            "patches": [
                {
                    "old_start": 439,
                    "old_length": 6,
                    "new_start": 439,
                    "new_length": 10,
                    "hunk": "@@ -439,6 +439,10 @@ Status HasTPUReplication(const EagerOperation& op, const EagerContext& ctx,\n \n Status MustCompileWithXLA(const EagerOperation* op, const EagerContext& ctx,\n                           bool* compile_with_xla) {\n+\n+#if defined( PLUGGABLE_DEVICE_SUPPORTED_MACOS )\n+  *compile_with_xla = false;\n+#else\n   if (!op->is_function()) {\n     *compile_with_xla = false;\n     return OkStatus();\n"
                },
                {
                    "old_start": 468,
                    "old_length": 6,
                    "new_start": 472,
                    "new_length": 7,
                    "hunk": "@@ -468,6 +472,7 @@ Status MustCompileWithXLA(const EagerOperation* op, const EagerContext& ctx,\n   } else {\n     *compile_with_xla = false;\n   }\n+#endif\n \n   return OkStatus();\n }\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+#if defined( PLUGGABLE_DEVICE_SUPPORTED_MACOS )\n+  *compile_with_xla = false;\n+#else\n+#endif\n",
            "whole_hunk": "@@ -439,6 +439,10 @@ Status HasTPUReplication(const EagerOperation& op, const EagerContext& ctx,\n \n Status MustCompileWithXLA(const EagerOperation* op, const EagerContext& ctx,\n                           bool* compile_with_xla) {\n+\n+#if defined( PLUGGABLE_DEVICE_SUPPORTED_MACOS )\n+  *compile_with_xla = false;\n+#else\n   if (!op->is_function()) {\n     *compile_with_xla = false;\n     return OkStatus();\n@@ -468,6 +472,7 @@ Status MustCompileWithXLA(const EagerOperation* op, const EagerContext& ctx,\n   } else {\n     *compile_with_xla = false;\n   }\n+#endif\n \n   return OkStatus();\n }\n"
        },
        {
            "name": "tensor.h",
            "path": "tensorflow/core/framework/tensor.h",
            "patches": [
                {
                    "old_start": 35,
                    "old_length": 6,
                    "new_start": 35,
                    "new_length": 12,
                    "hunk": "@@ -35,6 +35,12 @@ limitations under the License.\n #include \"tensorflow/core/platform/mem.h\"\n #include \"tensorflow/core/platform/types.h\"\n \n+#if !defined(PLUGGABLE_DEVICE_SUPPORTED_MACOS) &&  \\\n+    defined(__APPLE__) && !defined(ANDROID) &&     \\\n+    !defined(__ANDROID__) && !TARGET_OS_IOS\n+#define PLUGGABLE_DEVICE_SUPPORTED_MACOS 1\n+#endif\n+\n namespace tensorflow {\n \n // Forward declarations.  In particular, we forward declare protos so that their\n"
                },
                {
                    "old_start": 314,
                    "old_length": 7,
                    "new_start": 320,
                    "new_length": 7,
                    "hunk": "@@ -314,7 +320,7 @@ class Tensor {\n \n   /// Returns true iff this tensor is aligned.\n   bool IsAligned() const {\n-#if EIGEN_MAX_ALIGN_BYTES == 0\n+#if EIGEN_MAX_ALIGN_BYTES == 0 || PLUGGABLE_DEVICE_SUPPORTED_MACOS\n     return true;\n #else\n     void* ptr = base<void>();\n"
                }
            ],
            "whole_deleted": "-#if EIGEN_MAX_ALIGN_BYTES == 0\n",
            "whole_added": "+#if !defined(PLUGGABLE_DEVICE_SUPPORTED_MACOS) &&  \\\n+    defined(__APPLE__) && !defined(ANDROID) &&     \\\n+    !defined(__ANDROID__) && !TARGET_OS_IOS\n+#define PLUGGABLE_DEVICE_SUPPORTED_MACOS 1\n+#endif\n+\n+#if EIGEN_MAX_ALIGN_BYTES == 0 || PLUGGABLE_DEVICE_SUPPORTED_MACOS\n",
            "whole_hunk": "@@ -35,6 +35,12 @@ limitations under the License.\n #include \"tensorflow/core/platform/mem.h\"\n #include \"tensorflow/core/platform/types.h\"\n \n+#if !defined(PLUGGABLE_DEVICE_SUPPORTED_MACOS) &&  \\\n+    defined(__APPLE__) && !defined(ANDROID) &&     \\\n+    !defined(__ANDROID__) && !TARGET_OS_IOS\n+#define PLUGGABLE_DEVICE_SUPPORTED_MACOS 1\n+#endif\n+\n namespace tensorflow {\n \n // Forward declarations.  In particular, we forward declare protos so that their\n@@ -314,7 +320,7 @@ class Tensor {\n \n   /// Returns true iff this tensor is aligned.\n   bool IsAligned() const {\n-#if EIGEN_MAX_ALIGN_BYTES == 0\n+#if EIGEN_MAX_ALIGN_BYTES == 0 || PLUGGABLE_DEVICE_SUPPORTED_MACOS\n     return true;\n #else\n     void* ptr = base<void>();\n"
        },
        {
            "name": "list_kernels.cc",
            "path": "tensorflow/core/kernels/list_kernels.cc",
            "patches": [
                {
                    "old_start": 711,
                    "old_length": 6,
                    "new_start": 711,
                    "new_length": 7,
                    "hunk": "@@ -711,6 +711,7 @@ REGISTER_LIST_COPY(VariantDeviceCopyDirection::DEVICE_TO_DEVICE);\n \n REGISTER_UNARY_VARIANT_DECODE_FUNCTION(TensorList, TensorList::kTypeName);\n \n+#if !defined( PLUGGABLE_DEVICE_SUPPORTED_MACOS )\n #define REGISTER_TENSOR_LIST_OPS_DEFAULT(T)                                \\\n   REGISTER_KERNEL_BUILDER(Name(\"TensorListStack\")                          \\\n                               .TypeConstraint<T>(\"element_dtype\")          \\\n"
                },
                {
                    "old_start": 785,
                    "old_length": 4,
                    "new_start": 786,
                    "new_length": 5,
                    "hunk": "@@ -785,4 +786,5 @@ TF_CALL_int64(REGISTER_TENSOR_LIST_OPS_DEFAULT);\n TF_CALL_GPU_NUMBER_TYPES(REGISTER_TENSOR_LIST_OPS_DEFAULT);\n \n #undef REGISTER_TENSOR_LIST_OPS_DEFAULT\n+#endif\n }  // namespace tensorflow"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#if !defined( PLUGGABLE_DEVICE_SUPPORTED_MACOS )\n+#endif\n",
            "whole_hunk": "@@ -711,6 +711,7 @@ REGISTER_LIST_COPY(VariantDeviceCopyDirection::DEVICE_TO_DEVICE);\n \n REGISTER_UNARY_VARIANT_DECODE_FUNCTION(TensorList, TensorList::kTypeName);\n \n+#if !defined( PLUGGABLE_DEVICE_SUPPORTED_MACOS )\n #define REGISTER_TENSOR_LIST_OPS_DEFAULT(T)                                \\\n   REGISTER_KERNEL_BUILDER(Name(\"TensorListStack\")                          \\\n                               .TypeConstraint<T>(\"element_dtype\")          \\\n@@ -785,4 +786,5 @@ TF_CALL_int64(REGISTER_TENSOR_LIST_OPS_DEFAULT);\n TF_CALL_GPU_NUMBER_TYPES(REGISTER_TENSOR_LIST_OPS_DEFAULT);\n \n #undef REGISTER_TENSOR_LIST_OPS_DEFAULT\n+#endif\n }  // namespace tensorflow"
        }
    ]
},
{
    "Id": 664,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/22ca232e2f59fa9a9c49100e7976a9a82f026f69",
    "date": "2022-10-07T09:17:31-07:00",
    "message": "Added additional checks to model initialization. Corrected error codes for the checks inside model loader.\n\nPiperOrigin-RevId: 479595356",
    "label": "YES",
    "changes": [
        {
            "name": "model_loader.cc",
            "path": "tensorflow/lite/experimental/acceleration/mini_benchmark/model_loader.cc",
            "patches": [
                {
                    "old_start": 32,
                    "old_length": 7,
                    "new_start": 32,
                    "new_length": 7,
                    "hunk": "@@ -32,7 +32,7 @@ namespace tflite {\n namespace acceleration {\n \n MinibenchmarkStatus ModelLoader::Init() {\n-  if (model_) {\n+  if (model_ && model_->initialized()) {\n     // Already done.\n     return kMinibenchmarkSuccess;\n   }\n"
                },
                {
                    "old_start": 40,
                    "old_length": 7,
                    "new_start": 40,
                    "new_length": 7,
                    "hunk": "@@ -40,7 +40,7 @@ MinibenchmarkStatus ModelLoader::Init() {\n   if (status != kMinibenchmarkSuccess) {\n     return status;\n   }\n-  if (!model_) {\n+  if (!model_ || !model_->initialized()) {\n     return kMinibenchmarkModelBuildFailed;\n   }\n   return kMinibenchmarkSuccess;\n"
                },
                {
                    "old_start": 58,
                    "old_length": 7,
                    "new_start": 58,
                    "new_length": 7,
                    "hunk": "@@ -58,7 +58,7 @@ MinibenchmarkStatus PathModelLoader::InitInternal() {\n \n MinibenchmarkStatus MmapModelLoader::InitInternal() {\n   if (model_fd_ < 0 || model_offset_ < 0 || model_size_ < 0) {\n-    return kMinibenchmarkModelReadFailed;\n+    return kMinibenchmarkPreconditionNotMet;\n   }\n   if (!MMAPAllocation::IsSupported()) {\n     return kMinibenchmarkUnsupportedPlatform;\n"
                },
                {
                    "old_start": 74,
                    "old_length": 7,
                    "new_start": 74,
                    "new_length": 7,
                    "hunk": "@@ -74,7 +74,7 @@ MinibenchmarkStatus MmapModelLoader::InitInternal() {\n \n MinibenchmarkStatus PipeModelLoader::InitInternal() {\n   if (pipe_fd_ < 0) {\n-    return kMinibenchmarkModelReadFailed;\n+    return kMinibenchmarkPreconditionNotMet;\n   }\n \n   std::free(model_buffer_);\n"
                }
            ],
            "whole_deleted": "-  if (model_) {\n-  if (!model_) {\n-    return kMinibenchmarkModelReadFailed;\n-    return kMinibenchmarkModelReadFailed;\n",
            "whole_added": "+  if (model_ && model_->initialized()) {\n+  if (!model_ || !model_->initialized()) {\n+    return kMinibenchmarkPreconditionNotMet;\n+    return kMinibenchmarkPreconditionNotMet;\n",
            "whole_hunk": "@@ -32,7 +32,7 @@ namespace tflite {\n namespace acceleration {\n \n MinibenchmarkStatus ModelLoader::Init() {\n-  if (model_) {\n+  if (model_ && model_->initialized()) {\n     // Already done.\n     return kMinibenchmarkSuccess;\n   }\n@@ -40,7 +40,7 @@ MinibenchmarkStatus ModelLoader::Init() {\n   if (status != kMinibenchmarkSuccess) {\n     return status;\n   }\n-  if (!model_) {\n+  if (!model_ || !model_->initialized()) {\n     return kMinibenchmarkModelBuildFailed;\n   }\n   return kMinibenchmarkSuccess;\n@@ -58,7 +58,7 @@ MinibenchmarkStatus PathModelLoader::InitInternal() {\n \n MinibenchmarkStatus MmapModelLoader::InitInternal() {\n   if (model_fd_ < 0 || model_offset_ < 0 || model_size_ < 0) {\n-    return kMinibenchmarkModelReadFailed;\n+    return kMinibenchmarkPreconditionNotMet;\n   }\n   if (!MMAPAllocation::IsSupported()) {\n     return kMinibenchmarkUnsupportedPlatform;\n@@ -74,7 +74,7 @@ MinibenchmarkStatus MmapModelLoader::InitInternal() {\n \n MinibenchmarkStatus PipeModelLoader::InitInternal() {\n   if (pipe_fd_ < 0) {\n-    return kMinibenchmarkModelReadFailed;\n+    return kMinibenchmarkPreconditionNotMet;\n   }\n \n   std::free(model_buffer_);\n"
        },
        {
            "name": "model_loader_test.cc",
            "path": "tensorflow/lite/experimental/acceleration/mini_benchmark/model_loader_test.cc",
            "patches": [
                {
                    "old_start": 130,
                    "old_length": 7,
                    "new_start": 130,
                    "new_length": 7,
                    "hunk": "@@ -130,7 +130,7 @@ TEST_F(ModelLoaderTest, InvalidPipe) {\n   auto model_loader = std::make_unique<PipeModelLoader>(-1, 10);\n \n   ASSERT_NE(model_loader, nullptr);\n-  EXPECT_THAT(model_loader->Init(), kMinibenchmarkModelReadFailed);\n+  EXPECT_THAT(model_loader->Init(), kMinibenchmarkPreconditionNotMet);\n }\n \n TEST_F(ModelLoaderTest, CreateModelLoaderFromValidPath) {"
                }
            ],
            "whole_deleted": "-  EXPECT_THAT(model_loader->Init(), kMinibenchmarkModelReadFailed);\n",
            "whole_added": "+  EXPECT_THAT(model_loader->Init(), kMinibenchmarkPreconditionNotMet);\n",
            "whole_hunk": "@@ -130,7 +130,7 @@ TEST_F(ModelLoaderTest, InvalidPipe) {\n   auto model_loader = std::make_unique<PipeModelLoader>(-1, 10);\n \n   ASSERT_NE(model_loader, nullptr);\n-  EXPECT_THAT(model_loader->Init(), kMinibenchmarkModelReadFailed);\n+  EXPECT_THAT(model_loader->Init(), kMinibenchmarkPreconditionNotMet);\n }\n \n TEST_F(ModelLoaderTest, CreateModelLoaderFromValidPath) {"
        }
    ]
},
{
    "Id": 257,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e44f8a08051baa58bde9130a844a1b82a8179526",
    "date": "2023-10-31T12:18:59-07:00",
    "message": "check hasattr on the type, not the instance.\n\nhasattr on the instance triggers __getattr__ which carries very undesirable\neffects, such as running Ops on a donated buffer.\n\nLong term, we may want to audit all uses of hasattr on TensorFlow instances\nthat overrides __getattr__ in nontrival (e.g. running tf Ops) ways. They will\nalmost always cause trouble here and there because TensorFlow is quite far\nfrom being able guarantee if an Op returns or consumes is actually valid in all cases. Things will improve give it time, but if we can avoid such strong assumptions the system tend to get more robust.\n\nPiperOrigin-RevId: 578261984",
    "label": "YES",
    "changes": [
        {
            "name": "async_checkpoint_helper.py",
            "path": "tensorflow/python/checkpoint/async_checkpoint_helper.py",
            "patches": [
                {
                    "old_start": 263,
                    "old_length": 10,
                    "new_start": 263,
                    "new_length": 10,
                    "hunk": "@@ -263,10 +263,10 @@ class AsyncCheckpointHelper:\n     # custom __getattr__ code, see b/152031870 for context.\n     for t in all_trackables:\n       # Special case 1: TPU Embedding, populate object_map here\n-    # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n-    # object map. Also add TPUEmbedding to separate list for special handling\n-    # with values copy.\n-      if hasattr(t, _TPU_EMBEDDING_ATTR):\n+      # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n+      # object map. Also add TPUEmbedding to separate list for special handling\n+      # with values copy.\n+      if hasattr(type(t), _TPU_EMBEDDING_ATTR):\n         self._handle_tpu_embedding(t)\n       # Special case 2: handle slot variables. The object_map is populated later\n       # when the variable values are being copied to host CPU for the first\n"
                },
                {
                    "old_start": 414,
                    "old_length": 9,
                    "new_start": 414,
                    "new_length": 9,
                    "hunk": "@@ -414,9 +414,9 @@ class AsyncCheckpointHelper:\n     Raises:\n       AttributeError: if the input trackable is not TPUEmbedding type.\n     \"\"\"\n-    if not hasattr(\n-        tpu_embedding, _TPU_EMBEDDING_ATTR\n-    ) or not callable(tpu_embedding._create_copy_for_async_checkpoint):  # pylint: disable=protected-access\n+    if not hasattr(type(tpu_embedding), _TPU_EMBEDDING_ATTR) or not callable(\n+        tpu_embedding._create_copy_for_async_checkpoint  # pylint: disable=protected-access\n+    ):\n       raise AttributeError(\n           \"Expecting TPUEmbedding type; got %s\" % type(tpu_embedding)\n       )"
                }
            ],
            "whole_deleted": "-    # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n-    # object map. Also add TPUEmbedding to separate list for special handling\n-    # with values copy.\n-      if hasattr(t, _TPU_EMBEDDING_ATTR):\n-    if not hasattr(\n-        tpu_embedding, _TPU_EMBEDDING_ATTR\n-    ) or not callable(tpu_embedding._create_copy_for_async_checkpoint):  # pylint: disable=protected-access\n",
            "whole_added": "+      # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n+      # object map. Also add TPUEmbedding to separate list for special handling\n+      # with values copy.\n+      if hasattr(type(t), _TPU_EMBEDDING_ATTR):\n+    if not hasattr(type(tpu_embedding), _TPU_EMBEDDING_ATTR) or not callable(\n+        tpu_embedding._create_copy_for_async_checkpoint  # pylint: disable=protected-access\n+    ):\n",
            "whole_hunk": "@@ -263,10 +263,10 @@ class AsyncCheckpointHelper:\n     # custom __getattr__ code, see b/152031870 for context.\n     for t in all_trackables:\n       # Special case 1: TPU Embedding, populate object_map here\n-    # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n-    # object map. Also add TPUEmbedding to separate list for special handling\n-    # with values copy.\n-      if hasattr(t, _TPU_EMBEDDING_ATTR):\n+      # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n+      # object map. Also add TPUEmbedding to separate list for special handling\n+      # with values copy.\n+      if hasattr(type(t), _TPU_EMBEDDING_ATTR):\n         self._handle_tpu_embedding(t)\n       # Special case 2: handle slot variables. The object_map is populated later\n       # when the variable values are being copied to host CPU for the first\n@@ -414,9 +414,9 @@ class AsyncCheckpointHelper:\n     Raises:\n       AttributeError: if the input trackable is not TPUEmbedding type.\n     \"\"\"\n-    if not hasattr(\n-        tpu_embedding, _TPU_EMBEDDING_ATTR\n-    ) or not callable(tpu_embedding._create_copy_for_async_checkpoint):  # pylint: disable=protected-access\n+    if not hasattr(type(tpu_embedding), _TPU_EMBEDDING_ATTR) or not callable(\n+        tpu_embedding._create_copy_for_async_checkpoint  # pylint: disable=protected-access\n+    ):\n       raise AttributeError(\n           \"Expecting TPUEmbedding type; got %s\" % type(tpu_embedding)\n       )"
        }
    ]
},
{
    "Id": 437,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8b742f8559e88474735d0a2c03e00da65e40b412",
    "date": "2023-05-03T10:28:05-07:00",
    "message": "Fix check error on shape overflow.\n\nFixes #60198, #60275\n\nPiperOrigin-RevId: 529127714",
    "label": "YES",
    "changes": [
        {
            "name": "linalg_ops_common.cc",
            "path": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
            "patches": [
                {
                    "old_start": 153,
                    "old_length": 8,
                    "new_start": 153,
                    "new_length": 10,
                    "hunk": "@@ -153,8 +153,10 @@ void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n     const int col_dimension = input_rank - 1;\n     const int64_t num_rows = in.dim_size(row_dimension);\n     const int64_t num_cols = in.dim_size(col_dimension);\n-    input_matrix_shapes->emplace_back(\n-        std::initializer_list<int64_t>({num_rows, num_cols}));\n+    TensorShape input_shape;\n+    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape({num_rows, num_cols},\n+                                                          &input_shape));\n+    input_matrix_shapes->push_back(std::move(input_shape));\n     inputs->emplace_back(&in);\n     OP_REQUIRES(\n         context, in.dtype() == DataTypeToEnum<InputScalar>::v(),"
                }
            ],
            "whole_deleted": "-    input_matrix_shapes->emplace_back(\n-        std::initializer_list<int64_t>({num_rows, num_cols}));\n",
            "whole_added": "+    TensorShape input_shape;\n+    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape({num_rows, num_cols},\n+                                                          &input_shape));\n+    input_matrix_shapes->push_back(std::move(input_shape));\n",
            "whole_hunk": "@@ -153,8 +153,10 @@ void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n     const int col_dimension = input_rank - 1;\n     const int64_t num_rows = in.dim_size(row_dimension);\n     const int64_t num_cols = in.dim_size(col_dimension);\n-    input_matrix_shapes->emplace_back(\n-        std::initializer_list<int64_t>({num_rows, num_cols}));\n+    TensorShape input_shape;\n+    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape({num_rows, num_cols},\n+                                                          &input_shape));\n+    input_matrix_shapes->push_back(std::move(input_shape));\n     inputs->emplace_back(&in);\n     OP_REQUIRES(\n         context, in.dtype() == DataTypeToEnum<InputScalar>::v(),"
        }
    ]
},
{
    "Id": 528,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/48d3e51a1bd128554dd129251a51b6e12918a604",
    "date": "2023-02-16T14:56:06-08:00",
    "message": "Add a check to HandleFromInput to ensure that the resource isn't empty.\n\nPiperOrigin-RevId: 510250667",
    "label": "NO",
    "changes": [
        {
            "name": "resource_mgr.cc",
            "path": "tensorflow/core/framework/resource_mgr.cc",
            "patches": [
                {
                    "old_start": 380,
                    "old_length": 6,
                    "new_start": 380,
                    "new_length": 7,
                    "hunk": "@@ -380,6 +380,7 @@ string ContainerInfo::DebugString() const {\n                          \"]\");\n }\n \n+// TODO(b/228388547) users of this method should be migrated to the one below.\n const ResourceHandle& HandleFromInput(OpKernelContext* ctx, int input) {\n   return ctx->input(input).flat<ResourceHandle>()(0);\n }\n"
                },
                {
                    "old_start": 388,
                    "old_length": 6,
                    "new_start": 389,
                    "new_length": 9,
                    "hunk": "@@ -388,6 +389,9 @@ Status HandleFromInput(OpKernelContext* ctx, StringPiece input,\n                        ResourceHandle* handle) {\n   const Tensor* tensor;\n   TF_RETURN_IF_ERROR(ctx->input(input, &tensor));\n+  if (tensor->NumElements() == 0) {\n+    return errors::InvalidArgument(\"Empty resouce handle\");\n+  }\n   *handle = tensor->flat<ResourceHandle>()(0);\n   return OkStatus();\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// TODO(b/228388547) users of this method should be migrated to the one below.\n+  if (tensor->NumElements() == 0) {\n+    return errors::InvalidArgument(\"Empty resouce handle\");\n+  }\n",
            "whole_hunk": "@@ -380,6 +380,7 @@ string ContainerInfo::DebugString() const {\n                          \"]\");\n }\n \n+// TODO(b/228388547) users of this method should be migrated to the one below.\n const ResourceHandle& HandleFromInput(OpKernelContext* ctx, int input) {\n   return ctx->input(input).flat<ResourceHandle>()(0);\n }\n@@ -388,6 +389,9 @@ Status HandleFromInput(OpKernelContext* ctx, StringPiece input,\n                        ResourceHandle* handle) {\n   const Tensor* tensor;\n   TF_RETURN_IF_ERROR(ctx->input(input, &tensor));\n+  if (tensor->NumElements() == 0) {\n+    return errors::InvalidArgument(\"Empty resouce handle\");\n+  }\n   *handle = tensor->flat<ResourceHandle>()(0);\n   return OkStatus();\n }"
        }
    ]
},
{
    "Id": 61,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/bc195281edfbe5c93fe1e7a05354d3747215ea91",
    "date": "2024-04-30T16:52:49-07:00",
    "message": "Ensure that all instructions which are inserted by HloRematerialization have .remat in their name.\n\nCloned computations are excluded from this check.\n\nPiperOrigin-RevId: 629553524",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/BUILD",
            "patches": [
                {
                    "old_start": 4940,
                    "old_length": 9,
                    "new_start": 4940,
                    "new_length": 12,
                    "hunk": "@@ -4940,9 +4940,12 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_matchers\",\n         \"//xla/tests:xla_internal_test_main\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\",\n         \"@local_tsl//tsl/lib/core:status_test_util\",\n         \"@local_tsl//tsl/platform:statusor\",\n+        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@local_tsl//tsl/platform:test\",\n",
            "whole_hunk": "@@ -4940,9 +4940,12 @@ xla_cc_test(\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/utils:hlo_matchers\",\n         \"//xla/tests:xla_internal_test_main\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_googletest//:gtest\",\n         \"@local_tsl//tsl/lib/core:status_test_util\",\n         \"@local_tsl//tsl/platform:statusor\",\n+        \"@local_tsl//tsl/platform:test\",\n     ],\n )\n \n"
        },
        {
            "name": "hlo_rematerialization.cc",
            "path": "third_party/xla/xla/service/hlo_rematerialization.cc",
            "patches": [
                {
                    "old_start": 2060,
                    "old_length": 7,
                    "new_start": 2060,
                    "new_length": 8,
                    "hunk": "@@ -2060,7 +2060,8 @@ absl::StatusOr<int64_t> RematerializeInstructions(\n                 HloInstruction::CreateGetTupleElement(\n                     ShapeUtil::GetTupleElementShape(remat_use->shape(),\n                                                     *user.index),\n-                    remat_use, *user.index));\n+                    remat_use, *user.index),\n+                /*new_name=*/\"gte.remat\");\n             indirect_users.push_back(instruction_list->CreateItem(remat_use));\n             gte_cache[*user.index] = remat_use;\n           } else {\n"
                },
                {
                    "old_start": 2069,
                    "old_length": 7,
                    "new_start": 2070,
                    "new_length": 8,
                    "hunk": "@@ -2069,7 +2070,8 @@ absl::StatusOr<int64_t> RematerializeInstructions(\n         }\n         if (user_operand->shape() != remat_use->shape()) {\n           remat_use = computation->AddInstruction(\n-              HloInstruction::CreateBitcast(user_operand->shape(), remat_use));\n+              HloInstruction::CreateBitcast(user_operand->shape(), remat_use),\n+              /*new_name=*/\"bitcast.remat\");\n           indirect_users.push_back(instruction_list->CreateItem(remat_use));\n         }\n         TF_RETURN_IF_ERROR(user.user->instruction->ReplaceOperandWith(\n"
                }
            ],
            "whole_deleted": "-                    remat_use, *user.index));\n-              HloInstruction::CreateBitcast(user_operand->shape(), remat_use));\n",
            "whole_added": "+                    remat_use, *user.index),\n+                /*new_name=*/\"gte.remat\");\n+              HloInstruction::CreateBitcast(user_operand->shape(), remat_use),\n+              /*new_name=*/\"bitcast.remat\");\n",
            "whole_hunk": "@@ -2060,7 +2060,8 @@ absl::StatusOr<int64_t> RematerializeInstructions(\n                 HloInstruction::CreateGetTupleElement(\n                     ShapeUtil::GetTupleElementShape(remat_use->shape(),\n                                                     *user.index),\n-                    remat_use, *user.index));\n+                    remat_use, *user.index),\n+                /*new_name=*/\"gte.remat\");\n             indirect_users.push_back(instruction_list->CreateItem(remat_use));\n             gte_cache[*user.index] = remat_use;\n           } else {\n@@ -2069,7 +2070,8 @@ absl::StatusOr<int64_t> RematerializeInstructions(\n         }\n         if (user_operand->shape() != remat_use->shape()) {\n           remat_use = computation->AddInstruction(\n-              HloInstruction::CreateBitcast(user_operand->shape(), remat_use));\n+              HloInstruction::CreateBitcast(user_operand->shape(), remat_use),\n+              /*new_name=*/\"bitcast.remat\");\n           indirect_users.push_back(instruction_list->CreateItem(remat_use));\n         }\n         TF_RETURN_IF_ERROR(user.user->instruction->ReplaceOperandWith(\n"
        },
        {
            "name": "hlo_rematerialization_test.cc",
            "path": "third_party/xla/xla/service/hlo_rematerialization_test.cc",
            "patches": [
                {
                    "old_start": 22,
                    "old_length": 6,
                    "new_start": 22,
                    "new_length": 9,
                    "hunk": "@@ -22,6 +22,9 @@ limitations under the License.\n #include <string>\n \n #include <gmock/gmock.h>\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/strings/match.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n"
                },
                {
                    "old_start": 35,
                    "old_length": 6,
                    "new_start": 38,
                    "new_length": 7,
                    "hunk": "@@ -35,6 +38,7 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"tsl/lib/core/status_test_util.h\"\n #include \"tsl/platform/statusor.h\"\n+#include \"tsl/platform/test.h\"\n \n namespace xla {\n namespace {\n"
                },
                {
                    "old_start": 58,
                    "old_length": 6,
                    "new_start": 62,
                    "new_length": 16,
                    "hunk": "@@ -58,6 +62,16 @@ class RecomputeAndCompressHloRematerializationTest\n           ComputationSchedulerToModuleScheduler(DefaultMemoryScheduler));\n       TF_EXPECT_OK(scheduler.Run(module).status());\n     }\n+\n+    // First, get a set of instruction names before running remat.\n+    for (const HloComputation* computation : module->computations()) {\n+      before_computation_names_.insert(computation->name());\n+      for (const HloInstruction* instruction : computation->instructions()) {\n+        before_instruction_names_.insert(instruction->name());\n+      }\n+    }\n+\n+    // Run remat.\n     HloRematerialization::RematerializationModeConfig config(\n         /*recompute=*/true, /*compress=*/true, /*host_offload=*/false);\n     auto shape_size_func = [](const Shape& shape) { return ByteSizeOf(shape); };\n"
                },
                {
                    "old_start": 69,
                    "old_length": 8,
                    "new_start": 83,
                    "new_length": 41,
                    "hunk": "@@ -69,8 +83,41 @@ class RecomputeAndCompressHloRematerializationTest\n         /*host_memory_offload_config=*/std::nullopt);\n     HloRematerialization::RematerializationSizes sizes;\n     HloRematerialization remat(options, sizes);\n-    return remat.Run(module);\n+    absl::StatusOr<bool> result = remat.Run(module);\n+\n+    // Finally, get a set of instruction names after running remat.\n+    for (const HloComputation* computation : module->computations()) {\n+      if (!before_computation_names_.contains(computation->name())) {\n+        // This computation was cloned by remat. Skip.\n+        continue;\n+      }\n+      for (const HloInstruction* instruction : computation->instructions()) {\n+        after_instruction_names_.insert(instruction->name());\n+      }\n+    }\n+\n+    return result;\n+  }\n+\n+  void CheckForRematInInstructionNames(absl::string_view test_case_name) {\n+    constexpr const absl::string_view kRematInstructionNameMustContain =\n+        \".remat\";\n+    for (const auto& instruction_name : after_instruction_names_) {\n+      if (!before_instruction_names_.contains(instruction_name)) {\n+        // This is a newly inserted instruction by remat, check that it contains\n+        // the target name.\n+        EXPECT_TRUE(absl::StrContains(instruction_name,\n+                                      kRematInstructionNameMustContain))\n+            << \"[\" << test_case_name << \"] Instruction \\\"\" << instruction_name\n+            << \"\\\" must contain \\\"\" << kRematInstructionNameMustContain << \"\\\"\";\n+      }\n+    }\n   }\n+\n+ private:\n+  absl::flat_hash_set<absl::string_view> before_computation_names_;\n+  absl::flat_hash_set<absl::string_view> before_instruction_names_;\n+  absl::flat_hash_set<absl::string_view> after_instruction_names_;\n };\n \n // Test rematerialization of a single computation produced by\n"
                },
                {
                    "old_start": 111,
                    "old_length": 6,
                    "new_start": 158,
                    "new_length": 8,
                    "hunk": "@@ -111,6 +158,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest, SingleComputation) {\n                 .sequence(computation)\n                 .instructions()[computation->instruction_count() - 3],\n             remat_bcast);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Test rematerialization of a single computation that contains nodes that\n"
                },
                {
                    "old_start": 191,
                    "old_length": 6,
                    "new_start": 240,
                    "new_length": 8,
                    "hunk": "@@ -191,6 +240,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest, RematerializeAroundWhile) {\n   // Only the entry computation should have a rematerialized instruction added.\n   EXPECT_EQ(entry_computation->instruction_count(), 8);\n   EXPECT_EQ(body_computation->instruction_count(), 8);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Test rematerialization of a computation which calls another computation via a\n"
                },
                {
                    "old_start": 225,
                    "old_length": 6,
                    "new_start": 276,
                    "new_length": 8,
                    "hunk": "@@ -225,6 +276,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest,\n   // Both computations should have rematerialized instructions added.\n   EXPECT_EQ(entry_computation->instruction_count(), 9);\n   EXPECT_EQ(body_computation->instruction_count(), 9);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Test rematerialization of a doubly nested computation. All computations\n"
                },
                {
                    "old_start": 268,
                    "old_length": 6,
                    "new_start": 321,
                    "new_length": 8,
                    "hunk": "@@ -268,6 +321,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest,\n   EXPECT_EQ(entry_computation->instruction_count(), 9);\n   EXPECT_EQ(middle_computation->instruction_count(), 9);\n   EXPECT_EQ(inner_computation->instruction_count(), 9);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, RngNotRematerialized) {\n"
                },
                {
                    "old_start": 336,
                    "old_length": 6,
                    "new_start": 391,
                    "new_length": 8,
                    "hunk": "@@ -336,6 +391,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest, RngNotRematerialized) {\n   EXPECT_EQ(count_rngs(entry_computation), 1);\n   // There should have been rematerialization.\n   EXPECT_GT(entry_computation->instruction_count(), original_instruction_count);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest,\n"
                },
                {
                    "old_start": 438,
                    "old_length": 6,
                    "new_start": 495,
                    "new_length": 8,
                    "hunk": "@@ -438,6 +495,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest,\n   EXPECT_THAT(add_3->operand(0), op::Broadcast(param));\n   EXPECT_NE(add_4->operand(0), bcast);\n   EXPECT_THAT(add_4->operand(0), op::Broadcast(param));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, CopyNotRematerialized) {\n"
                },
                {
                    "old_start": 484,
                    "old_length": 6,
                    "new_start": 543,
                    "new_length": 8,
                    "hunk": "@@ -484,6 +543,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest, CopyNotRematerialized) {\n   EXPECT_TRUE(changed);\n \n   EXPECT_EQ(count_copies(entry_computation), 1);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Test rematerialization of values through bitcasts\n"
                },
                {
                    "old_start": 549,
                    "old_length": 6,
                    "new_start": 610,
                    "new_length": 8,
                    "hunk": "@@ -549,6 +610,8 @@ ENTRY %mycomp (param: f32[1]) -> f32[1] {\n                 .sequence(computation)\n                 .instructions()[computation->instruction_count() - 4],\n             remat_broadcast);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Test that the \"deny list for move remats\" engages when we rematerialize\n"
                },
                {
                    "old_start": 584,
                    "old_length": 6,
                    "new_start": 647,
                    "new_length": 8,
                    "hunk": "@@ -584,6 +647,8 @@ ENTRY %mycomp (param: f32[1]) -> f32[1024] {\n   ASSERT_THAT(add, op::Add(op::Bitcast(op::Broadcast(_)),\n                            op::Bitcast(op::Broadcast(_))));\n   EXPECT_TRUE(changed);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, RematTupleShape) {\n"
                },
                {
                    "old_start": 627,
                    "old_length": 6,
                    "new_start": 692,
                    "new_length": 8,
                    "hunk": "@@ -627,6 +692,8 @@ ENTRY %entry {\n   ASSERT_THAT(\n       add, op::Add(op::Multiply(), op::GetTupleElement(AllOf(\n                                        op::Fusion(), ::testing::Ne(fusion)))));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, RematTupleShapeDoubleUse) {\n"
                },
                {
                    "old_start": 680,
                    "old_length": 6,
                    "new_start": 747,
                    "new_length": 8,
                    "hunk": "@@ -680,6 +747,8 @@ ENTRY %entry {\n   // Check that the rematerialized fusion is the same for both ops.\n   EXPECT_EQ(add->operand(0)->operand(1)->operand(0),\n             add->operand(1)->operand(0));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest,\n"
                },
                {
                    "old_start": 728,
                    "old_length": 6,
                    "new_start": 797,
                    "new_length": 8,
                    "hunk": "@@ -728,6 +797,8 @@ ENTRY %entry {\n   ASSERT_THAT(add, op::Add(op::Bitcast(op::Multiply()),\n                            op::Bitcast(op::GetTupleElement(\n                                AllOf(op::Fusion(), ::testing::Ne(fusion))))));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, RematThroughTuple) {\n"
                },
                {
                    "old_start": 779,
                    "old_length": 6,
                    "new_start": 850,
                    "new_length": 8,
                    "hunk": "@@ -779,6 +850,8 @@ ENTRY %entry {\n       add, op::Add(op::GetTupleElement(AllOf(op::Fusion(), ::testing::Ne(tuple),\n                                              ::testing::Ne(fusion))),\n                    op::Add()));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Make sure when rematerializing all-gathers we increment channel_ids properly.\n"
                },
                {
                    "old_start": 834,
                    "old_length": 6,
                    "new_start": 907,
                    "new_length": 8,
                    "hunk": "@@ -834,6 +907,8 @@ ENTRY %mycomp (param: f32[1]) -> f32[1] {\n   EXPECT_TRUE(original_ag->channel_id().has_value());\n   EXPECT_TRUE(remat_ag->channel_id().has_value());\n   EXPECT_EQ(*remat_ag->channel_id(), *original_ag->channel_id() + 1);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, RematTupleArgFusion) {\n"
                },
                {
                    "old_start": 895,
                    "old_length": 6,
                    "new_start": 970,
                    "new_length": 8,
                    "hunk": "@@ -895,6 +970,8 @@ ENTRY %entry {\n   ASSERT_THAT(\n       root, op::Tuple(op::Reduce(),\n                       op::Fusion(AllOf(op::Fusion(), ::testing::Ne(fusion0)))));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest,\n"
                },
                {
                    "old_start": 969,
                    "old_length": 6,
                    "new_start": 1046,
                    "new_length": 8,
                    "hunk": "@@ -969,6 +1046,8 @@ ENTRY %entry {\n       (*it)->called_computations()[0]));\n   EXPECT_TRUE(module->schedule().is_computation_scheduled(\n       (*it2)->called_computations()[0]));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n class CompressingRematerializationTest : public RematerializationTestBase {\n"
                },
                {
                    "old_start": 1463,
                    "old_length": 6,
                    "new_start": 1542,
                    "new_length": 8,
                    "hunk": "@@ -1463,6 +1542,8 @@ TEST_P(IndirectUseTest, IndirectUseRematerialized) {\n     EXPECT_TRUE(changed);\n     EXPECT_EQ(entry_computation->instruction_count(), 9);\n   }\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n INSTANTIATE_TEST_SUITE_P(IndirectUseTestInstantiation, IndirectUseTest,"
                }
            ],
            "whole_deleted": "-    return remat.Run(module);\n",
            "whole_added": "+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/strings/match.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"tsl/platform/test.h\"\n+\n+    // First, get a set of instruction names before running remat.\n+    for (const HloComputation* computation : module->computations()) {\n+      before_computation_names_.insert(computation->name());\n+      for (const HloInstruction* instruction : computation->instructions()) {\n+        before_instruction_names_.insert(instruction->name());\n+      }\n+    }\n+\n+    // Run remat.\n+    absl::StatusOr<bool> result = remat.Run(module);\n+\n+    // Finally, get a set of instruction names after running remat.\n+    for (const HloComputation* computation : module->computations()) {\n+      if (!before_computation_names_.contains(computation->name())) {\n+        // This computation was cloned by remat. Skip.\n+        continue;\n+      }\n+      for (const HloInstruction* instruction : computation->instructions()) {\n+        after_instruction_names_.insert(instruction->name());\n+      }\n+    }\n+\n+    return result;\n+  }\n+\n+  void CheckForRematInInstructionNames(absl::string_view test_case_name) {\n+    constexpr const absl::string_view kRematInstructionNameMustContain =\n+        \".remat\";\n+    for (const auto& instruction_name : after_instruction_names_) {\n+      if (!before_instruction_names_.contains(instruction_name)) {\n+        // This is a newly inserted instruction by remat, check that it contains\n+        // the target name.\n+        EXPECT_TRUE(absl::StrContains(instruction_name,\n+                                      kRematInstructionNameMustContain))\n+            << \"[\" << test_case_name << \"] Instruction \\\"\" << instruction_name\n+            << \"\\\" must contain \\\"\" << kRematInstructionNameMustContain << \"\\\"\";\n+      }\n+    }\n+\n+ private:\n+  absl::flat_hash_set<absl::string_view> before_computation_names_;\n+  absl::flat_hash_set<absl::string_view> before_instruction_names_;\n+  absl::flat_hash_set<absl::string_view> after_instruction_names_;\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n",
            "whole_hunk": "@@ -22,6 +22,9 @@ limitations under the License.\n #include <string>\n \n #include <gmock/gmock.h>\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/strings/match.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n@@ -35,6 +38,7 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"tsl/lib/core/status_test_util.h\"\n #include \"tsl/platform/statusor.h\"\n+#include \"tsl/platform/test.h\"\n \n namespace xla {\n namespace {\n@@ -58,6 +62,16 @@ class RecomputeAndCompressHloRematerializationTest\n           ComputationSchedulerToModuleScheduler(DefaultMemoryScheduler));\n       TF_EXPECT_OK(scheduler.Run(module).status());\n     }\n+\n+    // First, get a set of instruction names before running remat.\n+    for (const HloComputation* computation : module->computations()) {\n+      before_computation_names_.insert(computation->name());\n+      for (const HloInstruction* instruction : computation->instructions()) {\n+        before_instruction_names_.insert(instruction->name());\n+      }\n+    }\n+\n+    // Run remat.\n     HloRematerialization::RematerializationModeConfig config(\n         /*recompute=*/true, /*compress=*/true, /*host_offload=*/false);\n     auto shape_size_func = [](const Shape& shape) { return ByteSizeOf(shape); };\n@@ -69,8 +83,41 @@ class RecomputeAndCompressHloRematerializationTest\n         /*host_memory_offload_config=*/std::nullopt);\n     HloRematerialization::RematerializationSizes sizes;\n     HloRematerialization remat(options, sizes);\n-    return remat.Run(module);\n+    absl::StatusOr<bool> result = remat.Run(module);\n+\n+    // Finally, get a set of instruction names after running remat.\n+    for (const HloComputation* computation : module->computations()) {\n+      if (!before_computation_names_.contains(computation->name())) {\n+        // This computation was cloned by remat. Skip.\n+        continue;\n+      }\n+      for (const HloInstruction* instruction : computation->instructions()) {\n+        after_instruction_names_.insert(instruction->name());\n+      }\n+    }\n+\n+    return result;\n+  }\n+\n+  void CheckForRematInInstructionNames(absl::string_view test_case_name) {\n+    constexpr const absl::string_view kRematInstructionNameMustContain =\n+        \".remat\";\n+    for (const auto& instruction_name : after_instruction_names_) {\n+      if (!before_instruction_names_.contains(instruction_name)) {\n+        // This is a newly inserted instruction by remat, check that it contains\n+        // the target name.\n+        EXPECT_TRUE(absl::StrContains(instruction_name,\n+                                      kRematInstructionNameMustContain))\n+            << \"[\" << test_case_name << \"] Instruction \\\"\" << instruction_name\n+            << \"\\\" must contain \\\"\" << kRematInstructionNameMustContain << \"\\\"\";\n+      }\n+    }\n   }\n+\n+ private:\n+  absl::flat_hash_set<absl::string_view> before_computation_names_;\n+  absl::flat_hash_set<absl::string_view> before_instruction_names_;\n+  absl::flat_hash_set<absl::string_view> after_instruction_names_;\n };\n \n // Test rematerialization of a single computation produced by\n@@ -111,6 +158,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest, SingleComputation) {\n                 .sequence(computation)\n                 .instructions()[computation->instruction_count() - 3],\n             remat_bcast);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Test rematerialization of a single computation that contains nodes that\n@@ -191,6 +240,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest, RematerializeAroundWhile) {\n   // Only the entry computation should have a rematerialized instruction added.\n   EXPECT_EQ(entry_computation->instruction_count(), 8);\n   EXPECT_EQ(body_computation->instruction_count(), 8);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Test rematerialization of a computation which calls another computation via a\n@@ -225,6 +276,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest,\n   // Both computations should have rematerialized instructions added.\n   EXPECT_EQ(entry_computation->instruction_count(), 9);\n   EXPECT_EQ(body_computation->instruction_count(), 9);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Test rematerialization of a doubly nested computation. All computations\n@@ -268,6 +321,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest,\n   EXPECT_EQ(entry_computation->instruction_count(), 9);\n   EXPECT_EQ(middle_computation->instruction_count(), 9);\n   EXPECT_EQ(inner_computation->instruction_count(), 9);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, RngNotRematerialized) {\n@@ -336,6 +391,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest, RngNotRematerialized) {\n   EXPECT_EQ(count_rngs(entry_computation), 1);\n   // There should have been rematerialization.\n   EXPECT_GT(entry_computation->instruction_count(), original_instruction_count);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest,\n@@ -438,6 +495,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest,\n   EXPECT_THAT(add_3->operand(0), op::Broadcast(param));\n   EXPECT_NE(add_4->operand(0), bcast);\n   EXPECT_THAT(add_4->operand(0), op::Broadcast(param));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, CopyNotRematerialized) {\n@@ -484,6 +543,8 @@ TEST_F(RecomputeAndCompressHloRematerializationTest, CopyNotRematerialized) {\n   EXPECT_TRUE(changed);\n \n   EXPECT_EQ(count_copies(entry_computation), 1);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Test rematerialization of values through bitcasts\n@@ -549,6 +610,8 @@ ENTRY %mycomp (param: f32[1]) -> f32[1] {\n                 .sequence(computation)\n                 .instructions()[computation->instruction_count() - 4],\n             remat_broadcast);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Test that the \"deny list for move remats\" engages when we rematerialize\n@@ -584,6 +647,8 @@ ENTRY %mycomp (param: f32[1]) -> f32[1024] {\n   ASSERT_THAT(add, op::Add(op::Bitcast(op::Broadcast(_)),\n                            op::Bitcast(op::Broadcast(_))));\n   EXPECT_TRUE(changed);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, RematTupleShape) {\n@@ -627,6 +692,8 @@ ENTRY %entry {\n   ASSERT_THAT(\n       add, op::Add(op::Multiply(), op::GetTupleElement(AllOf(\n                                        op::Fusion(), ::testing::Ne(fusion)))));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, RematTupleShapeDoubleUse) {\n@@ -680,6 +747,8 @@ ENTRY %entry {\n   // Check that the rematerialized fusion is the same for both ops.\n   EXPECT_EQ(add->operand(0)->operand(1)->operand(0),\n             add->operand(1)->operand(0));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest,\n@@ -728,6 +797,8 @@ ENTRY %entry {\n   ASSERT_THAT(add, op::Add(op::Bitcast(op::Multiply()),\n                            op::Bitcast(op::GetTupleElement(\n                                AllOf(op::Fusion(), ::testing::Ne(fusion))))));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, RematThroughTuple) {\n@@ -779,6 +850,8 @@ ENTRY %entry {\n       add, op::Add(op::GetTupleElement(AllOf(op::Fusion(), ::testing::Ne(tuple),\n                                              ::testing::Ne(fusion))),\n                    op::Add()));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n // Make sure when rematerializing all-gathers we increment channel_ids properly.\n@@ -834,6 +907,8 @@ ENTRY %mycomp (param: f32[1]) -> f32[1] {\n   EXPECT_TRUE(original_ag->channel_id().has_value());\n   EXPECT_TRUE(remat_ag->channel_id().has_value());\n   EXPECT_EQ(*remat_ag->channel_id(), *original_ag->channel_id() + 1);\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest, RematTupleArgFusion) {\n@@ -895,6 +970,8 @@ ENTRY %entry {\n   ASSERT_THAT(\n       root, op::Tuple(op::Reduce(),\n                       op::Fusion(AllOf(op::Fusion(), ::testing::Ne(fusion0)))));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n TEST_F(RecomputeAndCompressHloRematerializationTest,\n@@ -969,6 +1046,8 @@ ENTRY %entry {\n       (*it)->called_computations()[0]));\n   EXPECT_TRUE(module->schedule().is_computation_scheduled(\n       (*it2)->called_computations()[0]));\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n class CompressingRematerializationTest : public RematerializationTestBase {\n@@ -1463,6 +1542,8 @@ TEST_P(IndirectUseTest, IndirectUseRematerialized) {\n     EXPECT_TRUE(changed);\n     EXPECT_EQ(entry_computation->instruction_count(), 9);\n   }\n+  CheckForRematInInstructionNames(\n+      ::testing::UnitTest::GetInstance()->current_test_info()->name());\n }\n \n INSTANTIATE_TEST_SUITE_P(IndirectUseTestInstantiation, IndirectUseTest,"
        }
    ]
},
{
    "Id": 425,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/d070aa7f0f409825e42358b6b29491f4842ceba3",
    "date": "2023-05-16T16:18:40-07:00",
    "message": "Use the generic printer and disable validation when printing debug operations.\n\nOp validation can be time-consuming on large models.\n\nPiperOrigin-RevId: 532602582",
    "label": "NO",
    "changes": [
        {
            "name": "mark_ops_for_outside_compilation.cc",
            "path": "tensorflow/compiler/mlir/tensorflow/transforms/mark_ops_for_outside_compilation.cc",
            "patches": [
                {
                    "old_start": 398,
                    "old_length": 6,
                    "new_start": 398,
                    "new_length": 7,
                    "hunk": "@@ -398,6 +398,7 @@ void UnmarkChildren(ModuleOp module) {\n }\n \n constexpr int kTooManyOutsideCompileRegionThreshold = 32;\n+constexpr int kOpDetailCount = 8;\n \n void WarnOnExcessOutsideCompilationOps(ModuleOp module) {\n   // Count the number of outside compilation ops. If it exceeds the reporting\n"
                },
                {
                    "old_start": 410,
                    "old_length": 8,
                    "new_start": 411,
                    "new_length": 9,
                    "hunk": "@@ -410,8 +411,9 @@ void WarnOnExcessOutsideCompilationOps(ModuleOp module) {\n   });\n \n   if (outside_compile_ops.size() > kTooManyOutsideCompileRegionThreshold) {\n-    llvm::SmallVector<std::string, 8> op_info;\n-    for (auto& op : outside_compile_ops) {\n+    llvm::SmallVector<std::string, kOpDetailCount> op_info;\n+    for (int i = 0; i < kOpDetailCount; ++i) {\n+      auto& op = outside_compile_ops[i];\n       op_info.push_back(tensorflow::OpAsString(*op));\n     }\n \n"
                }
            ],
            "whole_deleted": "-    llvm::SmallVector<std::string, 8> op_info;\n-    for (auto& op : outside_compile_ops) {\n",
            "whole_added": "+constexpr int kOpDetailCount = 8;\n+    llvm::SmallVector<std::string, kOpDetailCount> op_info;\n+    for (int i = 0; i < kOpDetailCount; ++i) {\n+      auto& op = outside_compile_ops[i];\n",
            "whole_hunk": "@@ -398,6 +398,7 @@ void UnmarkChildren(ModuleOp module) {\n }\n \n constexpr int kTooManyOutsideCompileRegionThreshold = 32;\n+constexpr int kOpDetailCount = 8;\n \n void WarnOnExcessOutsideCompilationOps(ModuleOp module) {\n   // Count the number of outside compilation ops. If it exceeds the reporting\n@@ -410,8 +411,9 @@ void WarnOnExcessOutsideCompilationOps(ModuleOp module) {\n   });\n \n   if (outside_compile_ops.size() > kTooManyOutsideCompileRegionThreshold) {\n-    llvm::SmallVector<std::string, 8> op_info;\n-    for (auto& op : outside_compile_ops) {\n+    llvm::SmallVector<std::string, kOpDetailCount> op_info;\n+    for (int i = 0; i < kOpDetailCount; ++i) {\n+      auto& op = outside_compile_ops[i];\n       op_info.push_back(tensorflow::OpAsString(*op));\n     }\n \n"
        },
        {
            "name": "string_util.cc",
            "path": "tensorflow/compiler/mlir/tensorflow/utils/string_util.cc",
            "patches": [
                {
                    "old_start": 27,
                    "old_length": 8,
                    "new_start": 27,
                    "new_length": 11,
                    "hunk": "@@ -27,8 +27,11 @@ namespace tensorflow {\n std::string OpAsString(mlir::Operation& op) {\n   std::string out;\n   llvm::raw_string_ostream op_stream(out);\n-  op.print(op_stream,\n-           mlir::OpPrintingFlags().enableDebugInfo(true, /*prettyForm=*/true));\n+  op.print(op_stream, mlir::OpPrintingFlags()\n+                          .elideLargeElementsAttrs()\n+                          .assumeVerified()\n+                          .skipRegions()\n+                          .printGenericOpForm());\n   return out;\n }\n "
                }
            ],
            "whole_deleted": "-  op.print(op_stream,\n-           mlir::OpPrintingFlags().enableDebugInfo(true, /*prettyForm=*/true));\n",
            "whole_added": "+  op.print(op_stream, mlir::OpPrintingFlags()\n+                          .elideLargeElementsAttrs()\n+                          .assumeVerified()\n+                          .skipRegions()\n+                          .printGenericOpForm());\n",
            "whole_hunk": "@@ -27,8 +27,11 @@ namespace tensorflow {\n std::string OpAsString(mlir::Operation& op) {\n   std::string out;\n   llvm::raw_string_ostream op_stream(out);\n-  op.print(op_stream,\n-           mlir::OpPrintingFlags().enableDebugInfo(true, /*prettyForm=*/true));\n+  op.print(op_stream, mlir::OpPrintingFlags()\n+                          .elideLargeElementsAttrs()\n+                          .assumeVerified()\n+                          .skipRegions()\n+                          .printGenericOpForm());\n   return out;\n }\n "
        }
    ]
},
{
    "Id": 694,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/83f96361c8ea039f8a44aad7f378dc805f3ed3ce",
    "date": "2022-09-15T07:47:37-07:00",
    "message": "Update Eigen to commit:dceb779ecd822f55b4ae78f760371b0e08a889f2\n\nCHANGELOG\n=========\ndceb779ec - Fix test for pow with mixed integer types. We do not convert the exponent if it is an integer type.\nafc014f1b - Allow mixed types for pow(), as long as the exponent is exactly representable in the base type.\nb2c82a934 - Remove bad skew_symmetric_matrix3 test.\ne8a2aa24a - Fix a couple of issues with unary pow():\n07d075995 - [ROCm] Fix for sparse matrix related breakage on ROCm.\nfb212c745 - Fix g++-6 constexpr and c++20 constexpr build errors.\nec9c7163a - Feature/skew symmetric matrix3\n311ba66f7 - Fix realloc for non-trivial types.\n3c37dd2a1 - Tweak bound for pow to account for floating-point types.\nf9dfda28a - Add missing comparison operators for GPU packets.\n242325eca - Remove unused variable.\n133498c32 - Add constexpr, test for C++14 constexpr.\n69f50e3a6 - Adjust overflow threshold bound for pow tests.\n3e44f960e - Reduce compiler warnings for tests.\nb7e21d4e3 - Call check_that_malloc_is_allowed() in aligned_realloc()\n6e83e906c - fix typo in doc/TutorialSparse.dox\n525f06667 - fixed msvc compilation error in GeneralizedEigenSolver.h\nf241a2c18 - Add asserts for index-out-of-bounds in IndexedView.\nf5364331e - Fix some cmake issues.\nd816044b6 - Fix mixingtypes tests.\n94cc83faa - 2 typos fix in the 3rd table.\n30c42222a - Fix some test build errors in new unary pow.\nbd393e15c - Vectorize acos, asin, and atan for float.\ne5af9f87f - Vectorize pow for integer base / exponent types\n8acbf5c11 - re-enable pow for complex types\n7064ed134 - Specialize psign<Packet8i> for AVX2, don't vectorize psign<bool>.\n98e51c9e2 - Avoid undefined behavior in array_cwise test due to signed integer overflow\na7c1cac18 - Fix GeneralizedEigenSolver::info() and Asserts\n714678fc6 - Add missing ptr in realloc call.\nb2a13c9dd - Sparse Core: Replace malloc/free with conditional_aligned\n6aad0f821 - Fix psign for unsigned integer types, such as bool.\n1a09defce - Protect new pblend implementation with EIGEN_VECTORIZE_AVX2\n7c67dc67a - Use proper double word division algorithm for pow<double>. Gives 11-15% speedup.\n7a3b667c4 - Add support for AVX512-FP16 for vectorizing half precision math\n76a669fb4 - add fixed power unary operation\n39fcc8979 - Removed unnecessary checks for FP16C\n2f7cce2dd - [SYCL] Fix some SYCL tests\n27367017b - Disable bad \"deprecated warning\" edge-case in BDCSVD\nb8e93bf58 - Eliminate bool bitwise warnings.\n66ea0c09f - Don't double-define Half functions on aarch64\n97e0784dc - Vectorize the sign operator in Eigen.\nbe20207d1 - Fix vectorized Jacobi Rotation\n7a87ed1b6 - Fix code and unit test for a few corner cases in vectorized pow()\n9e0afe0f0 - Fix non-VSX PowerPC build\n84a9d6fac - Fix use of Packet2d type for non-VSX.\nce60a7be8 - Partial Packet support for GEMM real-only (PowerPC).  Also fix compilation warnings & errors for some conditions in new API.\n5a1c7807e - Fix inner iterator for sparse block.\n39d22ef46 - Fix flaky packetmath_1 test.\n7896c7dc6 - Use numext::sqrt in ConjugateGradient.\ne618c4a5e - Improve pblend AVX implementation\nef4654bae - Add true determinant to QR and it's variants\nb7668c037 - Avoid including <sstream> with EIGEN_NO_IO\n7dd3dda3d - Updated AccelerateSupport documentation after PR 966.\n69714ff61 - Add Sparse Subset of Matrix Inverse\n\nPiperOrigin-RevId: 474559050",
    "label": "NO",
    "changes": [
        {
            "name": "eigen.cmake",
            "path": "tensorflow/lite/tools/cmake/modules/eigen.cmake",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 7,
                    "new_start": 23,
                    "new_length": 7,
                    "hunk": "@@ -23,7 +23,7 @@ OverridableFetchContent_Declare(\n   eigen\n   GIT_REPOSITORY https://gitlab.com/libeigen/eigen.git\n   # Sync with tensorflow/third_party/eigen3/workspace.bzl\n-  GIT_TAG 34780d8bd13d0af0cf17a22789ef286e8512594d\n+  GIT_TAG dceb779ecd822f55b4ae78f760371b0e08a889f2\n   # It's not currently (cmake 3.17) possible to shallow clone with a GIT TAG\n   # as cmake attempts to git checkout the commit hash after the clone\n   # which doesn't work as it's a shallow clone hence a different commit hash.\n"
                }
            ],
            "whole_deleted": "-  GIT_TAG 34780d8bd13d0af0cf17a22789ef286e8512594d\n",
            "whole_added": "+  GIT_TAG dceb779ecd822f55b4ae78f760371b0e08a889f2\n",
            "whole_hunk": "@@ -23,7 +23,7 @@ OverridableFetchContent_Declare(\n   eigen\n   GIT_REPOSITORY https://gitlab.com/libeigen/eigen.git\n   # Sync with tensorflow/third_party/eigen3/workspace.bzl\n-  GIT_TAG 34780d8bd13d0af0cf17a22789ef286e8512594d\n+  GIT_TAG dceb779ecd822f55b4ae78f760371b0e08a889f2\n   # It's not currently (cmake 3.17) possible to shallow clone with a GIT TAG\n   # as cmake attempts to git checkout the commit hash after the clone\n   # which doesn't work as it's a shallow clone hence a different commit hash.\n"
        },
        {
            "name": "workspace.bzl",
            "path": "third_party/eigen3/workspace.bzl",
            "patches": [
                {
                    "old_start": 7,
                    "old_length": 8,
                    "new_start": 7,
                    "new_length": 8,
                    "hunk": "@@ -7,8 +7,8 @@ def repo():\n \n     # Attention: tools parse and update these lines.\n     # LINT.IfChange\n-    EIGEN_COMMIT = \"34780d8bd13d0af0cf17a22789ef286e8512594d\"\n-    EIGEN_SHA256 = \"e5da7faf0f603dac718fca8d6f564cd8ed57cfe79aa40d165b2393fe31b5f6f4\"\n+    EIGEN_COMMIT = \"dceb779ecd822f55b4ae78f760371b0e08a889f2\"\n+    EIGEN_SHA256 = \"470feafb15df971bb91df6f23dace4f0ec7f3c8d00fb69adafa72cd18bb2715e\"\n     # LINT.ThenChange(//tensorflow/lite/tools/cmake/modules/eigen.cmake)\n \n     tf_http_archive("
                }
            ],
            "whole_deleted": "-    EIGEN_COMMIT = \"34780d8bd13d0af0cf17a22789ef286e8512594d\"\n-    EIGEN_SHA256 = \"e5da7faf0f603dac718fca8d6f564cd8ed57cfe79aa40d165b2393fe31b5f6f4\"\n",
            "whole_added": "+    EIGEN_COMMIT = \"dceb779ecd822f55b4ae78f760371b0e08a889f2\"\n+    EIGEN_SHA256 = \"470feafb15df971bb91df6f23dace4f0ec7f3c8d00fb69adafa72cd18bb2715e\"\n",
            "whole_hunk": "@@ -7,8 +7,8 @@ def repo():\n \n     # Attention: tools parse and update these lines.\n     # LINT.IfChange\n-    EIGEN_COMMIT = \"34780d8bd13d0af0cf17a22789ef286e8512594d\"\n-    EIGEN_SHA256 = \"e5da7faf0f603dac718fca8d6f564cd8ed57cfe79aa40d165b2393fe31b5f6f4\"\n+    EIGEN_COMMIT = \"dceb779ecd822f55b4ae78f760371b0e08a889f2\"\n+    EIGEN_SHA256 = \"470feafb15df971bb91df6f23dace4f0ec7f3c8d00fb69adafa72cd18bb2715e\"\n     # LINT.ThenChange(//tensorflow/lite/tools/cmake/modules/eigen.cmake)\n \n     tf_http_archive("
        }
    ]
},
{
    "Id": 29,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1123fb3da1b2a8977bb01fc285a7be8f81914201",
    "date": "2024-06-10T04:10:55-07:00",
    "message": "Replace Interval::NumElements with ::GetLoopTripCount.\n\nThe latter has some safety checks built in. Replace unsafe\nuses of NumElements with `IsFeasible`.\n\nPiperOrigin-RevId: 641840653",
    "label": "YES",
    "changes": [
        {
            "name": "elemental_hlo_to_mlir.cc",
            "path": "third_party/xla/xla/service/gpu/fusions/mlir/elemental_hlo_to_mlir.cc",
            "patches": [
                {
                    "old_start": 1465,
                    "old_length": 7,
                    "new_start": 1465,
                    "new_length": 7,
                    "hunk": "@@ -1465,7 +1465,7 @@ SmallVector<Value> EmitLoopNest(\n   for (int sym_index = indexing_map.GetSymbolCount() - 1;\n        sym_index >= 0 && cumulative_loop_size < 64; --sym_index) {\n     auto& bound = indexing_map.GetSymbolBound(sym_index);\n-    cumulative_loop_size *= bound.NumElements();\n+    cumulative_loop_size *= bound.GetLoopTripCount();\n     if (!IsSymbolConstrained(indexing_map, sym_index)) continue;\n \n     IndexingMap peeled_map = indexing_map;\n"
                }
            ],
            "whole_deleted": "-    cumulative_loop_size *= bound.NumElements();\n",
            "whole_added": "+    cumulative_loop_size *= bound.GetLoopTripCount();\n",
            "whole_hunk": "@@ -1465,7 +1465,7 @@ SmallVector<Value> EmitLoopNest(\n   for (int sym_index = indexing_map.GetSymbolCount() - 1;\n        sym_index >= 0 && cumulative_loop_size < 64; --sym_index) {\n     auto& bound = indexing_map.GetSymbolBound(sym_index);\n-    cumulative_loop_size *= bound.NumElements();\n+    cumulative_loop_size *= bound.GetLoopTripCount();\n     if (!IsSymbolConstrained(indexing_map, sym_index)) continue;\n \n     IndexingMap peeled_map = indexing_map;\n"
        },
        {
            "name": "coalescing_analysis.cc",
            "path": "third_party/xla/xla/service/gpu/model/coalescing_analysis.cc",
            "patches": [
                {
                    "old_start": 472,
                    "old_length": 7,
                    "new_start": 472,
                    "new_length": 7,
                    "hunk": "@@ -472,7 +472,7 @@ std::vector<Interval> FindContiguousIntervals(\n       // Case 1.3: |multiplier| != 1 and g(s) = s.\n       if (partitioned_expr.func_of_s0 == range) {\n         Interval range_interval = indexing_map.GetSymbolBound(0);\n-        int64_t num_elems = range_interval.NumElements();\n+        int64_t num_elems = range_interval.GetLoopTripCount();\n         // In this case we get a single interval, because the ranges that every\n         // thread is reading overlap.\n         if (num_elems >= std::abs(multiplier.getValue())) {\n"
                },
                {
                    "old_start": 505,
                    "old_length": 7,
                    "new_start": 505,
                    "new_length": 7,
                    "hunk": "@@ -505,7 +505,7 @@ std::vector<Interval> FindContiguousIntervals(\n   }\n   // Case 2.2: g(s) = s.\n   Interval range_interval = indexing_map.GetSymbolBound(0);\n-  return ExtendIntervals(intervals, range_interval.NumElements() - 1);\n+  return ExtendIntervals(intervals, range_interval.GetLoopTripCount() - 1);\n }\n \n bool IsIndexingCoalesced(IndexingMap& thread_x_to_linearized_input,\n"
                }
            ],
            "whole_deleted": "-        int64_t num_elems = range_interval.NumElements();\n-  return ExtendIntervals(intervals, range_interval.NumElements() - 1);\n",
            "whole_added": "+        int64_t num_elems = range_interval.GetLoopTripCount();\n+  return ExtendIntervals(intervals, range_interval.GetLoopTripCount() - 1);\n",
            "whole_hunk": "@@ -472,7 +472,7 @@ std::vector<Interval> FindContiguousIntervals(\n       // Case 1.3: |multiplier| != 1 and g(s) = s.\n       if (partitioned_expr.func_of_s0 == range) {\n         Interval range_interval = indexing_map.GetSymbolBound(0);\n-        int64_t num_elems = range_interval.NumElements();\n+        int64_t num_elems = range_interval.GetLoopTripCount();\n         // In this case we get a single interval, because the ranges that every\n         // thread is reading overlap.\n         if (num_elems >= std::abs(multiplier.getValue())) {\n@@ -505,7 +505,7 @@ std::vector<Interval> FindContiguousIntervals(\n   }\n   // Case 2.2: g(s) = s.\n   Interval range_interval = indexing_map.GetSymbolBound(0);\n-  return ExtendIntervals(intervals, range_interval.NumElements() - 1);\n+  return ExtendIntervals(intervals, range_interval.GetLoopTripCount() - 1);\n }\n \n bool IsIndexingCoalesced(IndexingMap& thread_x_to_linearized_input,\n"
        },
        {
            "name": "indexing_map.cc",
            "path": "third_party/xla/xla/service/gpu/model/indexing_map.cc",
            "patches": [
                {
                    "old_start": 784,
                    "old_length": 8,
                    "new_start": 784,
                    "new_length": 17,
                    "hunk": "@@ -784,8 +784,17 @@ void Interval::Print(std::ostream& out) const {\n   out << '[' << lower << \", \" << upper << \"]\";\n }\n \n+int64_t Interval::GetLoopTripCount() const {\n+  if (!IsFeasible()) {\n+    return 0;\n+  }\n+  DCHECK((static_cast<absl::int128>(upper) - lower + 1) <=\n+         std::numeric_limits<int64_t>::max());\n+  return upper - lower + 1;\n+}\n+\n Interval::ComparisonResult Interval::operator>(const Interval& b) const {\n-  if (NumElements() == 0 || b.NumElements() == 0) {\n+  if (!IsFeasible() || !b.IsFeasible()) {\n     return {std::nullopt};\n   }\n   if (lower > b.upper) {\n"
                },
                {
                    "old_start": 799,
                    "old_length": 7,
                    "new_start": 808,
                    "new_length": 7,
                    "hunk": "@@ -799,7 +808,7 @@ Interval::ComparisonResult Interval::operator>(const Interval& b) const {\n \n Interval::ComparisonResult Interval::operator==(const Interval& b) const {\n   Interval intersection = Intersect(b);\n-  if (intersection.NumElements() == 0) return {false};\n+  if (!intersection.IsFeasible()) return {false};\n   if (intersection.IsPoint() && IsPoint() && b.IsPoint()) {\n     return {true};\n   }\n"
                }
            ],
            "whole_deleted": "-  if (NumElements() == 0 || b.NumElements() == 0) {\n-  if (intersection.NumElements() == 0) return {false};\n",
            "whole_added": "+int64_t Interval::GetLoopTripCount() const {\n+  if (!IsFeasible()) {\n+    return 0;\n+  }\n+  DCHECK((static_cast<absl::int128>(upper) - lower + 1) <=\n+         std::numeric_limits<int64_t>::max());\n+  return upper - lower + 1;\n+}\n+\n+  if (!IsFeasible() || !b.IsFeasible()) {\n+  if (!intersection.IsFeasible()) return {false};\n",
            "whole_hunk": "@@ -784,8 +784,17 @@ void Interval::Print(std::ostream& out) const {\n   out << '[' << lower << \", \" << upper << \"]\";\n }\n \n+int64_t Interval::GetLoopTripCount() const {\n+  if (!IsFeasible()) {\n+    return 0;\n+  }\n+  DCHECK((static_cast<absl::int128>(upper) - lower + 1) <=\n+         std::numeric_limits<int64_t>::max());\n+  return upper - lower + 1;\n+}\n+\n Interval::ComparisonResult Interval::operator>(const Interval& b) const {\n-  if (NumElements() == 0 || b.NumElements() == 0) {\n+  if (!IsFeasible() || !b.IsFeasible()) {\n     return {std::nullopt};\n   }\n   if (lower > b.upper) {\n@@ -799,7 +808,7 @@ Interval::ComparisonResult Interval::operator>(const Interval& b) const {\n \n Interval::ComparisonResult Interval::operator==(const Interval& b) const {\n   Interval intersection = Intersect(b);\n-  if (intersection.NumElements() == 0) return {false};\n+  if (!intersection.IsFeasible()) return {false};\n   if (intersection.IsPoint() && IsPoint() && b.IsPoint()) {\n     return {true};\n   }\n"
        },
        {
            "name": "indexing_map.h",
            "path": "third_party/xla/xla/service/gpu/model/indexing_map.h",
            "patches": [
                {
                    "old_start": 46,
                    "old_length": 9,
                    "new_start": 46,
                    "new_length": 14,
                    "hunk": "@@ -46,9 +46,14 @@ struct Interval {\n   void Print(std::ostream& out) const;\n \n   bool IsPoint() const { return lower == upper; }\n-  int64_t NumElements() const { return upper - lower + 1; }\n   bool IsFeasible() const { return lower <= upper; }\n \n+  // Returns the number of elements in the interval. Asserts that the number of\n+  // elements fits in an int64_t. For this reason, this should only be used for\n+  // intervals corresponding to symbols, not for general intervals. Use\n+  // `IsFeasible` to check if the interval is non-empty.\n+  int64_t GetLoopTripCount() const;\n+\n   bool Contains(int64_t value) const {\n     return value >= lower && value <= upper;\n   }"
                }
            ],
            "whole_deleted": "-  int64_t NumElements() const { return upper - lower + 1; }\n",
            "whole_added": "+  // Returns the number of elements in the interval. Asserts that the number of\n+  // elements fits in an int64_t. For this reason, this should only be used for\n+  // intervals corresponding to symbols, not for general intervals. Use\n+  // `IsFeasible` to check if the interval is non-empty.\n+  int64_t GetLoopTripCount() const;\n+\n",
            "whole_hunk": "@@ -46,9 +46,14 @@ struct Interval {\n   void Print(std::ostream& out) const;\n \n   bool IsPoint() const { return lower == upper; }\n-  int64_t NumElements() const { return upper - lower + 1; }\n   bool IsFeasible() const { return lower <= upper; }\n \n+  // Returns the number of elements in the interval. Asserts that the number of\n+  // elements fits in an int64_t. For this reason, this should only be used for\n+  // intervals corresponding to symbols, not for general intervals. Use\n+  // `IsFeasible` to check if the interval is non-empty.\n+  int64_t GetLoopTripCount() const;\n+\n   bool Contains(int64_t value) const {\n     return value >= lower && value <= upper;\n   }"
        }
    ]
},
{
    "Id": 254,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7e49357b9ebb225708fd75a29c261bb43f63e2a0",
    "date": "2023-11-02T11:38:51-07:00",
    "message": "Fix a bug in the `xla-call-module-deserialization` pass where function name uniquification didn't consider name conflicts correctly\n\nThe current `NewFuncName()` implementation identifies a new function name that is unique in the target module to which StableHLO functions are inserted, but it does not check whether the newly generated name clashes with any existing function in the original StableHLO module. For example, if there are two functions `@foo` and `@foo0` in the StableHLO module and the target TF module has `@foo`, `NewFuncName()` will rename the first function `@foo` into `@foo0` as this name does not exist in the TF module, but this causes name collision between the two StableHLO functions.\n\nThis CL rewrites the renaming logic to leverage `mlir::SymbolTable::renameToUnique()`. This allows us to rename func names to be unique to both modules without having to manually generate func names, which simplifies the code.\n\nPiperOrigin-RevId: 578922049",
    "label": "YES",
    "changes": [
        {
            "name": "xla_call_module_deserialization.mlir",
            "path": "tensorflow/compiler/mlir/tensorflow/tests/xla_call_module_deserialization.mlir",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 7,
                    "new_start": 20,
                    "new_length": 7,
                    "hunk": "@@ -20,7 +20,7 @@ module {\n     // CHECK:      %[[RESULT:.*]] = \"tf.XlaCallModule\"(%[[ARG0]], %[[ARG1]])\n     // CHECK-NOT:    function_list\n     // CHECK-SAME:   module = \"\"\n-    // CHECK-SAME:   _entry_function = @main0,\n+    // CHECK-SAME:   _entry_function = @main_0,\n \n     // `module` is stablehlo bytecode for:\n     //  func.func @main(%arg0: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) {\n"
                },
                {
                    "old_start": 38,
                    "old_length": 7,
                    "new_start": 38,
                    "new_length": 7,
                    "hunk": "@@ -38,7 +38,7 @@ module {\n     // CHECK:      %[[RESULT:.*]] = \"tf.XlaCallModule\"(%[[ARG0]], %[[ARG1]])\n     // CHECK-NOT:    function_list\n     // CHECK-SAME:   module = \"\"\n-    // CHECK-SAME:   _entry_function = @main1,\n+    // CHECK-SAME:   _entry_function = @main_1,\n \n     // `module` is stablehlo bytecode for:\n     //  func.func @main(%arg0: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) {\n"
                },
                {
                    "old_start": 50,
                    "old_length": 13,
                    "new_start": 50,
                    "new_length": 13,
                    "hunk": "@@ -50,13 +50,13 @@ module {\n     func.return %0 : tensor<10xi32>\n   }\n \n-  // CHECK-LABEL: func private @main0\n+  // CHECK-LABEL: func private @main_0\n   // CHECK-SAME:    (%[[ARG0:.*]]: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %[[ARG1:.*]]: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) attributes {_from_xla_call_module} {\n   // CHECK:         stablehlo.custom_call @tf.call_tf_function(%[[ARG0]], %[[ARG1]]) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {called_func = @_tf_func}} : (tensor<?xi32>, tensor<*xi32>) -> ()\n   // CHECK:         return %arg0 : tensor<?xi32>\n   // CHECK:       }\n \n-  // CHECK-LABEL: func private @main1\n+  // CHECK-LABEL: func private @main_1\n   // CHECK-SAME:    (%[[ARG0:.*]]: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %[[ARG1:.*]]: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) attributes {_from_xla_call_module} {\n   // CHECK:         stablehlo.custom_call @tf.call_tf_function(%[[ARG0]], %[[ARG1]]) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {called_func = @_tf_func}} : (tensor<?xi32>, tensor<*xi32>) -> ()\n   // CHECK:         return %arg0 : tensor<?xi32>\n"
                }
            ],
            "whole_deleted": "-    // CHECK-SAME:   _entry_function = @main0,\n-    // CHECK-SAME:   _entry_function = @main1,\n-  // CHECK-LABEL: func private @main0\n-  // CHECK-LABEL: func private @main1\n",
            "whole_added": "+    // CHECK-SAME:   _entry_function = @main_0,\n+    // CHECK-SAME:   _entry_function = @main_1,\n+  // CHECK-LABEL: func private @main_0\n+  // CHECK-LABEL: func private @main_1\n",
            "whole_hunk": "@@ -20,7 +20,7 @@ module {\n     // CHECK:      %[[RESULT:.*]] = \"tf.XlaCallModule\"(%[[ARG0]], %[[ARG1]])\n     // CHECK-NOT:    function_list\n     // CHECK-SAME:   module = \"\"\n-    // CHECK-SAME:   _entry_function = @main0,\n+    // CHECK-SAME:   _entry_function = @main_0,\n \n     // `module` is stablehlo bytecode for:\n     //  func.func @main(%arg0: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) {\n@@ -38,7 +38,7 @@ module {\n     // CHECK:      %[[RESULT:.*]] = \"tf.XlaCallModule\"(%[[ARG0]], %[[ARG1]])\n     // CHECK-NOT:    function_list\n     // CHECK-SAME:   module = \"\"\n-    // CHECK-SAME:   _entry_function = @main1,\n+    // CHECK-SAME:   _entry_function = @main_1,\n \n     // `module` is stablehlo bytecode for:\n     //  func.func @main(%arg0: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) {\n@@ -50,13 +50,13 @@ module {\n     func.return %0 : tensor<10xi32>\n   }\n \n-  // CHECK-LABEL: func private @main0\n+  // CHECK-LABEL: func private @main_0\n   // CHECK-SAME:    (%[[ARG0:.*]]: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %[[ARG1:.*]]: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) attributes {_from_xla_call_module} {\n   // CHECK:         stablehlo.custom_call @tf.call_tf_function(%[[ARG0]], %[[ARG1]]) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {called_func = @_tf_func}} : (tensor<?xi32>, tensor<*xi32>) -> ()\n   // CHECK:         return %arg0 : tensor<?xi32>\n   // CHECK:       }\n \n-  // CHECK-LABEL: func private @main1\n+  // CHECK-LABEL: func private @main_1\n   // CHECK-SAME:    (%[[ARG0:.*]]: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %[[ARG1:.*]]: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) attributes {_from_xla_call_module} {\n   // CHECK:         stablehlo.custom_call @tf.call_tf_function(%[[ARG0]], %[[ARG1]]) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {called_func = @_tf_func}} : (tensor<?xi32>, tensor<*xi32>) -> ()\n   // CHECK:         return %arg0 : tensor<?xi32>\n"
        },
        {
            "name": "xla_call_module_round_trip.mlir",
            "path": "tensorflow/compiler/mlir/tensorflow/tests/xla_call_module_round_trip.mlir",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 10,
                    "new_start": 17,
                    "new_length": 10,
                    "hunk": "@@ -17,10 +17,10 @@ module {\n     // CHECK-SAME:   module = \"\"\n     // CHECK-SAME:   platforms = []\n     // CHECK-SAME:   version = 5\n-    // CHECK-SAME:   _entry_function = @main0\n+    // CHECK-SAME:   _entry_function = @main_0\n     // CHECK-SAME:   _stablehlo_module_attrs = {}\n \n-    %0 = \"tf.XlaCallModule\"(%arg0, %arg1) {Sout = [#tf_type.shape<?>], dim_args_spec = [], _entry_function = @main0, module = \"\", platforms = [], version = 5 : i64} : (tensor<10xi32>, tensor<10xi32>) -> tensor<10xi32>\n+    %0 = \"tf.XlaCallModule\"(%arg0, %arg1) {Sout = [#tf_type.shape<?>], dim_args_spec = [], _entry_function = @main_0, module = \"\", platforms = [], version = 5 : i64} : (tensor<10xi32>, tensor<10xi32>) -> tensor<10xi32>\n     // CHECK: return %[[RESULT]]\n     func.return %0 : tensor<10xi32>\n   }\n"
                },
                {
                    "old_start": 34,
                    "old_length": 12,
                    "new_start": 34,
                    "new_length": 12,
                    "hunk": "@@ -34,12 +34,12 @@ module {\n     func.return\n   }\n \n-  // CHECK-LABEL: func private @main0\n+  // CHECK-LABEL: func private @main_0\n   // CHECK-SAME:    %[[ARG0:.*]]: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}\n   // CHECK-SAME:    %[[ARG1:.*]]: tensor<*xi32>)\n   // CHECK-SAME:    (tensor<?xi32> {jax.result_info = \"\"})\n   // CHECK-SAME:    attributes {_from_xla_call_module}\n-  func.func private @main0(%arg0: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) attributes {_from_xla_call_module} {\n+  func.func private @main_0(%arg0: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) attributes {_from_xla_call_module} {\n     // CHECK:      stablehlo.custom_call @tf.call_tf_function(%[[ARG0]], %[[ARG1]])\n     // CHECK-SAME: {\n     // CHECK-SAME:  api_version = 2 : i32,\n"
                }
            ],
            "whole_deleted": "-    // CHECK-SAME:   _entry_function = @main0\n-    %0 = \"tf.XlaCallModule\"(%arg0, %arg1) {Sout = [#tf_type.shape<?>], dim_args_spec = [], _entry_function = @main0, module = \"\", platforms = [], version = 5 : i64} : (tensor<10xi32>, tensor<10xi32>) -> tensor<10xi32>\n-  // CHECK-LABEL: func private @main0\n-  func.func private @main0(%arg0: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) attributes {_from_xla_call_module} {\n",
            "whole_added": "+    // CHECK-SAME:   _entry_function = @main_0\n+    %0 = \"tf.XlaCallModule\"(%arg0, %arg1) {Sout = [#tf_type.shape<?>], dim_args_spec = [], _entry_function = @main_0, module = \"\", platforms = [], version = 5 : i64} : (tensor<10xi32>, tensor<10xi32>) -> tensor<10xi32>\n+  // CHECK-LABEL: func private @main_0\n+  func.func private @main_0(%arg0: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) attributes {_from_xla_call_module} {\n",
            "whole_hunk": "@@ -17,10 +17,10 @@ module {\n     // CHECK-SAME:   module = \"\"\n     // CHECK-SAME:   platforms = []\n     // CHECK-SAME:   version = 5\n-    // CHECK-SAME:   _entry_function = @main0\n+    // CHECK-SAME:   _entry_function = @main_0\n     // CHECK-SAME:   _stablehlo_module_attrs = {}\n \n-    %0 = \"tf.XlaCallModule\"(%arg0, %arg1) {Sout = [#tf_type.shape<?>], dim_args_spec = [], _entry_function = @main0, module = \"\", platforms = [], version = 5 : i64} : (tensor<10xi32>, tensor<10xi32>) -> tensor<10xi32>\n+    %0 = \"tf.XlaCallModule\"(%arg0, %arg1) {Sout = [#tf_type.shape<?>], dim_args_spec = [], _entry_function = @main_0, module = \"\", platforms = [], version = 5 : i64} : (tensor<10xi32>, tensor<10xi32>) -> tensor<10xi32>\n     // CHECK: return %[[RESULT]]\n     func.return %0 : tensor<10xi32>\n   }\n@@ -34,12 +34,12 @@ module {\n     func.return\n   }\n \n-  // CHECK-LABEL: func private @main0\n+  // CHECK-LABEL: func private @main_0\n   // CHECK-SAME:    %[[ARG0:.*]]: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}\n   // CHECK-SAME:    %[[ARG1:.*]]: tensor<*xi32>)\n   // CHECK-SAME:    (tensor<?xi32> {jax.result_info = \"\"})\n   // CHECK-SAME:    attributes {_from_xla_call_module}\n-  func.func private @main0(%arg0: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) attributes {_from_xla_call_module} {\n+  func.func private @main_0(%arg0: tensor<?xi32> {jax.arg_info = \"x\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<*xi32>) -> (tensor<?xi32> {jax.result_info = \"\"}) attributes {_from_xla_call_module} {\n     // CHECK:      stablehlo.custom_call @tf.call_tf_function(%[[ARG0]], %[[ARG1]])\n     // CHECK-SAME: {\n     // CHECK-SAME:  api_version = 2 : i32,\n"
        },
        {
            "name": "xla_call_module_deserialization.cc",
            "path": "tensorflow/compiler/mlir/tensorflow/transforms/xla_call_module_deserialization.cc",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 16,
                    "new_start": 18,
                    "new_length": 21,
                    "hunk": "@@ -18,16 +18,21 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/StringRef.h\"\n-#include \"llvm/Support/FormatVariadic.h\"\n #include \"mlir/Dialect/Func/Extensions/AllExtensions.h\"  // from @llvm-project\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n #include \"mlir/Dialect/Quant/QuantOps.h\"  // from @llvm-project  // IWYU pragma: keep\n+#include \"mlir/IR/Attributes.h\"  // from @llvm-project\n+#include \"mlir/IR/Builders.h\"  // from @llvm-project\n #include \"mlir/IR/BuiltinAttributes.h\"  // from @llvm-project\n #include \"mlir/IR/BuiltinOps.h\"  // from @llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n #include \"mlir/IR/OwningOpRef.h\"  // from @llvm-project\n #include \"mlir/IR/SymbolTable.h\"  // from @llvm-project\n+#include \"mlir/IR/Visitors.h\"  // from @llvm-project\n #include \"mlir/Pass/Pass.h\"  // from @llvm-project\n #include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n #include \"stablehlo/dialect/ChloOps.h\"  // from @stablehlo  // IWYU pragma: keep\n"
                },
                {
                    "old_start": 81,
                    "old_length": 19,
                    "new_start": 86,
                    "new_length": 6,
                    "hunk": "@@ -81,19 +86,6 @@ tsl::StatusOr<OwningOpRef<ModuleOp>> DeserializeStablehlo(MLIRContext *context,\n   return std::move(*loader).module();\n }\n \n-// If `func_name` exists in `symbol_table`, returns a new name that doesn't\n-// exist. Otherwise, returns `func_name` as is.\n-StringAttr NewFuncName(const SymbolTable &symbol_table, StringAttr func_name) {\n-  int index = 0;\n-  StringAttr new_func_name = func_name;\n-  while (symbol_table.lookup(new_func_name)) {\n-    new_func_name =\n-        StringAttr::get(func_name.getContext(),\n-                        llvm::formatv(\"{0}{1}\", func_name.getValue(), index++));\n-  }\n-  return new_func_name;\n-}\n-\n // Renames functions in the stablehlo module to avoid naming conflicts with\n // existing functions in the tf module.\n // Sets _from_xla_call_module attribute for each stablehlo function.\n"
                },
                {
                    "old_start": 108,
                    "old_length": 20,
                    "new_start": 100,
                    "new_length": 21,
                    "hunk": "@@ -108,20 +100,21 @@ FailureOr<StringAttr> RenameStablehloFunctions(\n     MLIRContext *context, SymbolTableCollection &symbol_tables,\n     ModuleOp tf_module, ModuleOp stablehlo_module) {\n   SymbolTable &tf_symbol_table = symbol_tables.getSymbolTable(tf_module);\n+  SymbolTable &stablehlo_symbol_table =\n+      symbol_tables.getSymbolTable(stablehlo_module);\n   Builder builder(context);\n   StringAttr main_func_name;\n   for (auto func : stablehlo_module.getOps<func::FuncOp>()) {\n-    StringAttr func_name = NewFuncName(tf_symbol_table, func.getSymNameAttr());\n-    if (func.getSymName() == kStablehloMainFunctionName) {\n-      main_func_name = func_name;\n-    }\n-    if (func_name != func.getSymNameAttr()) {\n-      if (failed(SymbolTable::replaceAllSymbolUses(func, func_name,\n-                                                   stablehlo_module))) {\n+    const bool is_main_func = func.getSymName() == kStablehloMainFunctionName;\n+    if (tf_symbol_table.lookup(func.getSymName())) {\n+      if (failed(stablehlo_symbol_table.renameToUnique(\n+              func, {&tf_symbol_table, &stablehlo_symbol_table}))) {\n         return func.emitError()\n                << \"failed to rename StableHLO function \" << func.getSymName();\n       }\n-      func.setName(func_name);\n+    }\n+    if (is_main_func) {\n+      main_func_name = func.getSymNameAttr();\n     }\n     func->setAttr(kFromXlaCallModuleAttrName, builder.getUnitAttr());\n   }\n"
                },
                {
                    "old_start": 225,
                    "old_length": 7,
                    "new_start": 218,
                    "new_length": 7,
                    "hunk": "@@ -225,7 +218,7 @@ LogicalResult DeserializeXlaCallModule(MLIRContext *context,\n \n   // Translate `called_index` in TF function custom calls into symbol\n   // references. `function_list` attribute is needed after that.\n-  SmallVector<SymbolRefAttr> function_list(\n+  llvm::SmallVector<SymbolRefAttr> function_list(\n       op.getFunctionList().getAsRange<SymbolRefAttr>());\n   if (failed(\n           SymbolizeCustomCallCalledIndex(*stablehlo_module, function_list))) {"
                }
            ],
            "whole_deleted": "-#include \"llvm/Support/FormatVariadic.h\"\n-// If `func_name` exists in `symbol_table`, returns a new name that doesn't\n-// exist. Otherwise, returns `func_name` as is.\n-StringAttr NewFuncName(const SymbolTable &symbol_table, StringAttr func_name) {\n-  int index = 0;\n-  StringAttr new_func_name = func_name;\n-  while (symbol_table.lookup(new_func_name)) {\n-    new_func_name =\n-        StringAttr::get(func_name.getContext(),\n-                        llvm::formatv(\"{0}{1}\", func_name.getValue(), index++));\n-  }\n-  return new_func_name;\n-}\n-\n-    StringAttr func_name = NewFuncName(tf_symbol_table, func.getSymNameAttr());\n-    if (func.getSymName() == kStablehloMainFunctionName) {\n-      main_func_name = func_name;\n-    }\n-    if (func_name != func.getSymNameAttr()) {\n-      if (failed(SymbolTable::replaceAllSymbolUses(func, func_name,\n-                                                   stablehlo_module))) {\n-      func.setName(func_name);\n-  SmallVector<SymbolRefAttr> function_list(\n",
            "whole_added": "+#include \"llvm/ADT/ArrayRef.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n+#include \"mlir/IR/Attributes.h\"  // from @llvm-project\n+#include \"mlir/IR/Builders.h\"  // from @llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n+#include \"mlir/IR/Visitors.h\"  // from @llvm-project\n+  SymbolTable &stablehlo_symbol_table =\n+      symbol_tables.getSymbolTable(stablehlo_module);\n+    const bool is_main_func = func.getSymName() == kStablehloMainFunctionName;\n+    if (tf_symbol_table.lookup(func.getSymName())) {\n+      if (failed(stablehlo_symbol_table.renameToUnique(\n+              func, {&tf_symbol_table, &stablehlo_symbol_table}))) {\n+    }\n+    if (is_main_func) {\n+      main_func_name = func.getSymNameAttr();\n+  llvm::SmallVector<SymbolRefAttr> function_list(\n",
            "whole_hunk": "@@ -18,16 +18,21 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/StringRef.h\"\n-#include \"llvm/Support/FormatVariadic.h\"\n #include \"mlir/Dialect/Func/Extensions/AllExtensions.h\"  // from @llvm-project\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n #include \"mlir/Dialect/Quant/QuantOps.h\"  // from @llvm-project  // IWYU pragma: keep\n+#include \"mlir/IR/Attributes.h\"  // from @llvm-project\n+#include \"mlir/IR/Builders.h\"  // from @llvm-project\n #include \"mlir/IR/BuiltinAttributes.h\"  // from @llvm-project\n #include \"mlir/IR/BuiltinOps.h\"  // from @llvm-project\n+#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n #include \"mlir/IR/OwningOpRef.h\"  // from @llvm-project\n #include \"mlir/IR/SymbolTable.h\"  // from @llvm-project\n+#include \"mlir/IR/Visitors.h\"  // from @llvm-project\n #include \"mlir/Pass/Pass.h\"  // from @llvm-project\n #include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n #include \"stablehlo/dialect/ChloOps.h\"  // from @stablehlo  // IWYU pragma: keep\n@@ -81,19 +86,6 @@ tsl::StatusOr<OwningOpRef<ModuleOp>> DeserializeStablehlo(MLIRContext *context,\n   return std::move(*loader).module();\n }\n \n-// If `func_name` exists in `symbol_table`, returns a new name that doesn't\n-// exist. Otherwise, returns `func_name` as is.\n-StringAttr NewFuncName(const SymbolTable &symbol_table, StringAttr func_name) {\n-  int index = 0;\n-  StringAttr new_func_name = func_name;\n-  while (symbol_table.lookup(new_func_name)) {\n-    new_func_name =\n-        StringAttr::get(func_name.getContext(),\n-                        llvm::formatv(\"{0}{1}\", func_name.getValue(), index++));\n-  }\n-  return new_func_name;\n-}\n-\n // Renames functions in the stablehlo module to avoid naming conflicts with\n // existing functions in the tf module.\n // Sets _from_xla_call_module attribute for each stablehlo function.\n@@ -108,20 +100,21 @@ FailureOr<StringAttr> RenameStablehloFunctions(\n     MLIRContext *context, SymbolTableCollection &symbol_tables,\n     ModuleOp tf_module, ModuleOp stablehlo_module) {\n   SymbolTable &tf_symbol_table = symbol_tables.getSymbolTable(tf_module);\n+  SymbolTable &stablehlo_symbol_table =\n+      symbol_tables.getSymbolTable(stablehlo_module);\n   Builder builder(context);\n   StringAttr main_func_name;\n   for (auto func : stablehlo_module.getOps<func::FuncOp>()) {\n-    StringAttr func_name = NewFuncName(tf_symbol_table, func.getSymNameAttr());\n-    if (func.getSymName() == kStablehloMainFunctionName) {\n-      main_func_name = func_name;\n-    }\n-    if (func_name != func.getSymNameAttr()) {\n-      if (failed(SymbolTable::replaceAllSymbolUses(func, func_name,\n-                                                   stablehlo_module))) {\n+    const bool is_main_func = func.getSymName() == kStablehloMainFunctionName;\n+    if (tf_symbol_table.lookup(func.getSymName())) {\n+      if (failed(stablehlo_symbol_table.renameToUnique(\n+              func, {&tf_symbol_table, &stablehlo_symbol_table}))) {\n         return func.emitError()\n                << \"failed to rename StableHLO function \" << func.getSymName();\n       }\n-      func.setName(func_name);\n+    }\n+    if (is_main_func) {\n+      main_func_name = func.getSymNameAttr();\n     }\n     func->setAttr(kFromXlaCallModuleAttrName, builder.getUnitAttr());\n   }\n@@ -225,7 +218,7 @@ LogicalResult DeserializeXlaCallModule(MLIRContext *context,\n \n   // Translate `called_index` in TF function custom calls into symbol\n   // references. `function_list` attribute is needed after that.\n-  SmallVector<SymbolRefAttr> function_list(\n+  llvm::SmallVector<SymbolRefAttr> function_list(\n       op.getFunctionList().getAsRange<SymbolRefAttr>());\n   if (failed(\n           SymbolizeCustomCallCalledIndex(*stablehlo_module, function_list))) {"
        }
    ]
},
{
    "Id": 247,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f25c22bc494573699c05ee7c38b6154da8c8fc62",
    "date": "2023-11-13T13:26:44-08:00",
    "message": "Support per-channel quantization for data movement ops\n\nThis is needed for per-channel quantized weight. Skip op/result type check in the pattern because some op may change for per-channel quantized type. E.g. Quantization axis may change after broadcast_in_dims.\n\nPiperOrigin-RevId: 582064121",
    "label": "NO",
    "changes": [
        {
            "name": "convert_mhlo_quant_to_int.cc",
            "path": "tensorflow/compiler/mlir/quantization/stablehlo/passes/bridge/convert_mhlo_quant_to_int.cc",
            "patches": [
                {
                    "old_start": 1160,
                    "old_length": 6,
                    "new_start": 1160,
                    "new_length": 7,
                    "hunk": "@@ -1160,6 +1160,7 @@ class ConvertUniformQuantizedConvolutionOp\n \n // This pattern lowers a generic MHLO op for uq->int.\n // This pattern essentially just performs type change, with no algorithm change.\n+// TODO: b/310685906 - Add operand/result type validations.\n class ConvertGenericOp : public ConversionPattern {\n  public:\n   explicit ConvertGenericOp(MLIRContext *ctx)\n"
                },
                {
                    "old_start": 1174,
                    "old_length": 28,
                    "new_start": 1175,
                    "new_length": 6,
                    "hunk": "@@ -1174,28 +1175,6 @@ class ConvertGenericOp : public ConversionPattern {\n       return failure();\n     }\n \n-    // Check that all operands and result uq types are the same.\n-    llvm::SmallVector<Type> uq_types;\n-    for (auto result_type : op->getResultTypes()) {\n-      auto type =\n-          getElementTypeOrSelf(result_type).dyn_cast<UniformQuantizedType>();\n-      if (type) {\n-        uq_types.push_back(type);\n-      }\n-    }\n-    for (auto operand : op->getOperands()) {\n-      auto type = getElementTypeOrSelf(operand.getType())\n-                      .dyn_cast<UniformQuantizedType>();\n-      if (type) {\n-        uq_types.push_back(type);\n-      }\n-    }\n-    for (auto type : uq_types) {\n-      if (type != uq_types.front()) {\n-        return failure();\n-      }\n-    }\n-\n     // Determine new result type: use storage type for uq types; use original\n     // type otherwise.\n     llvm::SmallVector<Type, 4> new_result_types;\n"
                }
            ],
            "whole_deleted": "-    // Check that all operands and result uq types are the same.\n-    llvm::SmallVector<Type> uq_types;\n-    for (auto result_type : op->getResultTypes()) {\n-      auto type =\n-          getElementTypeOrSelf(result_type).dyn_cast<UniformQuantizedType>();\n-      if (type) {\n-        uq_types.push_back(type);\n-      }\n-    }\n-    for (auto operand : op->getOperands()) {\n-      auto type = getElementTypeOrSelf(operand.getType())\n-                      .dyn_cast<UniformQuantizedType>();\n-      if (type) {\n-        uq_types.push_back(type);\n-      }\n-    }\n-    for (auto type : uq_types) {\n-      if (type != uq_types.front()) {\n-        return failure();\n-      }\n-    }\n-\n",
            "whole_added": "+// TODO: b/310685906 - Add operand/result type validations.\n",
            "whole_hunk": "@@ -1160,6 +1160,7 @@ class ConvertUniformQuantizedConvolutionOp\n \n // This pattern lowers a generic MHLO op for uq->int.\n // This pattern essentially just performs type change, with no algorithm change.\n+// TODO: b/310685906 - Add operand/result type validations.\n class ConvertGenericOp : public ConversionPattern {\n  public:\n   explicit ConvertGenericOp(MLIRContext *ctx)\n@@ -1174,28 +1175,6 @@ class ConvertGenericOp : public ConversionPattern {\n       return failure();\n     }\n \n-    // Check that all operands and result uq types are the same.\n-    llvm::SmallVector<Type> uq_types;\n-    for (auto result_type : op->getResultTypes()) {\n-      auto type =\n-          getElementTypeOrSelf(result_type).dyn_cast<UniformQuantizedType>();\n-      if (type) {\n-        uq_types.push_back(type);\n-      }\n-    }\n-    for (auto operand : op->getOperands()) {\n-      auto type = getElementTypeOrSelf(operand.getType())\n-                      .dyn_cast<UniformQuantizedType>();\n-      if (type) {\n-        uq_types.push_back(type);\n-      }\n-    }\n-    for (auto type : uq_types) {\n-      if (type != uq_types.front()) {\n-        return failure();\n-      }\n-    }\n-\n     // Determine new result type: use storage type for uq types; use original\n     // type otherwise.\n     llvm::SmallVector<Type, 4> new_result_types;\n"
        },
        {
            "name": "convert-mhlo-quant-to-int.mlir",
            "path": "tensorflow/compiler/mlir/quantization/stablehlo/tests/bridge/convert-mhlo-quant-to-int.mlir",
            "patches": [
                {
                    "old_start": 1660,
                    "old_length": 6,
                    "new_start": 1660,
                    "new_length": 21,
                    "hunk": "@@ -1660,6 +1660,21 @@ func.func @broadcast(\n \n // -----\n \n+// CHECK-LABEL: func @broadcast_per_channel\n+func.func @broadcast_per_channel(\n+    %arg0: tensor<2x!quant.uniform<i32:f32:0, {4.000000e+00:0, 2.000000e+00:0}>>\n+  ) -> tensor<128x26x26x2x!quant.uniform<i32:f32:3, {4.000000e+00:0, 2.000000e+00:0}>>  {\n+  // CHECK: \"mhlo.broadcast_in_dim\"\n+  // CHECK-SAME: broadcast_dimensions = dense<3> : tensor<1xi64>\n+  // CHECK-SAME: (tensor<2xi32>) -> tensor<128x26x26x2xi32>\n+  %0 = \"mhlo.broadcast_in_dim\"(%arg0) {broadcast_dimensions = dense<3> : tensor<1xi64>}: (\n+      tensor<2x!quant.uniform<i32:f32:0, {4.000000e+00:0, 2.000000e+00:0}>>\n+    ) -> tensor<128x26x26x2x!quant.uniform<i32:f32:3, {4.000000e+00:0, 2.000000e+00:0}>>\n+  return %0 : tensor<128x26x26x2x!quant.uniform<i32:f32:3, {4.000000e+00:0, 2.000000e+00:0}>>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @max\n func.func @max(\n     %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n"
                },
                {
                    "old_start": 1675,
                    "old_length": 6,
                    "new_start": 1690,
                    "new_length": 21,
                    "hunk": "@@ -1675,6 +1690,21 @@ func.func @max(\n \n // -----\n \n+// CHECK-LABEL: func @max_per_channel\n+func.func @max_per_channel(\n+    %arg0: tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>> {\n+  // CHECK: mhlo.maximum\n+  // CHECK-SAME: tensor<1x2xi8>\n+  %0 = \"mhlo.maximum\"(%arg0, %arg0) : (\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>,\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  return %0 : tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @min\n func.func @min(\n     %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n"
                },
                {
                    "old_start": 1690,
                    "old_length": 6,
                    "new_start": 1720,
                    "new_length": 21,
                    "hunk": "@@ -1690,6 +1720,21 @@ func.func @min(\n \n // -----\n \n+// CHECK-LABEL: func @min_per_channel\n+func.func @min_per_channel(\n+    %arg0: tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>> {\n+  // CHECK: mhlo.minimum\n+  // CHECK-SAME: tensor<1x2xi8>\n+  %0 = \"mhlo.minimum\"(%arg0, %arg0) : (\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>,\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  return %0 : tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @function(%arg0: tensor<1x2xi8>) -> tensor<1x2xi8>\n func.func @function(\n     %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n"
                },
                {
                    "old_start": 1697,
                    "old_length": 30,
                    "new_start": 1742,
                    "new_length": 3,
                    "hunk": "@@ -1697,30 +1742,3 @@ func.func @function(\n   // CHECK: return %arg0 : tensor<1x2xi8>\n   return %arg0 : tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n }\n-\n-// -----\n-\n-func.func @min_mix_uq_type1(\n-    %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n-    %arg1: tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>> {\n-  // expected-error@+1 {{failed to legalize operation 'mhlo.minimum' that was explicitly marked illegal}}\n-  %0 = \"mhlo.minimum\"(%arg0, %arg1) : (\n-    tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n-    tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-  return %0 : tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-}\n-\n-// -----\n-\n-func.func @min_mix_uq_type2(\n-    %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>> {\n-  // expected-error@+1 {{failed to legalize operation 'mhlo.minimum' that was explicitly marked illegal}}\n-  %0 = \"mhlo.minimum\"(%arg0, %arg0) : (\n-    tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n-    tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-  return %0 : tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-}"
                }
            ],
            "whole_deleted": "-\n-// -----\n-\n-func.func @min_mix_uq_type1(\n-    %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n-    %arg1: tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>> {\n-  // expected-error@+1 {{failed to legalize operation 'mhlo.minimum' that was explicitly marked illegal}}\n-  %0 = \"mhlo.minimum\"(%arg0, %arg1) : (\n-    tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n-    tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-  return %0 : tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-}\n-\n-// -----\n-\n-func.func @min_mix_uq_type2(\n-    %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>> {\n-  // expected-error@+1 {{failed to legalize operation 'mhlo.minimum' that was explicitly marked illegal}}\n-  %0 = \"mhlo.minimum\"(%arg0, %arg0) : (\n-    tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n-    tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-  return %0 : tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-}\n",
            "whole_added": "+// CHECK-LABEL: func @broadcast_per_channel\n+func.func @broadcast_per_channel(\n+    %arg0: tensor<2x!quant.uniform<i32:f32:0, {4.000000e+00:0, 2.000000e+00:0}>>\n+  ) -> tensor<128x26x26x2x!quant.uniform<i32:f32:3, {4.000000e+00:0, 2.000000e+00:0}>>  {\n+  // CHECK: \"mhlo.broadcast_in_dim\"\n+  // CHECK-SAME: broadcast_dimensions = dense<3> : tensor<1xi64>\n+  // CHECK-SAME: (tensor<2xi32>) -> tensor<128x26x26x2xi32>\n+  %0 = \"mhlo.broadcast_in_dim\"(%arg0) {broadcast_dimensions = dense<3> : tensor<1xi64>}: (\n+      tensor<2x!quant.uniform<i32:f32:0, {4.000000e+00:0, 2.000000e+00:0}>>\n+    ) -> tensor<128x26x26x2x!quant.uniform<i32:f32:3, {4.000000e+00:0, 2.000000e+00:0}>>\n+  return %0 : tensor<128x26x26x2x!quant.uniform<i32:f32:3, {4.000000e+00:0, 2.000000e+00:0}>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: func @max_per_channel\n+func.func @max_per_channel(\n+    %arg0: tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>> {\n+  // CHECK: mhlo.maximum\n+  // CHECK-SAME: tensor<1x2xi8>\n+  %0 = \"mhlo.maximum\"(%arg0, %arg0) : (\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>,\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  return %0 : tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: func @min_per_channel\n+func.func @min_per_channel(\n+    %arg0: tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>> {\n+  // CHECK: mhlo.minimum\n+  // CHECK-SAME: tensor<1x2xi8>\n+  %0 = \"mhlo.minimum\"(%arg0, %arg0) : (\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>,\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  return %0 : tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+}\n+\n+// -----\n+\n",
            "whole_hunk": "@@ -1660,6 +1660,21 @@ func.func @broadcast(\n \n // -----\n \n+// CHECK-LABEL: func @broadcast_per_channel\n+func.func @broadcast_per_channel(\n+    %arg0: tensor<2x!quant.uniform<i32:f32:0, {4.000000e+00:0, 2.000000e+00:0}>>\n+  ) -> tensor<128x26x26x2x!quant.uniform<i32:f32:3, {4.000000e+00:0, 2.000000e+00:0}>>  {\n+  // CHECK: \"mhlo.broadcast_in_dim\"\n+  // CHECK-SAME: broadcast_dimensions = dense<3> : tensor<1xi64>\n+  // CHECK-SAME: (tensor<2xi32>) -> tensor<128x26x26x2xi32>\n+  %0 = \"mhlo.broadcast_in_dim\"(%arg0) {broadcast_dimensions = dense<3> : tensor<1xi64>}: (\n+      tensor<2x!quant.uniform<i32:f32:0, {4.000000e+00:0, 2.000000e+00:0}>>\n+    ) -> tensor<128x26x26x2x!quant.uniform<i32:f32:3, {4.000000e+00:0, 2.000000e+00:0}>>\n+  return %0 : tensor<128x26x26x2x!quant.uniform<i32:f32:3, {4.000000e+00:0, 2.000000e+00:0}>>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @max\n func.func @max(\n     %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n@@ -1675,6 +1690,21 @@ func.func @max(\n \n // -----\n \n+// CHECK-LABEL: func @max_per_channel\n+func.func @max_per_channel(\n+    %arg0: tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>> {\n+  // CHECK: mhlo.maximum\n+  // CHECK-SAME: tensor<1x2xi8>\n+  %0 = \"mhlo.maximum\"(%arg0, %arg0) : (\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>,\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  return %0 : tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @min\n func.func @min(\n     %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n@@ -1690,6 +1720,21 @@ func.func @min(\n \n // -----\n \n+// CHECK-LABEL: func @min_per_channel\n+func.func @min_per_channel(\n+    %arg0: tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>> {\n+  // CHECK: mhlo.minimum\n+  // CHECK-SAME: tensor<1x2xi8>\n+  %0 = \"mhlo.minimum\"(%arg0, %arg0) : (\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>,\n+    tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  ) -> tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+  return %0 : tensor<1x2x!quant.uniform<i8:f32:1, {2.000000e+00:3, 1.000000e+00:-2}>>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @function(%arg0: tensor<1x2xi8>) -> tensor<1x2xi8>\n func.func @function(\n     %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n@@ -1697,30 +1742,3 @@ func.func @function(\n   // CHECK: return %arg0 : tensor<1x2xi8>\n   return %arg0 : tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n }\n-\n-// -----\n-\n-func.func @min_mix_uq_type1(\n-    %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n-    %arg1: tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>> {\n-  // expected-error@+1 {{failed to legalize operation 'mhlo.minimum' that was explicitly marked illegal}}\n-  %0 = \"mhlo.minimum\"(%arg0, %arg1) : (\n-    tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n-    tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-  return %0 : tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-}\n-\n-// -----\n-\n-func.func @min_mix_uq_type2(\n-    %arg0: tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>> {\n-  // expected-error@+1 {{failed to legalize operation 'mhlo.minimum' that was explicitly marked illegal}}\n-  %0 = \"mhlo.minimum\"(%arg0, %arg0) : (\n-    tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>,\n-    tensor<1x2x!quant.uniform<i8:f32, 2.000000e+00:3>>\n-  ) -> tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-  return %0 : tensor<1x2x!quant.uniform<i8:f32, 1.000000e+00:2>>\n-}"
        }
    ]
},
{
    "Id": 336,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/53fb0130851ad40d544105432f414b4ebe9e729d",
    "date": "2023-08-03T00:19:20-07:00",
    "message": "Sliced prefetch tuning: When we check if we have enough copy resources for a sliced prefetch, inflate the required amount of resources for slices.\n\nPiperOrigin-RevId: 553382368",
    "label": "NO",
    "changes": [
        {
            "name": "memory_space_assignment.cc",
            "path": "tensorflow/compiler/xla/service/memory_space_assignment.cc",
            "patches": [
                {
                    "old_start": 6293,
                    "old_length": 9,
                    "new_start": 6293,
                    "new_length": 21,
                    "hunk": "@@ -6293,9 +6293,21 @@ bool DoWeHaveEnoughCopyResource(\n   // The specs must be in slice start time order because that's the order\n   // they'll be added to prefetch_async_copy_resource_ in\n   // AddAsyncSlicesForPrefetch(), if the solution is selected.\n+  static const float kSlicedCopyResourceInflation = 1.8;\n   for (int i = 0; i < slice_start_times.size(); ++i) {\n+    float original_copy_resource = copy_resource_per_slice[i];\n+    float new_copy_resource = original_copy_resource;\n+    if (slice_start_times.size() > 1) {\n+      // This is a hack that makes us more conservative about using sliced\n+      // prefetching vs unsliced prefetching.\n+      new_copy_resource = original_copy_resource * kSlicedCopyResourceInflation;\n+      VLOG(5)\n+          << \"Inflating required copy resources DoWeHaveEnoughCopyResource() \"\n+             \"slice check from \"\n+          << original_copy_resource << \" to \" << new_copy_resource;\n+    }\n     specs.push_back(\n-        {slice_start_times[i], prefetch_end_time, copy_resource_per_slice[i]});\n+        {slice_start_times[i], prefetch_end_time, new_copy_resource});\n   }\n \n   auto specs_to_string = [&specs]() {"
                }
            ],
            "whole_deleted": "-        {slice_start_times[i], prefetch_end_time, copy_resource_per_slice[i]});\n",
            "whole_added": "+  static const float kSlicedCopyResourceInflation = 1.8;\n+    float original_copy_resource = copy_resource_per_slice[i];\n+    float new_copy_resource = original_copy_resource;\n+    if (slice_start_times.size() > 1) {\n+      // This is a hack that makes us more conservative about using sliced\n+      // prefetching vs unsliced prefetching.\n+      new_copy_resource = original_copy_resource * kSlicedCopyResourceInflation;\n+      VLOG(5)\n+          << \"Inflating required copy resources DoWeHaveEnoughCopyResource() \"\n+             \"slice check from \"\n+          << original_copy_resource << \" to \" << new_copy_resource;\n+    }\n+        {slice_start_times[i], prefetch_end_time, new_copy_resource});\n",
            "whole_hunk": "@@ -6293,9 +6293,21 @@ bool DoWeHaveEnoughCopyResource(\n   // The specs must be in slice start time order because that's the order\n   // they'll be added to prefetch_async_copy_resource_ in\n   // AddAsyncSlicesForPrefetch(), if the solution is selected.\n+  static const float kSlicedCopyResourceInflation = 1.8;\n   for (int i = 0; i < slice_start_times.size(); ++i) {\n+    float original_copy_resource = copy_resource_per_slice[i];\n+    float new_copy_resource = original_copy_resource;\n+    if (slice_start_times.size() > 1) {\n+      // This is a hack that makes us more conservative about using sliced\n+      // prefetching vs unsliced prefetching.\n+      new_copy_resource = original_copy_resource * kSlicedCopyResourceInflation;\n+      VLOG(5)\n+          << \"Inflating required copy resources DoWeHaveEnoughCopyResource() \"\n+             \"slice check from \"\n+          << original_copy_resource << \" to \" << new_copy_resource;\n+    }\n     specs.push_back(\n-        {slice_start_times[i], prefetch_end_time, copy_resource_per_slice[i]});\n+        {slice_start_times[i], prefetch_end_time, new_copy_resource});\n   }\n \n   auto specs_to_string = [&specs]() {"
        }
    ]
},
{
    "Id": 444,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ae74390104b5c7bc2c78da5cf39c7e93ff422958",
    "date": "2023-04-26T06:04:06-07:00",
    "message": "Add 64-bit triton matmul indexing mode\n\nIf any of the (M*K, K*N, M*N) sizes cross the 32 bit boundary, we must use 64-bit indexing, otherwise integer overflow will cause illegal memory accesses.\n\nPiperOrigin-RevId: 527242751",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "tensorflow/compiler/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 456,
                    "old_length": 6,
                    "new_start": 456,
                    "new_length": 28,
                    "hunk": "@@ -456,6 +456,28 @@ xla_test(\n     ],\n )\n \n+xla_test(\n+    name = \"ir_emitter_triton_large_test\",\n+    srcs = if_cuda_is_configured([\"ir_emitter_triton_large_test.cc\"]),\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-sm70\",\n+    ]},\n+    backends = [\n+        \"gpu\",\n+    ],\n+    tags = [\n+        \"large\",\n+        \"no_oss\",\n+        \"nomac\",\n+        \"notap\",\n+    ],\n+    deps = [\n+        \"//tensorflow/compiler/xla:error_spec\",\n+        \"//tensorflow/compiler/xla/service/gpu/tests:gpu_codegen_test\",\n+        \"//tensorflow/compiler/xla/tests:xla_internal_test_main\",  # fixdeps: keep\n+    ],\n+)\n+\n xla_test(\n     name = \"ir_emitter_triton_parametrized_test\",\n     srcs = if_cuda_is_configured([\"ir_emitter_triton_parametrized_test.cc\"]),\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+xla_test(\n+    name = \"ir_emitter_triton_large_test\",\n+    srcs = if_cuda_is_configured([\"ir_emitter_triton_large_test.cc\"]),\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-sm70\",\n+    ]},\n+    backends = [\n+        \"gpu\",\n+    ],\n+    tags = [\n+        \"large\",\n+        \"no_oss\",\n+        \"nomac\",\n+        \"notap\",\n+    ],\n+    deps = [\n+        \"//tensorflow/compiler/xla:error_spec\",\n+        \"//tensorflow/compiler/xla/service/gpu/tests:gpu_codegen_test\",\n+        \"//tensorflow/compiler/xla/tests:xla_internal_test_main\",  # fixdeps: keep\n+    ],\n+)\n+\n",
            "whole_hunk": "@@ -456,6 +456,28 @@ xla_test(\n     ],\n )\n \n+xla_test(\n+    name = \"ir_emitter_triton_large_test\",\n+    srcs = if_cuda_is_configured([\"ir_emitter_triton_large_test.cc\"]),\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-sm70\",\n+    ]},\n+    backends = [\n+        \"gpu\",\n+    ],\n+    tags = [\n+        \"large\",\n+        \"no_oss\",\n+        \"nomac\",\n+        \"notap\",\n+    ],\n+    deps = [\n+        \"//tensorflow/compiler/xla:error_spec\",\n+        \"//tensorflow/compiler/xla/service/gpu/tests:gpu_codegen_test\",\n+        \"//tensorflow/compiler/xla/tests:xla_internal_test_main\",  # fixdeps: keep\n+    ],\n+)\n+\n xla_test(\n     name = \"ir_emitter_triton_parametrized_test\",\n     srcs = if_cuda_is_configured([\"ir_emitter_triton_parametrized_test.cc\"]),\n"
        },
        {
            "name": "ir_emitter_triton.cc",
            "path": "tensorflow/compiler/xla/service/gpu/ir_emitter_triton.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"tensorflow/compiler/xla/service/gpu/ir_emitter_triton.h\"\n \n+#include <climits>\n #include <cstdint>\n #include <functional>\n #include <memory>\n"
                },
                {
                    "old_start": 50,
                    "old_length": 6,
                    "new_start": 51,
                    "new_length": 7,
                    "hunk": "@@ -50,6 +51,7 @@ limitations under the License.\n #include \"mlir/IR/Location.h\"  // from @llvm-project\n #include \"mlir/IR/PatternMatch.h\"  // from @llvm-project\n #include \"mlir/IR/Types.h\"  // from @llvm-project\n+#include \"mlir/IR/Value.h\"  // from @llvm-project\n #include \"mlir/IR/ValueRange.h\"  // from @llvm-project\n #include \"mlir/IR/Verifier.h\"  // from @llvm-project\n #include \"mlir/Pass/PassManager.h\"  // from @llvm-project\n"
                },
                {
                    "old_start": 66,
                    "old_length": 6,
                    "new_start": 68,
                    "new_length": 7,
                    "hunk": "@@ -66,6 +68,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/service/gpu/ir_emission_utils.h\"\n #include \"tensorflow/compiler/xla/service/gpu/matmul_utils.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/llvm_util.h\"\n+#include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/xla_data.pb.h\"\n #include \"tensorflow/tsl/platform/path.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n"
                },
                {
                    "old_start": 168,
                    "old_length": 7,
                    "new_start": 171,
                    "new_length": 8,
                    "hunk": "@@ -168,7 +171,8 @@ Value Cast(mlir::OpBuilder b, mlir::Location loc, Value value,\n }\n \n // Create a scalar constant.\n-ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type, int value) {\n+ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type,\n+                           int64_t value) {\n   if (type.isa<mlir::IntegerType>()) {\n     return b.create<ma::ConstantOp>(b.getIntegerAttr(type, value));\n   }\n"
                },
                {
                    "old_start": 180,
                    "old_length": 8,
                    "new_start": 184,
                    "new_length": 8,
                    "hunk": "@@ -180,8 +184,8 @@ ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type, int value) {\n }\n \n // Create a tensor constant.\n-ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type, int value,\n-                           mlir::ArrayRef<int64_t> shape) {\n+ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type,\n+                           int64_t value, mlir::ArrayRef<int64_t> shape) {\n   auto tensor_type = mlir::RankedTensorType::get(shape, type);\n   if (auto int_type = type.dyn_cast<mlir::IntegerType>()) {\n     return b.create<ma::ConstantOp>(mlir::DenseElementsAttr::get(\n"
                },
                {
                    "old_start": 314,
                    "old_length": 11,
                    "new_start": 318,
                    "new_length": 10,
                    "hunk": "@@ -314,11 +318,10 @@ struct GeneralizeKernelSignaturePass\n   }\n };\n \n-}  // namespace\n-\n // Variable naming: lhs [m, k] x rhs [k, n] -> out [m, n].\n // TODO(b/270937368): Split this up into smaller functions.\n-StatusOr<LaunchDimensions> MatMul(\n+template <typename IndexT>\n+StatusOr<LaunchDimensions> MatMulImpl(\n     mlir::OpBuilder builder, const HloDotInstruction* dot_instr,\n     mlir::triton::FuncOp fn,\n     const tensorflow::AutotuneResult::TritonGemmKey& config, int shmem_budget) {\n"
                },
                {
                    "old_start": 328,
                    "old_length": 6,
                    "new_start": 331,
                    "new_length": 12,
                    "hunk": "@@ -328,6 +331,12 @@ StatusOr<LaunchDimensions> MatMul(\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(dot_instr->name()));\n   mlir::ImplicitLocOpBuilder b(loc, builder);\n   Type i32_ty = b.getI32Type();\n+  Type int_ty;\n+  if constexpr (std::is_same_v<IndexT, int64_t>) {\n+    int_ty = b.getI64Type();\n+  } else {\n+    int_ty = b.getI32Type();\n+  }\n   const DotDimensionNumbers& dims = dot_instr->dot_dimension_numbers();\n   const DotFusionAnalysis analysis(dot_instr, config.split_k());\n   const HloInstruction* hlo_lhs_param = analysis.OperandToParameter(0);\n"
                },
                {
                    "old_start": 393,
                    "old_length": 19,
                    "new_start": 402,
                    "new_length": 19,
                    "hunk": "@@ -393,19 +402,19 @@ StatusOr<LaunchDimensions> MatMul(\n   CHECK_EQ(analysis.IterSpec(1, rhs_noncontracting_dim_idx).size(), 1);\n   CHECK_EQ(analysis.IterSpec(0, dims.lhs_contracting_dimensions(0)).size(), 1);\n \n-  const int stride_lhs_m =\n+  const IndexT stride_lhs_m =\n       analysis.IterSpec(0, lhs_noncontracting_dim_idx)[0].stride;\n-  const int stride_lhs_k =\n+  const IndexT stride_lhs_k =\n       analysis.IterSpec(0, dims.lhs_contracting_dimensions(0))[0].stride;\n-  const int stride_rhs_k =\n+  const IndexT stride_rhs_k =\n       analysis.IterSpec(1, dims.rhs_contracting_dimensions(0))[0].stride;\n-  const int stride_rhs_n =\n+  const IndexT stride_rhs_n =\n       analysis.IterSpec(1, rhs_noncontracting_dim_idx)[0].stride;\n \n   // Either batch size or upper part of the length of a split nc dimension.\n   int batch_size = 1;\n-  int stride_batch_lhs = 0;\n-  int stride_batch_rhs = 0;\n+  IndexT stride_batch_lhs = 0;\n+  IndexT stride_batch_rhs = 0;\n   // LHS non-contracting can be split, so this holds its full size unlike the\n   // m_minor.\n   int m_full = m_minor;\n"
                },
                {
                    "old_start": 426,
                    "old_length": 7,
                    "new_start": 435,
                    "new_length": 7,
                    "hunk": "@@ -426,7 +435,7 @@ StatusOr<LaunchDimensions> MatMul(\n         analysis.IterSpec(1, dims.rhs_batch_dimensions(0))[0].stride;\n   }\n \n-  constexpr int64_t group_m = 8;\n+  constexpr int group_m = 8;\n \n   // Logical output dimensions are always ordered as:\n   //   batch, split-K, non-contracting LHS, non-contracting RHS,\n"
                },
                {
                    "old_start": 436,
                    "old_length": 10,
                    "new_start": 445,
                    "new_length": 10,
                    "hunk": "@@ -436,10 +445,10 @@ StatusOr<LaunchDimensions> MatMul(\n   const int split_k_out_logical_idx = have_split_k ? (have_batch ? 1 : 0) : -1;\n   const int batch_out_logical_idx = have_batch ? 0 : -1;\n \n-  int64_t stride_out_m = 0;\n-  int64_t stride_out_n = 0;\n-  int64_t stride_out_split_k = 0;\n-  int64_t stride_out_batch = 0;\n+  IndexT stride_out_m = 0;\n+  IndexT stride_out_n = 0;\n+  IndexT stride_out_split_k = 0;\n+  IndexT stride_out_batch = 0;\n \n   // Iterate over output's physical dimension starting from the fastest\n   // varying one; detect their types and populate the strides accordingly.\n"
                },
                {
                    "old_start": 546,
                    "old_length": 9,
                    "new_start": 555,
                    "new_length": 9,
                    "hunk": "@@ -546,9 +555,9 @@ StatusOr<LaunchDimensions> MatMul(\n     return b.create<mt::SplatOp>(type, value);\n   };\n \n-  auto build_range = [&](uint32_t start, uint32_t end) {\n-    auto type = mlir::RankedTensorType::get(end - start, b.getI32Type());\n-    return b.create<mt::MakeRangeOp>(type, start, end);\n+  auto build_range = [&](int32_t limit) {\n+    auto type = mlir::RankedTensorType::get(limit, b.getI32Type());\n+    return b.create<mt::MakeRangeOp>(type, 0, limit);\n   };\n \n   using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n"
                },
                {
                    "old_start": 562,
                    "old_length": 6,
                    "new_start": 571,
                    "new_length": 22,
                    "hunk": "@@ -562,6 +571,22 @@ StatusOr<LaunchDimensions> MatMul(\n     return b.create<mt::AddPtrOp>(ptr.getType(), ptr, offset);\n   };\n \n+  // Extend int32 indexes to int64, if necessary.\n+  auto convert_scalar = [&](Value value) -> Value {\n+    if constexpr (std::is_same_v<IndexT, int64_t>) {\n+      return b.create<ma::ExtSIOp>(int_ty, value);\n+    }\n+    return value;\n+  };\n+  auto convert_range = [&](Value value) -> Value {\n+    if constexpr (std::is_same_v<IndexT, int64_t>) {\n+      auto type = mlir::RankedTensorType::get(\n+          value.dyn_cast<TensorValue>().getType().getShape(), int_ty);\n+      return b.create<ma::ExtSIOp>(type, value);\n+    }\n+    return value;\n+  };\n+\n   auto pid_m = b.create<ma::AddIOp>(first_pid_m,\n                                     b.create<ma::RemSIOp>(pid0, group_size));\n   auto pid_m_stride =\n"
                },
                {
                    "old_start": 569,
                    "old_length": 55,
                    "new_start": 594,
                    "new_length": 59,
                    "hunk": "@@ -569,55 +594,59 @@ StatusOr<LaunchDimensions> MatMul(\n   // TODO(b/270351731): Consider regenerating range_m to reduce register\n   // pressure if we figure out how to make this optimization survive CSE.\n   auto range_m = b.create<ma::AddIOp>(build_splat(pid_m_stride, block_m),\n-                                      build_range(0, block_m));\n+                                      build_range(block_m));\n \n   auto pid_n = b.create<ma::DivSIOp>(\n       b.create<ma::RemSIOp>(pid0, CreateConst(b, i32_ty, width)), group_size);\n   auto pid_n_stride =\n       b.create<ma::MulIOp>(pid_n, CreateConst(b, i32_ty, block_n));\n   auto range_n = b.create<ma::AddIOp>(build_splat(pid_n_stride, block_n),\n-                                      build_range(0, block_n));\n+                                      build_range(block_n));\n \n   auto range_k = b.create<ma::AddIOp>(\n       build_splat(b.create<ma::MulIOp>(pid1, CreateConst(b, i32_ty, block_k)),\n                   block_k),\n-      build_range(0, block_k));\n+      build_range(block_k));\n \n   SmallVector<int64_t, 2> shape_m_1{block_m, 1};\n-  auto range_lhs_m =\n-      b.create<ma::RemSIOp>(range_m, CreateConst(b, i32_ty, m_minor, block_m));\n+  auto range_lhs_m = convert_range(\n+      b.create<ma::RemSIOp>(range_m, CreateConst(b, i32_ty, m_minor, block_m)));\n   auto lhs_offset_m =\n       b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_lhs_m, 1),\n-                           CreateConst(b, i32_ty, stride_lhs_m, shape_m_1));\n+                           CreateConst(b, int_ty, stride_lhs_m, shape_m_1));\n   SmallVector<int64_t, 2> shape_1_k{1, block_k};\n-  auto lhs_offset_k =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_k, 0),\n-                           CreateConst(b, i32_ty, stride_lhs_k, shape_1_k));\n+  auto lhs_offset_k = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_k), 0),\n+      CreateConst(b, int_ty, stride_lhs_k, shape_1_k));\n   SmallVector<int64_t, 2> shape_m_k{block_m, block_k};\n   auto lhs_offset = b.create<ma::AddIOp>(\n-      build_bcast(lhs_offset_m.getResult().cast<TensorValue>(), shape_m_k),\n-      build_bcast(lhs_offset_k.getResult().cast<TensorValue>(), shape_m_k));\n-  auto lhs_offset_batch =\n-      b.create<ma::MulIOp>(pid2, CreateConst(b, i32_ty, stride_batch_lhs));\n+      build_bcast(lhs_offset_m.getResult().template cast<TensorValue>(),\n+                  shape_m_k),\n+      build_bcast(lhs_offset_k.getResult().template cast<TensorValue>(),\n+                  shape_m_k));\n+  auto lhs_offset_batch = b.create<ma::MulIOp>(\n+      convert_scalar(pid2), CreateConst(b, int_ty, stride_batch_lhs));\n   mt::AddPtrOp lhs_ptrs_base = build_addptr(\n       build_splat(build_addptr(lhs, lhs_offset_batch), shape_m_k), lhs_offset);\n \n   SmallVector<int64_t, 2> shape_k_1{block_k, 1};\n-  auto rhs_off_k =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_k, 1),\n-                           CreateConst(b, i32_ty, stride_rhs_k, shape_k_1));\n+  auto rhs_off_k = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_k), 1),\n+      CreateConst(b, int_ty, stride_rhs_k, shape_k_1));\n   SmallVector<int64_t, 2> shape_1_n{1, block_n};\n-  auto range_rhs_n =\n-      b.create<ma::RemSIOp>(range_n, CreateConst(b, i32_ty, n, block_n));\n+  auto range_rhs_n = convert_range(\n+      b.create<ma::RemSIOp>(range_n, CreateConst(b, i32_ty, n, block_n)));\n   auto rhs_offset_n =\n       b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_rhs_n, 0),\n-                           CreateConst(b, i32_ty, stride_rhs_n, shape_1_n));\n+                           CreateConst(b, int_ty, stride_rhs_n, shape_1_n));\n   SmallVector<int64_t, 2> shape_k_n{block_k, block_n};\n   auto rhs_offset = b.create<ma::AddIOp>(\n-      build_bcast(rhs_off_k.getResult().cast<TensorValue>(), shape_k_n),\n-      build_bcast(rhs_offset_n.getResult().cast<TensorValue>(), shape_k_n));\n-  auto rhs_offset_batch =\n-      b.create<ma::MulIOp>(pid2, CreateConst(b, i32_ty, stride_batch_rhs));\n+      build_bcast(rhs_off_k.getResult().template cast<TensorValue>(),\n+                  shape_k_n),\n+      build_bcast(rhs_offset_n.getResult().template cast<TensorValue>(),\n+                  shape_k_n));\n+  auto rhs_offset_batch = b.create<ma::MulIOp>(\n+      convert_scalar(pid2), CreateConst(b, int_ty, stride_batch_rhs));\n   mt::AddPtrOp rhs_ptrs_base = build_addptr(\n       build_splat(build_addptr(rhs, rhs_offset_batch), shape_k_n), rhs_offset);\n   SmallVector<int64_t, 2> shape_m_n{block_m, block_n};\n"
                },
                {
                    "old_start": 644,
                    "old_length": 14,
                    "new_start": 673,
                    "new_length": 14,
                    "hunk": "@@ -644,14 +673,14 @@ StatusOr<LaunchDimensions> MatMul(\n                                b.create<mt::ExpandDimsOp>(range_k, 0),\n                                build_splat(elements_in_tile, shape_1_k))\n               .getResult()\n-              .cast<TensorValue>(),\n+              .template cast<TensorValue>(),\n           shape_m_k);\n       rhs_mask = build_bcast(\n           b.create<ma::CmpIOp>(ma::CmpIPredicate::slt,\n                                b.create<mt::ExpandDimsOp>(range_k, 1),\n                                build_splat(elements_in_tile, shape_k_1))\n               .getResult()\n-              .cast<TensorValue>(),\n+              .template cast<TensorValue>(),\n           shape_k_n);\n     }\n     auto lhs_tile = b.create<mt::LoadOp>(lhs_ptrs, lhs_mask, zeros_like_lhs,\n"
                },
                {
                    "old_start": 670,
                    "old_length": 11,
                    "new_start": 699,
                    "new_length": 11,
                    "hunk": "@@ -670,11 +699,11 @@ StatusOr<LaunchDimensions> MatMul(\n \n     mt::AddPtrOp lhs_ptrs_inc = build_addptr(\n         lhs_ptrs,\n-        CreateConst(b, i32_ty, block_k * config.split_k() * stride_lhs_k,\n+        CreateConst(b, int_ty, block_k * config.split_k() * stride_lhs_k,\n                     shape_m_k));\n     mt::AddPtrOp rhs_ptrs_inc = build_addptr(\n         rhs_ptrs,\n-        CreateConst(b, i32_ty, block_k * config.split_k() * stride_rhs_k,\n+        CreateConst(b, int_ty, block_k * config.split_k() * stride_rhs_k,\n                     shape_k_n));\n \n     b.create<mlir::scf::YieldOp>(\n"
                },
                {
                    "old_start": 693,
                    "old_length": 25,
                    "new_start": 722,
                    "new_length": 26,
                    "hunk": "@@ -693,25 +722,26 @@ StatusOr<LaunchDimensions> MatMul(\n           .getResult(2);\n \n   // Output tile offsets.\n-  auto out_offset_batch =\n-      b.create<ma::MulIOp>(pid2, CreateConst(b, i32_ty, stride_out_batch));\n-  auto out_offset_split_k =\n-      b.create<ma::MulIOp>(pid1, CreateConst(b, i32_ty, stride_out_split_k));\n-  auto out_offset_m =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_m, 1),\n-                           CreateConst(b, i32_ty, stride_out_m, shape_m_1));\n+  auto out_offset_batch = b.create<ma::MulIOp>(\n+      convert_scalar(pid2), CreateConst(b, int_ty, stride_out_batch));\n+  auto out_offset_split_k = b.create<ma::MulIOp>(\n+      convert_scalar(pid1), CreateConst(b, int_ty, stride_out_split_k));\n+  auto out_offset_m = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_m), 1),\n+      CreateConst(b, int_ty, stride_out_m, shape_m_1));\n   mt::AddPtrOp out_ptrs_m = build_addptr(\n       build_splat(\n           build_addptr(build_addptr(out, out_offset_batch), out_offset_split_k),\n           shape_m_1),\n       out_offset_m);\n \n-  auto out_offset_n =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_n, 0),\n-                           CreateConst(b, i32_ty, stride_out_n, shape_1_n));\n+  auto out_offset_n = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_n), 0),\n+      CreateConst(b, int_ty, stride_out_n, shape_1_n));\n   mt::AddPtrOp out_ptrs = build_addptr(\n       build_bcast(out_ptrs_m.getResult().cast<TensorValue>(), shape_m_n),\n-      build_bcast(out_offset_n.getResult().cast<TensorValue>(), shape_m_n));\n+      build_bcast(out_offset_n.getResult().template cast<TensorValue>(),\n+                  shape_m_n));\n \n   // Output tile store mask: check that the indices are within [M, N].\n   auto rm_cmp = b.create<ma::CmpIOp>(\n"
                },
                {
                    "old_start": 721,
                    "old_length": 14,
                    "new_start": 751,
                    "new_length": 34,
                    "hunk": "@@ -721,14 +751,34 @@ StatusOr<LaunchDimensions> MatMul(\n                                      b.create<mt::ExpandDimsOp>(range_n, 0),\n                                      CreateConst(b, i32_ty, n, shape_1_n));\n   auto mask = b.create<ma::AndIOp>(\n-      build_bcast(rm_cmp.getResult().cast<TensorValue>(), shape_m_n),\n-      build_bcast(rn_cmp.getResult().cast<TensorValue>(), shape_m_n));\n+      build_bcast(rm_cmp.getResult().template cast<TensorValue>(), shape_m_n),\n+      build_bcast(rn_cmp.getResult().template cast<TensorValue>(), shape_m_n));\n \n   b.create<mt::StoreOp>(out_ptrs, Cast(b, loc, acc_final, root_ty), mask,\n                         mt::CacheModifier::NONE, mt::EvictionPolicy::NORMAL);\n   return launch_dimensions;\n }\n \n+}  // namespace\n+\n+StatusOr<LaunchDimensions> MatMul(\n+    mlir::OpBuilder builder, const HloDotInstruction* dot_instr,\n+    mlir::triton::FuncOp fn,\n+    const tensorflow::AutotuneResult::TritonGemmKey& config, int shmem_budget) {\n+  // Use 32-bit indexing if addressing any of the inputs or the output (which\n+  // could grow if split_k is set) does not cross the INT_MAX boundary.\n+  // Otherwise, fall back to 64-bit indexing, which is slower.\n+  bool use_64bit_indexing =\n+      ShapeUtil::ElementsIn(dot_instr->operand(0)->shape()) > INT_MAX ||\n+      ShapeUtil::ElementsIn(dot_instr->operand(1)->shape()) > INT_MAX ||\n+      ShapeUtil::ElementsIn(dot_instr->shape()) * config.split_k() > INT_MAX;\n+  if (use_64bit_indexing) {\n+    return MatMulImpl<int64_t>(builder, dot_instr, fn, config, shmem_budget);\n+  } else {\n+    return MatMulImpl<int32_t>(builder, dot_instr, fn, config, shmem_budget);\n+  }\n+}\n+\n StatusOr<LaunchDimensions> TritonWrapper(\n     absl::string_view fn_name, const HloComputation* hlo_computation,\n     const se::CudaComputeCapability& cc, const GpuDeviceInfo& device_info,\n"
                }
            ],
            "whole_deleted": "-ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type, int value) {\n-ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type, int value,\n-                           mlir::ArrayRef<int64_t> shape) {\n-}  // namespace\n-\n-StatusOr<LaunchDimensions> MatMul(\n-  const int stride_lhs_m =\n-  const int stride_lhs_k =\n-  const int stride_rhs_k =\n-  const int stride_rhs_n =\n-  int stride_batch_lhs = 0;\n-  int stride_batch_rhs = 0;\n-  constexpr int64_t group_m = 8;\n-  int64_t stride_out_m = 0;\n-  int64_t stride_out_n = 0;\n-  int64_t stride_out_split_k = 0;\n-  int64_t stride_out_batch = 0;\n-  auto build_range = [&](uint32_t start, uint32_t end) {\n-    auto type = mlir::RankedTensorType::get(end - start, b.getI32Type());\n-    return b.create<mt::MakeRangeOp>(type, start, end);\n-                                      build_range(0, block_m));\n-                                      build_range(0, block_n));\n-      build_range(0, block_k));\n-  auto range_lhs_m =\n-      b.create<ma::RemSIOp>(range_m, CreateConst(b, i32_ty, m_minor, block_m));\n-                           CreateConst(b, i32_ty, stride_lhs_m, shape_m_1));\n-  auto lhs_offset_k =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_k, 0),\n-                           CreateConst(b, i32_ty, stride_lhs_k, shape_1_k));\n-      build_bcast(lhs_offset_m.getResult().cast<TensorValue>(), shape_m_k),\n-      build_bcast(lhs_offset_k.getResult().cast<TensorValue>(), shape_m_k));\n-  auto lhs_offset_batch =\n-      b.create<ma::MulIOp>(pid2, CreateConst(b, i32_ty, stride_batch_lhs));\n-  auto rhs_off_k =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_k, 1),\n-                           CreateConst(b, i32_ty, stride_rhs_k, shape_k_1));\n-  auto range_rhs_n =\n-      b.create<ma::RemSIOp>(range_n, CreateConst(b, i32_ty, n, block_n));\n-                           CreateConst(b, i32_ty, stride_rhs_n, shape_1_n));\n-      build_bcast(rhs_off_k.getResult().cast<TensorValue>(), shape_k_n),\n-      build_bcast(rhs_offset_n.getResult().cast<TensorValue>(), shape_k_n));\n-  auto rhs_offset_batch =\n-      b.create<ma::MulIOp>(pid2, CreateConst(b, i32_ty, stride_batch_rhs));\n-              .cast<TensorValue>(),\n-              .cast<TensorValue>(),\n-        CreateConst(b, i32_ty, block_k * config.split_k() * stride_lhs_k,\n-        CreateConst(b, i32_ty, block_k * config.split_k() * stride_rhs_k,\n-  auto out_offset_batch =\n-      b.create<ma::MulIOp>(pid2, CreateConst(b, i32_ty, stride_out_batch));\n-  auto out_offset_split_k =\n-      b.create<ma::MulIOp>(pid1, CreateConst(b, i32_ty, stride_out_split_k));\n-  auto out_offset_m =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_m, 1),\n-                           CreateConst(b, i32_ty, stride_out_m, shape_m_1));\n-  auto out_offset_n =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_n, 0),\n-                           CreateConst(b, i32_ty, stride_out_n, shape_1_n));\n-      build_bcast(out_offset_n.getResult().cast<TensorValue>(), shape_m_n));\n-      build_bcast(rm_cmp.getResult().cast<TensorValue>(), shape_m_n),\n-      build_bcast(rn_cmp.getResult().cast<TensorValue>(), shape_m_n));\n",
            "whole_added": "+#include <climits>\n+#include \"mlir/IR/Value.h\"  // from @llvm-project\n+#include \"tensorflow/compiler/xla/shape_util.h\"\n+ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type,\n+                           int64_t value) {\n+ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type,\n+                           int64_t value, mlir::ArrayRef<int64_t> shape) {\n+template <typename IndexT>\n+StatusOr<LaunchDimensions> MatMulImpl(\n+  Type int_ty;\n+  if constexpr (std::is_same_v<IndexT, int64_t>) {\n+    int_ty = b.getI64Type();\n+  } else {\n+    int_ty = b.getI32Type();\n+  }\n+  const IndexT stride_lhs_m =\n+  const IndexT stride_lhs_k =\n+  const IndexT stride_rhs_k =\n+  const IndexT stride_rhs_n =\n+  IndexT stride_batch_lhs = 0;\n+  IndexT stride_batch_rhs = 0;\n+  constexpr int group_m = 8;\n+  IndexT stride_out_m = 0;\n+  IndexT stride_out_n = 0;\n+  IndexT stride_out_split_k = 0;\n+  IndexT stride_out_batch = 0;\n+  auto build_range = [&](int32_t limit) {\n+    auto type = mlir::RankedTensorType::get(limit, b.getI32Type());\n+    return b.create<mt::MakeRangeOp>(type, 0, limit);\n+  // Extend int32 indexes to int64, if necessary.\n+  auto convert_scalar = [&](Value value) -> Value {\n+    if constexpr (std::is_same_v<IndexT, int64_t>) {\n+      return b.create<ma::ExtSIOp>(int_ty, value);\n+    }\n+    return value;\n+  };\n+  auto convert_range = [&](Value value) -> Value {\n+    if constexpr (std::is_same_v<IndexT, int64_t>) {\n+      auto type = mlir::RankedTensorType::get(\n+          value.dyn_cast<TensorValue>().getType().getShape(), int_ty);\n+      return b.create<ma::ExtSIOp>(type, value);\n+    }\n+    return value;\n+  };\n+\n+                                      build_range(block_m));\n+                                      build_range(block_n));\n+      build_range(block_k));\n+  auto range_lhs_m = convert_range(\n+      b.create<ma::RemSIOp>(range_m, CreateConst(b, i32_ty, m_minor, block_m)));\n+                           CreateConst(b, int_ty, stride_lhs_m, shape_m_1));\n+  auto lhs_offset_k = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_k), 0),\n+      CreateConst(b, int_ty, stride_lhs_k, shape_1_k));\n+      build_bcast(lhs_offset_m.getResult().template cast<TensorValue>(),\n+                  shape_m_k),\n+      build_bcast(lhs_offset_k.getResult().template cast<TensorValue>(),\n+                  shape_m_k));\n+  auto lhs_offset_batch = b.create<ma::MulIOp>(\n+      convert_scalar(pid2), CreateConst(b, int_ty, stride_batch_lhs));\n+  auto rhs_off_k = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_k), 1),\n+      CreateConst(b, int_ty, stride_rhs_k, shape_k_1));\n+  auto range_rhs_n = convert_range(\n+      b.create<ma::RemSIOp>(range_n, CreateConst(b, i32_ty, n, block_n)));\n+                           CreateConst(b, int_ty, stride_rhs_n, shape_1_n));\n+      build_bcast(rhs_off_k.getResult().template cast<TensorValue>(),\n+                  shape_k_n),\n+      build_bcast(rhs_offset_n.getResult().template cast<TensorValue>(),\n+                  shape_k_n));\n+  auto rhs_offset_batch = b.create<ma::MulIOp>(\n+      convert_scalar(pid2), CreateConst(b, int_ty, stride_batch_rhs));\n+              .template cast<TensorValue>(),\n+              .template cast<TensorValue>(),\n+        CreateConst(b, int_ty, block_k * config.split_k() * stride_lhs_k,\n+        CreateConst(b, int_ty, block_k * config.split_k() * stride_rhs_k,\n+  auto out_offset_batch = b.create<ma::MulIOp>(\n+      convert_scalar(pid2), CreateConst(b, int_ty, stride_out_batch));\n+  auto out_offset_split_k = b.create<ma::MulIOp>(\n+      convert_scalar(pid1), CreateConst(b, int_ty, stride_out_split_k));\n+  auto out_offset_m = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_m), 1),\n+      CreateConst(b, int_ty, stride_out_m, shape_m_1));\n+  auto out_offset_n = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_n), 0),\n+      CreateConst(b, int_ty, stride_out_n, shape_1_n));\n+      build_bcast(out_offset_n.getResult().template cast<TensorValue>(),\n+                  shape_m_n));\n+      build_bcast(rm_cmp.getResult().template cast<TensorValue>(), shape_m_n),\n+      build_bcast(rn_cmp.getResult().template cast<TensorValue>(), shape_m_n));\n+}  // namespace\n+\n+StatusOr<LaunchDimensions> MatMul(\n+    mlir::OpBuilder builder, const HloDotInstruction* dot_instr,\n+    mlir::triton::FuncOp fn,\n+    const tensorflow::AutotuneResult::TritonGemmKey& config, int shmem_budget) {\n+  // Use 32-bit indexing if addressing any of the inputs or the output (which\n+  // could grow if split_k is set) does not cross the INT_MAX boundary.\n+  // Otherwise, fall back to 64-bit indexing, which is slower.\n+  bool use_64bit_indexing =\n+      ShapeUtil::ElementsIn(dot_instr->operand(0)->shape()) > INT_MAX ||\n+      ShapeUtil::ElementsIn(dot_instr->operand(1)->shape()) > INT_MAX ||\n+      ShapeUtil::ElementsIn(dot_instr->shape()) * config.split_k() > INT_MAX;\n+  if (use_64bit_indexing) {\n+    return MatMulImpl<int64_t>(builder, dot_instr, fn, config, shmem_budget);\n+  } else {\n+    return MatMulImpl<int32_t>(builder, dot_instr, fn, config, shmem_budget);\n+  }\n+}\n+\n",
            "whole_hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"tensorflow/compiler/xla/service/gpu/ir_emitter_triton.h\"\n \n+#include <climits>\n #include <cstdint>\n #include <functional>\n #include <memory>\n@@ -50,6 +51,7 @@ limitations under the License.\n #include \"mlir/IR/Location.h\"  // from @llvm-project\n #include \"mlir/IR/PatternMatch.h\"  // from @llvm-project\n #include \"mlir/IR/Types.h\"  // from @llvm-project\n+#include \"mlir/IR/Value.h\"  // from @llvm-project\n #include \"mlir/IR/ValueRange.h\"  // from @llvm-project\n #include \"mlir/IR/Verifier.h\"  // from @llvm-project\n #include \"mlir/Pass/PassManager.h\"  // from @llvm-project\n@@ -66,6 +68,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/service/gpu/ir_emission_utils.h\"\n #include \"tensorflow/compiler/xla/service/gpu/matmul_utils.h\"\n #include \"tensorflow/compiler/xla/service/llvm_ir/llvm_util.h\"\n+#include \"tensorflow/compiler/xla/shape_util.h\"\n #include \"tensorflow/compiler/xla/xla_data.pb.h\"\n #include \"tensorflow/tsl/platform/path.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n@@ -168,7 +171,8 @@ Value Cast(mlir::OpBuilder b, mlir::Location loc, Value value,\n }\n \n // Create a scalar constant.\n-ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type, int value) {\n+ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type,\n+                           int64_t value) {\n   if (type.isa<mlir::IntegerType>()) {\n     return b.create<ma::ConstantOp>(b.getIntegerAttr(type, value));\n   }\n@@ -180,8 +184,8 @@ ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type, int value) {\n }\n \n // Create a tensor constant.\n-ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type, int value,\n-                           mlir::ArrayRef<int64_t> shape) {\n+ma::ConstantOp CreateConst(mlir::ImplicitLocOpBuilder b, Type type,\n+                           int64_t value, mlir::ArrayRef<int64_t> shape) {\n   auto tensor_type = mlir::RankedTensorType::get(shape, type);\n   if (auto int_type = type.dyn_cast<mlir::IntegerType>()) {\n     return b.create<ma::ConstantOp>(mlir::DenseElementsAttr::get(\n@@ -314,11 +318,10 @@ struct GeneralizeKernelSignaturePass\n   }\n };\n \n-}  // namespace\n-\n // Variable naming: lhs [m, k] x rhs [k, n] -> out [m, n].\n // TODO(b/270937368): Split this up into smaller functions.\n-StatusOr<LaunchDimensions> MatMul(\n+template <typename IndexT>\n+StatusOr<LaunchDimensions> MatMulImpl(\n     mlir::OpBuilder builder, const HloDotInstruction* dot_instr,\n     mlir::triton::FuncOp fn,\n     const tensorflow::AutotuneResult::TritonGemmKey& config, int shmem_budget) {\n@@ -328,6 +331,12 @@ StatusOr<LaunchDimensions> MatMul(\n   auto loc = mlir::NameLoc::get(builder.getStringAttr(dot_instr->name()));\n   mlir::ImplicitLocOpBuilder b(loc, builder);\n   Type i32_ty = b.getI32Type();\n+  Type int_ty;\n+  if constexpr (std::is_same_v<IndexT, int64_t>) {\n+    int_ty = b.getI64Type();\n+  } else {\n+    int_ty = b.getI32Type();\n+  }\n   const DotDimensionNumbers& dims = dot_instr->dot_dimension_numbers();\n   const DotFusionAnalysis analysis(dot_instr, config.split_k());\n   const HloInstruction* hlo_lhs_param = analysis.OperandToParameter(0);\n@@ -393,19 +402,19 @@ StatusOr<LaunchDimensions> MatMul(\n   CHECK_EQ(analysis.IterSpec(1, rhs_noncontracting_dim_idx).size(), 1);\n   CHECK_EQ(analysis.IterSpec(0, dims.lhs_contracting_dimensions(0)).size(), 1);\n \n-  const int stride_lhs_m =\n+  const IndexT stride_lhs_m =\n       analysis.IterSpec(0, lhs_noncontracting_dim_idx)[0].stride;\n-  const int stride_lhs_k =\n+  const IndexT stride_lhs_k =\n       analysis.IterSpec(0, dims.lhs_contracting_dimensions(0))[0].stride;\n-  const int stride_rhs_k =\n+  const IndexT stride_rhs_k =\n       analysis.IterSpec(1, dims.rhs_contracting_dimensions(0))[0].stride;\n-  const int stride_rhs_n =\n+  const IndexT stride_rhs_n =\n       analysis.IterSpec(1, rhs_noncontracting_dim_idx)[0].stride;\n \n   // Either batch size or upper part of the length of a split nc dimension.\n   int batch_size = 1;\n-  int stride_batch_lhs = 0;\n-  int stride_batch_rhs = 0;\n+  IndexT stride_batch_lhs = 0;\n+  IndexT stride_batch_rhs = 0;\n   // LHS non-contracting can be split, so this holds its full size unlike the\n   // m_minor.\n   int m_full = m_minor;\n@@ -426,7 +435,7 @@ StatusOr<LaunchDimensions> MatMul(\n         analysis.IterSpec(1, dims.rhs_batch_dimensions(0))[0].stride;\n   }\n \n-  constexpr int64_t group_m = 8;\n+  constexpr int group_m = 8;\n \n   // Logical output dimensions are always ordered as:\n   //   batch, split-K, non-contracting LHS, non-contracting RHS,\n@@ -436,10 +445,10 @@ StatusOr<LaunchDimensions> MatMul(\n   const int split_k_out_logical_idx = have_split_k ? (have_batch ? 1 : 0) : -1;\n   const int batch_out_logical_idx = have_batch ? 0 : -1;\n \n-  int64_t stride_out_m = 0;\n-  int64_t stride_out_n = 0;\n-  int64_t stride_out_split_k = 0;\n-  int64_t stride_out_batch = 0;\n+  IndexT stride_out_m = 0;\n+  IndexT stride_out_n = 0;\n+  IndexT stride_out_split_k = 0;\n+  IndexT stride_out_batch = 0;\n \n   // Iterate over output's physical dimension starting from the fastest\n   // varying one; detect their types and populate the strides accordingly.\n@@ -546,9 +555,9 @@ StatusOr<LaunchDimensions> MatMul(\n     return b.create<mt::SplatOp>(type, value);\n   };\n \n-  auto build_range = [&](uint32_t start, uint32_t end) {\n-    auto type = mlir::RankedTensorType::get(end - start, b.getI32Type());\n-    return b.create<mt::MakeRangeOp>(type, start, end);\n+  auto build_range = [&](int32_t limit) {\n+    auto type = mlir::RankedTensorType::get(limit, b.getI32Type());\n+    return b.create<mt::MakeRangeOp>(type, 0, limit);\n   };\n \n   using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n@@ -562,6 +571,22 @@ StatusOr<LaunchDimensions> MatMul(\n     return b.create<mt::AddPtrOp>(ptr.getType(), ptr, offset);\n   };\n \n+  // Extend int32 indexes to int64, if necessary.\n+  auto convert_scalar = [&](Value value) -> Value {\n+    if constexpr (std::is_same_v<IndexT, int64_t>) {\n+      return b.create<ma::ExtSIOp>(int_ty, value);\n+    }\n+    return value;\n+  };\n+  auto convert_range = [&](Value value) -> Value {\n+    if constexpr (std::is_same_v<IndexT, int64_t>) {\n+      auto type = mlir::RankedTensorType::get(\n+          value.dyn_cast<TensorValue>().getType().getShape(), int_ty);\n+      return b.create<ma::ExtSIOp>(type, value);\n+    }\n+    return value;\n+  };\n+\n   auto pid_m = b.create<ma::AddIOp>(first_pid_m,\n                                     b.create<ma::RemSIOp>(pid0, group_size));\n   auto pid_m_stride =\n@@ -569,55 +594,59 @@ StatusOr<LaunchDimensions> MatMul(\n   // TODO(b/270351731): Consider regenerating range_m to reduce register\n   // pressure if we figure out how to make this optimization survive CSE.\n   auto range_m = b.create<ma::AddIOp>(build_splat(pid_m_stride, block_m),\n-                                      build_range(0, block_m));\n+                                      build_range(block_m));\n \n   auto pid_n = b.create<ma::DivSIOp>(\n       b.create<ma::RemSIOp>(pid0, CreateConst(b, i32_ty, width)), group_size);\n   auto pid_n_stride =\n       b.create<ma::MulIOp>(pid_n, CreateConst(b, i32_ty, block_n));\n   auto range_n = b.create<ma::AddIOp>(build_splat(pid_n_stride, block_n),\n-                                      build_range(0, block_n));\n+                                      build_range(block_n));\n \n   auto range_k = b.create<ma::AddIOp>(\n       build_splat(b.create<ma::MulIOp>(pid1, CreateConst(b, i32_ty, block_k)),\n                   block_k),\n-      build_range(0, block_k));\n+      build_range(block_k));\n \n   SmallVector<int64_t, 2> shape_m_1{block_m, 1};\n-  auto range_lhs_m =\n-      b.create<ma::RemSIOp>(range_m, CreateConst(b, i32_ty, m_minor, block_m));\n+  auto range_lhs_m = convert_range(\n+      b.create<ma::RemSIOp>(range_m, CreateConst(b, i32_ty, m_minor, block_m)));\n   auto lhs_offset_m =\n       b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_lhs_m, 1),\n-                           CreateConst(b, i32_ty, stride_lhs_m, shape_m_1));\n+                           CreateConst(b, int_ty, stride_lhs_m, shape_m_1));\n   SmallVector<int64_t, 2> shape_1_k{1, block_k};\n-  auto lhs_offset_k =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_k, 0),\n-                           CreateConst(b, i32_ty, stride_lhs_k, shape_1_k));\n+  auto lhs_offset_k = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_k), 0),\n+      CreateConst(b, int_ty, stride_lhs_k, shape_1_k));\n   SmallVector<int64_t, 2> shape_m_k{block_m, block_k};\n   auto lhs_offset = b.create<ma::AddIOp>(\n-      build_bcast(lhs_offset_m.getResult().cast<TensorValue>(), shape_m_k),\n-      build_bcast(lhs_offset_k.getResult().cast<TensorValue>(), shape_m_k));\n-  auto lhs_offset_batch =\n-      b.create<ma::MulIOp>(pid2, CreateConst(b, i32_ty, stride_batch_lhs));\n+      build_bcast(lhs_offset_m.getResult().template cast<TensorValue>(),\n+                  shape_m_k),\n+      build_bcast(lhs_offset_k.getResult().template cast<TensorValue>(),\n+                  shape_m_k));\n+  auto lhs_offset_batch = b.create<ma::MulIOp>(\n+      convert_scalar(pid2), CreateConst(b, int_ty, stride_batch_lhs));\n   mt::AddPtrOp lhs_ptrs_base = build_addptr(\n       build_splat(build_addptr(lhs, lhs_offset_batch), shape_m_k), lhs_offset);\n \n   SmallVector<int64_t, 2> shape_k_1{block_k, 1};\n-  auto rhs_off_k =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_k, 1),\n-                           CreateConst(b, i32_ty, stride_rhs_k, shape_k_1));\n+  auto rhs_off_k = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_k), 1),\n+      CreateConst(b, int_ty, stride_rhs_k, shape_k_1));\n   SmallVector<int64_t, 2> shape_1_n{1, block_n};\n-  auto range_rhs_n =\n-      b.create<ma::RemSIOp>(range_n, CreateConst(b, i32_ty, n, block_n));\n+  auto range_rhs_n = convert_range(\n+      b.create<ma::RemSIOp>(range_n, CreateConst(b, i32_ty, n, block_n)));\n   auto rhs_offset_n =\n       b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_rhs_n, 0),\n-                           CreateConst(b, i32_ty, stride_rhs_n, shape_1_n));\n+                           CreateConst(b, int_ty, stride_rhs_n, shape_1_n));\n   SmallVector<int64_t, 2> shape_k_n{block_k, block_n};\n   auto rhs_offset = b.create<ma::AddIOp>(\n-      build_bcast(rhs_off_k.getResult().cast<TensorValue>(), shape_k_n),\n-      build_bcast(rhs_offset_n.getResult().cast<TensorValue>(), shape_k_n));\n-  auto rhs_offset_batch =\n-      b.create<ma::MulIOp>(pid2, CreateConst(b, i32_ty, stride_batch_rhs));\n+      build_bcast(rhs_off_k.getResult().template cast<TensorValue>(),\n+                  shape_k_n),\n+      build_bcast(rhs_offset_n.getResult().template cast<TensorValue>(),\n+                  shape_k_n));\n+  auto rhs_offset_batch = b.create<ma::MulIOp>(\n+      convert_scalar(pid2), CreateConst(b, int_ty, stride_batch_rhs));\n   mt::AddPtrOp rhs_ptrs_base = build_addptr(\n       build_splat(build_addptr(rhs, rhs_offset_batch), shape_k_n), rhs_offset);\n   SmallVector<int64_t, 2> shape_m_n{block_m, block_n};\n@@ -644,14 +673,14 @@ StatusOr<LaunchDimensions> MatMul(\n                                b.create<mt::ExpandDimsOp>(range_k, 0),\n                                build_splat(elements_in_tile, shape_1_k))\n               .getResult()\n-              .cast<TensorValue>(),\n+              .template cast<TensorValue>(),\n           shape_m_k);\n       rhs_mask = build_bcast(\n           b.create<ma::CmpIOp>(ma::CmpIPredicate::slt,\n                                b.create<mt::ExpandDimsOp>(range_k, 1),\n                                build_splat(elements_in_tile, shape_k_1))\n               .getResult()\n-              .cast<TensorValue>(),\n+              .template cast<TensorValue>(),\n           shape_k_n);\n     }\n     auto lhs_tile = b.create<mt::LoadOp>(lhs_ptrs, lhs_mask, zeros_like_lhs,\n@@ -670,11 +699,11 @@ StatusOr<LaunchDimensions> MatMul(\n \n     mt::AddPtrOp lhs_ptrs_inc = build_addptr(\n         lhs_ptrs,\n-        CreateConst(b, i32_ty, block_k * config.split_k() * stride_lhs_k,\n+        CreateConst(b, int_ty, block_k * config.split_k() * stride_lhs_k,\n                     shape_m_k));\n     mt::AddPtrOp rhs_ptrs_inc = build_addptr(\n         rhs_ptrs,\n-        CreateConst(b, i32_ty, block_k * config.split_k() * stride_rhs_k,\n+        CreateConst(b, int_ty, block_k * config.split_k() * stride_rhs_k,\n                     shape_k_n));\n \n     b.create<mlir::scf::YieldOp>(\n@@ -693,25 +722,26 @@ StatusOr<LaunchDimensions> MatMul(\n           .getResult(2);\n \n   // Output tile offsets.\n-  auto out_offset_batch =\n-      b.create<ma::MulIOp>(pid2, CreateConst(b, i32_ty, stride_out_batch));\n-  auto out_offset_split_k =\n-      b.create<ma::MulIOp>(pid1, CreateConst(b, i32_ty, stride_out_split_k));\n-  auto out_offset_m =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_m, 1),\n-                           CreateConst(b, i32_ty, stride_out_m, shape_m_1));\n+  auto out_offset_batch = b.create<ma::MulIOp>(\n+      convert_scalar(pid2), CreateConst(b, int_ty, stride_out_batch));\n+  auto out_offset_split_k = b.create<ma::MulIOp>(\n+      convert_scalar(pid1), CreateConst(b, int_ty, stride_out_split_k));\n+  auto out_offset_m = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_m), 1),\n+      CreateConst(b, int_ty, stride_out_m, shape_m_1));\n   mt::AddPtrOp out_ptrs_m = build_addptr(\n       build_splat(\n           build_addptr(build_addptr(out, out_offset_batch), out_offset_split_k),\n           shape_m_1),\n       out_offset_m);\n \n-  auto out_offset_n =\n-      b.create<ma::MulIOp>(b.create<mt::ExpandDimsOp>(range_n, 0),\n-                           CreateConst(b, i32_ty, stride_out_n, shape_1_n));\n+  auto out_offset_n = b.create<ma::MulIOp>(\n+      b.create<mt::ExpandDimsOp>(convert_range(range_n), 0),\n+      CreateConst(b, int_ty, stride_out_n, shape_1_n));\n   mt::AddPtrOp out_ptrs = build_addptr(\n       build_bcast(out_ptrs_m.getResult().cast<TensorValue>(), shape_m_n),\n-      build_bcast(out_offset_n.getResult().cast<TensorValue>(), shape_m_n));\n+      build_bcast(out_offset_n.getResult().template cast<TensorValue>(),\n+                  shape_m_n));\n \n   // Output tile store mask: check that the indices are within [M, N].\n   auto rm_cmp = b.create<ma::CmpIOp>(\n@@ -721,14 +751,34 @@ StatusOr<LaunchDimensions> MatMul(\n                                      b.create<mt::ExpandDimsOp>(range_n, 0),\n                                      CreateConst(b, i32_ty, n, shape_1_n));\n   auto mask = b.create<ma::AndIOp>(\n-      build_bcast(rm_cmp.getResult().cast<TensorValue>(), shape_m_n),\n-      build_bcast(rn_cmp.getResult().cast<TensorValue>(), shape_m_n));\n+      build_bcast(rm_cmp.getResult().template cast<TensorValue>(), shape_m_n),\n+      build_bcast(rn_cmp.getResult().template cast<TensorValue>(), shape_m_n));\n \n   b.create<mt::StoreOp>(out_ptrs, Cast(b, loc, acc_final, root_ty), mask,\n                         mt::CacheModifier::NONE, mt::EvictionPolicy::NORMAL);\n   return launch_dimensions;\n }\n \n+}  // namespace\n+\n+StatusOr<LaunchDimensions> MatMul(\n+    mlir::OpBuilder builder, const HloDotInstruction* dot_instr,\n+    mlir::triton::FuncOp fn,\n+    const tensorflow::AutotuneResult::TritonGemmKey& config, int shmem_budget) {\n+  // Use 32-bit indexing if addressing any of the inputs or the output (which\n+  // could grow if split_k is set) does not cross the INT_MAX boundary.\n+  // Otherwise, fall back to 64-bit indexing, which is slower.\n+  bool use_64bit_indexing =\n+      ShapeUtil::ElementsIn(dot_instr->operand(0)->shape()) > INT_MAX ||\n+      ShapeUtil::ElementsIn(dot_instr->operand(1)->shape()) > INT_MAX ||\n+      ShapeUtil::ElementsIn(dot_instr->shape()) * config.split_k() > INT_MAX;\n+  if (use_64bit_indexing) {\n+    return MatMulImpl<int64_t>(builder, dot_instr, fn, config, shmem_budget);\n+  } else {\n+    return MatMulImpl<int32_t>(builder, dot_instr, fn, config, shmem_budget);\n+  }\n+}\n+\n StatusOr<LaunchDimensions> TritonWrapper(\n     absl::string_view fn_name, const HloComputation* hlo_computation,\n     const se::CudaComputeCapability& cc, const GpuDeviceInfo& device_info,\n"
        },
        {
            "name": "ir_emitter_triton_large_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/ir_emitter_triton_large_test.cc",
            "patches": [
                {
                    "old_start": 0,
                    "old_length": 0,
                    "new_start": 1,
                    "new_length": 63,
                    "hunk": "@@ -0,0 +1,63 @@\n+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/compiler/xla/error_spec.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/tests/gpu_codegen_test.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class CompareTest : public GpuCodegenTest {};\n+\n+TEST_F(CompareTest, IndexUsing64Bits) {\n+  const char* hlo_text_ref = R\"(\n+HloModule r\n+\n+ENTRY e {\n+  arg0 = f16[65536,32800] parameter(0)\n+  arg1 = f16[32800,32] parameter(1)\n+  ROOT custom-call = f16[65536,32] custom-call(arg0, arg1),\n+    custom_call_target=\"__cublas$gemm\",\n+    backend_config=\"{\\\"alpha_real\\\":1,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"alpha_imag\\\":0,\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\"\n+}\n+)\";\n+\n+  const char* hlo_text_triton = R\"(\n+HloModule t\n+\n+triton_dot {\n+  p0 = f16[65536,32800] parameter(0)\n+  p1 = f16[32800,32] parameter(1)\n+  ROOT dot = f16[65536,32] dot(p0, p1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+\n+ENTRY e {\n+  p0 = f16[65536,32800] parameter(0)\n+  p1 = f16[32800,32] parameter(1)\n+  ROOT _ = f16[65536,32] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n+    backend_config=\"{\\\"block_m\\\":\\\"32\\\",\\\"block_n\\\":\\\"32\\\",\\\"block_k\\\":\\\"32\\\",\\\"split_k\\\":\\\"1\\\",\\\"num_stages\\\":\\\"1\\\",\\\"num_warps\\\":\\\"1\\\"}\"\n+}\n+)\";\n+\n+  EXPECT_TRUE(RunAndCompareTwoModules(hlo_text_ref, hlo_text_triton,\n+                                      ErrorSpec{1e-3, 1e-3},\n+                                      /*run_hlo_passes=*/false));\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/compiler/xla/error_spec.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/tests/gpu_codegen_test.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class CompareTest : public GpuCodegenTest {};\n+\n+TEST_F(CompareTest, IndexUsing64Bits) {\n+  const char* hlo_text_ref = R\"(\n+HloModule r\n+\n+ENTRY e {\n+  arg0 = f16[65536,32800] parameter(0)\n+  arg1 = f16[32800,32] parameter(1)\n+  ROOT custom-call = f16[65536,32] custom-call(arg0, arg1),\n+    custom_call_target=\"__cublas$gemm\",\n+    backend_config=\"{\\\"alpha_real\\\":1,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"alpha_imag\\\":0,\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\"\n+}\n+)\";\n+\n+  const char* hlo_text_triton = R\"(\n+HloModule t\n+\n+triton_dot {\n+  p0 = f16[65536,32800] parameter(0)\n+  p1 = f16[32800,32] parameter(1)\n+  ROOT dot = f16[65536,32] dot(p0, p1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+\n+ENTRY e {\n+  p0 = f16[65536,32800] parameter(0)\n+  p1 = f16[32800,32] parameter(1)\n+  ROOT _ = f16[65536,32] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n+    backend_config=\"{\\\"block_m\\\":\\\"32\\\",\\\"block_n\\\":\\\"32\\\",\\\"block_k\\\":\\\"32\\\",\\\"split_k\\\":\\\"1\\\",\\\"num_stages\\\":\\\"1\\\",\\\"num_warps\\\":\\\"1\\\"}\"\n+}\n+)\";\n+\n+  EXPECT_TRUE(RunAndCompareTwoModules(hlo_text_ref, hlo_text_triton,\n+                                      ErrorSpec{1e-3, 1e-3},\n+                                      /*run_hlo_passes=*/false));\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla\n",
            "whole_hunk": "@@ -0,0 +1,63 @@\n+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/compiler/xla/error_spec.h\"\n+#include \"tensorflow/compiler/xla/service/gpu/tests/gpu_codegen_test.h\"\n+\n+namespace xla {\n+namespace gpu {\n+namespace {\n+\n+class CompareTest : public GpuCodegenTest {};\n+\n+TEST_F(CompareTest, IndexUsing64Bits) {\n+  const char* hlo_text_ref = R\"(\n+HloModule r\n+\n+ENTRY e {\n+  arg0 = f16[65536,32800] parameter(0)\n+  arg1 = f16[32800,32] parameter(1)\n+  ROOT custom-call = f16[65536,32] custom-call(arg0, arg1),\n+    custom_call_target=\"__cublas$gemm\",\n+    backend_config=\"{\\\"alpha_real\\\":1,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"alpha_imag\\\":0,\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\"\n+}\n+)\";\n+\n+  const char* hlo_text_triton = R\"(\n+HloModule t\n+\n+triton_dot {\n+  p0 = f16[65536,32800] parameter(0)\n+  p1 = f16[32800,32] parameter(1)\n+  ROOT dot = f16[65536,32] dot(p0, p1),\n+    lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+}\n+\n+ENTRY e {\n+  p0 = f16[65536,32800] parameter(0)\n+  p1 = f16[32800,32] parameter(1)\n+  ROOT _ = f16[65536,32] fusion(p0, p1), kind=kCustom, calls=triton_dot,\n+    backend_config=\"{\\\"block_m\\\":\\\"32\\\",\\\"block_n\\\":\\\"32\\\",\\\"block_k\\\":\\\"32\\\",\\\"split_k\\\":\\\"1\\\",\\\"num_stages\\\":\\\"1\\\",\\\"num_warps\\\":\\\"1\\\"}\"\n+}\n+)\";\n+\n+  EXPECT_TRUE(RunAndCompareTwoModules(hlo_text_ref, hlo_text_triton,\n+                                      ErrorSpec{1e-3, 1e-3},\n+                                      /*run_hlo_passes=*/false));\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        }
    ]
},
{
    "Id": 613,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7b9d773d28ba4062b79a75408f58a2143e80d64d",
    "date": "2022-12-09T18:22:31-08:00",
    "message": "Disable fallback legalization for ops containing a list of SymbolRef attributes\n\nFallback legalization is not supported for such ops and this check makes sure that it is not attempted even if the op is added to the allow list.\n\nPiperOrigin-RevId: 494306878",
    "label": "NO",
    "changes": [
        {
            "name": "legalize-tf-with-tf2xla.mlir",
            "path": "tensorflow/compiler/mlir/xla/tests/legalize-tf-with-tf2xla.mlir",
            "patches": [
                {
                    "old_start": 357,
                    "old_length": 6,
                    "new_start": 357,
                    "new_length": 16,
                    "hunk": "@@ -357,6 +357,16 @@ func.func @atan2_with_symbol_ref(%arg0: tensor<2xf32>) -> tensor<2xf32> {\n   func.return %0 : tensor<2xf32>\n }\n \n+func.func private @branch0(tensor<2xf32>) -> tensor<2xf32>\n+func.func private @branch1(tensor<2xf32>) -> tensor<2xf32>\n+\n+func.func @case_with_symbol_ref(%arg0: tensor<i32>, %arg1: tensor<2xf32>) -> tensor<2xf32> {\n+  // CHECK: tf.Case\n+  // expected-remark@+1 {{ops with symbol references are not supported}}\n+  %0 = \"tf.Case\"(%arg0, %arg1) {branches = [@branch0, @branch1], is_stateless = false} : (tensor<i32>, tensor<2xf32>) -> tensor<2xf32>\n+  func.return %0 : tensor<2xf32>\n+}\n+\n // CHECK-LABEL: const\n func.func @const() -> tensor<2xf32> {\n   // CHECK: mhlo.const\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+func.func private @branch0(tensor<2xf32>) -> tensor<2xf32>\n+func.func private @branch1(tensor<2xf32>) -> tensor<2xf32>\n+\n+func.func @case_with_symbol_ref(%arg0: tensor<i32>, %arg1: tensor<2xf32>) -> tensor<2xf32> {\n+  // CHECK: tf.Case\n+  // expected-remark@+1 {{ops with symbol references are not supported}}\n+  %0 = \"tf.Case\"(%arg0, %arg1) {branches = [@branch0, @branch1], is_stateless = false} : (tensor<i32>, tensor<2xf32>) -> tensor<2xf32>\n+  func.return %0 : tensor<2xf32>\n+}\n+\n",
            "whole_hunk": "@@ -357,6 +357,16 @@ func.func @atan2_with_symbol_ref(%arg0: tensor<2xf32>) -> tensor<2xf32> {\n   func.return %0 : tensor<2xf32>\n }\n \n+func.func private @branch0(tensor<2xf32>) -> tensor<2xf32>\n+func.func private @branch1(tensor<2xf32>) -> tensor<2xf32>\n+\n+func.func @case_with_symbol_ref(%arg0: tensor<i32>, %arg1: tensor<2xf32>) -> tensor<2xf32> {\n+  // CHECK: tf.Case\n+  // expected-remark@+1 {{ops with symbol references are not supported}}\n+  %0 = \"tf.Case\"(%arg0, %arg1) {branches = [@branch0, @branch1], is_stateless = false} : (tensor<i32>, tensor<2xf32>) -> tensor<2xf32>\n+  func.return %0 : tensor<2xf32>\n+}\n+\n // CHECK-LABEL: const\n func.func @const() -> tensor<2xf32> {\n   // CHECK: mhlo.const\n"
        },
        {
            "name": "legalize_tf_with_tf2xla.cc",
            "path": "tensorflow/compiler/mlir/xla/transforms/legalize_tf_with_tf2xla.cc",
            "patches": [
                {
                    "old_start": 406,
                    "old_length": 6,
                    "new_start": 406,
                    "new_length": 7,
                    "hunk": "@@ -406,6 +406,7 @@ bool IsOpAllowedForTesting(Operation* op) {\n       new llvm::SmallDenseSet<mlir::TypeID, 16>{\n     // Op used to verify handling of XlaExpression of kind constant.\n     TypeID::get<TF::ConstOp>(),\n+    TypeID::get<TF::CaseOp>(),\n   };\n   // clang-format on\n   auto abstractOp = op->getRegisteredInfo();\n"
                },
                {
                    "old_start": 559,
                    "old_length": 7,
                    "new_start": 560,
                    "new_length": 16,
                    "hunk": "@@ -559,7 +560,16 @@ LogicalResult Tf2XlaRewriter::LegalizeOp() {\n   }\n \n   for (const auto& attr : op_->getAttrs()) {\n-    if (attr.getValue().isa<SymbolRefAttr>()) {\n+    Attribute attr_value = attr.getValue();\n+    bool has_symbol_ref = false;\n+    if (attr_value.isa<SymbolRefAttr>()) {\n+      has_symbol_ref = true;\n+    } else if (auto array_attr = attr_value.dyn_cast<ArrayAttr>()) {\n+      if (!array_attr.empty() && array_attr.begin()->isa<SymbolRefAttr>()) {\n+        has_symbol_ref = true;\n+      }\n+    }\n+    if (has_symbol_ref) {\n       return op_->emitRemark()\n              << \"ops with symbol references are not supported\";\n     }"
                }
            ],
            "whole_deleted": "-    if (attr.getValue().isa<SymbolRefAttr>()) {\n",
            "whole_added": "+    TypeID::get<TF::CaseOp>(),\n+    Attribute attr_value = attr.getValue();\n+    bool has_symbol_ref = false;\n+    if (attr_value.isa<SymbolRefAttr>()) {\n+      has_symbol_ref = true;\n+    } else if (auto array_attr = attr_value.dyn_cast<ArrayAttr>()) {\n+      if (!array_attr.empty() && array_attr.begin()->isa<SymbolRefAttr>()) {\n+        has_symbol_ref = true;\n+      }\n+    }\n+    if (has_symbol_ref) {\n",
            "whole_hunk": "@@ -406,6 +406,7 @@ bool IsOpAllowedForTesting(Operation* op) {\n       new llvm::SmallDenseSet<mlir::TypeID, 16>{\n     // Op used to verify handling of XlaExpression of kind constant.\n     TypeID::get<TF::ConstOp>(),\n+    TypeID::get<TF::CaseOp>(),\n   };\n   // clang-format on\n   auto abstractOp = op->getRegisteredInfo();\n@@ -559,7 +560,16 @@ LogicalResult Tf2XlaRewriter::LegalizeOp() {\n   }\n \n   for (const auto& attr : op_->getAttrs()) {\n-    if (attr.getValue().isa<SymbolRefAttr>()) {\n+    Attribute attr_value = attr.getValue();\n+    bool has_symbol_ref = false;\n+    if (attr_value.isa<SymbolRefAttr>()) {\n+      has_symbol_ref = true;\n+    } else if (auto array_attr = attr_value.dyn_cast<ArrayAttr>()) {\n+      if (!array_attr.empty() && array_attr.begin()->isa<SymbolRefAttr>()) {\n+        has_symbol_ref = true;\n+      }\n+    }\n+    if (has_symbol_ref) {\n       return op_->emitRemark()\n              << \"ops with symbol references are not supported\";\n     }"
        }
    ]
},
{
    "Id": 375,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/586392c0fdff62e8086267a5bf8327bc372366e7",
    "date": "2023-07-04T00:39:52-07:00",
    "message": "Reland: Fix check in ShouldFuseInPlaceOp().\n\nWhen checking whether the consumer has an in-place operand that is shared with\nthe producer, it was missing the case that the in-place operand was the\nproducer itself.\n\nPiperOrigin-RevId: 545364078",
    "label": "YES",
    "changes": [
        {
            "name": "fusion_merger_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/fusion_merger_test.cc",
            "patches": [
                {
                    "old_start": 1061,
                    "old_length": 6,
                    "new_start": 1061,
                    "new_length": 72,
                    "hunk": "@@ -1061,6 +1061,72 @@ TEST_F(FusionMergerTest, IncompatibleNonTrivialHeroes) {\n   EXPECT_FALSE(fusion_merger_.Run(module.get()).value());\n }\n \n+TEST_F(FusionMergerTest, DoNotMergeDUSFusions) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    %fused_computation (param_0: f32[8], param_1.2: f32[], param_2.3: f32[8]) -> f32[8] {\n+      %param_0 = f32[8]{0} parameter(0)\n+      %param_2.3 = f32[8]{0} parameter(2)\n+      %slice.2 = f32[5]{0} slice(f32[8]{0} %param_2.3), slice={[0:5]}\n+      %param_1.2 = f32[] parameter(1)\n+      %broadcast.2 = f32[5]{0} broadcast(f32[] %param_1.2), dimensions={}\n+      %add.2 = f32[5]{0} add(f32[5]{0} %slice.2, f32[5]{0} %broadcast.2)\n+      %two.1 = s32[] constant(2)\n+      ROOT %dynamic-update-slice.2 = f32[8]{0} dynamic-update-slice(f32[8]{0} %param_0, f32[5]{0} %add.2, s32[] %two.1)\n+    }\n+\n+    %fused_computation.1 (param_0.1: f32[8], param_1.4: f32[6], param_2.6: f32[]) -> f32[8] {\n+      %param_0.1 = f32[8]{0} parameter(0)\n+      %param_1.4 = f32[6]{0} parameter(1)\n+      %param_2.6 = f32[] parameter(2)\n+      %broadcast.3 = f32[6]{0} broadcast(f32[] %param_2.6), dimensions={}\n+      %add.3 = f32[6]{0} add(f32[6]{0} %param_1.4, f32[6]{0} %broadcast.3)\n+      %three.1 = s32[] constant(3)\n+      ROOT %dynamic-update-slice.3 = f32[8]{0} dynamic-update-slice(f32[8]{0} %param_0.1, f32[6]{0} %add.3, s32[] %three.1)\n+    }\n+\n+    ENTRY %Test (parameter: f32[8]) -> f32[8] {\n+      %parameter = f32[8]{0} parameter(0)\n+      %slice.1 = f32[6]{0} slice(f32[8]{0} %parameter), slice={[0:6]}\n+      %one = f32[] constant(1)\n+      %fusion.1 = f32[8]{0} fusion(f32[8]{0} %parameter, f32[6]{0} %slice.1, f32[] %one), kind=kLoop, calls=%fused_computation.1\n+      ROOT %fusion = f32[8]{0} fusion(f32[8]{0} %fusion.1, f32[] %one, f32[8]{0} %parameter), kind=kLoop, calls=%fused_computation\n+    }\n+    )\")\n+                    .value();\n+  EXPECT_FALSE(fusion_merger_.Run(module.get()).value());\n+}\n+\n+TEST_F(FusionMergerTest, MergeDUSFusionWithElementwiseFusion) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    %fused_computation {\n+      %param_0 = f32[1,8]{1,0} parameter(0)\n+      %bitcast = f32[8]{0} bitcast(%param_0)\n+      ROOT %neg = f32[8]{0} negate(%bitcast)\n+    }\n+\n+    %fused_computation.1 {\n+      %param_0.1 = f32[8]{0} parameter(0)\n+      %param_1.4 = f32[5]{0} parameter(1)\n+      %three.1 = s32[] constant(3)\n+      %exp = f32[5]{0} exponential(%param_1.4)\n+      ROOT %dynamic-update-slice.3 = f32[8]{0} dynamic-update-slice(f32[8]{0} %param_0.1, f32[5]{0} %exp, s32[] %three.1)\n+    }\n+\n+    ENTRY %Test {\n+      %parameter = f32[5]{0} parameter(0)\n+      %parameter.1 = f32[1,8]{1,0} parameter(1)\n+      %fusion = f32[8]{0} fusion(f32[1,8]{1,0} %parameter.1), kind=kLoop, calls=%fused_computation\n+      ROOT %fusion.1 = f32[8]{0} fusion(f32[8]{0} %fusion, f32[5]{0} %parameter), kind=kLoop, calls=%fused_computation.1\n+    }\n+    )\")\n+                    .value();\n+  EXPECT_TRUE(fusion_merger_.Run(module.get()).value());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(FusionMergerTest, DoNotMergeDUSFusions) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    %fused_computation (param_0: f32[8], param_1.2: f32[], param_2.3: f32[8]) -> f32[8] {\n+      %param_0 = f32[8]{0} parameter(0)\n+      %param_2.3 = f32[8]{0} parameter(2)\n+      %slice.2 = f32[5]{0} slice(f32[8]{0} %param_2.3), slice={[0:5]}\n+      %param_1.2 = f32[] parameter(1)\n+      %broadcast.2 = f32[5]{0} broadcast(f32[] %param_1.2), dimensions={}\n+      %add.2 = f32[5]{0} add(f32[5]{0} %slice.2, f32[5]{0} %broadcast.2)\n+      %two.1 = s32[] constant(2)\n+      ROOT %dynamic-update-slice.2 = f32[8]{0} dynamic-update-slice(f32[8]{0} %param_0, f32[5]{0} %add.2, s32[] %two.1)\n+    }\n+\n+    %fused_computation.1 (param_0.1: f32[8], param_1.4: f32[6], param_2.6: f32[]) -> f32[8] {\n+      %param_0.1 = f32[8]{0} parameter(0)\n+      %param_1.4 = f32[6]{0} parameter(1)\n+      %param_2.6 = f32[] parameter(2)\n+      %broadcast.3 = f32[6]{0} broadcast(f32[] %param_2.6), dimensions={}\n+      %add.3 = f32[6]{0} add(f32[6]{0} %param_1.4, f32[6]{0} %broadcast.3)\n+      %three.1 = s32[] constant(3)\n+      ROOT %dynamic-update-slice.3 = f32[8]{0} dynamic-update-slice(f32[8]{0} %param_0.1, f32[6]{0} %add.3, s32[] %three.1)\n+    }\n+\n+    ENTRY %Test (parameter: f32[8]) -> f32[8] {\n+      %parameter = f32[8]{0} parameter(0)\n+      %slice.1 = f32[6]{0} slice(f32[8]{0} %parameter), slice={[0:6]}\n+      %one = f32[] constant(1)\n+      %fusion.1 = f32[8]{0} fusion(f32[8]{0} %parameter, f32[6]{0} %slice.1, f32[] %one), kind=kLoop, calls=%fused_computation.1\n+      ROOT %fusion = f32[8]{0} fusion(f32[8]{0} %fusion.1, f32[] %one, f32[8]{0} %parameter), kind=kLoop, calls=%fused_computation\n+    }\n+    )\")\n+                    .value();\n+  EXPECT_FALSE(fusion_merger_.Run(module.get()).value());\n+}\n+\n+TEST_F(FusionMergerTest, MergeDUSFusionWithElementwiseFusion) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    %fused_computation {\n+      %param_0 = f32[1,8]{1,0} parameter(0)\n+      %bitcast = f32[8]{0} bitcast(%param_0)\n+      ROOT %neg = f32[8]{0} negate(%bitcast)\n+    }\n+\n+    %fused_computation.1 {\n+      %param_0.1 = f32[8]{0} parameter(0)\n+      %param_1.4 = f32[5]{0} parameter(1)\n+      %three.1 = s32[] constant(3)\n+      %exp = f32[5]{0} exponential(%param_1.4)\n+      ROOT %dynamic-update-slice.3 = f32[8]{0} dynamic-update-slice(f32[8]{0} %param_0.1, f32[5]{0} %exp, s32[] %three.1)\n+    }\n+\n+    ENTRY %Test {\n+      %parameter = f32[5]{0} parameter(0)\n+      %parameter.1 = f32[1,8]{1,0} parameter(1)\n+      %fusion = f32[8]{0} fusion(f32[1,8]{1,0} %parameter.1), kind=kLoop, calls=%fused_computation\n+      ROOT %fusion.1 = f32[8]{0} fusion(f32[8]{0} %fusion, f32[5]{0} %parameter), kind=kLoop, calls=%fused_computation.1\n+    }\n+    )\")\n+                    .value();\n+  EXPECT_TRUE(fusion_merger_.Run(module.get()).value());\n+}\n+\n",
            "whole_hunk": "@@ -1061,6 +1061,72 @@ TEST_F(FusionMergerTest, IncompatibleNonTrivialHeroes) {\n   EXPECT_FALSE(fusion_merger_.Run(module.get()).value());\n }\n \n+TEST_F(FusionMergerTest, DoNotMergeDUSFusions) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    %fused_computation (param_0: f32[8], param_1.2: f32[], param_2.3: f32[8]) -> f32[8] {\n+      %param_0 = f32[8]{0} parameter(0)\n+      %param_2.3 = f32[8]{0} parameter(2)\n+      %slice.2 = f32[5]{0} slice(f32[8]{0} %param_2.3), slice={[0:5]}\n+      %param_1.2 = f32[] parameter(1)\n+      %broadcast.2 = f32[5]{0} broadcast(f32[] %param_1.2), dimensions={}\n+      %add.2 = f32[5]{0} add(f32[5]{0} %slice.2, f32[5]{0} %broadcast.2)\n+      %two.1 = s32[] constant(2)\n+      ROOT %dynamic-update-slice.2 = f32[8]{0} dynamic-update-slice(f32[8]{0} %param_0, f32[5]{0} %add.2, s32[] %two.1)\n+    }\n+\n+    %fused_computation.1 (param_0.1: f32[8], param_1.4: f32[6], param_2.6: f32[]) -> f32[8] {\n+      %param_0.1 = f32[8]{0} parameter(0)\n+      %param_1.4 = f32[6]{0} parameter(1)\n+      %param_2.6 = f32[] parameter(2)\n+      %broadcast.3 = f32[6]{0} broadcast(f32[] %param_2.6), dimensions={}\n+      %add.3 = f32[6]{0} add(f32[6]{0} %param_1.4, f32[6]{0} %broadcast.3)\n+      %three.1 = s32[] constant(3)\n+      ROOT %dynamic-update-slice.3 = f32[8]{0} dynamic-update-slice(f32[8]{0} %param_0.1, f32[6]{0} %add.3, s32[] %three.1)\n+    }\n+\n+    ENTRY %Test (parameter: f32[8]) -> f32[8] {\n+      %parameter = f32[8]{0} parameter(0)\n+      %slice.1 = f32[6]{0} slice(f32[8]{0} %parameter), slice={[0:6]}\n+      %one = f32[] constant(1)\n+      %fusion.1 = f32[8]{0} fusion(f32[8]{0} %parameter, f32[6]{0} %slice.1, f32[] %one), kind=kLoop, calls=%fused_computation.1\n+      ROOT %fusion = f32[8]{0} fusion(f32[8]{0} %fusion.1, f32[] %one, f32[8]{0} %parameter), kind=kLoop, calls=%fused_computation\n+    }\n+    )\")\n+                    .value();\n+  EXPECT_FALSE(fusion_merger_.Run(module.get()).value());\n+}\n+\n+TEST_F(FusionMergerTest, MergeDUSFusionWithElementwiseFusion) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    %fused_computation {\n+      %param_0 = f32[1,8]{1,0} parameter(0)\n+      %bitcast = f32[8]{0} bitcast(%param_0)\n+      ROOT %neg = f32[8]{0} negate(%bitcast)\n+    }\n+\n+    %fused_computation.1 {\n+      %param_0.1 = f32[8]{0} parameter(0)\n+      %param_1.4 = f32[5]{0} parameter(1)\n+      %three.1 = s32[] constant(3)\n+      %exp = f32[5]{0} exponential(%param_1.4)\n+      ROOT %dynamic-update-slice.3 = f32[8]{0} dynamic-update-slice(f32[8]{0} %param_0.1, f32[5]{0} %exp, s32[] %three.1)\n+    }\n+\n+    ENTRY %Test {\n+      %parameter = f32[5]{0} parameter(0)\n+      %parameter.1 = f32[1,8]{1,0} parameter(1)\n+      %fusion = f32[8]{0} fusion(f32[1,8]{1,0} %parameter.1), kind=kLoop, calls=%fused_computation\n+      ROOT %fusion.1 = f32[8]{0} fusion(f32[8]{0} %fusion, f32[5]{0} %parameter), kind=kLoop, calls=%fused_computation.1\n+    }\n+    )\")\n+                    .value();\n+  EXPECT_TRUE(fusion_merger_.Run(module.get()).value());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla\n"
        },
        {
            "name": "instruction_fusion.cc",
            "path": "tensorflow/compiler/xla/service/instruction_fusion.cc",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 6,
                    "new_start": 20,
                    "new_length": 7,
                    "hunk": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <list>\n #include <memory>\n #include <numeric>\n+#include <optional>\n #include <string>\n #include <utility>\n #include <vector>\n"
                },
                {
                    "old_start": 835,
                    "old_length": 33,
                    "new_start": 836,
                    "new_length": 28,
                    "hunk": "@@ -835,33 +836,28 @@ bool InstructionFusion::MultiOutputFusionCreatesCycle(\n \n namespace {\n \n-// Extracts instruction from the fusion that satisfies filter. If no or multiple\n-// instructions in the fusion satisfy filter, returns nullptr.\n-const HloInstruction* ExtractInstruction(const HloInstruction* hlo,\n-                                         const HloPredicate& filter) {\n+// Extracts instructions from the fusion that satisfy the filter.\n+std::vector<const HloInstruction*> ExtractInstructions(\n+    const HloInstruction* hlo, const HloPredicate& filter) {\n   if (filter(hlo)) {\n-    return hlo;\n+    return {hlo};\n   }\n   if (hlo->opcode() != HloOpcode::kFusion) {\n-    return nullptr;\n+    return {};\n   }\n-  const HloInstruction* match = nullptr;\n+  std::vector<const HloInstruction*> matches;\n   for (HloInstruction* inst :\n        hlo->fused_instructions_computation()->instructions()) {\n     if (filter(inst)) {\n-      if (match == nullptr) {\n-        match = inst;\n-      } else {\n-        return nullptr;\n-      }\n+      matches.push_back(inst);\n     }\n   }\n-  return match;\n+  return matches;\n }\n \n-const HloInstruction* ExtractInstruction(const HloInstruction* hlo,\n-                                         HloOpcode opcode) {\n-  return ExtractInstruction(hlo, [opcode](const HloInstruction* inst) {\n+std::vector<const HloInstruction*> ExtractInstructions(\n+    const HloInstruction* hlo, HloOpcode opcode) {\n+  return ExtractInstructions(hlo, [opcode](const HloInstruction* inst) {\n     return inst->opcode() == opcode;\n   });\n }\n"
                },
                {
                    "old_start": 871,
                    "old_length": 12,
                    "new_start": 867,
                    "new_length": 8,
                    "hunk": "@@ -871,12 +867,8 @@ const HloInstruction* ExtractInstruction(const HloInstruction* hlo,\n // DUS when fusing will cause another non-elementwise op to share operands with\n // the DUS in-place buffer.\n bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,\n-                                    const HloInstruction* consumer) {\n-  CHECK_EQ(consumer->opcode(), HloOpcode::kFusion);\n-  const HloInstruction* dus =\n-      ExtractInstruction(consumer, HloOpcode::kDynamicUpdateSlice);\n-  CHECK_NE(dus, nullptr);\n-\n+                                    const HloInstruction* consumer,\n+                                    const HloInstruction* dus) {\n   // Use a memoization map to avoid exponential runtime.\n   absl::flat_hash_map<const HloInstruction*, bool> nonelementwise_memo;\n   // Recursively check if the instruction or its users (or their users) are\n"
                },
                {
                    "old_start": 936,
                    "old_length": 9,
                    "new_start": 928,
                    "new_length": 11,
                    "hunk": "@@ -936,9 +928,11 @@ bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,\n       }\n     }\n     if (!producer->IsElementwise() &&\n-        absl::c_find(producer->operands(), consumer->operand(operand_number)) !=\n-            producer->operands().end()) {\n-      VLOG(4) << \"Found non-elementwise operand that uses the same operand of \"\n+        (consumer->operand(operand_number) == producer ||\n+         absl::c_find(producer->operands(),\n+                      consumer->operand(operand_number)) !=\n+             producer->operands().end())) {\n+      VLOG(4) << \"Found non-elementwise producer that uses the same operand of \"\n                  \"an in-place consumer\";\n       auto get_real_operand = [](const HloInstruction* op,\n                                  const HloInstruction* operand) {\n"
                },
                {
                    "old_start": 958,
                    "old_length": 19,
                    "new_start": 952,
                    "new_length": 40,
                    "hunk": "@@ -958,19 +952,40 @@ bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,\n       };\n       // A common special case is a slice or dynamic-slice and a\n       // dynamic-update-slice that use the same indices. This pattern is safe.\n-      const HloInstruction* dus =\n-          ExtractInstruction(consumer, HloOpcode::kDynamicUpdateSlice);\n-      const HloInstruction* producer_nonelementwise =\n-          ExtractInstruction(producer, [](const HloInstruction* inst) {\n+\n+      auto producer_nonelementwise_ops =\n+          ExtractInstructions(producer, [](const HloInstruction* inst) {\n             return inst->opcode() != HloOpcode::kFusion &&\n-                   !inst->IsElementwise();\n+                   !inst->IsElementwise() &&\n+                   inst->opcode() != HloOpcode::kBitcast &&\n+                   inst->opcode() != HloOpcode::kParameter;\n           });\n-      if (dus == nullptr || producer_nonelementwise == nullptr ||\n-          producer_nonelementwise->shape() != dus->operand(1)->shape()) {\n-        return \"Consumer is not a dus or the producer fusion has multiple \"\n-               \"non-elementwise ops, bailing.\";\n+      if (producer_nonelementwise_ops.size() > 1) {\n+        return \"Producer fusion has multiple non-elementwise ops, bailing.\";\n+      }\n+      // If the producer has only elementwise ops or bitcasts, we can fuse.\n+      if (producer_nonelementwise_ops.empty()) {\n+        return {};\n       }\n+      auto dus_ops =\n+          ExtractInstructions(consumer, HloOpcode::kDynamicUpdateSlice);\n+      // Check whether we have a DUS, in which case we should not fuse to make\n+      // sure we can compute the fusion in place.\n+      // TODO(akuegel): Are there other ops than dynamic update slice where we\n+      // have a special emitter if it can be done in-place?\n+      if (dus_ops.empty()) {\n+        return {};\n+      }\n+      if (dus_ops.size() > 1) {\n+        return \"multiple dus ops, bailing.\";\n+      }\n+      auto dus = dus_ops[0];\n+      auto producer_nonelementwise = producer_nonelementwise_ops[0];\n       if (producer_nonelementwise->opcode() == HloOpcode::kSlice) {\n+        if (producer_nonelementwise->shape() != dus->operand(1)->shape()) {\n+          return \"Slice op has a different shape than the update shape of the \"\n+                 \"dus op, bailing.\";\n+        }\n         for (int i = 0; i < dus->shape().rank(); ++i) {\n           const HloInstruction* dus_operand =\n               get_real_operand(consumer, dus->operand(2 + i));\n"
                },
                {
                    "old_start": 983,
                    "old_length": 13,
                    "new_start": 998,
                    "new_length": 17,
                    "hunk": "@@ -983,13 +998,17 @@ bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,\n         }\n         VLOG(4) << \"DUS and slice index match\";\n         if (consumer->opcode() == HloOpcode::kFusion &&\n-            !IsSafeToFuseSliceIntoDusFusion(producer, consumer)) {\n+            !IsSafeToFuseSliceIntoDusFusion(producer, consumer, dus)) {\n           return \"Fusing slice into DUS will also fuse another non-elementwise \"\n                  \"op with shared operand as DUS.\";\n         }\n         return {};\n       }\n       if (producer_nonelementwise->opcode() == HloOpcode::kDynamicSlice) {\n+        if (producer_nonelementwise->shape() != dus->operand(1)->shape()) {\n+          return \"Dynamic slice op has a different shape than the update shape \"\n+                 \"of the dus op, bailing.\";\n+        }\n         for (int i = 0; i < dus->shape().rank(); ++i) {\n           const HloInstruction* ds_operand = get_real_operand(\n               producer, producer_nonelementwise->operand(1 + i));\n"
                },
                {
                    "old_start": 1004,
                    "old_length": 7,
                    "new_start": 1023,
                    "new_length": 7,
                    "hunk": "@@ -1004,7 +1023,7 @@ bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,\n         }\n         VLOG(4) << \"DUS and DS index match\";\n         if (consumer->opcode() == HloOpcode::kFusion &&\n-            !IsSafeToFuseSliceIntoDusFusion(producer, consumer)) {\n+            !IsSafeToFuseSliceIntoDusFusion(producer, consumer, dus)) {\n           return \"Fusing DS into DUS will also fuse another non-elementwise op \"\n                  \"with shared operand as DUS.\";\n         }\n"
                }
            ],
            "whole_deleted": "-// Extracts instruction from the fusion that satisfies filter. If no or multiple\n-// instructions in the fusion satisfy filter, returns nullptr.\n-const HloInstruction* ExtractInstruction(const HloInstruction* hlo,\n-                                         const HloPredicate& filter) {\n-    return hlo;\n-    return nullptr;\n-  const HloInstruction* match = nullptr;\n-      if (match == nullptr) {\n-        match = inst;\n-      } else {\n-        return nullptr;\n-      }\n-  return match;\n-const HloInstruction* ExtractInstruction(const HloInstruction* hlo,\n-                                         HloOpcode opcode) {\n-  return ExtractInstruction(hlo, [opcode](const HloInstruction* inst) {\n-                                    const HloInstruction* consumer) {\n-  CHECK_EQ(consumer->opcode(), HloOpcode::kFusion);\n-  const HloInstruction* dus =\n-      ExtractInstruction(consumer, HloOpcode::kDynamicUpdateSlice);\n-  CHECK_NE(dus, nullptr);\n-\n-        absl::c_find(producer->operands(), consumer->operand(operand_number)) !=\n-            producer->operands().end()) {\n-      VLOG(4) << \"Found non-elementwise operand that uses the same operand of \"\n-      const HloInstruction* dus =\n-          ExtractInstruction(consumer, HloOpcode::kDynamicUpdateSlice);\n-      const HloInstruction* producer_nonelementwise =\n-          ExtractInstruction(producer, [](const HloInstruction* inst) {\n-                   !inst->IsElementwise();\n-      if (dus == nullptr || producer_nonelementwise == nullptr ||\n-          producer_nonelementwise->shape() != dus->operand(1)->shape()) {\n-        return \"Consumer is not a dus or the producer fusion has multiple \"\n-               \"non-elementwise ops, bailing.\";\n-            !IsSafeToFuseSliceIntoDusFusion(producer, consumer)) {\n-            !IsSafeToFuseSliceIntoDusFusion(producer, consumer)) {\n",
            "whole_added": "+#include <optional>\n+// Extracts instructions from the fusion that satisfy the filter.\n+std::vector<const HloInstruction*> ExtractInstructions(\n+    const HloInstruction* hlo, const HloPredicate& filter) {\n+    return {hlo};\n+    return {};\n+  std::vector<const HloInstruction*> matches;\n+      matches.push_back(inst);\n+  return matches;\n+std::vector<const HloInstruction*> ExtractInstructions(\n+    const HloInstruction* hlo, HloOpcode opcode) {\n+  return ExtractInstructions(hlo, [opcode](const HloInstruction* inst) {\n+                                    const HloInstruction* consumer,\n+                                    const HloInstruction* dus) {\n+        (consumer->operand(operand_number) == producer ||\n+         absl::c_find(producer->operands(),\n+                      consumer->operand(operand_number)) !=\n+             producer->operands().end())) {\n+      VLOG(4) << \"Found non-elementwise producer that uses the same operand of \"\n+\n+      auto producer_nonelementwise_ops =\n+          ExtractInstructions(producer, [](const HloInstruction* inst) {\n+                   !inst->IsElementwise() &&\n+                   inst->opcode() != HloOpcode::kBitcast &&\n+                   inst->opcode() != HloOpcode::kParameter;\n+      if (producer_nonelementwise_ops.size() > 1) {\n+        return \"Producer fusion has multiple non-elementwise ops, bailing.\";\n+      }\n+      // If the producer has only elementwise ops or bitcasts, we can fuse.\n+      if (producer_nonelementwise_ops.empty()) {\n+        return {};\n+      auto dus_ops =\n+          ExtractInstructions(consumer, HloOpcode::kDynamicUpdateSlice);\n+      // Check whether we have a DUS, in which case we should not fuse to make\n+      // sure we can compute the fusion in place.\n+      // TODO(akuegel): Are there other ops than dynamic update slice where we\n+      // have a special emitter if it can be done in-place?\n+      if (dus_ops.empty()) {\n+        return {};\n+      }\n+      if (dus_ops.size() > 1) {\n+        return \"multiple dus ops, bailing.\";\n+      }\n+      auto dus = dus_ops[0];\n+      auto producer_nonelementwise = producer_nonelementwise_ops[0];\n+        if (producer_nonelementwise->shape() != dus->operand(1)->shape()) {\n+          return \"Slice op has a different shape than the update shape of the \"\n+                 \"dus op, bailing.\";\n+        }\n+            !IsSafeToFuseSliceIntoDusFusion(producer, consumer, dus)) {\n+        if (producer_nonelementwise->shape() != dus->operand(1)->shape()) {\n+          return \"Dynamic slice op has a different shape than the update shape \"\n+                 \"of the dus op, bailing.\";\n+        }\n+            !IsSafeToFuseSliceIntoDusFusion(producer, consumer, dus)) {\n",
            "whole_hunk": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <list>\n #include <memory>\n #include <numeric>\n+#include <optional>\n #include <string>\n #include <utility>\n #include <vector>\n@@ -835,33 +836,28 @@ bool InstructionFusion::MultiOutputFusionCreatesCycle(\n \n namespace {\n \n-// Extracts instruction from the fusion that satisfies filter. If no or multiple\n-// instructions in the fusion satisfy filter, returns nullptr.\n-const HloInstruction* ExtractInstruction(const HloInstruction* hlo,\n-                                         const HloPredicate& filter) {\n+// Extracts instructions from the fusion that satisfy the filter.\n+std::vector<const HloInstruction*> ExtractInstructions(\n+    const HloInstruction* hlo, const HloPredicate& filter) {\n   if (filter(hlo)) {\n-    return hlo;\n+    return {hlo};\n   }\n   if (hlo->opcode() != HloOpcode::kFusion) {\n-    return nullptr;\n+    return {};\n   }\n-  const HloInstruction* match = nullptr;\n+  std::vector<const HloInstruction*> matches;\n   for (HloInstruction* inst :\n        hlo->fused_instructions_computation()->instructions()) {\n     if (filter(inst)) {\n-      if (match == nullptr) {\n-        match = inst;\n-      } else {\n-        return nullptr;\n-      }\n+      matches.push_back(inst);\n     }\n   }\n-  return match;\n+  return matches;\n }\n \n-const HloInstruction* ExtractInstruction(const HloInstruction* hlo,\n-                                         HloOpcode opcode) {\n-  return ExtractInstruction(hlo, [opcode](const HloInstruction* inst) {\n+std::vector<const HloInstruction*> ExtractInstructions(\n+    const HloInstruction* hlo, HloOpcode opcode) {\n+  return ExtractInstructions(hlo, [opcode](const HloInstruction* inst) {\n     return inst->opcode() == opcode;\n   });\n }\n@@ -871,12 +867,8 @@ const HloInstruction* ExtractInstruction(const HloInstruction* hlo,\n // DUS when fusing will cause another non-elementwise op to share operands with\n // the DUS in-place buffer.\n bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,\n-                                    const HloInstruction* consumer) {\n-  CHECK_EQ(consumer->opcode(), HloOpcode::kFusion);\n-  const HloInstruction* dus =\n-      ExtractInstruction(consumer, HloOpcode::kDynamicUpdateSlice);\n-  CHECK_NE(dus, nullptr);\n-\n+                                    const HloInstruction* consumer,\n+                                    const HloInstruction* dus) {\n   // Use a memoization map to avoid exponential runtime.\n   absl::flat_hash_map<const HloInstruction*, bool> nonelementwise_memo;\n   // Recursively check if the instruction or its users (or their users) are\n@@ -936,9 +928,11 @@ bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,\n       }\n     }\n     if (!producer->IsElementwise() &&\n-        absl::c_find(producer->operands(), consumer->operand(operand_number)) !=\n-            producer->operands().end()) {\n-      VLOG(4) << \"Found non-elementwise operand that uses the same operand of \"\n+        (consumer->operand(operand_number) == producer ||\n+         absl::c_find(producer->operands(),\n+                      consumer->operand(operand_number)) !=\n+             producer->operands().end())) {\n+      VLOG(4) << \"Found non-elementwise producer that uses the same operand of \"\n                  \"an in-place consumer\";\n       auto get_real_operand = [](const HloInstruction* op,\n                                  const HloInstruction* operand) {\n@@ -958,19 +952,40 @@ bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,\n       };\n       // A common special case is a slice or dynamic-slice and a\n       // dynamic-update-slice that use the same indices. This pattern is safe.\n-      const HloInstruction* dus =\n-          ExtractInstruction(consumer, HloOpcode::kDynamicUpdateSlice);\n-      const HloInstruction* producer_nonelementwise =\n-          ExtractInstruction(producer, [](const HloInstruction* inst) {\n+\n+      auto producer_nonelementwise_ops =\n+          ExtractInstructions(producer, [](const HloInstruction* inst) {\n             return inst->opcode() != HloOpcode::kFusion &&\n-                   !inst->IsElementwise();\n+                   !inst->IsElementwise() &&\n+                   inst->opcode() != HloOpcode::kBitcast &&\n+                   inst->opcode() != HloOpcode::kParameter;\n           });\n-      if (dus == nullptr || producer_nonelementwise == nullptr ||\n-          producer_nonelementwise->shape() != dus->operand(1)->shape()) {\n-        return \"Consumer is not a dus or the producer fusion has multiple \"\n-               \"non-elementwise ops, bailing.\";\n+      if (producer_nonelementwise_ops.size() > 1) {\n+        return \"Producer fusion has multiple non-elementwise ops, bailing.\";\n+      }\n+      // If the producer has only elementwise ops or bitcasts, we can fuse.\n+      if (producer_nonelementwise_ops.empty()) {\n+        return {};\n       }\n+      auto dus_ops =\n+          ExtractInstructions(consumer, HloOpcode::kDynamicUpdateSlice);\n+      // Check whether we have a DUS, in which case we should not fuse to make\n+      // sure we can compute the fusion in place.\n+      // TODO(akuegel): Are there other ops than dynamic update slice where we\n+      // have a special emitter if it can be done in-place?\n+      if (dus_ops.empty()) {\n+        return {};\n+      }\n+      if (dus_ops.size() > 1) {\n+        return \"multiple dus ops, bailing.\";\n+      }\n+      auto dus = dus_ops[0];\n+      auto producer_nonelementwise = producer_nonelementwise_ops[0];\n       if (producer_nonelementwise->opcode() == HloOpcode::kSlice) {\n+        if (producer_nonelementwise->shape() != dus->operand(1)->shape()) {\n+          return \"Slice op has a different shape than the update shape of the \"\n+                 \"dus op, bailing.\";\n+        }\n         for (int i = 0; i < dus->shape().rank(); ++i) {\n           const HloInstruction* dus_operand =\n               get_real_operand(consumer, dus->operand(2 + i));\n@@ -983,13 +998,17 @@ bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,\n         }\n         VLOG(4) << \"DUS and slice index match\";\n         if (consumer->opcode() == HloOpcode::kFusion &&\n-            !IsSafeToFuseSliceIntoDusFusion(producer, consumer)) {\n+            !IsSafeToFuseSliceIntoDusFusion(producer, consumer, dus)) {\n           return \"Fusing slice into DUS will also fuse another non-elementwise \"\n                  \"op with shared operand as DUS.\";\n         }\n         return {};\n       }\n       if (producer_nonelementwise->opcode() == HloOpcode::kDynamicSlice) {\n+        if (producer_nonelementwise->shape() != dus->operand(1)->shape()) {\n+          return \"Dynamic slice op has a different shape than the update shape \"\n+                 \"of the dus op, bailing.\";\n+        }\n         for (int i = 0; i < dus->shape().rank(); ++i) {\n           const HloInstruction* ds_operand = get_real_operand(\n               producer, producer_nonelementwise->operand(1 + i));\n@@ -1004,7 +1023,7 @@ bool IsSafeToFuseSliceIntoDusFusion(const HloInstruction* producer,\n         }\n         VLOG(4) << \"DUS and DS index match\";\n         if (consumer->opcode() == HloOpcode::kFusion &&\n-            !IsSafeToFuseSliceIntoDusFusion(producer, consumer)) {\n+            !IsSafeToFuseSliceIntoDusFusion(producer, consumer, dus)) {\n           return \"Fusing DS into DUS will also fuse another non-elementwise op \"\n                  \"with shared operand as DUS.\";\n         }\n"
        },
        {
            "name": "instruction_fusion_test.cc",
            "path": "tensorflow/compiler/xla/service/instruction_fusion_test.cc",
            "patches": [
                {
                    "old_start": 521,
                    "old_length": 6,
                    "new_start": 521,
                    "new_length": 66,
                    "hunk": "@@ -521,6 +521,66 @@ TEST_F(InstructionFusionTest,\n   EXPECT_THAT(root, op::Fusion(op::Parameter(), op::Slice()));\n }\n \n+TEST_F(InstructionFusionTest,\n+       DynamicUpdateSliceShouldNotFuseWithDynamicUpdateSlice) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule test_module\n+\n+  ENTRY Test {\n+    parameter = f32[8] parameter(0)\n+    slice = f32[5] slice(parameter), slice={[0:5]}\n+    one = f32[] constant(1)\n+    broadcast = f32[5] broadcast(one), dimensions={}\n+    add = f32[5] add(slice, broadcast)\n+    three = s32[] constant(3)\n+    update = f32[6] parameter(1)\n+    two = s32[] constant(2)\n+    dynamic-update-slice = f32[8] dynamic-update-slice(parameter, update, two)\n+    ROOT dynamic-update-slice.1 = f32[8] dynamic-update-slice(dynamic-update-slice, add, three)\n+  })\")\n+                    .value();\n+  EXPECT_TRUE(\n+      InstructionFusion(InstructionFusion::IsExpensive, /*may_duplicate=*/false)\n+          .Run(module.get())\n+          .value())\n+      << module->ToString();\n+  // Verify that we don't fuse both dynamic-update-slice ops together, as they\n+  // have different index values.\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root, op::Fusion(op::Fusion(), op::Parameter()));\n+  EXPECT_THAT(root->fused_expression_root(), op::DynamicUpdateSlice());\n+  EXPECT_THAT(root->operand(0)->fused_expression_root(),\n+              op::DynamicUpdateSlice());\n+}\n+\n+TEST_F(InstructionFusionTest, DynamicUpdateSliceShouldFuseWithElementwise) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule test_module\n+\n+  ENTRY Test {\n+    parameter = f32[8] parameter(0)\n+    slice = f32[5] slice(parameter), slice={[0:5]}\n+    one = f32[] constant(1)\n+    broadcast = f32[5] broadcast(one), dimensions={}\n+    add = f32[5] add(slice, broadcast)\n+    three = s32[] constant(3)\n+    parameter.1 = f32[1,8] parameter(1)\n+    bitcast = f32[8] bitcast(parameter.1)\n+    neg = f32[8] negate(bitcast)\n+    ROOT dynamic-update-slice.1 = f32[8] dynamic-update-slice(neg, add, three)\n+  })\")\n+                    .value();\n+  EXPECT_TRUE(\n+      InstructionFusion(InstructionFusion::IsExpensive, /*may_duplicate=*/false)\n+          .Run(module.get())\n+          .value())\n+      << module->ToString();\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root, op::Fusion(op::Parameter(), op::Parameter()));\n+  EXPECT_THAT(root->fused_expression_root(),\n+              op::DynamicUpdateSlice(op::Negate(), op::Add(), op::Constant()));\n+}\n+\n TEST_F(InstructionFusionTest, InPlaceOpShouldFuseWithSliceSameIndex) {\n   auto module = ParseAndReturnVerifiedModule(R\"(\n   HloModule test_module"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(InstructionFusionTest,\n+       DynamicUpdateSliceShouldNotFuseWithDynamicUpdateSlice) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule test_module\n+\n+  ENTRY Test {\n+    parameter = f32[8] parameter(0)\n+    slice = f32[5] slice(parameter), slice={[0:5]}\n+    one = f32[] constant(1)\n+    broadcast = f32[5] broadcast(one), dimensions={}\n+    add = f32[5] add(slice, broadcast)\n+    three = s32[] constant(3)\n+    update = f32[6] parameter(1)\n+    two = s32[] constant(2)\n+    dynamic-update-slice = f32[8] dynamic-update-slice(parameter, update, two)\n+    ROOT dynamic-update-slice.1 = f32[8] dynamic-update-slice(dynamic-update-slice, add, three)\n+  })\")\n+                    .value();\n+  EXPECT_TRUE(\n+      InstructionFusion(InstructionFusion::IsExpensive, /*may_duplicate=*/false)\n+          .Run(module.get())\n+          .value())\n+      << module->ToString();\n+  // Verify that we don't fuse both dynamic-update-slice ops together, as they\n+  // have different index values.\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root, op::Fusion(op::Fusion(), op::Parameter()));\n+  EXPECT_THAT(root->fused_expression_root(), op::DynamicUpdateSlice());\n+  EXPECT_THAT(root->operand(0)->fused_expression_root(),\n+              op::DynamicUpdateSlice());\n+}\n+\n+TEST_F(InstructionFusionTest, DynamicUpdateSliceShouldFuseWithElementwise) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule test_module\n+\n+  ENTRY Test {\n+    parameter = f32[8] parameter(0)\n+    slice = f32[5] slice(parameter), slice={[0:5]}\n+    one = f32[] constant(1)\n+    broadcast = f32[5] broadcast(one), dimensions={}\n+    add = f32[5] add(slice, broadcast)\n+    three = s32[] constant(3)\n+    parameter.1 = f32[1,8] parameter(1)\n+    bitcast = f32[8] bitcast(parameter.1)\n+    neg = f32[8] negate(bitcast)\n+    ROOT dynamic-update-slice.1 = f32[8] dynamic-update-slice(neg, add, three)\n+  })\")\n+                    .value();\n+  EXPECT_TRUE(\n+      InstructionFusion(InstructionFusion::IsExpensive, /*may_duplicate=*/false)\n+          .Run(module.get())\n+          .value())\n+      << module->ToString();\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root, op::Fusion(op::Parameter(), op::Parameter()));\n+  EXPECT_THAT(root->fused_expression_root(),\n+              op::DynamicUpdateSlice(op::Negate(), op::Add(), op::Constant()));\n+}\n+\n",
            "whole_hunk": "@@ -521,6 +521,66 @@ TEST_F(InstructionFusionTest,\n   EXPECT_THAT(root, op::Fusion(op::Parameter(), op::Slice()));\n }\n \n+TEST_F(InstructionFusionTest,\n+       DynamicUpdateSliceShouldNotFuseWithDynamicUpdateSlice) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule test_module\n+\n+  ENTRY Test {\n+    parameter = f32[8] parameter(0)\n+    slice = f32[5] slice(parameter), slice={[0:5]}\n+    one = f32[] constant(1)\n+    broadcast = f32[5] broadcast(one), dimensions={}\n+    add = f32[5] add(slice, broadcast)\n+    three = s32[] constant(3)\n+    update = f32[6] parameter(1)\n+    two = s32[] constant(2)\n+    dynamic-update-slice = f32[8] dynamic-update-slice(parameter, update, two)\n+    ROOT dynamic-update-slice.1 = f32[8] dynamic-update-slice(dynamic-update-slice, add, three)\n+  })\")\n+                    .value();\n+  EXPECT_TRUE(\n+      InstructionFusion(InstructionFusion::IsExpensive, /*may_duplicate=*/false)\n+          .Run(module.get())\n+          .value())\n+      << module->ToString();\n+  // Verify that we don't fuse both dynamic-update-slice ops together, as they\n+  // have different index values.\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root, op::Fusion(op::Fusion(), op::Parameter()));\n+  EXPECT_THAT(root->fused_expression_root(), op::DynamicUpdateSlice());\n+  EXPECT_THAT(root->operand(0)->fused_expression_root(),\n+              op::DynamicUpdateSlice());\n+}\n+\n+TEST_F(InstructionFusionTest, DynamicUpdateSliceShouldFuseWithElementwise) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+  HloModule test_module\n+\n+  ENTRY Test {\n+    parameter = f32[8] parameter(0)\n+    slice = f32[5] slice(parameter), slice={[0:5]}\n+    one = f32[] constant(1)\n+    broadcast = f32[5] broadcast(one), dimensions={}\n+    add = f32[5] add(slice, broadcast)\n+    three = s32[] constant(3)\n+    parameter.1 = f32[1,8] parameter(1)\n+    bitcast = f32[8] bitcast(parameter.1)\n+    neg = f32[8] negate(bitcast)\n+    ROOT dynamic-update-slice.1 = f32[8] dynamic-update-slice(neg, add, three)\n+  })\")\n+                    .value();\n+  EXPECT_TRUE(\n+      InstructionFusion(InstructionFusion::IsExpensive, /*may_duplicate=*/false)\n+          .Run(module.get())\n+          .value())\n+      << module->ToString();\n+  HloInstruction* root = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(root, op::Fusion(op::Parameter(), op::Parameter()));\n+  EXPECT_THAT(root->fused_expression_root(),\n+              op::DynamicUpdateSlice(op::Negate(), op::Add(), op::Constant()));\n+}\n+\n TEST_F(InstructionFusionTest, InPlaceOpShouldFuseWithSliceSameIndex) {\n   auto module = ParseAndReturnVerifiedModule(R\"(\n   HloModule test_module"
        }
    ]
},
{
    "Id": 657,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9bb678d2bd3d27e2ddc74fcf7270b0834cf008ad",
    "date": "2022-10-18T08:39:39-07:00",
    "message": "[xla:gpu:runtime] Remove incorrect channel id check\n\nChannel id properly handled by GetNcclCollectiveConfig and there is no need to pass it to the custom call handler.\n\nPiperOrigin-RevId: 481928094",
    "label": "NO",
    "changes": [
        {
            "name": "lmhlo_to_gpu_runtime.cc",
            "path": "tensorflow/compiler/xla/mlir/transforms/gpu/lmhlo_to_gpu_runtime.cc",
            "patches": [
                {
                    "old_start": 640,
                    "old_length": 10,
                    "new_start": 640,
                    "new_length": 6,
                    "hunk": "@@ -640,10 +640,6 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {\n \n   LogicalResult matchAndRewrite(CollectiveOp op,\n                                 PatternRewriter& rewriter) const override {\n-    // Channel ID is not supported.\n-    if (op.getChannelId().has_value())\n-      return op.emitOpError() << \"Collective channel id is not supported\";\n-\n     // Construct an NCCL collective config from the parent func attributes.\n     func::FuncOp fn = op->template getParentOfType<func::FuncOp>();\n     auto replica_count_attr = fn->getAttrOfType<IntegerAttr>(\"replica_count\");"
                }
            ],
            "whole_deleted": "-    // Channel ID is not supported.\n-    if (op.getChannelId().has_value())\n-      return op.emitOpError() << \"Collective channel id is not supported\";\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -640,10 +640,6 @@ class CollectiveOpLowering : public OpRewritePattern<CollectiveOp> {\n \n   LogicalResult matchAndRewrite(CollectiveOp op,\n                                 PatternRewriter& rewriter) const override {\n-    // Channel ID is not supported.\n-    if (op.getChannelId().has_value())\n-      return op.emitOpError() << \"Collective channel id is not supported\";\n-\n     // Construct an NCCL collective config from the parent func attributes.\n     func::FuncOp fn = op->template getParentOfType<func::FuncOp>();\n     auto replica_count_attr = fn->getAttrOfType<IntegerAttr>(\"replica_count\");"
        }
    ]
},
{
    "Id": 333,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/87ded6c7697af6fdb9dbc022b385dbbaf7a4892a",
    "date": "2023-08-07T16:02:21-07:00",
    "message": "Fix bool type inference.\n\nisinstance(True, int) == True, so check bool type before int type check.\n\nPiperOrigin-RevId: 554619002",
    "label": "YES",
    "changes": [
        {
            "name": "flexible_dtypes.py",
            "path": "tensorflow/python/framework/flexible_dtypes.py",
            "patches": [
                {
                    "old_start": 429,
                    "old_length": 6,
                    "new_start": 429,
                    "new_length": 10,
                    "hunk": "@@ -429,6 +429,10 @@ def _get_dtype_and_weakness(x):\n       return (_NP_TO_TF[x], False)\n   except TypeError:\n     pass\n+  # bool type check must happen before int type check because\n+  # isinstance(True, int) == True (https://peps.python.org/pep-0285/).\n+  if isinstance(x, bool) or x == bool:\n+    return _b8\n   # TODO(b/286585058): Update implementation depending on whether Python\n   # scalars are inferred to 32 bit or 64 bit.\n   if isinstance(x, _pi):\n"
                },
                {
                    "old_start": 441,
                    "old_length": 8,
                    "new_start": 445,
                    "new_length": 6,
                    "hunk": "@@ -441,8 +445,6 @@ def _get_dtype_and_weakness(x):\n     return _f32w\n   if isinstance(x, _pc) or x == complex:\n     return _c128w\n-  if isinstance(x, bool) or x == bool:\n-    return _b8\n   if isinstance(x, tensor_shape.TensorShape):\n     # Since TensorShape is always integer value, return int32.\n     return _i32\n"
                }
            ],
            "whole_deleted": "-  if isinstance(x, bool) or x == bool:\n-    return _b8\n",
            "whole_added": "+  # bool type check must happen before int type check because\n+  # isinstance(True, int) == True (https://peps.python.org/pep-0285/).\n+  if isinstance(x, bool) or x == bool:\n+    return _b8\n",
            "whole_hunk": "@@ -429,6 +429,10 @@ def _get_dtype_and_weakness(x):\n       return (_NP_TO_TF[x], False)\n   except TypeError:\n     pass\n+  # bool type check must happen before int type check because\n+  # isinstance(True, int) == True (https://peps.python.org/pep-0285/).\n+  if isinstance(x, bool) or x == bool:\n+    return _b8\n   # TODO(b/286585058): Update implementation depending on whether Python\n   # scalars are inferred to 32 bit or 64 bit.\n   if isinstance(x, _pi):\n@@ -441,8 +445,6 @@ def _get_dtype_and_weakness(x):\n     return _f32w\n   if isinstance(x, _pc) or x == complex:\n     return _c128w\n-  if isinstance(x, bool) or x == bool:\n-    return _b8\n   if isinstance(x, tensor_shape.TensorShape):\n     # Since TensorShape is always integer value, return int32.\n     return _i32\n"
        },
        {
            "name": "flexible_dtypes_test.py",
            "path": "tensorflow/python/framework/flexible_dtypes_test.py",
            "patches": [
                {
                    "old_start": 828,
                    "old_length": 6,
                    "new_start": 828,
                    "new_length": 14,
                    "hunk": "@@ -828,6 +828,14 @@ class DtypesUtilTest(tf_test.TestCase, parameterized.TestCase):\n           (dtypes.float32, False),\n       )\n \n+  # Test bool type inference.\n+  def testResultTypeBool(self):\n+    with DtypeConversionTestEnv('all'):\n+      self.assertEqual(\n+          flexible_dtypes.result_type(True, False),\n+          (dtypes.bool, False),\n+      )\n+\n   # Test Tensor shape type inference.\n   def testResultTypeTensorShape(self):\n     with DtypeConversionTestEnv('all'):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  # Test bool type inference.\n+  def testResultTypeBool(self):\n+    with DtypeConversionTestEnv('all'):\n+      self.assertEqual(\n+          flexible_dtypes.result_type(True, False),\n+          (dtypes.bool, False),\n+      )\n+\n",
            "whole_hunk": "@@ -828,6 +828,14 @@ class DtypesUtilTest(tf_test.TestCase, parameterized.TestCase):\n           (dtypes.float32, False),\n       )\n \n+  # Test bool type inference.\n+  def testResultTypeBool(self):\n+    with DtypeConversionTestEnv('all'):\n+      self.assertEqual(\n+          flexible_dtypes.result_type(True, False),\n+          (dtypes.bool, False),\n+      )\n+\n   # Test Tensor shape type inference.\n   def testResultTypeTensorShape(self):\n     with DtypeConversionTestEnv('all'):"
        }
    ]
},
{
    "Id": 176,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/d4fc6f9cad02c96a69dff07d5daa0cd04929aeb0",
    "date": "2024-01-29T17:18:36-08:00",
    "message": "Fix `FuseFullyConnectedAndMul` to check if FullyConnected has only one use, that is the LHS of Mul Op. Otherwise this will duplicate the fullyconnected op to serve the remaining uses.\n\nFor example, consider this MLIR graph with the output of FC used by multiple ops:\n```\n  %0 = \"tfl.fully_connected\"(%arg0, %cst0, %cst1) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n  %1 = \"tfl.reshape\"(%0, %cst3) : (tensor<4x2xf32>, tensor<2xi32>) -> tensor<1x8xf32>\n  %2 = \"tfl.mul\"(%0, %cst2) {fused_activation_function = \"RELU6\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n```\n\nThe expected output for the fusion is a `NEW` FC op with the Mul op fused into the FC kernel, and the `OLD` FC op deleted. But the reshape-op above does not need/expect a fc with mul fused. So, the 'OLD' fc op is still around to feed into the ReshapeOp. Hence we are computing two fully-connected ops in the graph instead of one.\n\nThis particular bug has always been there but got triggered on this customer model(b/318735772) by this `recent` change cl/553508906.\n\nThis is a broader issue that needs to be fixed for all the BinaryOp(*FC_Op*) fusion optimization variants on FullyConnected Op, Convolutions, etc.\n\nPiperOrigin-RevId: 602536477",
    "label": "YES",
    "changes": [
        {
            "name": "optimize.mlir",
            "path": "tensorflow/compiler/mlir/lite/tests/optimize.mlir",
            "patches": [
                {
                    "old_start": 377,
                    "old_length": 6,
                    "new_start": 377,
                    "new_length": 28,
                    "hunk": "@@ -377,6 +377,28 @@ func.func @DontFuseMulIntoFullyConnectedForLargeFilter(%arg0: tensor<128x256000x\n }\n \n \n+// CHECK-LABEL: @skipFuseMulIntoFullyConnected\n+func.func @skipFuseMulIntoFullyConnected(%arg0: tensor<4x2xf32>) -> (tensor<1x8xf32>, tensor<4x2xf32>) {\n+  %cst0 = arith.constant dense<[[1.0, 2.0], [3.0, 4.0]]> : tensor<2x2xf32>\n+  %cst1 = arith.constant dense<2.0> : tensor<2xf32>\n+  %cst2 = arith.constant dense<[1.0, 2.0]> : tensor<2xf32>\n+  %cst3 = arith.constant dense<[1, 8]> : tensor<2xi32>\n+\n+  %0 = \"tfl.fully_connected\"(%arg0, %cst0, %cst1) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  %1 = \"tfl.reshape\"(%0, %cst3) : (tensor<4x2xf32>, tensor<2xi32>) -> tensor<1x8xf32>\n+  %2 = \"tfl.mul\"(%0, %cst2) {fused_activation_function = \"RELU6\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+\n+  func.return %1, %2 : tensor<1x8xf32>, tensor<4x2xf32>\n+  // CHECK:  %cst = arith.constant dense<{{\\[}}[1.000000e+00, 2.000000e+00], [3.000000e+00, 4.000000e+00]]> : tensor<2x2xf32>\n+  // CHECK:  %cst_0 = arith.constant dense<2.000000e+00> : tensor<2xf32>\n+  // CHECK:  %cst_1 = arith.constant dense<[1.000000e+00, 2.000000e+00]> : tensor<2xf32>\n+  // CHECK:  %cst_2 = arith.constant dense<[1, 8]> : tensor<2xi32>\n+  // CHECK:  %0 = \"tfl.fully_connected\"(%arg0, %cst, %cst_0) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  // CHECK:  %1 = \"tfl.reshape\"(%0, %cst_2) : (tensor<4x2xf32>, tensor<2xi32>) -> tensor<1x8xf32>\n+  // CHECK:  %2 = tfl.mul(%0, %cst_1) {fused_activation_function = \"RELU6\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  // CHECK:  return %1, %2 : tensor<1x8xf32>, tensor<4x2xf32>\n+}\n+\n // CHECK-LABEL: @fuseAddIntoFollowingFullyConnectedWithQDQs\n func.func @fuseAddIntoFollowingFullyConnectedWithQDQs(%arg0: tensor<4x2xf32>) -> tensor<4x2xf32> {\n   %cst2 = arith.constant dense<1.5> : tensor<f32>\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// CHECK-LABEL: @skipFuseMulIntoFullyConnected\n+func.func @skipFuseMulIntoFullyConnected(%arg0: tensor<4x2xf32>) -> (tensor<1x8xf32>, tensor<4x2xf32>) {\n+  %cst0 = arith.constant dense<[[1.0, 2.0], [3.0, 4.0]]> : tensor<2x2xf32>\n+  %cst1 = arith.constant dense<2.0> : tensor<2xf32>\n+  %cst2 = arith.constant dense<[1.0, 2.0]> : tensor<2xf32>\n+  %cst3 = arith.constant dense<[1, 8]> : tensor<2xi32>\n+\n+  %0 = \"tfl.fully_connected\"(%arg0, %cst0, %cst1) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  %1 = \"tfl.reshape\"(%0, %cst3) : (tensor<4x2xf32>, tensor<2xi32>) -> tensor<1x8xf32>\n+  %2 = \"tfl.mul\"(%0, %cst2) {fused_activation_function = \"RELU6\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+\n+  func.return %1, %2 : tensor<1x8xf32>, tensor<4x2xf32>\n+  // CHECK:  %cst = arith.constant dense<{{\\[}}[1.000000e+00, 2.000000e+00], [3.000000e+00, 4.000000e+00]]> : tensor<2x2xf32>\n+  // CHECK:  %cst_0 = arith.constant dense<2.000000e+00> : tensor<2xf32>\n+  // CHECK:  %cst_1 = arith.constant dense<[1.000000e+00, 2.000000e+00]> : tensor<2xf32>\n+  // CHECK:  %cst_2 = arith.constant dense<[1, 8]> : tensor<2xi32>\n+  // CHECK:  %0 = \"tfl.fully_connected\"(%arg0, %cst, %cst_0) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  // CHECK:  %1 = \"tfl.reshape\"(%0, %cst_2) : (tensor<4x2xf32>, tensor<2xi32>) -> tensor<1x8xf32>\n+  // CHECK:  %2 = tfl.mul(%0, %cst_1) {fused_activation_function = \"RELU6\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  // CHECK:  return %1, %2 : tensor<1x8xf32>, tensor<4x2xf32>\n+}\n+\n",
            "whole_hunk": "@@ -377,6 +377,28 @@ func.func @DontFuseMulIntoFullyConnectedForLargeFilter(%arg0: tensor<128x256000x\n }\n \n \n+// CHECK-LABEL: @skipFuseMulIntoFullyConnected\n+func.func @skipFuseMulIntoFullyConnected(%arg0: tensor<4x2xf32>) -> (tensor<1x8xf32>, tensor<4x2xf32>) {\n+  %cst0 = arith.constant dense<[[1.0, 2.0], [3.0, 4.0]]> : tensor<2x2xf32>\n+  %cst1 = arith.constant dense<2.0> : tensor<2xf32>\n+  %cst2 = arith.constant dense<[1.0, 2.0]> : tensor<2xf32>\n+  %cst3 = arith.constant dense<[1, 8]> : tensor<2xi32>\n+\n+  %0 = \"tfl.fully_connected\"(%arg0, %cst0, %cst1) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  %1 = \"tfl.reshape\"(%0, %cst3) : (tensor<4x2xf32>, tensor<2xi32>) -> tensor<1x8xf32>\n+  %2 = \"tfl.mul\"(%0, %cst2) {fused_activation_function = \"RELU6\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+\n+  func.return %1, %2 : tensor<1x8xf32>, tensor<4x2xf32>\n+  // CHECK:  %cst = arith.constant dense<{{\\[}}[1.000000e+00, 2.000000e+00], [3.000000e+00, 4.000000e+00]]> : tensor<2x2xf32>\n+  // CHECK:  %cst_0 = arith.constant dense<2.000000e+00> : tensor<2xf32>\n+  // CHECK:  %cst_1 = arith.constant dense<[1.000000e+00, 2.000000e+00]> : tensor<2xf32>\n+  // CHECK:  %cst_2 = arith.constant dense<[1, 8]> : tensor<2xi32>\n+  // CHECK:  %0 = \"tfl.fully_connected\"(%arg0, %cst, %cst_0) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  // CHECK:  %1 = \"tfl.reshape\"(%0, %cst_2) : (tensor<4x2xf32>, tensor<2xi32>) -> tensor<1x8xf32>\n+  // CHECK:  %2 = tfl.mul(%0, %cst_1) {fused_activation_function = \"RELU6\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n+  // CHECK:  return %1, %2 : tensor<1x8xf32>, tensor<4x2xf32>\n+}\n+\n // CHECK-LABEL: @fuseAddIntoFollowingFullyConnectedWithQDQs\n func.func @fuseAddIntoFollowingFullyConnectedWithQDQs(%arg0: tensor<4x2xf32>) -> tensor<4x2xf32> {\n   %cst2 = arith.constant dense<1.5> : tensor<f32>\n"
        },
        {
            "name": "optimize.cc",
            "path": "tensorflow/compiler/mlir/lite/transforms/optimize.cc",
            "patches": [
                {
                    "old_start": 1249,
                    "old_length": 6,
                    "new_start": 1249,
                    "new_length": 12,
                    "hunk": "@@ -1249,6 +1249,12 @@ struct FuseFullyConnectedAndMul : public OpRewritePattern<TFL::MulOp> {\n     auto fc_op = dyn_cast_or_null<TFL::FullyConnectedOp>(\n         mul_op.getLhs().getDefiningOp());\n     if (!fc_op) return failure();\n+\n+    // Check if FullyConnected has only one use, that is the LHS of Mul Op.\n+    // Otherwise this will duplicate the fullyconnected op to serve the\n+    // remaining uses.\n+    if (!fc_op->hasOneUse()) return failure();\n+\n     Value filter = fc_op.getFilter();\n     Value bias = fc_op.getBias();\n     ElementsAttr cst_tmp;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+\n+    // Check if FullyConnected has only one use, that is the LHS of Mul Op.\n+    // Otherwise this will duplicate the fullyconnected op to serve the\n+    // remaining uses.\n+    if (!fc_op->hasOneUse()) return failure();\n+\n",
            "whole_hunk": "@@ -1249,6 +1249,12 @@ struct FuseFullyConnectedAndMul : public OpRewritePattern<TFL::MulOp> {\n     auto fc_op = dyn_cast_or_null<TFL::FullyConnectedOp>(\n         mul_op.getLhs().getDefiningOp());\n     if (!fc_op) return failure();\n+\n+    // Check if FullyConnected has only one use, that is the LHS of Mul Op.\n+    // Otherwise this will duplicate the fullyconnected op to serve the\n+    // remaining uses.\n+    if (!fc_op->hasOneUse()) return failure();\n+\n     Value filter = fc_op.getFilter();\n     Value bias = fc_op.getBias();\n     ElementsAttr cst_tmp;"
        }
    ]
},
{
    "Id": 223,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f3819de491f740984b2975294bed0e642c66c802",
    "date": "2023-12-12T16:51:32-08:00",
    "message": "[PJRT C API] Fix missing mesh shape and mesh ids in ExecutableBuildOptions proto.\n\nAlso check whether compile_thread_pool is nullptr and fail the serialization if it is not nullptr.\n\nPiperOrigin-RevId: 590380587",
    "label": "YES",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/client/BUILD",
            "patches": [
                {
                    "old_start": 105,
                    "old_length": 14,
                    "new_start": 105,
                    "new_length": 19,
                    "hunk": "@@ -105,14 +105,19 @@ cc_library(\n         \"//xla:debug_options_flags\",\n         \"//xla:execution_options_util\",\n         \"//xla:shape_util\",\n+        \"//xla:statusor\",\n+        \"//xla:util\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/pjrt:compile_options_proto_cc\",\n         \"//xla/service:compilation_environments\",\n         \"//xla/service:computation_placer\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@local_tsl//tsl/platform:env\",\n+        \"@local_tsl//tsl/platform:errors\",\n+        \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//xla:statusor\",\n+        \"//xla:util\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@local_tsl//tsl/platform:errors\",\n+        \"@local_tsl//tsl/platform:status\",\n",
            "whole_hunk": "@@ -105,14 +105,19 @@ cc_library(\n         \"//xla:debug_options_flags\",\n         \"//xla:execution_options_util\",\n         \"//xla:shape_util\",\n+        \"//xla:statusor\",\n+        \"//xla:util\",\n         \"//xla:xla_proto_cc\",\n         \"//xla/pjrt:compile_options_proto_cc\",\n         \"//xla/service:compilation_environments\",\n         \"//xla/service:computation_placer\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:inlined_vector\",\n+        \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@local_tsl//tsl/platform:env\",\n+        \"@local_tsl//tsl/platform:errors\",\n+        \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n     ],\n )\n"
        },
        {
            "name": "executable_build_options.cc",
            "path": "third_party/xla/xla/client/executable_build_options.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 16,
                    "new_start": 15,
                    "new_length": 26,
                    "hunk": "@@ -15,16 +15,26 @@ limitations under the License.\n \n #include \"xla/client/executable_build_options.h\"\n \n+#include <cstdint>\n #include <memory>\n #include <string>\n #include <utility>\n #include <vector>\n \n+#include \"absl/log/check.h\"\n #include \"absl/strings/str_format.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/execution_options_util.h\"\n+#include \"xla/layout_util.h\"\n+#include \"xla/service/compilation_environments.h\"\n+#include \"xla/service/computation_placer.h\"\n+#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/statusor.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n+#include \"tsl/platform/errors.h\"\n+#include \"tsl/platform/status.h\"\n #include \"tsl/platform/statusor.h\"\n \n namespace xla {\n"
                },
                {
                    "old_start": 151,
                    "old_length": 6,
                    "new_start": 161,
                    "new_length": 10,
                    "hunk": "@@ -151,6 +161,10 @@ StatusOr<ExecutableBuildOptionsProto> ExecutableBuildOptions::ToProto() const {\n         \"Cannot serialize \"\n         \"ExecutableBuildOptions::layout_canonicalization_callback\");\n   }\n+  if (compile_thread_pool() != nullptr) {\n+    return InvalidArgument(\n+        \"Cannot serialize ExecutableBuildOptions::compile_thread_pool\");\n+  }\n   output.set_num_replicas(num_replicas());\n   output.set_num_partitions(num_partitions());\n   output.set_use_spmd_partitioning(use_spmd_partitioning());\n"
                },
                {
                    "old_start": 170,
                    "old_length": 6,
                    "new_start": 184,
                    "new_length": 12,
                    "hunk": "@@ -170,6 +184,12 @@ StatusOr<ExecutableBuildOptionsProto> ExecutableBuildOptions::ToProto() const {\n   }\n   *output.mutable_fdo_profile() = fdo_profile();\n   output.set_device_memory_size(device_memory_size());\n+  for (int64_t s : auto_spmd_partitioning_mesh_shape()) {\n+    output.mutable_auto_spmd_partitioning_mesh_shape()->Add(s);\n+  }\n+  for (int64_t s : auto_spmd_partitioning_mesh_ids()) {\n+    output.mutable_auto_spmd_partitioning_mesh_ids()->Add(s);\n+  }\n   return output;\n }\n \n"
                },
                {
                    "old_start": 208,
                    "old_length": 6,
                    "new_start": 228,
                    "new_length": 12,
                    "hunk": "@@ -208,6 +228,12 @@ StatusOr<ExecutableBuildOptions> ExecutableBuildOptionsFromProto(\n       input.allow_spmd_sharding_propagation_to_output());\n   *output.mutable_fdo_profile() = input.fdo_profile();\n   output.set_device_memory_size(input.device_memory_size());\n+  output.set_auto_spmd_partitioning_mesh_shape(\n+      std::vector<int64_t>(input.auto_spmd_partitioning_mesh_shape().begin(),\n+                           input.auto_spmd_partitioning_mesh_shape().end()));\n+  output.set_auto_spmd_partitioning_mesh_ids(\n+      std::vector<int64_t>(input.auto_spmd_partitioning_mesh_ids().begin(),\n+                           input.auto_spmd_partitioning_mesh_ids().end()));\n   return output;\n }\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include <cstdint>\n+#include \"absl/log/check.h\"\n+#include \"xla/layout_util.h\"\n+#include \"xla/service/compilation_environments.h\"\n+#include \"xla/service/computation_placer.h\"\n+#include \"xla/shape.h\"\n+#include \"xla/statusor.h\"\n+#include \"xla/util.h\"\n+#include \"tsl/platform/errors.h\"\n+#include \"tsl/platform/status.h\"\n+  if (compile_thread_pool() != nullptr) {\n+    return InvalidArgument(\n+        \"Cannot serialize ExecutableBuildOptions::compile_thread_pool\");\n+  }\n+  for (int64_t s : auto_spmd_partitioning_mesh_shape()) {\n+    output.mutable_auto_spmd_partitioning_mesh_shape()->Add(s);\n+  }\n+  for (int64_t s : auto_spmd_partitioning_mesh_ids()) {\n+    output.mutable_auto_spmd_partitioning_mesh_ids()->Add(s);\n+  }\n+  output.set_auto_spmd_partitioning_mesh_shape(\n+      std::vector<int64_t>(input.auto_spmd_partitioning_mesh_shape().begin(),\n+                           input.auto_spmd_partitioning_mesh_shape().end()));\n+  output.set_auto_spmd_partitioning_mesh_ids(\n+      std::vector<int64_t>(input.auto_spmd_partitioning_mesh_ids().begin(),\n+                           input.auto_spmd_partitioning_mesh_ids().end()));\n",
            "whole_hunk": "@@ -15,16 +15,26 @@ limitations under the License.\n \n #include \"xla/client/executable_build_options.h\"\n \n+#include <cstdint>\n #include <memory>\n #include <string>\n #include <utility>\n #include <vector>\n \n+#include \"absl/log/check.h\"\n #include \"absl/strings/str_format.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/execution_options_util.h\"\n+#include \"xla/layout_util.h\"\n+#include \"xla/service/compilation_environments.h\"\n+#include \"xla/service/computation_placer.h\"\n+#include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n+#include \"xla/statusor.h\"\n+#include \"xla/util.h\"\n #include \"xla/xla.pb.h\"\n+#include \"tsl/platform/errors.h\"\n+#include \"tsl/platform/status.h\"\n #include \"tsl/platform/statusor.h\"\n \n namespace xla {\n@@ -151,6 +161,10 @@ StatusOr<ExecutableBuildOptionsProto> ExecutableBuildOptions::ToProto() const {\n         \"Cannot serialize \"\n         \"ExecutableBuildOptions::layout_canonicalization_callback\");\n   }\n+  if (compile_thread_pool() != nullptr) {\n+    return InvalidArgument(\n+        \"Cannot serialize ExecutableBuildOptions::compile_thread_pool\");\n+  }\n   output.set_num_replicas(num_replicas());\n   output.set_num_partitions(num_partitions());\n   output.set_use_spmd_partitioning(use_spmd_partitioning());\n@@ -170,6 +184,12 @@ StatusOr<ExecutableBuildOptionsProto> ExecutableBuildOptions::ToProto() const {\n   }\n   *output.mutable_fdo_profile() = fdo_profile();\n   output.set_device_memory_size(device_memory_size());\n+  for (int64_t s : auto_spmd_partitioning_mesh_shape()) {\n+    output.mutable_auto_spmd_partitioning_mesh_shape()->Add(s);\n+  }\n+  for (int64_t s : auto_spmd_partitioning_mesh_ids()) {\n+    output.mutable_auto_spmd_partitioning_mesh_ids()->Add(s);\n+  }\n   return output;\n }\n \n@@ -208,6 +228,12 @@ StatusOr<ExecutableBuildOptions> ExecutableBuildOptionsFromProto(\n       input.allow_spmd_sharding_propagation_to_output());\n   *output.mutable_fdo_profile() = input.fdo_profile();\n   output.set_device_memory_size(input.device_memory_size());\n+  output.set_auto_spmd_partitioning_mesh_shape(\n+      std::vector<int64_t>(input.auto_spmd_partitioning_mesh_shape().begin(),\n+                           input.auto_spmd_partitioning_mesh_shape().end()));\n+  output.set_auto_spmd_partitioning_mesh_ids(\n+      std::vector<int64_t>(input.auto_spmd_partitioning_mesh_ids().begin(),\n+                           input.auto_spmd_partitioning_mesh_ids().end()));\n   return output;\n }\n \n"
        },
        {
            "name": "compile_options.proto",
            "path": "third_party/xla/xla/pjrt/compile_options.proto",
            "patches": [
                {
                    "old_start": 7,
                    "old_length": 7,
                    "new_start": 7,
                    "new_length": 7,
                    "hunk": "@@ -7,7 +7,7 @@ import \"xla/xla.proto\";\n import \"xla/xla_data.proto\";\n \n // A serialization of xla::ExecutableBuildOptions.\n-// Next id: 16.\n+// Next id: 18.\n message ExecutableBuildOptionsProto {\n   // If set, this is the device to build the computation for. Valid\n   // device_ordinal values are: 0 to # of devices - 1. These values are\n"
                },
                {
                    "old_start": 84,
                    "old_length": 6,
                    "new_start": 84,
                    "new_length": 12,
                    "hunk": "@@ -84,6 +84,12 @@ message ExecutableBuildOptionsProto {\n   bytes fdo_profile = 14;\n \n   int64 device_memory_size = 15;\n+\n+  // Mesh shape in auto sharding options.\n+  repeated int64 auto_spmd_partitioning_mesh_shape = 16;\n+\n+  // Mesh ids in auto sharding options.\n+  repeated int64 auto_spmd_partitioning_mesh_ids = 17;\n }\n \n message OptionOverrideProto {"
                }
            ],
            "whole_deleted": "-// Next id: 16.\n",
            "whole_added": "+// Next id: 18.\n+\n+  // Mesh shape in auto sharding options.\n+  repeated int64 auto_spmd_partitioning_mesh_shape = 16;\n+\n+  // Mesh ids in auto sharding options.\n+  repeated int64 auto_spmd_partitioning_mesh_ids = 17;\n",
            "whole_hunk": "@@ -7,7 +7,7 @@ import \"xla/xla.proto\";\n import \"xla/xla_data.proto\";\n \n // A serialization of xla::ExecutableBuildOptions.\n-// Next id: 16.\n+// Next id: 18.\n message ExecutableBuildOptionsProto {\n   // If set, this is the device to build the computation for. Valid\n   // device_ordinal values are: 0 to # of devices - 1. These values are\n@@ -84,6 +84,12 @@ message ExecutableBuildOptionsProto {\n   bytes fdo_profile = 14;\n \n   int64 device_memory_size = 15;\n+\n+  // Mesh shape in auto sharding options.\n+  repeated int64 auto_spmd_partitioning_mesh_shape = 16;\n+\n+  // Mesh ids in auto sharding options.\n+  repeated int64 auto_spmd_partitioning_mesh_ids = 17;\n }\n \n message OptionOverrideProto {"
        }
    ]
},
{
    "Id": 213,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b7e107eaa6dffb649d055d893a1fce734ee50d55",
    "date": "2024-01-03T08:27:12-08:00",
    "message": "PR #7922: [XLA:GPU] Error out for ptxas version - Update ptxas version check\n\nImported from GitHub PR https://github.com/openxla/xla/pull/7922\n\nThe version check is extended to all ptxas version `12.3.103`\nCopybara import of the project:\n\n--\n50c9aeb611685f1ee91771f031f9b8cafc58cb10 by Ayan Moitra <amoitra@nvidia.com>:\n\nUpdate ptxas version check\n\n--\n429b0866625761d13f51a70b404ab13714bba25b by Ayan Moitra <amoitra@nvidia.com>:\n\nnit fix\n\n--\n5dab1394c0a0370219c1d0b35f3cd9a949b5f3bd by Ayan Moitra <amoitra@nvidia.com>:\n\nSpecific ptxas version affected\n\nMerging this change closes #7922\n\nPiperOrigin-RevId: 595407179",
    "label": "NO",
    "changes": [
        {
            "name": "asm_compiler.cc",
            "path": "third_party/xla/xla/stream_executor/gpu/asm_compiler.cc",
            "patches": [
                {
                    "old_start": 258,
                    "old_length": 11,
                    "new_start": 258,
                    "new_length": 14,
                    "hunk": "@@ -258,11 +258,14 @@ tsl::StatusOr<std::vector<uint8_t>> CompileGpuAsm(int cc_major, int cc_minor,\n                                                   const char* ptx_contents,\n                                                   GpuAsmOpts options,\n                                                   bool cancel_if_reg_spill) {\n-  auto ptxas_version_tuple = GetAsmCompilerVersion(options.preferred_cuda_dir);\n-  if (ptxas_version_tuple.value() == std::array<int64_t, 3>{12, 3, 1}) {\n-    return tsl::errors::Internal(\n-        absl::StrFormat(\"ptxas 12.3.1 has a bug that we think can affect XLA. \"\n-                        \"Please use a different version.\"));\n+  TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n+                      GetAsmCompilerVersion(options.preferred_cuda_dir));\n+  if (ptxas_version_tuple == std::array<int64_t, 3>{12, 3, 103}) {\n+    return absl::InternalError(absl::StrFormat(\n+        \"ptxas %d.%d.%d has a bug that we think can affect XLA. \"\n+        \"Please use a different version.\",\n+        std::get<0>(ptxas_version_tuple), std::get<1>(ptxas_version_tuple),\n+        std::get<2>(ptxas_version_tuple)));\n   }\n   std::string ptxas_path =\n       FindCudaExecutable(\"ptxas\", options.preferred_cuda_dir);"
                }
            ],
            "whole_deleted": "-  auto ptxas_version_tuple = GetAsmCompilerVersion(options.preferred_cuda_dir);\n-  if (ptxas_version_tuple.value() == std::array<int64_t, 3>{12, 3, 1}) {\n-    return tsl::errors::Internal(\n-        absl::StrFormat(\"ptxas 12.3.1 has a bug that we think can affect XLA. \"\n-                        \"Please use a different version.\"));\n",
            "whole_added": "+  TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n+                      GetAsmCompilerVersion(options.preferred_cuda_dir));\n+  if (ptxas_version_tuple == std::array<int64_t, 3>{12, 3, 103}) {\n+    return absl::InternalError(absl::StrFormat(\n+        \"ptxas %d.%d.%d has a bug that we think can affect XLA. \"\n+        \"Please use a different version.\",\n+        std::get<0>(ptxas_version_tuple), std::get<1>(ptxas_version_tuple),\n+        std::get<2>(ptxas_version_tuple)));\n",
            "whole_hunk": "@@ -258,11 +258,14 @@ tsl::StatusOr<std::vector<uint8_t>> CompileGpuAsm(int cc_major, int cc_minor,\n                                                   const char* ptx_contents,\n                                                   GpuAsmOpts options,\n                                                   bool cancel_if_reg_spill) {\n-  auto ptxas_version_tuple = GetAsmCompilerVersion(options.preferred_cuda_dir);\n-  if (ptxas_version_tuple.value() == std::array<int64_t, 3>{12, 3, 1}) {\n-    return tsl::errors::Internal(\n-        absl::StrFormat(\"ptxas 12.3.1 has a bug that we think can affect XLA. \"\n-                        \"Please use a different version.\"));\n+  TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n+                      GetAsmCompilerVersion(options.preferred_cuda_dir));\n+  if (ptxas_version_tuple == std::array<int64_t, 3>{12, 3, 103}) {\n+    return absl::InternalError(absl::StrFormat(\n+        \"ptxas %d.%d.%d has a bug that we think can affect XLA. \"\n+        \"Please use a different version.\",\n+        std::get<0>(ptxas_version_tuple), std::get<1>(ptxas_version_tuple),\n+        std::get<2>(ptxas_version_tuple)));\n   }\n   std::string ptxas_path =\n       FindCudaExecutable(\"ptxas\", options.preferred_cuda_dir);"
        }
    ]
},
{
    "Id": 239,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/d1ad87b61c28834a48d84d709f21aeedbd9c6521",
    "date": "2023-11-27T07:54:13-08:00",
    "message": "[XLA:GPU][NFC] Transform dimension propagation to the functional paradigm\n\nThis intends to make it easier to implement new fusion strategies, such as \"Separate multiple uses of nodes within one scope when they are incompatible in Triton GEMM fusion\".\n\n\"Renames\":\nAnalyzeForFusion -> GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible\nRequireSupportedInstruction -> GetPropagatedDimOrdersAndRequirements\nHandleInstruction -> GetPropagatedDimOrders\nRequireSupportedDimOrder -> GetRequirementsIfSupportedOrder\nRequireSupportedDimOrders -> GetRequirementsIfSupportedOrders\nDimOrderUpdates -> DimOrdersAndReqs\n\nNotable logic changes:\n\nI split out the splittable_dimension_major_part_size from DotProperties to DotRequirements, because it's not really a property of the dot, but rather a requirement which can be imposed by the instructions of the fusion.\n\nI explicitly return an error if a dimension split would be needed for Softmax in GetRequirementsIfSupportedOrder.\n\nI don't check IsSupportedSplittableDimensionMajorPartSize in GetRequirementsIfSupportedOrder anymore, I just check that in CombineDimOrdersAndReqs after the propagation is done.\n\nPiperOrigin-RevId: 585650283",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 1298,
                    "old_length": 6,
                    "new_start": 1298,
                    "new_length": 7,
                    "hunk": "@@ -1298,6 +1298,7 @@ cc_library(\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"@com_google_absl//absl/container:inlined_vector\",\n",
            "whole_hunk": "@@ -1298,6 +1298,7 @@ cc_library(\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/container:inlined_vector\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/log:check\",\n         \"@com_google_absl//absl/strings\",\n"
        },
        {
            "name": "gemm_rewriter_triton.cc",
            "path": "third_party/xla/xla/service/gpu/gemm_rewriter_triton.cc",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 6,
                    "new_start": 20,
                    "new_length": 7,
                    "hunk": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <cstdint>\n #include <iterator>\n #include <list>\n+#include <optional>\n #include <queue>\n #include <string>\n #include <utility>\n"
                },
                {
                    "old_start": 29,
                    "old_length": 6,
                    "new_start": 30,
                    "new_length": 7,
                    "hunk": "@@ -29,6 +30,7 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/strings/str_cat.h\"\n"
                },
                {
                    "old_start": 106,
                    "old_length": 14,
                    "new_start": 108,
                    "new_length": 14,
                    "hunk": "@@ -106,14 +108,14 @@ bool IsDistributiveOverAddition(const HloInstruction& hlo) {\n \n namespace {\n \n-FusionDecision RequireTritonFusibleConvert(\n-    const HloInstruction* input, se::GpuComputeCapability gpu_version) {\n+FusionDecision IsConversionWorthFusing(const HloInstruction& input,\n+                                       se::GpuComputeCapability gpu_version) {\n   // TODO(b/266862494): Can pick up almost any\n   // convert, but if it's reducing the data volume it should rather be fused\n   // to the output of the producer kernel. However not all operations support\n   // output fusion - then it should be fused here anyway!\n-  if (ShapeUtil::ByteSizeOf(input->operand(0)->shape()) >\n-      ShapeUtil::ByteSizeOf(input->shape())) {\n+  if (ShapeUtil::ByteSizeOf(input.operand(0)->shape()) >\n+      ShapeUtil::ByteSizeOf(input.shape())) {\n     return \"Narrowing conversion.\";\n   }\n   return FusionDecision{};\n"
                },
                {
                    "old_start": 248,
                    "old_length": 11,
                    "new_start": 250,
                    "new_length": 89,
                    "hunk": "@@ -248,11 +250,89 @@ using Fragment = DimensionOrder::Fragment;\n using Fragments = DimensionOrder::Fragments;\n using FragmentOrders = DimensionOrder::FragmentOrders;\n using DimOrderMap = absl::flat_hash_map<const HloInstruction*, DimensionOrder>;\n+using DimOrderMapOrError = std::variant<DimOrderMap, FusionDecision>;\n \n-struct DimOrderUpdates {\n-  DimOrderMap map;\n-  int64_t splittable_dimension_major_part_size = 0;\n+// This represents an invalid dimension index.\n+constexpr int kNoDimensionIndex = -1;\n+struct DotProperties {\n+  // Index of dot dimension that can be split.\n+  // Currently typically LHS non-contracting one.\n+  const int splittable_dimension_index;\n+};\n+struct SoftmaxProperties {\n+  const int softmax_reduction_dimension;\n+  const int softmax_batch_dimension;\n+};\n+// HeroProperties depend only on the hero op and they don't change as we\n+// change the fusion.\n+using HeroProperties = std::variant<DotProperties, SoftmaxProperties>;\n+\n+// A special value for splittable_dimension_major_part_size.\n+constexpr int kNoSplitRequirement = 1;\n+struct DotRequirements {\n+  explicit DotRequirements(int64_t splittable_dimension_major_part_size)\n+      : splittable_dimension_major_part_size(\n+            splittable_dimension_major_part_size) {\n+    CHECK_GE(splittable_dimension_major_part_size, 1);\n+  }\n+  // If not kNoSplitRequirement, then the major part size of the splittable\n+  // dimension must be the given value.\n+  int64_t splittable_dimension_major_part_size;\n };\n+struct SoftmaxRequirements {};\n+// Requirements can change depending on what we fuse.\n+using Requirements = std::variant<DotRequirements, SoftmaxRequirements>;\n+using RequirementsOrError = std::variant<Requirements, FusionDecision>;\n+\n+// The dimension orders and requirements resulting from propagating the\n+// dimension orders through an HLO.\n+struct DimOrdersAndReqs {\n+  DimOrderMap dim_orders;\n+  Requirements requirements;\n+};\n+using DimOrdersAndReqsOrError = std::variant<DimOrdersAndReqs, FusionDecision>;\n+\n+using Int64OrError = std::variant<int64_t, FusionDecision>;\n+Int64OrError CombineSplitDimMajorPartSizeReqs(int64_t a, int64_t b) {\n+  if (a == b || b == kNoSplitRequirement) {\n+    return a;\n+  }\n+  if (a == kNoSplitRequirement) {\n+    return b;\n+  }\n+  return FusionDecision(\"Conflicting splits of splittable dimension\");\n+}\n+\n+RequirementsOrError CombineDotRequirements(DotRequirements a,\n+                                           DotRequirements b) {\n+  Int64OrError combined_size_req =\n+      CombineSplitDimMajorPartSizeReqs(a.splittable_dimension_major_part_size,\n+                                       b.splittable_dimension_major_part_size);\n+  if (std::holds_alternative<FusionDecision>(combined_size_req)) {\n+    return std::get<FusionDecision>(combined_size_req);\n+  }\n+  return DotRequirements(std::get<int64_t>(combined_size_req));\n+}\n+\n+RequirementsOrError CombineSoftmaxRequirements(SoftmaxRequirements a,\n+                                               SoftmaxRequirements b) {\n+  // SoftmaxRequirements is an empty class for now.\n+  return a;\n+}\n+\n+RequirementsOrError CombineRequirements(Requirements a,\n+                                        RequirementsOrError b_or_error) {\n+  if (std::holds_alternative<FusionDecision>(b_or_error)) {\n+    return b_or_error;\n+  }\n+  const Requirements& b = std::get<Requirements>(b_or_error);\n+  if (std::holds_alternative<DotRequirements>(b)) {\n+    return CombineDotRequirements(std::get<DotRequirements>(a),\n+                                  std::get<DotRequirements>(b));\n+  }\n+  return CombineSoftmaxRequirements(std::get<SoftmaxRequirements>(a),\n+                                    std::get<SoftmaxRequirements>(b));\n+}\n \n TensorIterationSpec DimensionOrderToTensorIterationSpec(\n     const DimensionOrder& order) {\n"
                },
                {
                    "old_start": 316,
                    "old_length": 33,
                    "new_start": 396,
                    "new_length": 12,
                    "hunk": "@@ -316,33 +396,12 @@ bool DimensionOrder::IsPhysicallyEquivalent(const DimensionOrder& other) const {\n \n enum class TransformDirection { kInputToOutput, kOutputToInput };\n \n-using DimOrderUpdatesOrError = std::variant<FusionDecision, DimOrderUpdates>;\n using OldToNewHloMap =\n     absl::flat_hash_map<const HloInstruction*, HloInstruction*>;\n \n class FusionContext {\n-  struct DotProperties {\n-    int splittable_dimension;\n-    int64_t splittable_dimension_supported_major_part_size;\n-  };\n-  struct SoftmaxProperties {\n-    int softmax_reduction_dimension;\n-    int softmax_batch_dimension;\n-  };\n-\n-  explicit FusionContext(DotProperties properties) : properties_(properties) {}\n-\n-  explicit FusionContext(SoftmaxProperties properties)\n-      : properties_(properties) {}\n-\n-  DimOrderUpdatesOrError HandleElementwise(const HloInstruction* hlo,\n-                                           const DimOrderMap& dim_orders) const;\n-  DimOrderUpdatesOrError HandleBitcast(const HloInstruction* hlo,\n-                                       const DimOrderMap& dim_orders,\n-                                       TransformDirection direction) const;\n-  DimOrderUpdatesOrError HandleDimensionAlteringOp(\n-      const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-      TransformDirection direction) const;\n+  FusionContext(HeroProperties properties, Requirements requirements)\n+      : properties_(properties), requirements_(requirements) {}\n \n  public:\n   // Create fusion context from a dot operand according to\n"
                },
                {
                    "old_start": 353,
                    "old_length": 41,
                    "new_start": 412,
                    "new_length": 52,
                    "hunk": "@@ -353,41 +412,52 @@ class FusionContext {\n   // Create fusion context from dot's output.\n   static FusionContext FromDotOutput(\n       const HloInstruction& dot, int split_k,\n-      int64_t splittable_dimension_supported_major_part_size);\n+      int64_t splittable_dimension_major_part_size);\n \n   static FusionContext FromSoftmaxRoot(const HloInstruction&);\n \n-  DimOrderUpdatesOrError HandleInstruction(const HloInstruction* hlo,\n-                                           const DimOrderMap& dim_orders,\n-                                           TransformDirection direction) const;\n-\n-  // Tells if the dimension order is supported by the triton emitters.\n-  // Only the dimension indicated by SplittableDimensionIndex() can be split\n-  // physically once by other dimensions. Other ones can be only split\n-  // logically. All subdimensions within a dimension have to be ordered.\n-  // Return major part of splittable dimension in split_dim_major_part if a\n-  // supported split is detected.\n-  FusionDecision RequireSupportedDimOrder(const DimensionOrder& order,\n-                                          int64_t& split_dim_major_part) const;\n-  // Apply RequireSupportedDimOrder() to all known dimension orders\n-  // around `hlo`.\n-  FusionDecision RequireSupportedDimOrders(const HloInstruction& hlo,\n-                                           DimOrderUpdates& updates) const;\n-  // Try to calculate transformations of dimensions defined by the\n-  // instruction, then check that the resulting dimension orders are supported.\n-  DimOrderUpdatesOrError RequireSupportedInstruction(\n+  // If possible, propagates `src_dim_order` (describing one side of `hlo`) to\n+  // the other side and returns those dim orders.\n+  static DimOrderMapOrError GetPropagatedDimOrders(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order, const HeroProperties& properties);\n+\n+  // If the dimension order is supported by the triton emitters, this returns\n+  // which requirements does this order impose on the fusion.\n+  //\n+  // All subdimensions within a dimension have to be ordered.\n+  static RequirementsOrError GetRequirementsIfSupportedOrder(\n+      const DimensionOrder& order, const HeroProperties& properties);\n+  // Apply GetRequirementsIfSupportedOrder() to all known\n+  // dimension orders around `hlo` and combine the result.\n+  static RequirementsOrError GetRequirementsIfSupportedOrders(\n       const HloInstruction& hlo, const DimOrderMap& dim_orders,\n-      TransformDirection direction) const;\n-  // Checks if the instruction is possible and profitable to fuse.\n-  // If so tries to transform dim_order describing one side of `hlo` into\n-  // description(s) of its other side if it is supported.\n-  DimOrderUpdatesOrError AnalyzeForFusion(\n+      const HeroProperties& properties);\n+  // If fusing the instruction is possible then it propagates\n+  // the `src_dim_order` (describing one side of `hlo`) to the other side and\n+  // returns those dim orders and the requirements that they impose on the\n+  // fusion.\n+  static DimOrdersAndReqsOrError GetPropagatedDimOrdersAndRequirements(\n+      const HloInstruction& hlo, const DimensionOrder& src_dim_order,\n+      TransformDirection direction, const HeroProperties& properties);\n+  // If fusing the instruction is possible *and profitable* then it propagates\n+  // the `src_dim_order` (describing one side of `hlo`) to the other side and\n+  // returns those dim orders and the requirements that they impose on the\n+  // fusion.\n+  //\n+  // `src_operand_index` must be set iff `transform_direction` is\n+  // kInputToOutput.\n+  static DimOrdersAndReqsOrError\n+  GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n       const HloInstruction& hlo, TransformDirection transform_direction,\n-      OldToNewHloMap& old_to_new_map,\n-      se::GpuComputeCapability gpu_version) const;\n-  // Add dimension orders from `updates` to `dim_orders_` and update the\n-  // splittable dimension ratio if all of them are compatible.\n-  bool MergeUpdates(const DimOrderUpdates& updates);\n+      const std::optional<int>& src_operand_index,\n+      const DimensionOrder& src_dim_order,\n+      const se::GpuComputeCapability& gpu_version,\n+      const HeroProperties& properties);\n+\n+  // Add dimension orders from `update` to `dim_orders_` and update\n+  // `requirements_` if all of them are compatible.\n+  bool CombineDimOrdersAndReqs(const DimOrdersAndReqs& update);\n   // Fuse an instruction with all its fusible inputs.\n   // If an input is not fusible stop there and make a parameter of the new\n   // fusion, otherwise put it onto stack and check its own inputs first.\n"
                },
                {
                    "old_start": 403,
                    "old_length": 44,
                    "new_start": 473,
                    "new_length": 27,
                    "hunk": "@@ -403,44 +473,27 @@ class FusionContext {\n       const HloInstruction& origin, ConstHloInstructionSet& parameters,\n       ConstHloInstructionMap<TensorIterationSpec>& iter_specs);\n \n-  // Index of dot dimension that can be split.\n-  // Currently typically LHS non-contracting one.\n-  int64_t SplittableDimensionIndex() const {\n-    CHECK(std::holds_alternative<DotProperties>(properties_));\n-    return std::get<DotProperties>(properties_).splittable_dimension;\n-  }\n-  // Tells whether `size` major part of a dimension can be physically split.\n-  bool IsSupportedSplittableDimensionMajorPartSize(const int64_t size) const {\n-    CHECK_NE(size, 0);\n-    CHECK(std::holds_alternative<DotProperties>(properties_));\n-    // 0 means no specific size requirement.\n-    return std::get<DotProperties>(properties_)\n-                   .splittable_dimension_supported_major_part_size == 0 ||\n-           std::get<DotProperties>(properties_)\n-                   .splittable_dimension_supported_major_part_size == size;\n-  }\n-  int SplittableDimensionMajorPartSize() const {\n-    CHECK(std::holds_alternative<DotProperties>(properties_));\n-    return std::get<DotProperties>(properties_)\n-        .splittable_dimension_supported_major_part_size;\n-  }\n-  const DimOrderMap& DimOrders() const { return dim_orders_; }\n-\n- private:\n-  DimOrderUpdatesOrError AnalyzeForFusionImpl(\n-      const HloInstruction& hlo, TransformDirection transform_direction,\n-      OldToNewHloMap& old_to_new_map, const DimOrderMap& dim_orders,\n-      se::GpuComputeCapability gpu_version) const;\n-  bool SetSplittableDimensionMajorPartSize(const int64_t size) {\n-    if (IsSupportedSplittableDimensionMajorPartSize(size)) {\n-      std::get<DotProperties>(properties_)\n-          .splittable_dimension_supported_major_part_size = size;\n-      return true;\n-    }\n-    return false;\n+  int64_t splittable_dimension_major_part_size() const {\n+    CHECK(std::holds_alternative<DotRequirements>(requirements_));\n+    return std::get<DotRequirements>(requirements_)\n+        .splittable_dimension_major_part_size;\n   }\n+  const HeroProperties& hero_properties() const { return properties_; }\n+  const DimOrderMap& dim_orders() const { return dim_orders_; }\n \n-  std::variant<DotProperties, SoftmaxProperties> properties_;\n+ private:\n+  static DimOrderMap GetPropagatedDimOrdersForElementwise(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order);\n+  static DimOrderMapOrError GetPropagatedDimOrdersForBitcast(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order, const HeroProperties& properties);\n+  static DimOrderMapOrError GetPropagatedDimOrdersForDimAlteringOp(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order, const HeroProperties& properties);\n+\n+  const HeroProperties properties_;\n+  Requirements requirements_;\n   DimOrderMap dim_orders_;\n };\n \n"
                },
                {
                    "old_start": 449,
                    "old_length": 12,
                    "new_start": 502,
                    "new_length": 12,
                    "hunk": "@@ -449,12 +502,12 @@ FusionContext FusionContext::FromDotOperand(const HloInstruction& dot,\n                                             const int split_k) {\n   // There can be either none or one split-K batch dimension.\n   const int num_split_k_batch_dims = split_k > 1;\n-  int split_k_dimension_index = -1;\n+  int split_k_dimension_index = kNoDimensionIndex;\n   if (split_k > 1) {\n     split_k_dimension_index =\n         ContractingDimensionIndex(dot, operand_number) - 1;\n   }\n-  int splittable_dimension_index = -1;\n+  int splittable_dimension_index = kNoDimensionIndex;\n   // LHS non-contracting dimension can be split if non-splitK batch is absent.\n   if (operand_number == 0 &&\n       dot.dot_dimension_numbers().lhs_batch_dimensions_size() -\n"
                },
                {
                    "old_start": 463,
                    "old_length": 9,
                    "new_start": 516,
                    "new_length": 8,
                    "hunk": "@@ -463,9 +516,8 @@ FusionContext FusionContext::FromDotOperand(const HloInstruction& dot,\n     splittable_dimension_index =\n         NonContractingDimensionIndex(dot, operand_number);\n   }\n-  FusionContext context(FusionContext::DotProperties{\n-      splittable_dimension_index,\n-      /*splittable_dimension_supported_major_size=*/0});\n+  FusionContext context(DotProperties{splittable_dimension_index},\n+                        DotRequirements(kNoSplitRequirement));\n   context.dim_orders_[dot.operand(operand_number)] =\n       DimensionOrder::FromDotOperandOrOutput(*dot.operand(operand_number),\n                                              split_k_dimension_index);\n"
                },
                {
                    "old_start": 474,
                    "old_length": 34,
                    "new_start": 526,
                    "new_length": 35,
                    "hunk": "@@ -474,34 +526,35 @@ FusionContext FusionContext::FromDotOperand(const HloInstruction& dot,\n \n FusionContext FusionContext::FromDotOutput(\n     const HloInstruction& dot, const int split_k,\n-    const int64_t splittable_dimension_supported_major_part_size) {\n+    const int64_t splittable_dimension_major_part_size) {\n   // Allow non-contracting dimension originating from LHS to split if\n   // this dimension is split at the output at the same ratio as\n   // at the input.\n-  int splittable_dimension_index = -1;\n-  if (splittable_dimension_supported_major_part_size > 1) {\n+  int splittable_dimension_index = kNoDimensionIndex;\n+  if (splittable_dimension_major_part_size > 1) {\n     // Split-K dimension is the first one in the output if present;\n     // LHS non-contracting follows (batch is absent in this case).\n     splittable_dimension_index = (split_k > 1) ? 1 : 0;\n   }\n-  FusionContext context(FusionContext::DotProperties{\n-      splittable_dimension_index,\n-      splittable_dimension_supported_major_part_size});\n+  FusionContext context(DotProperties{splittable_dimension_index},\n+                        DotRequirements(splittable_dimension_major_part_size));\n   context.dim_orders_[&dot] = DimensionOrder::FromDotOperandOrOutput(dot);\n   return context;\n }\n \n FusionContext FusionContext::FromSoftmaxRoot(const HloInstruction& root) {\n-  FusionContext context(FusionContext::SoftmaxProperties{\n-      DimensionOrder::kSoftmaxReductionDimension,\n-      DimensionOrder::kSoftmaxBatchDimension});\n+  FusionContext context(\n+      SoftmaxProperties{DimensionOrder::kSoftmaxReductionDimension,\n+                        DimensionOrder::kSoftmaxBatchDimension},\n+      SoftmaxRequirements{});\n   context.dim_orders_[&root] = DimensionOrder::FromSoftmaxRoot(root);\n   return context;\n }\n \n-FusionDecision FusionContext::RequireSupportedDimOrder(\n-    const DimensionOrder& order, int64_t& split_dim_major_part) const {\n+/*static*/ RequirementsOrError FusionContext::GetRequirementsIfSupportedOrder(\n+    const DimensionOrder& order, const HeroProperties& properties) {\n   VLOG(8) << order.ToString();\n+  int64_t split_dim_major_part = kNoSplitRequirement;\n   const Fragments& tensor_dim_fragments = order.TensorFragmentsOrder();\n   for (const auto& [dim_index, dim_fragments] : order.DimFragmentsOrders()) {\n     CHECK(!dim_fragments.empty());\n"
                },
                {
                    "old_start": 536,
                    "old_length": 10,
                    "new_start": 589,
                    "new_length": 17,
                    "hunk": "@@ -536,10 +589,17 @@ FusionDecision FusionContext::RequireSupportedDimOrder(\n \n       ++group_counter;\n       if (group_counter > 1) {\n-        if (dim_index == SplittableDimensionIndex() &&\n-            IsSupportedSplittableDimensionMajorPartSize(grouped_size)) {\n+        if (!std::holds_alternative<DotProperties>(properties)) {\n+          return \"Splitting a dimension is not supported for Softmax.\";\n+        }\n+        // Only the dimension indicated by `splittable_dimension_index` (if any)\n+        // can be split physically once by other dimensions. Other ones can be\n+        // only split logically.\n+        const int splittable_dimension_index =\n+            std::get<DotProperties>(properties).splittable_dimension_index;\n+        if (dim_index == splittable_dimension_index) {\n           if (group_counter == 2) {\n-            if (split_dim_major_part != 0 &&\n+            if (split_dim_major_part != kNoSplitRequirement &&\n                 split_dim_major_part != grouped_size) {\n               return \"Conflicting splits of splittable dimension\";\n             }\n"
                },
                {
                    "old_start": 556,
                    "old_length": 68,
                    "new_start": 616,
                    "new_length": 110,
                    "hunk": "@@ -556,68 +616,110 @@ FusionDecision FusionContext::RequireSupportedDimOrder(\n       ++fragment_it;\n     }\n   }\n-  return FusionDecision{};\n+\n+  if (std::holds_alternative<DotProperties>(properties)) {\n+    return DotRequirements(split_dim_major_part);\n+  }\n+  return SoftmaxRequirements{};\n }\n \n-FusionDecision FusionContext::RequireSupportedDimOrders(\n-    const HloInstruction& hlo, DimOrderUpdates& updates) const {\n-  auto check_if_present = [&](const HloInstruction* instr) {\n-    if (auto it = updates.map.find(instr); it != updates.map.end()) {\n-      return RequireSupportedDimOrder(\n-          it->second, updates.splittable_dimension_major_part_size);\n+/*static*/ RequirementsOrError FusionContext::GetRequirementsIfSupportedOrders(\n+    const HloInstruction& hlo, const DimOrderMap& dim_orders,\n+    const HeroProperties& properties) {\n+  const Requirements empty_requirements =\n+      std::holds_alternative<DotProperties>(properties)\n+          ? Requirements(DotRequirements(kNoSplitRequirement))\n+          : Requirements(SoftmaxRequirements{});\n+  auto get_requirements =\n+      [&](const HloInstruction& instr) -> RequirementsOrError {\n+    if (auto it = dim_orders.find(&instr); it != dim_orders.end()) {\n+      return GetRequirementsIfSupportedOrder(it->second, properties);\n     }\n-    return FusionDecision{};\n+    return empty_requirements;\n   };\n+\n+  Requirements requirements = empty_requirements;\n   for (const HloInstruction* operand : hlo.operands()) {\n-    if (auto result = check_if_present(operand); !result) {\n-      return result;\n+    RequirementsOrError requirements_or_error =\n+        CombineRequirements(requirements, get_requirements(*operand));\n+    if (std::holds_alternative<FusionDecision>(requirements_or_error)) {\n+      return requirements_or_error;\n     }\n+    requirements = std::get<Requirements>(requirements_or_error);\n   }\n-  return check_if_present(&hlo);\n+\n+  return CombineRequirements(requirements, get_requirements(hlo));\n }\n \n-DimOrderUpdatesOrError FusionContext::HandleElementwise(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders) const {\n-  // The output and all the input dimension orders of `hlo` have to be the same.\n-  const HloInstruction* src = nullptr;\n-  const DimensionOrder* src_dim_order;\n-  // Try using the output as a reference if it's already described, otherwise\n-  // scan through all operands.\n-  if (auto it = dim_orders.find(hlo); it != dim_orders.cend()) {\n-    src = it->first;\n-    src_dim_order = &it->second;\n-  } else {\n-    for (const HloInstruction* operand : hlo->operands()) {\n-      if (auto it = dim_orders.find(operand); it != dim_orders.cend()) {\n-        src = it->first;\n-        src_dim_order = &it->second;\n-        break;\n-      }\n+/*static*/ DimOrderMap FusionContext::GetPropagatedDimOrdersForElementwise(\n+    const HloInstruction& hlo, TransformDirection direction,\n+    const DimensionOrder& src_dim_order) {\n+  if (direction == TransformDirection::kOutputToInput) {\n+    DimOrderMap map;\n+    for (const HloInstruction* operand : hlo.operands()) {\n+      map.insert({operand, src_dim_order});\n     }\n-    CHECK_NE(src, nullptr);\n+    return map;\n+  }\n+\n+  DimOrderMap map;\n+  map.insert({&hlo, src_dim_order});\n+  // TODO(tdanyluk): For now, the \"input to output\" direction of this function\n+  // also returns the dim orders for the operands, not just the output. This is\n+  // needed to propagate the dim order of one input to the other(s) when fusing\n+  // elementwise ops to the output. Perhaps we can separate the \"input to\n+  // output\" and \"output to input\" directions of that in a later CL.\n+  for (const HloInstruction* operand : hlo.operands()) {\n+    map.insert({operand, src_dim_order});\n+  }\n+  return map;\n+}\n+\n+const HloInstruction& GetSourceHlo(const HloInstruction& hlo,\n+                                   TransformDirection direction) {\n+  CHECK_GE(hlo.operand_count(), 1);\n+\n+  if (direction == TransformDirection::kOutputToInput) {\n+    return hlo;\n+  }\n+  return *hlo.operand(0);\n+}\n+\n+using ConstInstructionVector = absl::InlinedVector<const HloInstruction*, 2>;\n+ConstInstructionVector GetDestHlos(const HloInstruction& hlo,\n+                                   TransformDirection direction) {\n+  if (direction == TransformDirection::kInputToOutput) {\n+    return {&hlo};\n   }\n \n-  DimOrderUpdates result;\n-  result.map.insert({hlo, DimensionOrder(*src_dim_order)});\n-  for (const HloInstruction* operand : hlo->operands()) {\n-    result.map.insert({operand, DimensionOrder(dim_orders.at(src))});\n+  ConstInstructionVector hlos;\n+  hlos.reserve(hlo.operands().size());\n+  for (const HloInstruction* operand : hlo.operands()) {\n+    hlos.push_back(operand);\n+  }\n+  return hlos;\n+}\n+\n+const HloInstruction& GetDestHlo(const HloInstruction& hlo,\n+                                 TransformDirection direction) {\n+  CHECK_EQ(hlo.operand_count(), 1);\n+\n+  if (direction == TransformDirection::kInputToOutput) {\n+    return hlo;\n   }\n-  return result;\n+\n+  return *hlo.operand(0);\n }\n \n-DimOrderUpdatesOrError FusionContext::HandleBitcast(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection direction) const {\n-  const HloInstruction* src =\n-      (direction == TransformDirection::kOutputToInput) ? hlo : hlo->operand(0);\n-  const HloInstruction* dst =\n-      (direction == TransformDirection::kOutputToInput) ? hlo->operand(0) : hlo;\n-  const Shape& dst_shape = dst->shape();\n-  const Fragments& src_fragments_order =\n-      dim_orders.at(src).TensorFragmentsOrder();\n-  DimOrderUpdates result;\n+/*static*/ DimOrderMapOrError FusionContext::GetPropagatedDimOrdersForBitcast(\n+    const HloInstruction& hlo, const TransformDirection direction,\n+    const DimensionOrder& src_dim_order, const HeroProperties& properties) {\n+  const HloInstruction& dst = GetDestHlo(hlo, direction);\n+  const Shape& dst_shape = dst.shape();\n+  const Fragments& src_fragments_order = src_dim_order.TensorFragmentsOrder();\n+  DimOrderMap dst_dim_orders;\n   DimensionOrder& dst_dim_order =\n-      result.map.insert({dst, DimensionOrder()}).first->second;\n+      dst_dim_orders.insert({&dst, DimensionOrder()}).first->second;\n   Fragments& dst_fragments_order = dst_dim_order.TensorFragmentsOrder();\n   // Size of not yet assigned part of current target dimension.\n   int64_t dst_remaining_size = 1;\n"
                },
                {
                    "old_start": 634,
                    "old_length": 9,
                    "new_start": 736,
                    "new_length": 9,
                    "hunk": "@@ -634,9 +736,9 @@ DimOrderUpdatesOrError FusionContext::HandleBitcast(\n       dst_fragments_order.push_back(fragment);\n       src_to_dst[&*src_dim].push_back(dst_fragments_order.size() - 1);\n     };\n-    if (std::holds_alternative<SoftmaxProperties>(properties_) &&\n+    if (std::holds_alternative<SoftmaxProperties>(properties) &&\n         src_dim->dst_dim_number() ==\n-            std::get<SoftmaxProperties>(properties_).softmax_batch_dimension) {\n+            std::get<SoftmaxProperties>(properties).softmax_batch_dimension) {\n       // Special handling for softmax batch dimension: allow arbitrary reshapes\n       // on it because it's guaranteed by the construction of the fusion to have\n       // no physical alterations like transposes.\n"
                },
                {
                    "old_start": 731,
                    "old_length": 7,
                    "new_start": 833,
                    "new_length": 7,
                    "hunk": "@@ -731,7 +833,7 @@ DimOrderUpdatesOrError FusionContext::HandleBitcast(\n \n   FragmentOrders& dst_dim_fragment_orders = dst_dim_order.DimFragmentsOrders();\n   for (const auto& [dim_index, dim_sequence] :\n-       dim_orders.at(src).DimFragmentsOrders()) {\n+       src_dim_order.DimFragmentsOrders()) {\n     std::vector<int>& dst = dst_dim_fragment_orders[dim_index];\n     dst.reserve(dim_sequence.size());\n     for (const int src : dim_sequence) {\n"
                },
                {
                    "old_start": 741,
                    "old_length": 15,
                    "new_start": 843,
                    "new_length": 16,
                    "hunk": "@@ -741,15 +843,16 @@ DimOrderUpdatesOrError FusionContext::HandleBitcast(\n     }\n   }\n \n-  return result;\n+  return dst_dim_orders;\n }\n \n // Handle copy, transpose, broadcast or reduce.\n // Common between them is that they alter the tensor dimensions or their order\n // and the way to handle layouts.\n-DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection direction) const {\n+/*static*/ DimOrderMapOrError\n+FusionContext::GetPropagatedDimOrdersForDimAlteringOp(\n+    const HloInstruction& hlo, const TransformDirection direction,\n+    const DimensionOrder& src_dim_order, const HeroProperties& properties) {\n   // Temporary storage for new fragments local to this function.\n   // Please keep this as the first local variable of this function, with type\n   // std::list to make sure that all pointers to elements of this remain valid\n"
                },
                {
                    "old_start": 757,
                    "old_length": 13,
                    "new_start": 860,
                    "new_length": 12,
                    "hunk": "@@ -757,13 +860,12 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n   // unnecessarily big for a typical size of 1.\n   std::list<Fragment> new_fragments;\n \n-  const HloInstruction* src =\n-      (direction == TransformDirection::kOutputToInput) ? hlo : hlo->operand(0);\n+  const HloInstruction& src = GetSourceHlo(hlo, direction);\n   // Note: copying instead of using a const reference because\n   // some operations (slice) will modify fragment properties in-place.\n-  Fragments src_fragments_order = dim_orders.at(src).TensorFragmentsOrder();\n-  if (hlo->opcode() == HloOpcode::kSlice &&\n-      ShapeUtil::IsEffectiveScalar(hlo->shape())) {\n+  Fragments src_fragments_order = src_dim_order.TensorFragmentsOrder();\n+  if (hlo.opcode() == HloOpcode::kSlice &&\n+      ShapeUtil::IsEffectiveScalar(hlo.shape())) {\n     return FusionDecision(\"Slice to scalar is not implemented yet.\");\n   }\n   // Every HLO dimension can correspond to a group of subdimensions in\n"
                },
                {
                    "old_start": 772,
                    "old_length": 10,
                    "new_start": 874,
                    "new_length": 10,
                    "hunk": "@@ -772,10 +874,10 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n   // Group subdimensions by iterating over them in the same order as over\n   // full dimensions and matching by total size.\n   std::vector<std::vector<Fragment*>> src_physical;\n-  src_physical.reserve(src->shape().rank());\n+  src_physical.reserve(src.shape().rank());\n   auto src_fragment_it = src_fragments_order.begin();\n-  for (int64_t dim_index : src->shape().layout().minor_to_major()) {\n-    const int64_t dim_size = src->shape().dimensions(dim_index);\n+  for (int64_t dim_index : src.shape().layout().minor_to_major()) {\n+    const int64_t dim_size = src.shape().dimensions(dim_index);\n     int64_t subdim_size_accumulator = 1;\n     std::vector<Fragment*> subdim_group;\n     do {\n"
                },
                {
                    "old_start": 792,
                    "old_length": 21,
                    "new_start": 894,
                    "new_length": 17,
                    "hunk": "@@ -792,21 +894,17 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n   std::vector<std::vector<Fragment*>> src_logical;\n   src_logical.resize(src_physical.size());\n   for (int i = 0; i < src_physical.size(); ++i) {\n-    src_logical[src->shape().layout().minor_to_major(i)] = src_physical[i];\n+    src_logical[src.shape().layout().minor_to_major(i)] = src_physical[i];\n   }\n \n-  HloInstruction::InstructionVector output;\n-  output.push_back(const_cast<HloInstruction*>(hlo));\n-  DimOrderUpdates result;\n-  for (const HloInstruction* dst :\n-       (direction == TransformDirection::kInputToOutput) ? output\n-                                                         : hlo->operands()) {\n+  DimOrderMap dst_dim_orders;\n+  for (const HloInstruction* dst : GetDestHlos(hlo, direction)) {\n     DimensionOrder& dst_dim_order =\n-        result.map.insert({dst, DimensionOrder()}).first->second;\n+        dst_dim_orders.insert({dst, DimensionOrder()}).first->second;\n     // Source logical -> destination logical.\n     std::vector<std::vector<Fragment*>> dst_logical;\n-    if (hlo->opcode() == HloOpcode::kTranspose) {\n-      const auto* transpose = Cast<HloTransposeInstruction>(hlo);\n+    if (hlo.opcode() == HloOpcode::kTranspose) {\n+      const auto* transpose = Cast<HloTransposeInstruction>(&hlo);\n       std::vector<int64_t> permutation(transpose->dimensions().cbegin(),\n                                        transpose->dimensions().cend());\n       if (direction == TransformDirection::kInputToOutput) {\n"
                },
                {
                    "old_start": 816,
                    "old_length": 18,
                    "new_start": 914,
                    "new_length": 18,
                    "hunk": "@@ -816,18 +914,18 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n       for (int i = 0; i < permutation.size(); ++i) {\n         dst_logical[permutation[i]] = src_logical[i];\n       }\n-    } else if (hlo->opcode() == HloOpcode::kBroadcast) {\n-      const auto* broadcast = Cast<HloBroadcastInstruction>(hlo);\n+    } else if (hlo.opcode() == HloOpcode::kBroadcast) {\n+      const auto* broadcast = Cast<HloBroadcastInstruction>(&hlo);\n       dst_logical.resize(broadcast->dimensions().size());\n       for (int i = 0; i < broadcast->dimensions().size(); ++i) {\n         dst_logical[i] = src_logical[broadcast->dimensions()[i]];\n       }\n-    } else if (hlo->opcode() == HloOpcode::kReduce) {\n+    } else if (hlo.opcode() == HloOpcode::kReduce) {\n       // Operand 1 (the neutral value) has to be a scalar.\n-      if (dst != hlo && hlo->operand_index(dst) == 1) {\n+      if (dst != &hlo && hlo.operand_index(dst) == 1) {\n         continue;\n       }\n-      const auto* reduce = Cast<HloReduceInstruction>(hlo);\n+      const auto* reduce = Cast<HloReduceInstruction>(&hlo);\n       dst_logical.resize(src_logical.size() + reduce->dimensions().size());\n       if (reduce->dimensions().size() != 1) {\n         return FusionDecision(\"Unsupported reduction.\");\n"
                },
                {
                    "old_start": 838,
                    "old_length": 23,
                    "new_start": 936,
                    "new_length": 23,
                    "hunk": "@@ -838,23 +936,23 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n           // softmax fusions with known patterns for now. Generally a reduction\n           // should create a new tiled dimension.\n           dst_logical[i] = {&new_fragments.emplace_back(\n-              std::get<SoftmaxProperties>(properties_)\n+              std::get<SoftmaxProperties>(properties)\n                   .softmax_reduction_dimension,\n               reduce->operand(0)->shape().dimensions(i))};\n         } else {\n           dst_logical[i] = src_logical[i];\n         }\n       }\n-    } else if (hlo->opcode() == HloOpcode::kCopy) {\n+    } else if (hlo.opcode() == HloOpcode::kCopy) {\n       // Copy preserves the logical shape, just permutes the layout.\n-      CHECK(ShapeUtil::SameDimensions(src->shape(), dst->shape()));\n+      CHECK(ShapeUtil::SameDimensions(src.shape(), dst->shape()));\n       dst_logical = src_logical;\n-    } else if (hlo->opcode() == HloOpcode::kPad) {\n+    } else if (hlo.opcode() == HloOpcode::kPad) {\n       // Operand 1 (the padding value) has to be a scalar.\n-      if (dst != hlo && hlo->operand_index(dst) == 1) {\n+      if (dst != &hlo && hlo.operand_index(dst) == 1) {\n         continue;\n       }\n-      const auto* pad = Cast<HloPadInstruction>(hlo);\n+      const auto* pad = Cast<HloPadInstruction>(&hlo);\n       dst_logical.resize(src_logical.size());\n       for (int i = 0; i < src_logical.size(); ++i) {\n         // This only handles the padding added by\n"
                },
                {
                    "old_start": 883,
                    "old_length": 8,
                    "new_start": 981,
                    "new_length": 8,
                    "hunk": "@@ -883,8 +981,8 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n           dst_logical[i] = {&new_fragments.back()};\n         }\n       }\n-    } else if (hlo->opcode() == HloOpcode::kSlice) {\n-      const auto slice = Cast<HloSliceInstruction>(hlo);\n+    } else if (hlo.opcode() == HloOpcode::kSlice) {\n+      const auto slice = Cast<HloSliceInstruction>(&hlo);\n       dst_logical.resize(src_logical.size());\n       for (int dim = 0; dim < src_logical.size(); ++dim) {\n         dst_logical[dim] = src_logical[dim];\n"
                },
                {
                    "old_start": 923,
                    "old_length": 11,
                    "new_start": 1021,
                    "new_length": 11,
                    "hunk": "@@ -923,11 +1021,11 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n       }\n     }\n     for (const auto& [dim_index, dim_sequence] :\n-         dim_orders.at(src).DimFragmentsOrders()) {\n+         src_dim_order.DimFragmentsOrders()) {\n       for (const int fragment_number : dim_sequence) {\n         const auto it = src_to_dst.find(&src_fragments_order[fragment_number]);\n         if (it == src_to_dst.cend()) {\n-          if (hlo->opcode() == HloOpcode::kBroadcast &&\n+          if (hlo.opcode() == HloOpcode::kBroadcast &&\n               src_fragments_order[fragment_number].full_size() > 1 &&\n               dim_numbers_present_in_dst.contains(dim_index)) {\n             return FusionDecision(\"Unsupported broadcast\");\n"
                },
                {
                    "old_start": 938,
                    "old_length": 55,
                    "new_start": 1036,
                    "new_length": 63,
                    "hunk": "@@ -938,55 +1036,63 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n       }\n     }\n   }\n-  return result;\n+  return dst_dim_orders;\n }\n \n // Infers DimensionOrders of all unknown sides (output, operands)\n // of `hlo` from the known ones.\n-DimOrderUpdatesOrError FusionContext::HandleInstruction(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection direction) const {\n-  VLOG(7) << \"Analyzing \" << hlo->ToString();\n-  if (hlo->opcode() == HloOpcode::kParameter ||\n-      hlo_query::IsScalarConstant(hlo)) {\n-    return DimOrderUpdates{};\n-  } else if (hlo->opcode() == HloOpcode::kTranspose ||\n-             hlo->opcode() == HloOpcode::kCopy) {\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kBroadcast) {\n+/*static*/ DimOrderMapOrError FusionContext::GetPropagatedDimOrders(\n+    const HloInstruction& hlo, const TransformDirection direction,\n+    const DimensionOrder& src_dim_order, const HeroProperties& properties) {\n+  VLOG(7) << \"Analyzing \" << hlo.ToString();\n+  if (hlo.opcode() == HloOpcode::kParameter ||\n+      hlo_query::IsScalarConstant(&hlo)) {\n+    CHECK(direction == TransformDirection::kOutputToInput);\n+    return DimOrderMap{};\n+  } else if (hlo.opcode() == HloOpcode::kTranspose ||\n+             hlo.opcode() == HloOpcode::kCopy) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kBroadcast) {\n     if (direction != TransformDirection::kOutputToInput) {\n       return \"Unsupported broadcast direction.\";\n     }\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kReduce) {\n-    if (!std::holds_alternative<SoftmaxProperties>(properties_)) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kReduce) {\n+    if (!std::holds_alternative<SoftmaxProperties>(properties)) {\n       return \"Reductions are not supported in GEMM fusions yet.\";\n     }\n     if (direction != TransformDirection::kOutputToInput) {\n       return \"Unsupported direction of reduction.\";\n     }\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kPad) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kPad) {\n     if (direction != TransformDirection::kOutputToInput) {\n       return \"Unsupported pad direction.\";\n     }\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->operand_count() > 0 &&\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.operand_count() > 0 &&\n              IsTritonSupportedElementwise(\n-                 hlo->opcode(), hlo->operand(0)->shape().element_type())) {\n-    return HandleElementwise(hlo, dim_orders);\n-  } else if (hlo->opcode() == HloOpcode::kBitcast) {\n-    return HandleBitcast(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kSlice) {\n+                 hlo.opcode(), hlo.operand(0)->shape().element_type())) {\n+    return GetPropagatedDimOrdersForElementwise(hlo, direction, src_dim_order);\n+  } else if (hlo.opcode() == HloOpcode::kBitcast) {\n+    return GetPropagatedDimOrdersForBitcast(hlo, direction, src_dim_order,\n+                                            properties);\n+  } else if (hlo.opcode() == HloOpcode::kSlice) {\n     if (direction != TransformDirection::kOutputToInput) {\n       return \"Unsupported slice direction.\";\n     }\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kReshape) {\n-    if (!ShapeUtil::ReshapeIsBitcast(hlo->operand(0)->shape(), hlo->shape())) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kReshape) {\n+    if (!ShapeUtil::ReshapeIsBitcast(hlo.operand(0)->shape(), hlo.shape())) {\n       return \"Non-bitcast reshape.\";\n     }\n-    return HandleBitcast(hlo, dim_orders, direction);\n+    return GetPropagatedDimOrdersForBitcast(hlo, direction, src_dim_order,\n+                                            properties);\n   }\n   return \"Unimplemented instruction.\";\n }\n"
                },
                {
                    "old_start": 1033,
                    "old_length": 34,
                    "new_start": 1139,
                    "new_length": 36,
                    "hunk": "@@ -1033,34 +1139,36 @@ bool IsOutputWorthFusing(const HloInstruction& hlo) {\n          InputMinusOutputBytes(hlo) >= -kIoToleranceBytes;\n }\n \n-DimOrderUpdatesOrError FusionContext::RequireSupportedInstruction(\n-    const HloInstruction& hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection transform_direction) const {\n-  auto result = HandleInstruction(&hlo, dim_orders, transform_direction);\n-  if (!std::holds_alternative<DimOrderUpdates>(result)) {\n-    return std::get<FusionDecision>(result);\n-  }\n-\n-  if (FusionDecision supported =\n-          RequireSupportedDimOrders(hlo, std::get<DimOrderUpdates>(result));\n-      !supported) {\n-    return supported;\n-  }\n-  return std::get<DimOrderUpdates>(result);\n+/*static*/ DimOrdersAndReqsOrError\n+FusionContext::GetPropagatedDimOrdersAndRequirements(\n+    const HloInstruction& hlo, const DimensionOrder& src_dim_order,\n+    TransformDirection direction, const HeroProperties& properties) {\n+  DimOrderMapOrError propagated_dim_orders_or_error =\n+      GetPropagatedDimOrders(hlo, direction, src_dim_order, properties);\n+  if (std::holds_alternative<FusionDecision>(propagated_dim_orders_or_error)) {\n+    return std::get<FusionDecision>(propagated_dim_orders_or_error);\n+  }\n+  DimOrderMap propagated_dim_orders =\n+      std::move(std::get<DimOrderMap>(propagated_dim_orders_or_error));\n+  RequirementsOrError requirements_or_error =\n+      GetRequirementsIfSupportedOrders(hlo, propagated_dim_orders, properties);\n+  if (std::holds_alternative<FusionDecision>(requirements_or_error)) {\n+    return std::get<FusionDecision>(requirements_or_error);\n+  }\n+  return DimOrdersAndReqs{propagated_dim_orders,\n+                          std::get<Requirements>(requirements_or_error)};\n }\n \n-DimOrderUpdatesOrError FusionContext::AnalyzeForFusion(\n-    const HloInstruction& hlo, const TransformDirection transform_direction,\n-    OldToNewHloMap& old_to_new_map,\n-    const se::GpuComputeCapability gpu_version) const {\n-  return AnalyzeForFusionImpl(hlo, transform_direction, old_to_new_map,\n-                              dim_orders_, gpu_version);\n-}\n+/*static*/ DimOrdersAndReqsOrError\n+FusionContext::GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+    const HloInstruction& hlo, TransformDirection transform_direction,\n+    const std::optional<int>& src_operand_index,\n+    const DimensionOrder& src_dim_order,\n+    const se::GpuComputeCapability& gpu_version,\n+    const HeroProperties& properties) {\n+  CHECK_EQ(transform_direction == TransformDirection::kInputToOutput,\n+           src_operand_index.has_value());\n \n-DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n-    const HloInstruction& hlo, const TransformDirection transform_direction,\n-    OldToNewHloMap& old_to_new_map, const DimOrderMap& dim_orders,\n-    const se::GpuComputeCapability gpu_version) const {\n   if (hlo.opcode() == HloOpcode::kTuple ||\n       hlo.opcode() == HloOpcode::kGetTupleElement) {\n     return \"Unsupported instruction.\";\n"
                },
                {
                    "old_start": 1080,
                    "old_length": 11,
                    "new_start": 1188,
                    "new_length": 14,
                    "hunk": "@@ -1080,11 +1188,14 @@ DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n   if (!IsTritonSupportedDataType(hlo.shape().element_type(), gpu_version)) {\n     return \"Unsupported output data type.\";\n   }\n-  DimOrderUpdatesOrError result =\n-      RequireSupportedInstruction(hlo, dim_orders, transform_direction);\n-  if (!std::holds_alternative<DimOrderUpdates>(result)) {\n-    return result;\n+  DimOrdersAndReqsOrError result_or_error =\n+      GetPropagatedDimOrdersAndRequirements(hlo, src_dim_order,\n+                                            transform_direction, properties);\n+  if (!std::holds_alternative<DimOrdersAndReqs>(result_or_error)) {\n+    return result_or_error;\n   }\n+  DimOrdersAndReqs dim_orders_and_requirements =\n+      std::move(std::get<DimOrdersAndReqs>(result_or_error));\n   int fusion_level =\n       hlo.GetModule()->config().debug_options().xla_gpu_triton_fusion_level();\n   if (!std::get<se::CudaComputeCapability>(gpu_version)\n"
                },
                {
                    "old_start": 1094,
                    "old_length": 8,
                    "new_start": 1205,
                    "new_length": 7,
                    "hunk": "@@ -1094,8 +1205,7 @@ DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n   if (transform_direction == TransformDirection::kOutputToInput) {\n     if (fusion_level < 2) {\n       if (hlo.opcode() == HloOpcode::kConvert) {\n-        if (FusionDecision decision =\n-                RequireTritonFusibleConvert(&hlo, gpu_version);\n+        if (FusionDecision decision = IsConversionWorthFusing(hlo, gpu_version);\n             !decision) {\n           return decision;\n         }\n"
                },
                {
                    "old_start": 1113,
                    "old_length": 9,
                    "new_start": 1223,
                    "new_length": 13,
                    "hunk": "@@ -1113,9 +1223,13 @@ DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n           if (operand->opcode() == HloOpcode::kBroadcast &&\n               (operand->operand(0)->opcode() == HloOpcode::kParameter ||\n                operand->operand(0)->opcode() == HloOpcode::kConstant) &&\n-              std::holds_alternative<DimOrderUpdates>(AnalyzeForFusionImpl(\n-                  *operand, transform_direction, old_to_new_map,\n-                  std::get<DimOrderUpdates>(result).map, gpu_version))) {\n+              std::holds_alternative<DimOrdersAndReqs>(\n+                  GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+                      *operand, TransformDirection::kOutputToInput,\n+                      /*src_operand_index=*/std::nullopt,\n+                      /*src_dim_order=*/\n+                      dim_orders_and_requirements.dim_orders.at(operand),\n+                      gpu_version, properties))) {\n             accepted = true;\n             break;\n           }\n"
                },
                {
                    "old_start": 1129,
                    "old_length": 9,
                    "new_start": 1243,
                    "new_length": 10,
                    "hunk": "@@ -1129,9 +1243,10 @@ DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n     if (fusion_level < 2) {\n       return \"Skipping fusing outputs at low fusion levels.\";\n     }\n-    for (const HloInstruction* operand : hlo.operands()) {\n-      // Skip already fused operands.\n-      if (old_to_new_map.contains(operand)) {\n+    for (int i = 0; i < hlo.operand_count(); ++i) {\n+      const HloInstruction* operand = hlo.operand(i);\n+      // Skip source operand.\n+      if (i == *src_operand_index) {\n         continue;\n       }\n       // Currently only broadcasts of scalar constants or parameters\n"
                },
                {
                    "old_start": 1147,
                    "old_length": 7,
                    "new_start": 1262,
                    "new_length": 7,
                    "hunk": "@@ -1147,7 +1262,7 @@ DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n       return \"Not obviously profitable to fuse as output.\";\n     }\n   }\n-  return std::get<DimOrderUpdates>(result);\n+  return dim_orders_and_requirements;\n }\n \n // Gets the fused HLO corresponding to `hlo` or adds a new parameter if not\n"
                },
                {
                    "old_start": 1223,
                    "old_length": 21,
                    "new_start": 1338,
                    "new_length": 24,
                    "hunk": "@@ -1223,21 +1338,24 @@ int64_t NumAddedParameters(const HloInstruction& hlo) {\n   return hlo.operand_count() - 1;\n }\n \n-bool FusionContext::MergeUpdates(const DimOrderUpdates& updates) {\n+bool FusionContext::CombineDimOrdersAndReqs(const DimOrdersAndReqs& update) {\n   // First check that all updates to insert are compatible to avoid\n   // incomplete merges.\n-  for (const auto& [key, value] : updates.map) {\n+  for (const auto& [key, value] : update.dim_orders) {\n     auto it = dim_orders_.find(key);\n     if (it != dim_orders_.cend() && !it->second.IsPhysicallyEquivalent(value)) {\n       return false;\n     }\n   }\n-  if (updates.splittable_dimension_major_part_size > 1 &&\n-      !SetSplittableDimensionMajorPartSize(\n-          updates.splittable_dimension_major_part_size)) {\n+\n+  RequirementsOrError requirements_or_error =\n+      CombineRequirements(requirements_, update.requirements);\n+  if (std::holds_alternative<FusionDecision>(requirements_or_error)) {\n     return false;\n   }\n-  dim_orders_.insert(updates.map.begin(), updates.map.end());\n+\n+  requirements_ = std::move(std::get<Requirements>(requirements_or_error));\n+  dim_orders_.insert(update.dim_orders.begin(), update.dim_orders.end());\n   return true;\n }\n \n"
                },
                {
                    "old_start": 1270,
                    "old_length": 10,
                    "new_start": 1388,
                    "new_length": 13,
                    "hunk": "@@ -1270,10 +1388,13 @@ void FusionContext::TryToFuseWithInputsRecursively(\n       continue;\n     }\n     num_requeued = 0;\n-    const DimOrderUpdatesOrError result = AnalyzeForFusion(\n-        *hlo, TransformDirection::kOutputToInput, old_to_new_map, gpu_version);\n-    if (!std::holds_alternative<DimOrderUpdates>(result) ||\n-        !MergeUpdates(std::get<DimOrderUpdates>(result))) {\n+    const DimOrdersAndReqsOrError result =\n+        GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+            *hlo, TransformDirection::kOutputToInput,\n+            /*src_operand_index=*/std::nullopt, dim_orders_.at(hlo),\n+            gpu_version, properties_);\n+    if (!std::holds_alternative<DimOrdersAndReqs>(result) ||\n+        !CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result))) {\n       continue;\n     }\n     if (hlo->opcode() != HloOpcode::kParameter) {\n"
                },
                {
                    "old_start": 1369,
                    "old_length": 7,
                    "new_start": 1490,
                    "new_length": 7,
                    "hunk": "@@ -1369,7 +1490,7 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n \n   // These describe _outputs_ of corresponding HLOs.\n   auto context = FusionContext::FromDotOutput(\n-      dot, /*split_k=*/1, lhs_context.SplittableDimensionMajorPartSize());\n+      dot, /*split_k=*/1, lhs_context.splittable_dimension_major_part_size());\n   HloInstruction* fusion_output = &dot;\n   bool output_changed = true;\n   while (output_changed) {\n"
                },
                {
                    "old_start": 1381,
                    "old_length": 13,
                    "new_start": 1502,
                    "new_length": 17,
                    "hunk": "@@ -1381,13 +1502,17 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n     if (!IsDistributiveOverAddition(*user)) {\n       break;\n     }\n-    auto result =\n-        context.AnalyzeForFusion(*user, TransformDirection::kInputToOutput,\n-                                 output_old_to_new_map, gpu_version);\n-    if (!std::holds_alternative<DimOrderUpdates>(result)) {\n+    DimOrdersAndReqsOrError result =\n+        FusionContext::GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+            *user, TransformDirection::kInputToOutput,\n+            user->operand_index(fusion_output),\n+            context.dim_orders().at(fusion_output), gpu_version,\n+            context.hero_properties());\n+    if (!std::holds_alternative<DimOrdersAndReqs>(result)) {\n       continue;\n     }\n-    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n+    TF_RET_CHECK(\n+        context.CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result)));\n     for (HloInstruction* operand : user->operands()) {\n       if (!output_old_to_new_map.contains(operand)) {\n         context.TryToFuseWithInputsRecursively(*operand, gpu_version,\n"
                },
                {
                    "old_start": 1509,
                    "old_length": 12,
                    "new_start": 1634,
                    "new_length": 11,
                    "hunk": "@@ -1509,12 +1634,11 @@ Status FusionContext::PropagateDimensionOrdersToParameters(\n       TF_RET_CHECK(parameters.insert(hlo).second);\n       VLOG(5) << hlo->ToString();\n     }\n-    auto result =\n-        HandleInstruction(hlo, dim_orders_, TransformDirection::kOutputToInput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n-    TF_RET_CHECK(\n-        RequireSupportedDimOrders(*hlo, std::get<DimOrderUpdates>(result)));\n-    TF_RET_CHECK(MergeUpdates(std::get<DimOrderUpdates>(result)));\n+    DimOrdersAndReqsOrError result = GetPropagatedDimOrdersAndRequirements(\n+        *hlo, dim_orders_.at(hlo), TransformDirection::kOutputToInput,\n+        properties_);\n+    TF_RET_CHECK(std::holds_alternative<DimOrdersAndReqs>(result));\n+    TF_RET_CHECK(CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result)));\n     iter_specs[hlo] = DimensionOrderToTensorIterationSpec(dim_orders_.at(hlo));\n     for (const HloInstruction* operand : hlo->operands()) {\n       if (!visited.insert(operand).second) {\n"
                },
                {
                    "old_start": 1531,
                    "old_length": 7,
                    "new_start": 1655,
                    "new_length": 7,
                    "hunk": "@@ -1531,7 +1655,7 @@ Status FusionContext::PropagateDimensionOrdersToParameters(\n   return OkStatus();\n }\n \n-}  // anonymous namespace\n+}  // namespace\n \n // Data types that are supported by the Triton emitters.\n bool IsTritonSupportedDataType(PrimitiveType type,\n"
                },
                {
                    "old_start": 1643,
                    "old_length": 14,
                    "new_start": 1767,
                    "new_length": 15,
                    "hunk": "@@ -1643,14 +1767,15 @@ Status TritonFusionAnalysis::ExecuteForSoftmaxFusion(\n \n Status TritonFusionAnalysis::ExecuteForDotFusion(const HloInstruction& dot,\n                                                  const int split_k) {\n-  int64_t lhs_nc_split_major_part_size = -1;\n+  int64_t lhs_nc_split_major_part_size = kNoSplitRequirement;\n   for (const Scope scope : {Scope::LHS, Scope::RHS}) {\n     const int operand_number = static_cast<int>(scope);\n     auto context = FusionContext::FromDotOperand(dot, operand_number, split_k);\n     TF_RETURN_IF_ERROR(context.PropagateDimensionOrdersToParameters(\n         *dot.operand(operand_number), parameters_[scope], iter_specs_[scope]));\n-    if (scope == Scope::LHS && context.SplittableDimensionMajorPartSize() > 1) {\n-      lhs_nc_split_major_part_size = context.SplittableDimensionMajorPartSize();\n+    if (scope == Scope::LHS) {\n+      lhs_nc_split_major_part_size =\n+          context.splittable_dimension_major_part_size();\n     }\n   }\n \n"
                },
                {
                    "old_start": 1661,
                    "old_length": 17,
                    "new_start": 1786,
                    "new_length": 19,
                    "hunk": "@@ -1661,17 +1786,19 @@ Status TritonFusionAnalysis::ExecuteForDotFusion(const HloInstruction& dot,\n   // Propagate dimension order from dot to root.\n   while (!output->IsRoot()) {\n     TF_RET_CHECK(output->user_count() == 1);\n+    const HloInstruction* input = output;\n     output = output->users()[0];\n-    auto result = context.HandleInstruction(output, context.DimOrders(),\n-                                            TransformDirection::kInputToOutput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n-    TF_RET_CHECK(context.RequireSupportedDimOrders(\n-        *output, std::get<DimOrderUpdates>(result)));\n-    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n+    DimOrdersAndReqsOrError result =\n+        context.GetPropagatedDimOrdersAndRequirements(\n+            *output, context.dim_orders().at(input),\n+            TransformDirection::kInputToOutput, context.hero_properties());\n+    TF_RET_CHECK(std::holds_alternative<DimOrdersAndReqs>(result));\n+    TF_RET_CHECK(\n+        context.CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result)));\n   }\n   TF_RET_CHECK(iter_specs_[Scope::OUTPUT]\n                    .insert({output, DimensionOrderToTensorIterationSpec(\n-                                        context.DimOrders().at(output))})\n+                                        context.dim_orders().at(output))})\n                    .second);\n   if (output != &dot) {\n     // Propagate back to parameters of the output fusion."
                }
            ],
            "whole_deleted": "-FusionDecision RequireTritonFusibleConvert(\n-    const HloInstruction* input, se::GpuComputeCapability gpu_version) {\n-  if (ShapeUtil::ByteSizeOf(input->operand(0)->shape()) >\n-      ShapeUtil::ByteSizeOf(input->shape())) {\n-struct DimOrderUpdates {\n-  DimOrderMap map;\n-  int64_t splittable_dimension_major_part_size = 0;\n-using DimOrderUpdatesOrError = std::variant<FusionDecision, DimOrderUpdates>;\n-  struct DotProperties {\n-    int splittable_dimension;\n-    int64_t splittable_dimension_supported_major_part_size;\n-  };\n-  struct SoftmaxProperties {\n-    int softmax_reduction_dimension;\n-    int softmax_batch_dimension;\n-  };\n-\n-  explicit FusionContext(DotProperties properties) : properties_(properties) {}\n-\n-  explicit FusionContext(SoftmaxProperties properties)\n-      : properties_(properties) {}\n-\n-  DimOrderUpdatesOrError HandleElementwise(const HloInstruction* hlo,\n-                                           const DimOrderMap& dim_orders) const;\n-  DimOrderUpdatesOrError HandleBitcast(const HloInstruction* hlo,\n-                                       const DimOrderMap& dim_orders,\n-                                       TransformDirection direction) const;\n-  DimOrderUpdatesOrError HandleDimensionAlteringOp(\n-      const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-      TransformDirection direction) const;\n-      int64_t splittable_dimension_supported_major_part_size);\n-  DimOrderUpdatesOrError HandleInstruction(const HloInstruction* hlo,\n-                                           const DimOrderMap& dim_orders,\n-                                           TransformDirection direction) const;\n-\n-  // Tells if the dimension order is supported by the triton emitters.\n-  // Only the dimension indicated by SplittableDimensionIndex() can be split\n-  // physically once by other dimensions. Other ones can be only split\n-  // logically. All subdimensions within a dimension have to be ordered.\n-  // Return major part of splittable dimension in split_dim_major_part if a\n-  // supported split is detected.\n-  FusionDecision RequireSupportedDimOrder(const DimensionOrder& order,\n-                                          int64_t& split_dim_major_part) const;\n-  // Apply RequireSupportedDimOrder() to all known dimension orders\n-  // around `hlo`.\n-  FusionDecision RequireSupportedDimOrders(const HloInstruction& hlo,\n-                                           DimOrderUpdates& updates) const;\n-  // Try to calculate transformations of dimensions defined by the\n-  // instruction, then check that the resulting dimension orders are supported.\n-  DimOrderUpdatesOrError RequireSupportedInstruction(\n-      TransformDirection direction) const;\n-  // Checks if the instruction is possible and profitable to fuse.\n-  // If so tries to transform dim_order describing one side of `hlo` into\n-  // description(s) of its other side if it is supported.\n-  DimOrderUpdatesOrError AnalyzeForFusion(\n-      OldToNewHloMap& old_to_new_map,\n-      se::GpuComputeCapability gpu_version) const;\n-  // Add dimension orders from `updates` to `dim_orders_` and update the\n-  // splittable dimension ratio if all of them are compatible.\n-  bool MergeUpdates(const DimOrderUpdates& updates);\n-  // Index of dot dimension that can be split.\n-  // Currently typically LHS non-contracting one.\n-  int64_t SplittableDimensionIndex() const {\n-    CHECK(std::holds_alternative<DotProperties>(properties_));\n-    return std::get<DotProperties>(properties_).splittable_dimension;\n-  }\n-  // Tells whether `size` major part of a dimension can be physically split.\n-  bool IsSupportedSplittableDimensionMajorPartSize(const int64_t size) const {\n-    CHECK_NE(size, 0);\n-    CHECK(std::holds_alternative<DotProperties>(properties_));\n-    // 0 means no specific size requirement.\n-    return std::get<DotProperties>(properties_)\n-                   .splittable_dimension_supported_major_part_size == 0 ||\n-           std::get<DotProperties>(properties_)\n-                   .splittable_dimension_supported_major_part_size == size;\n-  }\n-  int SplittableDimensionMajorPartSize() const {\n-    CHECK(std::holds_alternative<DotProperties>(properties_));\n-    return std::get<DotProperties>(properties_)\n-        .splittable_dimension_supported_major_part_size;\n-  }\n-  const DimOrderMap& DimOrders() const { return dim_orders_; }\n-\n- private:\n-  DimOrderUpdatesOrError AnalyzeForFusionImpl(\n-      const HloInstruction& hlo, TransformDirection transform_direction,\n-      OldToNewHloMap& old_to_new_map, const DimOrderMap& dim_orders,\n-      se::GpuComputeCapability gpu_version) const;\n-  bool SetSplittableDimensionMajorPartSize(const int64_t size) {\n-    if (IsSupportedSplittableDimensionMajorPartSize(size)) {\n-      std::get<DotProperties>(properties_)\n-          .splittable_dimension_supported_major_part_size = size;\n-      return true;\n-    }\n-    return false;\n-  std::variant<DotProperties, SoftmaxProperties> properties_;\n-  int split_k_dimension_index = -1;\n-  int splittable_dimension_index = -1;\n-  FusionContext context(FusionContext::DotProperties{\n-      splittable_dimension_index,\n-      /*splittable_dimension_supported_major_size=*/0});\n-    const int64_t splittable_dimension_supported_major_part_size) {\n-  int splittable_dimension_index = -1;\n-  if (splittable_dimension_supported_major_part_size > 1) {\n-  FusionContext context(FusionContext::DotProperties{\n-      splittable_dimension_index,\n-      splittable_dimension_supported_major_part_size});\n-  FusionContext context(FusionContext::SoftmaxProperties{\n-      DimensionOrder::kSoftmaxReductionDimension,\n-      DimensionOrder::kSoftmaxBatchDimension});\n-FusionDecision FusionContext::RequireSupportedDimOrder(\n-    const DimensionOrder& order, int64_t& split_dim_major_part) const {\n-        if (dim_index == SplittableDimensionIndex() &&\n-            IsSupportedSplittableDimensionMajorPartSize(grouped_size)) {\n-            if (split_dim_major_part != 0 &&\n-  return FusionDecision{};\n-FusionDecision FusionContext::RequireSupportedDimOrders(\n-    const HloInstruction& hlo, DimOrderUpdates& updates) const {\n-  auto check_if_present = [&](const HloInstruction* instr) {\n-    if (auto it = updates.map.find(instr); it != updates.map.end()) {\n-      return RequireSupportedDimOrder(\n-          it->second, updates.splittable_dimension_major_part_size);\n-    return FusionDecision{};\n-    if (auto result = check_if_present(operand); !result) {\n-      return result;\n-  return check_if_present(&hlo);\n-DimOrderUpdatesOrError FusionContext::HandleElementwise(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders) const {\n-  // The output and all the input dimension orders of `hlo` have to be the same.\n-  const HloInstruction* src = nullptr;\n-  const DimensionOrder* src_dim_order;\n-  // Try using the output as a reference if it's already described, otherwise\n-  // scan through all operands.\n-  if (auto it = dim_orders.find(hlo); it != dim_orders.cend()) {\n-    src = it->first;\n-    src_dim_order = &it->second;\n-  } else {\n-    for (const HloInstruction* operand : hlo->operands()) {\n-      if (auto it = dim_orders.find(operand); it != dim_orders.cend()) {\n-        src = it->first;\n-        src_dim_order = &it->second;\n-        break;\n-      }\n-    CHECK_NE(src, nullptr);\n-  DimOrderUpdates result;\n-  result.map.insert({hlo, DimensionOrder(*src_dim_order)});\n-  for (const HloInstruction* operand : hlo->operands()) {\n-    result.map.insert({operand, DimensionOrder(dim_orders.at(src))});\n-  return result;\n-DimOrderUpdatesOrError FusionContext::HandleBitcast(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection direction) const {\n-  const HloInstruction* src =\n-      (direction == TransformDirection::kOutputToInput) ? hlo : hlo->operand(0);\n-  const HloInstruction* dst =\n-      (direction == TransformDirection::kOutputToInput) ? hlo->operand(0) : hlo;\n-  const Shape& dst_shape = dst->shape();\n-  const Fragments& src_fragments_order =\n-      dim_orders.at(src).TensorFragmentsOrder();\n-  DimOrderUpdates result;\n-      result.map.insert({dst, DimensionOrder()}).first->second;\n-    if (std::holds_alternative<SoftmaxProperties>(properties_) &&\n-            std::get<SoftmaxProperties>(properties_).softmax_batch_dimension) {\n-       dim_orders.at(src).DimFragmentsOrders()) {\n-  return result;\n-DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection direction) const {\n-  const HloInstruction* src =\n-      (direction == TransformDirection::kOutputToInput) ? hlo : hlo->operand(0);\n-  Fragments src_fragments_order = dim_orders.at(src).TensorFragmentsOrder();\n-  if (hlo->opcode() == HloOpcode::kSlice &&\n-      ShapeUtil::IsEffectiveScalar(hlo->shape())) {\n-  src_physical.reserve(src->shape().rank());\n-  for (int64_t dim_index : src->shape().layout().minor_to_major()) {\n-    const int64_t dim_size = src->shape().dimensions(dim_index);\n-    src_logical[src->shape().layout().minor_to_major(i)] = src_physical[i];\n-  HloInstruction::InstructionVector output;\n-  output.push_back(const_cast<HloInstruction*>(hlo));\n-  DimOrderUpdates result;\n-  for (const HloInstruction* dst :\n-       (direction == TransformDirection::kInputToOutput) ? output\n-                                                         : hlo->operands()) {\n-        result.map.insert({dst, DimensionOrder()}).first->second;\n-    if (hlo->opcode() == HloOpcode::kTranspose) {\n-      const auto* transpose = Cast<HloTransposeInstruction>(hlo);\n-    } else if (hlo->opcode() == HloOpcode::kBroadcast) {\n-      const auto* broadcast = Cast<HloBroadcastInstruction>(hlo);\n-    } else if (hlo->opcode() == HloOpcode::kReduce) {\n-      if (dst != hlo && hlo->operand_index(dst) == 1) {\n-      const auto* reduce = Cast<HloReduceInstruction>(hlo);\n-              std::get<SoftmaxProperties>(properties_)\n-    } else if (hlo->opcode() == HloOpcode::kCopy) {\n-      CHECK(ShapeUtil::SameDimensions(src->shape(), dst->shape()));\n-    } else if (hlo->opcode() == HloOpcode::kPad) {\n-      if (dst != hlo && hlo->operand_index(dst) == 1) {\n-      const auto* pad = Cast<HloPadInstruction>(hlo);\n-    } else if (hlo->opcode() == HloOpcode::kSlice) {\n-      const auto slice = Cast<HloSliceInstruction>(hlo);\n-         dim_orders.at(src).DimFragmentsOrders()) {\n-          if (hlo->opcode() == HloOpcode::kBroadcast &&\n-  return result;\n-DimOrderUpdatesOrError FusionContext::HandleInstruction(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection direction) const {\n-  VLOG(7) << \"Analyzing \" << hlo->ToString();\n-  if (hlo->opcode() == HloOpcode::kParameter ||\n-      hlo_query::IsScalarConstant(hlo)) {\n-    return DimOrderUpdates{};\n-  } else if (hlo->opcode() == HloOpcode::kTranspose ||\n-             hlo->opcode() == HloOpcode::kCopy) {\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kBroadcast) {\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kReduce) {\n-    if (!std::holds_alternative<SoftmaxProperties>(properties_)) {\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kPad) {\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->operand_count() > 0 &&\n-                 hlo->opcode(), hlo->operand(0)->shape().element_type())) {\n-    return HandleElementwise(hlo, dim_orders);\n-  } else if (hlo->opcode() == HloOpcode::kBitcast) {\n-    return HandleBitcast(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kSlice) {\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kReshape) {\n-    if (!ShapeUtil::ReshapeIsBitcast(hlo->operand(0)->shape(), hlo->shape())) {\n-    return HandleBitcast(hlo, dim_orders, direction);\n-DimOrderUpdatesOrError FusionContext::RequireSupportedInstruction(\n-    const HloInstruction& hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection transform_direction) const {\n-  auto result = HandleInstruction(&hlo, dim_orders, transform_direction);\n-  if (!std::holds_alternative<DimOrderUpdates>(result)) {\n-    return std::get<FusionDecision>(result);\n-  }\n-\n-  if (FusionDecision supported =\n-          RequireSupportedDimOrders(hlo, std::get<DimOrderUpdates>(result));\n-      !supported) {\n-    return supported;\n-  }\n-  return std::get<DimOrderUpdates>(result);\n-DimOrderUpdatesOrError FusionContext::AnalyzeForFusion(\n-    const HloInstruction& hlo, const TransformDirection transform_direction,\n-    OldToNewHloMap& old_to_new_map,\n-    const se::GpuComputeCapability gpu_version) const {\n-  return AnalyzeForFusionImpl(hlo, transform_direction, old_to_new_map,\n-                              dim_orders_, gpu_version);\n-}\n-DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n-    const HloInstruction& hlo, const TransformDirection transform_direction,\n-    OldToNewHloMap& old_to_new_map, const DimOrderMap& dim_orders,\n-    const se::GpuComputeCapability gpu_version) const {\n-  DimOrderUpdatesOrError result =\n-      RequireSupportedInstruction(hlo, dim_orders, transform_direction);\n-  if (!std::holds_alternative<DimOrderUpdates>(result)) {\n-    return result;\n-        if (FusionDecision decision =\n-                RequireTritonFusibleConvert(&hlo, gpu_version);\n-              std::holds_alternative<DimOrderUpdates>(AnalyzeForFusionImpl(\n-                  *operand, transform_direction, old_to_new_map,\n-                  std::get<DimOrderUpdates>(result).map, gpu_version))) {\n-    for (const HloInstruction* operand : hlo.operands()) {\n-      // Skip already fused operands.\n-      if (old_to_new_map.contains(operand)) {\n-  return std::get<DimOrderUpdates>(result);\n-bool FusionContext::MergeUpdates(const DimOrderUpdates& updates) {\n-  for (const auto& [key, value] : updates.map) {\n-  if (updates.splittable_dimension_major_part_size > 1 &&\n-      !SetSplittableDimensionMajorPartSize(\n-          updates.splittable_dimension_major_part_size)) {\n-  dim_orders_.insert(updates.map.begin(), updates.map.end());\n-    const DimOrderUpdatesOrError result = AnalyzeForFusion(\n-        *hlo, TransformDirection::kOutputToInput, old_to_new_map, gpu_version);\n-    if (!std::holds_alternative<DimOrderUpdates>(result) ||\n-        !MergeUpdates(std::get<DimOrderUpdates>(result))) {\n-      dot, /*split_k=*/1, lhs_context.SplittableDimensionMajorPartSize());\n-    auto result =\n-        context.AnalyzeForFusion(*user, TransformDirection::kInputToOutput,\n-                                 output_old_to_new_map, gpu_version);\n-    if (!std::holds_alternative<DimOrderUpdates>(result)) {\n-    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n-    auto result =\n-        HandleInstruction(hlo, dim_orders_, TransformDirection::kOutputToInput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n-    TF_RET_CHECK(\n-        RequireSupportedDimOrders(*hlo, std::get<DimOrderUpdates>(result)));\n-    TF_RET_CHECK(MergeUpdates(std::get<DimOrderUpdates>(result)));\n-}  // anonymous namespace\n-  int64_t lhs_nc_split_major_part_size = -1;\n-    if (scope == Scope::LHS && context.SplittableDimensionMajorPartSize() > 1) {\n-      lhs_nc_split_major_part_size = context.SplittableDimensionMajorPartSize();\n-    auto result = context.HandleInstruction(output, context.DimOrders(),\n-                                            TransformDirection::kInputToOutput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n-    TF_RET_CHECK(context.RequireSupportedDimOrders(\n-        *output, std::get<DimOrderUpdates>(result)));\n-    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n-                                        context.DimOrders().at(output))})\n",
            "whole_added": "+#include <optional>\n+#include \"absl/container/inlined_vector.h\"\n+FusionDecision IsConversionWorthFusing(const HloInstruction& input,\n+                                       se::GpuComputeCapability gpu_version) {\n+  if (ShapeUtil::ByteSizeOf(input.operand(0)->shape()) >\n+      ShapeUtil::ByteSizeOf(input.shape())) {\n+using DimOrderMapOrError = std::variant<DimOrderMap, FusionDecision>;\n+// This represents an invalid dimension index.\n+constexpr int kNoDimensionIndex = -1;\n+struct DotProperties {\n+  // Index of dot dimension that can be split.\n+  // Currently typically LHS non-contracting one.\n+  const int splittable_dimension_index;\n+};\n+struct SoftmaxProperties {\n+  const int softmax_reduction_dimension;\n+  const int softmax_batch_dimension;\n+};\n+// HeroProperties depend only on the hero op and they don't change as we\n+// change the fusion.\n+using HeroProperties = std::variant<DotProperties, SoftmaxProperties>;\n+\n+// A special value for splittable_dimension_major_part_size.\n+constexpr int kNoSplitRequirement = 1;\n+struct DotRequirements {\n+  explicit DotRequirements(int64_t splittable_dimension_major_part_size)\n+      : splittable_dimension_major_part_size(\n+            splittable_dimension_major_part_size) {\n+    CHECK_GE(splittable_dimension_major_part_size, 1);\n+  }\n+  // If not kNoSplitRequirement, then the major part size of the splittable\n+  // dimension must be the given value.\n+  int64_t splittable_dimension_major_part_size;\n+struct SoftmaxRequirements {};\n+// Requirements can change depending on what we fuse.\n+using Requirements = std::variant<DotRequirements, SoftmaxRequirements>;\n+using RequirementsOrError = std::variant<Requirements, FusionDecision>;\n+\n+// The dimension orders and requirements resulting from propagating the\n+// dimension orders through an HLO.\n+struct DimOrdersAndReqs {\n+  DimOrderMap dim_orders;\n+  Requirements requirements;\n+};\n+using DimOrdersAndReqsOrError = std::variant<DimOrdersAndReqs, FusionDecision>;\n+\n+using Int64OrError = std::variant<int64_t, FusionDecision>;\n+Int64OrError CombineSplitDimMajorPartSizeReqs(int64_t a, int64_t b) {\n+  if (a == b || b == kNoSplitRequirement) {\n+    return a;\n+  }\n+  if (a == kNoSplitRequirement) {\n+    return b;\n+  }\n+  return FusionDecision(\"Conflicting splits of splittable dimension\");\n+}\n+\n+RequirementsOrError CombineDotRequirements(DotRequirements a,\n+                                           DotRequirements b) {\n+  Int64OrError combined_size_req =\n+      CombineSplitDimMajorPartSizeReqs(a.splittable_dimension_major_part_size,\n+                                       b.splittable_dimension_major_part_size);\n+  if (std::holds_alternative<FusionDecision>(combined_size_req)) {\n+    return std::get<FusionDecision>(combined_size_req);\n+  }\n+  return DotRequirements(std::get<int64_t>(combined_size_req));\n+}\n+\n+RequirementsOrError CombineSoftmaxRequirements(SoftmaxRequirements a,\n+                                               SoftmaxRequirements b) {\n+  // SoftmaxRequirements is an empty class for now.\n+  return a;\n+}\n+\n+RequirementsOrError CombineRequirements(Requirements a,\n+                                        RequirementsOrError b_or_error) {\n+  if (std::holds_alternative<FusionDecision>(b_or_error)) {\n+    return b_or_error;\n+  }\n+  const Requirements& b = std::get<Requirements>(b_or_error);\n+  if (std::holds_alternative<DotRequirements>(b)) {\n+    return CombineDotRequirements(std::get<DotRequirements>(a),\n+                                  std::get<DotRequirements>(b));\n+  }\n+  return CombineSoftmaxRequirements(std::get<SoftmaxRequirements>(a),\n+                                    std::get<SoftmaxRequirements>(b));\n+}\n+  FusionContext(HeroProperties properties, Requirements requirements)\n+      : properties_(properties), requirements_(requirements) {}\n+      int64_t splittable_dimension_major_part_size);\n+  // If possible, propagates `src_dim_order` (describing one side of `hlo`) to\n+  // the other side and returns those dim orders.\n+  static DimOrderMapOrError GetPropagatedDimOrders(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order, const HeroProperties& properties);\n+\n+  // If the dimension order is supported by the triton emitters, this returns\n+  // which requirements does this order impose on the fusion.\n+  //\n+  // All subdimensions within a dimension have to be ordered.\n+  static RequirementsOrError GetRequirementsIfSupportedOrder(\n+      const DimensionOrder& order, const HeroProperties& properties);\n+  // Apply GetRequirementsIfSupportedOrder() to all known\n+  // dimension orders around `hlo` and combine the result.\n+  static RequirementsOrError GetRequirementsIfSupportedOrders(\n+      const HeroProperties& properties);\n+  // If fusing the instruction is possible then it propagates\n+  // the `src_dim_order` (describing one side of `hlo`) to the other side and\n+  // returns those dim orders and the requirements that they impose on the\n+  // fusion.\n+  static DimOrdersAndReqsOrError GetPropagatedDimOrdersAndRequirements(\n+      const HloInstruction& hlo, const DimensionOrder& src_dim_order,\n+      TransformDirection direction, const HeroProperties& properties);\n+  // If fusing the instruction is possible *and profitable* then it propagates\n+  // the `src_dim_order` (describing one side of `hlo`) to the other side and\n+  // returns those dim orders and the requirements that they impose on the\n+  // fusion.\n+  //\n+  // `src_operand_index` must be set iff `transform_direction` is\n+  // kInputToOutput.\n+  static DimOrdersAndReqsOrError\n+  GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+      const std::optional<int>& src_operand_index,\n+      const DimensionOrder& src_dim_order,\n+      const se::GpuComputeCapability& gpu_version,\n+      const HeroProperties& properties);\n+\n+  // Add dimension orders from `update` to `dim_orders_` and update\n+  // `requirements_` if all of them are compatible.\n+  bool CombineDimOrdersAndReqs(const DimOrdersAndReqs& update);\n+  int64_t splittable_dimension_major_part_size() const {\n+    CHECK(std::holds_alternative<DotRequirements>(requirements_));\n+    return std::get<DotRequirements>(requirements_)\n+        .splittable_dimension_major_part_size;\n+  const HeroProperties& hero_properties() const { return properties_; }\n+  const DimOrderMap& dim_orders() const { return dim_orders_; }\n+ private:\n+  static DimOrderMap GetPropagatedDimOrdersForElementwise(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order);\n+  static DimOrderMapOrError GetPropagatedDimOrdersForBitcast(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order, const HeroProperties& properties);\n+  static DimOrderMapOrError GetPropagatedDimOrdersForDimAlteringOp(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order, const HeroProperties& properties);\n+\n+  const HeroProperties properties_;\n+  Requirements requirements_;\n+  int split_k_dimension_index = kNoDimensionIndex;\n+  int splittable_dimension_index = kNoDimensionIndex;\n+  FusionContext context(DotProperties{splittable_dimension_index},\n+                        DotRequirements(kNoSplitRequirement));\n+    const int64_t splittable_dimension_major_part_size) {\n+  int splittable_dimension_index = kNoDimensionIndex;\n+  if (splittable_dimension_major_part_size > 1) {\n+  FusionContext context(DotProperties{splittable_dimension_index},\n+                        DotRequirements(splittable_dimension_major_part_size));\n+  FusionContext context(\n+      SoftmaxProperties{DimensionOrder::kSoftmaxReductionDimension,\n+                        DimensionOrder::kSoftmaxBatchDimension},\n+      SoftmaxRequirements{});\n+/*static*/ RequirementsOrError FusionContext::GetRequirementsIfSupportedOrder(\n+    const DimensionOrder& order, const HeroProperties& properties) {\n+  int64_t split_dim_major_part = kNoSplitRequirement;\n+        if (!std::holds_alternative<DotProperties>(properties)) {\n+          return \"Splitting a dimension is not supported for Softmax.\";\n+        }\n+        // Only the dimension indicated by `splittable_dimension_index` (if any)\n+        // can be split physically once by other dimensions. Other ones can be\n+        // only split logically.\n+        const int splittable_dimension_index =\n+            std::get<DotProperties>(properties).splittable_dimension_index;\n+        if (dim_index == splittable_dimension_index) {\n+            if (split_dim_major_part != kNoSplitRequirement &&\n+\n+  if (std::holds_alternative<DotProperties>(properties)) {\n+    return DotRequirements(split_dim_major_part);\n+  }\n+  return SoftmaxRequirements{};\n+/*static*/ RequirementsOrError FusionContext::GetRequirementsIfSupportedOrders(\n+    const HloInstruction& hlo, const DimOrderMap& dim_orders,\n+    const HeroProperties& properties) {\n+  const Requirements empty_requirements =\n+      std::holds_alternative<DotProperties>(properties)\n+          ? Requirements(DotRequirements(kNoSplitRequirement))\n+          : Requirements(SoftmaxRequirements{});\n+  auto get_requirements =\n+      [&](const HloInstruction& instr) -> RequirementsOrError {\n+    if (auto it = dim_orders.find(&instr); it != dim_orders.end()) {\n+      return GetRequirementsIfSupportedOrder(it->second, properties);\n+    return empty_requirements;\n+\n+  Requirements requirements = empty_requirements;\n+    RequirementsOrError requirements_or_error =\n+        CombineRequirements(requirements, get_requirements(*operand));\n+    if (std::holds_alternative<FusionDecision>(requirements_or_error)) {\n+      return requirements_or_error;\n+    requirements = std::get<Requirements>(requirements_or_error);\n+\n+  return CombineRequirements(requirements, get_requirements(hlo));\n+/*static*/ DimOrderMap FusionContext::GetPropagatedDimOrdersForElementwise(\n+    const HloInstruction& hlo, TransformDirection direction,\n+    const DimensionOrder& src_dim_order) {\n+  if (direction == TransformDirection::kOutputToInput) {\n+    DimOrderMap map;\n+    for (const HloInstruction* operand : hlo.operands()) {\n+      map.insert({operand, src_dim_order});\n+    return map;\n+  }\n+\n+  DimOrderMap map;\n+  map.insert({&hlo, src_dim_order});\n+  // TODO(tdanyluk): For now, the \"input to output\" direction of this function\n+  // also returns the dim orders for the operands, not just the output. This is\n+  // needed to propagate the dim order of one input to the other(s) when fusing\n+  // elementwise ops to the output. Perhaps we can separate the \"input to\n+  // output\" and \"output to input\" directions of that in a later CL.\n+  for (const HloInstruction* operand : hlo.operands()) {\n+    map.insert({operand, src_dim_order});\n+  }\n+  return map;\n+}\n+\n+const HloInstruction& GetSourceHlo(const HloInstruction& hlo,\n+                                   TransformDirection direction) {\n+  CHECK_GE(hlo.operand_count(), 1);\n+\n+  if (direction == TransformDirection::kOutputToInput) {\n+    return hlo;\n+  }\n+  return *hlo.operand(0);\n+}\n+\n+using ConstInstructionVector = absl::InlinedVector<const HloInstruction*, 2>;\n+ConstInstructionVector GetDestHlos(const HloInstruction& hlo,\n+                                   TransformDirection direction) {\n+  if (direction == TransformDirection::kInputToOutput) {\n+    return {&hlo};\n+  ConstInstructionVector hlos;\n+  hlos.reserve(hlo.operands().size());\n+  for (const HloInstruction* operand : hlo.operands()) {\n+    hlos.push_back(operand);\n+  }\n+  return hlos;\n+}\n+\n+const HloInstruction& GetDestHlo(const HloInstruction& hlo,\n+                                 TransformDirection direction) {\n+  CHECK_EQ(hlo.operand_count(), 1);\n+\n+  if (direction == TransformDirection::kInputToOutput) {\n+    return hlo;\n+\n+  return *hlo.operand(0);\n+/*static*/ DimOrderMapOrError FusionContext::GetPropagatedDimOrdersForBitcast(\n+    const HloInstruction& hlo, const TransformDirection direction,\n+    const DimensionOrder& src_dim_order, const HeroProperties& properties) {\n+  const HloInstruction& dst = GetDestHlo(hlo, direction);\n+  const Shape& dst_shape = dst.shape();\n+  const Fragments& src_fragments_order = src_dim_order.TensorFragmentsOrder();\n+  DimOrderMap dst_dim_orders;\n+      dst_dim_orders.insert({&dst, DimensionOrder()}).first->second;\n+    if (std::holds_alternative<SoftmaxProperties>(properties) &&\n+            std::get<SoftmaxProperties>(properties).softmax_batch_dimension) {\n+       src_dim_order.DimFragmentsOrders()) {\n+  return dst_dim_orders;\n+/*static*/ DimOrderMapOrError\n+FusionContext::GetPropagatedDimOrdersForDimAlteringOp(\n+    const HloInstruction& hlo, const TransformDirection direction,\n+    const DimensionOrder& src_dim_order, const HeroProperties& properties) {\n+  const HloInstruction& src = GetSourceHlo(hlo, direction);\n+  Fragments src_fragments_order = src_dim_order.TensorFragmentsOrder();\n+  if (hlo.opcode() == HloOpcode::kSlice &&\n+      ShapeUtil::IsEffectiveScalar(hlo.shape())) {\n+  src_physical.reserve(src.shape().rank());\n+  for (int64_t dim_index : src.shape().layout().minor_to_major()) {\n+    const int64_t dim_size = src.shape().dimensions(dim_index);\n+    src_logical[src.shape().layout().minor_to_major(i)] = src_physical[i];\n+  DimOrderMap dst_dim_orders;\n+  for (const HloInstruction* dst : GetDestHlos(hlo, direction)) {\n+        dst_dim_orders.insert({dst, DimensionOrder()}).first->second;\n+    if (hlo.opcode() == HloOpcode::kTranspose) {\n+      const auto* transpose = Cast<HloTransposeInstruction>(&hlo);\n+    } else if (hlo.opcode() == HloOpcode::kBroadcast) {\n+      const auto* broadcast = Cast<HloBroadcastInstruction>(&hlo);\n+    } else if (hlo.opcode() == HloOpcode::kReduce) {\n+      if (dst != &hlo && hlo.operand_index(dst) == 1) {\n+      const auto* reduce = Cast<HloReduceInstruction>(&hlo);\n+              std::get<SoftmaxProperties>(properties)\n+    } else if (hlo.opcode() == HloOpcode::kCopy) {\n+      CHECK(ShapeUtil::SameDimensions(src.shape(), dst->shape()));\n+    } else if (hlo.opcode() == HloOpcode::kPad) {\n+      if (dst != &hlo && hlo.operand_index(dst) == 1) {\n+      const auto* pad = Cast<HloPadInstruction>(&hlo);\n+    } else if (hlo.opcode() == HloOpcode::kSlice) {\n+      const auto slice = Cast<HloSliceInstruction>(&hlo);\n+         src_dim_order.DimFragmentsOrders()) {\n+          if (hlo.opcode() == HloOpcode::kBroadcast &&\n+  return dst_dim_orders;\n+/*static*/ DimOrderMapOrError FusionContext::GetPropagatedDimOrders(\n+    const HloInstruction& hlo, const TransformDirection direction,\n+    const DimensionOrder& src_dim_order, const HeroProperties& properties) {\n+  VLOG(7) << \"Analyzing \" << hlo.ToString();\n+  if (hlo.opcode() == HloOpcode::kParameter ||\n+      hlo_query::IsScalarConstant(&hlo)) {\n+    CHECK(direction == TransformDirection::kOutputToInput);\n+    return DimOrderMap{};\n+  } else if (hlo.opcode() == HloOpcode::kTranspose ||\n+             hlo.opcode() == HloOpcode::kCopy) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kBroadcast) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kReduce) {\n+    if (!std::holds_alternative<SoftmaxProperties>(properties)) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kPad) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.operand_count() > 0 &&\n+                 hlo.opcode(), hlo.operand(0)->shape().element_type())) {\n+    return GetPropagatedDimOrdersForElementwise(hlo, direction, src_dim_order);\n+  } else if (hlo.opcode() == HloOpcode::kBitcast) {\n+    return GetPropagatedDimOrdersForBitcast(hlo, direction, src_dim_order,\n+                                            properties);\n+  } else if (hlo.opcode() == HloOpcode::kSlice) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kReshape) {\n+    if (!ShapeUtil::ReshapeIsBitcast(hlo.operand(0)->shape(), hlo.shape())) {\n+    return GetPropagatedDimOrdersForBitcast(hlo, direction, src_dim_order,\n+                                            properties);\n+/*static*/ DimOrdersAndReqsOrError\n+FusionContext::GetPropagatedDimOrdersAndRequirements(\n+    const HloInstruction& hlo, const DimensionOrder& src_dim_order,\n+    TransformDirection direction, const HeroProperties& properties) {\n+  DimOrderMapOrError propagated_dim_orders_or_error =\n+      GetPropagatedDimOrders(hlo, direction, src_dim_order, properties);\n+  if (std::holds_alternative<FusionDecision>(propagated_dim_orders_or_error)) {\n+    return std::get<FusionDecision>(propagated_dim_orders_or_error);\n+  }\n+  DimOrderMap propagated_dim_orders =\n+      std::move(std::get<DimOrderMap>(propagated_dim_orders_or_error));\n+  RequirementsOrError requirements_or_error =\n+      GetRequirementsIfSupportedOrders(hlo, propagated_dim_orders, properties);\n+  if (std::holds_alternative<FusionDecision>(requirements_or_error)) {\n+    return std::get<FusionDecision>(requirements_or_error);\n+  }\n+  return DimOrdersAndReqs{propagated_dim_orders,\n+                          std::get<Requirements>(requirements_or_error)};\n+/*static*/ DimOrdersAndReqsOrError\n+FusionContext::GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+    const HloInstruction& hlo, TransformDirection transform_direction,\n+    const std::optional<int>& src_operand_index,\n+    const DimensionOrder& src_dim_order,\n+    const se::GpuComputeCapability& gpu_version,\n+    const HeroProperties& properties) {\n+  CHECK_EQ(transform_direction == TransformDirection::kInputToOutput,\n+           src_operand_index.has_value());\n+  DimOrdersAndReqsOrError result_or_error =\n+      GetPropagatedDimOrdersAndRequirements(hlo, src_dim_order,\n+                                            transform_direction, properties);\n+  if (!std::holds_alternative<DimOrdersAndReqs>(result_or_error)) {\n+    return result_or_error;\n+  DimOrdersAndReqs dim_orders_and_requirements =\n+      std::move(std::get<DimOrdersAndReqs>(result_or_error));\n+        if (FusionDecision decision = IsConversionWorthFusing(hlo, gpu_version);\n+              std::holds_alternative<DimOrdersAndReqs>(\n+                  GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+                      *operand, TransformDirection::kOutputToInput,\n+                      /*src_operand_index=*/std::nullopt,\n+                      /*src_dim_order=*/\n+                      dim_orders_and_requirements.dim_orders.at(operand),\n+                      gpu_version, properties))) {\n+    for (int i = 0; i < hlo.operand_count(); ++i) {\n+      const HloInstruction* operand = hlo.operand(i);\n+      // Skip source operand.\n+      if (i == *src_operand_index) {\n+  return dim_orders_and_requirements;\n+bool FusionContext::CombineDimOrdersAndReqs(const DimOrdersAndReqs& update) {\n+  for (const auto& [key, value] : update.dim_orders) {\n+\n+  RequirementsOrError requirements_or_error =\n+      CombineRequirements(requirements_, update.requirements);\n+  if (std::holds_alternative<FusionDecision>(requirements_or_error)) {\n+\n+  requirements_ = std::move(std::get<Requirements>(requirements_or_error));\n+  dim_orders_.insert(update.dim_orders.begin(), update.dim_orders.end());\n+    const DimOrdersAndReqsOrError result =\n+        GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+            *hlo, TransformDirection::kOutputToInput,\n+            /*src_operand_index=*/std::nullopt, dim_orders_.at(hlo),\n+            gpu_version, properties_);\n+    if (!std::holds_alternative<DimOrdersAndReqs>(result) ||\n+        !CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result))) {\n+      dot, /*split_k=*/1, lhs_context.splittable_dimension_major_part_size());\n+    DimOrdersAndReqsOrError result =\n+        FusionContext::GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+            *user, TransformDirection::kInputToOutput,\n+            user->operand_index(fusion_output),\n+            context.dim_orders().at(fusion_output), gpu_version,\n+            context.hero_properties());\n+    if (!std::holds_alternative<DimOrdersAndReqs>(result)) {\n+    TF_RET_CHECK(\n+        context.CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result)));\n+    DimOrdersAndReqsOrError result = GetPropagatedDimOrdersAndRequirements(\n+        *hlo, dim_orders_.at(hlo), TransformDirection::kOutputToInput,\n+        properties_);\n+    TF_RET_CHECK(std::holds_alternative<DimOrdersAndReqs>(result));\n+    TF_RET_CHECK(CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result)));\n+}  // namespace\n+  int64_t lhs_nc_split_major_part_size = kNoSplitRequirement;\n+    if (scope == Scope::LHS) {\n+      lhs_nc_split_major_part_size =\n+          context.splittable_dimension_major_part_size();\n+    const HloInstruction* input = output;\n+    DimOrdersAndReqsOrError result =\n+        context.GetPropagatedDimOrdersAndRequirements(\n+            *output, context.dim_orders().at(input),\n+            TransformDirection::kInputToOutput, context.hero_properties());\n+    TF_RET_CHECK(std::holds_alternative<DimOrdersAndReqs>(result));\n+    TF_RET_CHECK(\n+        context.CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result)));\n+                                        context.dim_orders().at(output))})\n",
            "whole_hunk": "@@ -20,6 +20,7 @@ limitations under the License.\n #include <cstdint>\n #include <iterator>\n #include <list>\n+#include <optional>\n #include <queue>\n #include <string>\n #include <utility>\n@@ -29,6 +30,7 @@ limitations under the License.\n #include \"absl/algorithm/container.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/container/flat_hash_set.h\"\n+#include \"absl/container/inlined_vector.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/strings/str_cat.h\"\n@@ -106,14 +108,14 @@ bool IsDistributiveOverAddition(const HloInstruction& hlo) {\n \n namespace {\n \n-FusionDecision RequireTritonFusibleConvert(\n-    const HloInstruction* input, se::GpuComputeCapability gpu_version) {\n+FusionDecision IsConversionWorthFusing(const HloInstruction& input,\n+                                       se::GpuComputeCapability gpu_version) {\n   // TODO(b/266862494): Can pick up almost any\n   // convert, but if it's reducing the data volume it should rather be fused\n   // to the output of the producer kernel. However not all operations support\n   // output fusion - then it should be fused here anyway!\n-  if (ShapeUtil::ByteSizeOf(input->operand(0)->shape()) >\n-      ShapeUtil::ByteSizeOf(input->shape())) {\n+  if (ShapeUtil::ByteSizeOf(input.operand(0)->shape()) >\n+      ShapeUtil::ByteSizeOf(input.shape())) {\n     return \"Narrowing conversion.\";\n   }\n   return FusionDecision{};\n@@ -248,11 +250,89 @@ using Fragment = DimensionOrder::Fragment;\n using Fragments = DimensionOrder::Fragments;\n using FragmentOrders = DimensionOrder::FragmentOrders;\n using DimOrderMap = absl::flat_hash_map<const HloInstruction*, DimensionOrder>;\n+using DimOrderMapOrError = std::variant<DimOrderMap, FusionDecision>;\n \n-struct DimOrderUpdates {\n-  DimOrderMap map;\n-  int64_t splittable_dimension_major_part_size = 0;\n+// This represents an invalid dimension index.\n+constexpr int kNoDimensionIndex = -1;\n+struct DotProperties {\n+  // Index of dot dimension that can be split.\n+  // Currently typically LHS non-contracting one.\n+  const int splittable_dimension_index;\n+};\n+struct SoftmaxProperties {\n+  const int softmax_reduction_dimension;\n+  const int softmax_batch_dimension;\n+};\n+// HeroProperties depend only on the hero op and they don't change as we\n+// change the fusion.\n+using HeroProperties = std::variant<DotProperties, SoftmaxProperties>;\n+\n+// A special value for splittable_dimension_major_part_size.\n+constexpr int kNoSplitRequirement = 1;\n+struct DotRequirements {\n+  explicit DotRequirements(int64_t splittable_dimension_major_part_size)\n+      : splittable_dimension_major_part_size(\n+            splittable_dimension_major_part_size) {\n+    CHECK_GE(splittable_dimension_major_part_size, 1);\n+  }\n+  // If not kNoSplitRequirement, then the major part size of the splittable\n+  // dimension must be the given value.\n+  int64_t splittable_dimension_major_part_size;\n };\n+struct SoftmaxRequirements {};\n+// Requirements can change depending on what we fuse.\n+using Requirements = std::variant<DotRequirements, SoftmaxRequirements>;\n+using RequirementsOrError = std::variant<Requirements, FusionDecision>;\n+\n+// The dimension orders and requirements resulting from propagating the\n+// dimension orders through an HLO.\n+struct DimOrdersAndReqs {\n+  DimOrderMap dim_orders;\n+  Requirements requirements;\n+};\n+using DimOrdersAndReqsOrError = std::variant<DimOrdersAndReqs, FusionDecision>;\n+\n+using Int64OrError = std::variant<int64_t, FusionDecision>;\n+Int64OrError CombineSplitDimMajorPartSizeReqs(int64_t a, int64_t b) {\n+  if (a == b || b == kNoSplitRequirement) {\n+    return a;\n+  }\n+  if (a == kNoSplitRequirement) {\n+    return b;\n+  }\n+  return FusionDecision(\"Conflicting splits of splittable dimension\");\n+}\n+\n+RequirementsOrError CombineDotRequirements(DotRequirements a,\n+                                           DotRequirements b) {\n+  Int64OrError combined_size_req =\n+      CombineSplitDimMajorPartSizeReqs(a.splittable_dimension_major_part_size,\n+                                       b.splittable_dimension_major_part_size);\n+  if (std::holds_alternative<FusionDecision>(combined_size_req)) {\n+    return std::get<FusionDecision>(combined_size_req);\n+  }\n+  return DotRequirements(std::get<int64_t>(combined_size_req));\n+}\n+\n+RequirementsOrError CombineSoftmaxRequirements(SoftmaxRequirements a,\n+                                               SoftmaxRequirements b) {\n+  // SoftmaxRequirements is an empty class for now.\n+  return a;\n+}\n+\n+RequirementsOrError CombineRequirements(Requirements a,\n+                                        RequirementsOrError b_or_error) {\n+  if (std::holds_alternative<FusionDecision>(b_or_error)) {\n+    return b_or_error;\n+  }\n+  const Requirements& b = std::get<Requirements>(b_or_error);\n+  if (std::holds_alternative<DotRequirements>(b)) {\n+    return CombineDotRequirements(std::get<DotRequirements>(a),\n+                                  std::get<DotRequirements>(b));\n+  }\n+  return CombineSoftmaxRequirements(std::get<SoftmaxRequirements>(a),\n+                                    std::get<SoftmaxRequirements>(b));\n+}\n \n TensorIterationSpec DimensionOrderToTensorIterationSpec(\n     const DimensionOrder& order) {\n@@ -316,33 +396,12 @@ bool DimensionOrder::IsPhysicallyEquivalent(const DimensionOrder& other) const {\n \n enum class TransformDirection { kInputToOutput, kOutputToInput };\n \n-using DimOrderUpdatesOrError = std::variant<FusionDecision, DimOrderUpdates>;\n using OldToNewHloMap =\n     absl::flat_hash_map<const HloInstruction*, HloInstruction*>;\n \n class FusionContext {\n-  struct DotProperties {\n-    int splittable_dimension;\n-    int64_t splittable_dimension_supported_major_part_size;\n-  };\n-  struct SoftmaxProperties {\n-    int softmax_reduction_dimension;\n-    int softmax_batch_dimension;\n-  };\n-\n-  explicit FusionContext(DotProperties properties) : properties_(properties) {}\n-\n-  explicit FusionContext(SoftmaxProperties properties)\n-      : properties_(properties) {}\n-\n-  DimOrderUpdatesOrError HandleElementwise(const HloInstruction* hlo,\n-                                           const DimOrderMap& dim_orders) const;\n-  DimOrderUpdatesOrError HandleBitcast(const HloInstruction* hlo,\n-                                       const DimOrderMap& dim_orders,\n-                                       TransformDirection direction) const;\n-  DimOrderUpdatesOrError HandleDimensionAlteringOp(\n-      const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-      TransformDirection direction) const;\n+  FusionContext(HeroProperties properties, Requirements requirements)\n+      : properties_(properties), requirements_(requirements) {}\n \n  public:\n   // Create fusion context from a dot operand according to\n@@ -353,41 +412,52 @@ class FusionContext {\n   // Create fusion context from dot's output.\n   static FusionContext FromDotOutput(\n       const HloInstruction& dot, int split_k,\n-      int64_t splittable_dimension_supported_major_part_size);\n+      int64_t splittable_dimension_major_part_size);\n \n   static FusionContext FromSoftmaxRoot(const HloInstruction&);\n \n-  DimOrderUpdatesOrError HandleInstruction(const HloInstruction* hlo,\n-                                           const DimOrderMap& dim_orders,\n-                                           TransformDirection direction) const;\n-\n-  // Tells if the dimension order is supported by the triton emitters.\n-  // Only the dimension indicated by SplittableDimensionIndex() can be split\n-  // physically once by other dimensions. Other ones can be only split\n-  // logically. All subdimensions within a dimension have to be ordered.\n-  // Return major part of splittable dimension in split_dim_major_part if a\n-  // supported split is detected.\n-  FusionDecision RequireSupportedDimOrder(const DimensionOrder& order,\n-                                          int64_t& split_dim_major_part) const;\n-  // Apply RequireSupportedDimOrder() to all known dimension orders\n-  // around `hlo`.\n-  FusionDecision RequireSupportedDimOrders(const HloInstruction& hlo,\n-                                           DimOrderUpdates& updates) const;\n-  // Try to calculate transformations of dimensions defined by the\n-  // instruction, then check that the resulting dimension orders are supported.\n-  DimOrderUpdatesOrError RequireSupportedInstruction(\n+  // If possible, propagates `src_dim_order` (describing one side of `hlo`) to\n+  // the other side and returns those dim orders.\n+  static DimOrderMapOrError GetPropagatedDimOrders(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order, const HeroProperties& properties);\n+\n+  // If the dimension order is supported by the triton emitters, this returns\n+  // which requirements does this order impose on the fusion.\n+  //\n+  // All subdimensions within a dimension have to be ordered.\n+  static RequirementsOrError GetRequirementsIfSupportedOrder(\n+      const DimensionOrder& order, const HeroProperties& properties);\n+  // Apply GetRequirementsIfSupportedOrder() to all known\n+  // dimension orders around `hlo` and combine the result.\n+  static RequirementsOrError GetRequirementsIfSupportedOrders(\n       const HloInstruction& hlo, const DimOrderMap& dim_orders,\n-      TransformDirection direction) const;\n-  // Checks if the instruction is possible and profitable to fuse.\n-  // If so tries to transform dim_order describing one side of `hlo` into\n-  // description(s) of its other side if it is supported.\n-  DimOrderUpdatesOrError AnalyzeForFusion(\n+      const HeroProperties& properties);\n+  // If fusing the instruction is possible then it propagates\n+  // the `src_dim_order` (describing one side of `hlo`) to the other side and\n+  // returns those dim orders and the requirements that they impose on the\n+  // fusion.\n+  static DimOrdersAndReqsOrError GetPropagatedDimOrdersAndRequirements(\n+      const HloInstruction& hlo, const DimensionOrder& src_dim_order,\n+      TransformDirection direction, const HeroProperties& properties);\n+  // If fusing the instruction is possible *and profitable* then it propagates\n+  // the `src_dim_order` (describing one side of `hlo`) to the other side and\n+  // returns those dim orders and the requirements that they impose on the\n+  // fusion.\n+  //\n+  // `src_operand_index` must be set iff `transform_direction` is\n+  // kInputToOutput.\n+  static DimOrdersAndReqsOrError\n+  GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n       const HloInstruction& hlo, TransformDirection transform_direction,\n-      OldToNewHloMap& old_to_new_map,\n-      se::GpuComputeCapability gpu_version) const;\n-  // Add dimension orders from `updates` to `dim_orders_` and update the\n-  // splittable dimension ratio if all of them are compatible.\n-  bool MergeUpdates(const DimOrderUpdates& updates);\n+      const std::optional<int>& src_operand_index,\n+      const DimensionOrder& src_dim_order,\n+      const se::GpuComputeCapability& gpu_version,\n+      const HeroProperties& properties);\n+\n+  // Add dimension orders from `update` to `dim_orders_` and update\n+  // `requirements_` if all of them are compatible.\n+  bool CombineDimOrdersAndReqs(const DimOrdersAndReqs& update);\n   // Fuse an instruction with all its fusible inputs.\n   // If an input is not fusible stop there and make a parameter of the new\n   // fusion, otherwise put it onto stack and check its own inputs first.\n@@ -403,44 +473,27 @@ class FusionContext {\n       const HloInstruction& origin, ConstHloInstructionSet& parameters,\n       ConstHloInstructionMap<TensorIterationSpec>& iter_specs);\n \n-  // Index of dot dimension that can be split.\n-  // Currently typically LHS non-contracting one.\n-  int64_t SplittableDimensionIndex() const {\n-    CHECK(std::holds_alternative<DotProperties>(properties_));\n-    return std::get<DotProperties>(properties_).splittable_dimension;\n-  }\n-  // Tells whether `size` major part of a dimension can be physically split.\n-  bool IsSupportedSplittableDimensionMajorPartSize(const int64_t size) const {\n-    CHECK_NE(size, 0);\n-    CHECK(std::holds_alternative<DotProperties>(properties_));\n-    // 0 means no specific size requirement.\n-    return std::get<DotProperties>(properties_)\n-                   .splittable_dimension_supported_major_part_size == 0 ||\n-           std::get<DotProperties>(properties_)\n-                   .splittable_dimension_supported_major_part_size == size;\n-  }\n-  int SplittableDimensionMajorPartSize() const {\n-    CHECK(std::holds_alternative<DotProperties>(properties_));\n-    return std::get<DotProperties>(properties_)\n-        .splittable_dimension_supported_major_part_size;\n-  }\n-  const DimOrderMap& DimOrders() const { return dim_orders_; }\n-\n- private:\n-  DimOrderUpdatesOrError AnalyzeForFusionImpl(\n-      const HloInstruction& hlo, TransformDirection transform_direction,\n-      OldToNewHloMap& old_to_new_map, const DimOrderMap& dim_orders,\n-      se::GpuComputeCapability gpu_version) const;\n-  bool SetSplittableDimensionMajorPartSize(const int64_t size) {\n-    if (IsSupportedSplittableDimensionMajorPartSize(size)) {\n-      std::get<DotProperties>(properties_)\n-          .splittable_dimension_supported_major_part_size = size;\n-      return true;\n-    }\n-    return false;\n+  int64_t splittable_dimension_major_part_size() const {\n+    CHECK(std::holds_alternative<DotRequirements>(requirements_));\n+    return std::get<DotRequirements>(requirements_)\n+        .splittable_dimension_major_part_size;\n   }\n+  const HeroProperties& hero_properties() const { return properties_; }\n+  const DimOrderMap& dim_orders() const { return dim_orders_; }\n \n-  std::variant<DotProperties, SoftmaxProperties> properties_;\n+ private:\n+  static DimOrderMap GetPropagatedDimOrdersForElementwise(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order);\n+  static DimOrderMapOrError GetPropagatedDimOrdersForBitcast(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order, const HeroProperties& properties);\n+  static DimOrderMapOrError GetPropagatedDimOrdersForDimAlteringOp(\n+      const HloInstruction& hlo, TransformDirection direction,\n+      const DimensionOrder& src_dim_order, const HeroProperties& properties);\n+\n+  const HeroProperties properties_;\n+  Requirements requirements_;\n   DimOrderMap dim_orders_;\n };\n \n@@ -449,12 +502,12 @@ FusionContext FusionContext::FromDotOperand(const HloInstruction& dot,\n                                             const int split_k) {\n   // There can be either none or one split-K batch dimension.\n   const int num_split_k_batch_dims = split_k > 1;\n-  int split_k_dimension_index = -1;\n+  int split_k_dimension_index = kNoDimensionIndex;\n   if (split_k > 1) {\n     split_k_dimension_index =\n         ContractingDimensionIndex(dot, operand_number) - 1;\n   }\n-  int splittable_dimension_index = -1;\n+  int splittable_dimension_index = kNoDimensionIndex;\n   // LHS non-contracting dimension can be split if non-splitK batch is absent.\n   if (operand_number == 0 &&\n       dot.dot_dimension_numbers().lhs_batch_dimensions_size() -\n@@ -463,9 +516,8 @@ FusionContext FusionContext::FromDotOperand(const HloInstruction& dot,\n     splittable_dimension_index =\n         NonContractingDimensionIndex(dot, operand_number);\n   }\n-  FusionContext context(FusionContext::DotProperties{\n-      splittable_dimension_index,\n-      /*splittable_dimension_supported_major_size=*/0});\n+  FusionContext context(DotProperties{splittable_dimension_index},\n+                        DotRequirements(kNoSplitRequirement));\n   context.dim_orders_[dot.operand(operand_number)] =\n       DimensionOrder::FromDotOperandOrOutput(*dot.operand(operand_number),\n                                              split_k_dimension_index);\n@@ -474,34 +526,35 @@ FusionContext FusionContext::FromDotOperand(const HloInstruction& dot,\n \n FusionContext FusionContext::FromDotOutput(\n     const HloInstruction& dot, const int split_k,\n-    const int64_t splittable_dimension_supported_major_part_size) {\n+    const int64_t splittable_dimension_major_part_size) {\n   // Allow non-contracting dimension originating from LHS to split if\n   // this dimension is split at the output at the same ratio as\n   // at the input.\n-  int splittable_dimension_index = -1;\n-  if (splittable_dimension_supported_major_part_size > 1) {\n+  int splittable_dimension_index = kNoDimensionIndex;\n+  if (splittable_dimension_major_part_size > 1) {\n     // Split-K dimension is the first one in the output if present;\n     // LHS non-contracting follows (batch is absent in this case).\n     splittable_dimension_index = (split_k > 1) ? 1 : 0;\n   }\n-  FusionContext context(FusionContext::DotProperties{\n-      splittable_dimension_index,\n-      splittable_dimension_supported_major_part_size});\n+  FusionContext context(DotProperties{splittable_dimension_index},\n+                        DotRequirements(splittable_dimension_major_part_size));\n   context.dim_orders_[&dot] = DimensionOrder::FromDotOperandOrOutput(dot);\n   return context;\n }\n \n FusionContext FusionContext::FromSoftmaxRoot(const HloInstruction& root) {\n-  FusionContext context(FusionContext::SoftmaxProperties{\n-      DimensionOrder::kSoftmaxReductionDimension,\n-      DimensionOrder::kSoftmaxBatchDimension});\n+  FusionContext context(\n+      SoftmaxProperties{DimensionOrder::kSoftmaxReductionDimension,\n+                        DimensionOrder::kSoftmaxBatchDimension},\n+      SoftmaxRequirements{});\n   context.dim_orders_[&root] = DimensionOrder::FromSoftmaxRoot(root);\n   return context;\n }\n \n-FusionDecision FusionContext::RequireSupportedDimOrder(\n-    const DimensionOrder& order, int64_t& split_dim_major_part) const {\n+/*static*/ RequirementsOrError FusionContext::GetRequirementsIfSupportedOrder(\n+    const DimensionOrder& order, const HeroProperties& properties) {\n   VLOG(8) << order.ToString();\n+  int64_t split_dim_major_part = kNoSplitRequirement;\n   const Fragments& tensor_dim_fragments = order.TensorFragmentsOrder();\n   for (const auto& [dim_index, dim_fragments] : order.DimFragmentsOrders()) {\n     CHECK(!dim_fragments.empty());\n@@ -536,10 +589,17 @@ FusionDecision FusionContext::RequireSupportedDimOrder(\n \n       ++group_counter;\n       if (group_counter > 1) {\n-        if (dim_index == SplittableDimensionIndex() &&\n-            IsSupportedSplittableDimensionMajorPartSize(grouped_size)) {\n+        if (!std::holds_alternative<DotProperties>(properties)) {\n+          return \"Splitting a dimension is not supported for Softmax.\";\n+        }\n+        // Only the dimension indicated by `splittable_dimension_index` (if any)\n+        // can be split physically once by other dimensions. Other ones can be\n+        // only split logically.\n+        const int splittable_dimension_index =\n+            std::get<DotProperties>(properties).splittable_dimension_index;\n+        if (dim_index == splittable_dimension_index) {\n           if (group_counter == 2) {\n-            if (split_dim_major_part != 0 &&\n+            if (split_dim_major_part != kNoSplitRequirement &&\n                 split_dim_major_part != grouped_size) {\n               return \"Conflicting splits of splittable dimension\";\n             }\n@@ -556,68 +616,110 @@ FusionDecision FusionContext::RequireSupportedDimOrder(\n       ++fragment_it;\n     }\n   }\n-  return FusionDecision{};\n+\n+  if (std::holds_alternative<DotProperties>(properties)) {\n+    return DotRequirements(split_dim_major_part);\n+  }\n+  return SoftmaxRequirements{};\n }\n \n-FusionDecision FusionContext::RequireSupportedDimOrders(\n-    const HloInstruction& hlo, DimOrderUpdates& updates) const {\n-  auto check_if_present = [&](const HloInstruction* instr) {\n-    if (auto it = updates.map.find(instr); it != updates.map.end()) {\n-      return RequireSupportedDimOrder(\n-          it->second, updates.splittable_dimension_major_part_size);\n+/*static*/ RequirementsOrError FusionContext::GetRequirementsIfSupportedOrders(\n+    const HloInstruction& hlo, const DimOrderMap& dim_orders,\n+    const HeroProperties& properties) {\n+  const Requirements empty_requirements =\n+      std::holds_alternative<DotProperties>(properties)\n+          ? Requirements(DotRequirements(kNoSplitRequirement))\n+          : Requirements(SoftmaxRequirements{});\n+  auto get_requirements =\n+      [&](const HloInstruction& instr) -> RequirementsOrError {\n+    if (auto it = dim_orders.find(&instr); it != dim_orders.end()) {\n+      return GetRequirementsIfSupportedOrder(it->second, properties);\n     }\n-    return FusionDecision{};\n+    return empty_requirements;\n   };\n+\n+  Requirements requirements = empty_requirements;\n   for (const HloInstruction* operand : hlo.operands()) {\n-    if (auto result = check_if_present(operand); !result) {\n-      return result;\n+    RequirementsOrError requirements_or_error =\n+        CombineRequirements(requirements, get_requirements(*operand));\n+    if (std::holds_alternative<FusionDecision>(requirements_or_error)) {\n+      return requirements_or_error;\n     }\n+    requirements = std::get<Requirements>(requirements_or_error);\n   }\n-  return check_if_present(&hlo);\n+\n+  return CombineRequirements(requirements, get_requirements(hlo));\n }\n \n-DimOrderUpdatesOrError FusionContext::HandleElementwise(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders) const {\n-  // The output and all the input dimension orders of `hlo` have to be the same.\n-  const HloInstruction* src = nullptr;\n-  const DimensionOrder* src_dim_order;\n-  // Try using the output as a reference if it's already described, otherwise\n-  // scan through all operands.\n-  if (auto it = dim_orders.find(hlo); it != dim_orders.cend()) {\n-    src = it->first;\n-    src_dim_order = &it->second;\n-  } else {\n-    for (const HloInstruction* operand : hlo->operands()) {\n-      if (auto it = dim_orders.find(operand); it != dim_orders.cend()) {\n-        src = it->first;\n-        src_dim_order = &it->second;\n-        break;\n-      }\n+/*static*/ DimOrderMap FusionContext::GetPropagatedDimOrdersForElementwise(\n+    const HloInstruction& hlo, TransformDirection direction,\n+    const DimensionOrder& src_dim_order) {\n+  if (direction == TransformDirection::kOutputToInput) {\n+    DimOrderMap map;\n+    for (const HloInstruction* operand : hlo.operands()) {\n+      map.insert({operand, src_dim_order});\n     }\n-    CHECK_NE(src, nullptr);\n+    return map;\n+  }\n+\n+  DimOrderMap map;\n+  map.insert({&hlo, src_dim_order});\n+  // TODO(tdanyluk): For now, the \"input to output\" direction of this function\n+  // also returns the dim orders for the operands, not just the output. This is\n+  // needed to propagate the dim order of one input to the other(s) when fusing\n+  // elementwise ops to the output. Perhaps we can separate the \"input to\n+  // output\" and \"output to input\" directions of that in a later CL.\n+  for (const HloInstruction* operand : hlo.operands()) {\n+    map.insert({operand, src_dim_order});\n+  }\n+  return map;\n+}\n+\n+const HloInstruction& GetSourceHlo(const HloInstruction& hlo,\n+                                   TransformDirection direction) {\n+  CHECK_GE(hlo.operand_count(), 1);\n+\n+  if (direction == TransformDirection::kOutputToInput) {\n+    return hlo;\n+  }\n+  return *hlo.operand(0);\n+}\n+\n+using ConstInstructionVector = absl::InlinedVector<const HloInstruction*, 2>;\n+ConstInstructionVector GetDestHlos(const HloInstruction& hlo,\n+                                   TransformDirection direction) {\n+  if (direction == TransformDirection::kInputToOutput) {\n+    return {&hlo};\n   }\n \n-  DimOrderUpdates result;\n-  result.map.insert({hlo, DimensionOrder(*src_dim_order)});\n-  for (const HloInstruction* operand : hlo->operands()) {\n-    result.map.insert({operand, DimensionOrder(dim_orders.at(src))});\n+  ConstInstructionVector hlos;\n+  hlos.reserve(hlo.operands().size());\n+  for (const HloInstruction* operand : hlo.operands()) {\n+    hlos.push_back(operand);\n+  }\n+  return hlos;\n+}\n+\n+const HloInstruction& GetDestHlo(const HloInstruction& hlo,\n+                                 TransformDirection direction) {\n+  CHECK_EQ(hlo.operand_count(), 1);\n+\n+  if (direction == TransformDirection::kInputToOutput) {\n+    return hlo;\n   }\n-  return result;\n+\n+  return *hlo.operand(0);\n }\n \n-DimOrderUpdatesOrError FusionContext::HandleBitcast(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection direction) const {\n-  const HloInstruction* src =\n-      (direction == TransformDirection::kOutputToInput) ? hlo : hlo->operand(0);\n-  const HloInstruction* dst =\n-      (direction == TransformDirection::kOutputToInput) ? hlo->operand(0) : hlo;\n-  const Shape& dst_shape = dst->shape();\n-  const Fragments& src_fragments_order =\n-      dim_orders.at(src).TensorFragmentsOrder();\n-  DimOrderUpdates result;\n+/*static*/ DimOrderMapOrError FusionContext::GetPropagatedDimOrdersForBitcast(\n+    const HloInstruction& hlo, const TransformDirection direction,\n+    const DimensionOrder& src_dim_order, const HeroProperties& properties) {\n+  const HloInstruction& dst = GetDestHlo(hlo, direction);\n+  const Shape& dst_shape = dst.shape();\n+  const Fragments& src_fragments_order = src_dim_order.TensorFragmentsOrder();\n+  DimOrderMap dst_dim_orders;\n   DimensionOrder& dst_dim_order =\n-      result.map.insert({dst, DimensionOrder()}).first->second;\n+      dst_dim_orders.insert({&dst, DimensionOrder()}).first->second;\n   Fragments& dst_fragments_order = dst_dim_order.TensorFragmentsOrder();\n   // Size of not yet assigned part of current target dimension.\n   int64_t dst_remaining_size = 1;\n@@ -634,9 +736,9 @@ DimOrderUpdatesOrError FusionContext::HandleBitcast(\n       dst_fragments_order.push_back(fragment);\n       src_to_dst[&*src_dim].push_back(dst_fragments_order.size() - 1);\n     };\n-    if (std::holds_alternative<SoftmaxProperties>(properties_) &&\n+    if (std::holds_alternative<SoftmaxProperties>(properties) &&\n         src_dim->dst_dim_number() ==\n-            std::get<SoftmaxProperties>(properties_).softmax_batch_dimension) {\n+            std::get<SoftmaxProperties>(properties).softmax_batch_dimension) {\n       // Special handling for softmax batch dimension: allow arbitrary reshapes\n       // on it because it's guaranteed by the construction of the fusion to have\n       // no physical alterations like transposes.\n@@ -731,7 +833,7 @@ DimOrderUpdatesOrError FusionContext::HandleBitcast(\n \n   FragmentOrders& dst_dim_fragment_orders = dst_dim_order.DimFragmentsOrders();\n   for (const auto& [dim_index, dim_sequence] :\n-       dim_orders.at(src).DimFragmentsOrders()) {\n+       src_dim_order.DimFragmentsOrders()) {\n     std::vector<int>& dst = dst_dim_fragment_orders[dim_index];\n     dst.reserve(dim_sequence.size());\n     for (const int src : dim_sequence) {\n@@ -741,15 +843,16 @@ DimOrderUpdatesOrError FusionContext::HandleBitcast(\n     }\n   }\n \n-  return result;\n+  return dst_dim_orders;\n }\n \n // Handle copy, transpose, broadcast or reduce.\n // Common between them is that they alter the tensor dimensions or their order\n // and the way to handle layouts.\n-DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection direction) const {\n+/*static*/ DimOrderMapOrError\n+FusionContext::GetPropagatedDimOrdersForDimAlteringOp(\n+    const HloInstruction& hlo, const TransformDirection direction,\n+    const DimensionOrder& src_dim_order, const HeroProperties& properties) {\n   // Temporary storage for new fragments local to this function.\n   // Please keep this as the first local variable of this function, with type\n   // std::list to make sure that all pointers to elements of this remain valid\n@@ -757,13 +860,12 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n   // unnecessarily big for a typical size of 1.\n   std::list<Fragment> new_fragments;\n \n-  const HloInstruction* src =\n-      (direction == TransformDirection::kOutputToInput) ? hlo : hlo->operand(0);\n+  const HloInstruction& src = GetSourceHlo(hlo, direction);\n   // Note: copying instead of using a const reference because\n   // some operations (slice) will modify fragment properties in-place.\n-  Fragments src_fragments_order = dim_orders.at(src).TensorFragmentsOrder();\n-  if (hlo->opcode() == HloOpcode::kSlice &&\n-      ShapeUtil::IsEffectiveScalar(hlo->shape())) {\n+  Fragments src_fragments_order = src_dim_order.TensorFragmentsOrder();\n+  if (hlo.opcode() == HloOpcode::kSlice &&\n+      ShapeUtil::IsEffectiveScalar(hlo.shape())) {\n     return FusionDecision(\"Slice to scalar is not implemented yet.\");\n   }\n   // Every HLO dimension can correspond to a group of subdimensions in\n@@ -772,10 +874,10 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n   // Group subdimensions by iterating over them in the same order as over\n   // full dimensions and matching by total size.\n   std::vector<std::vector<Fragment*>> src_physical;\n-  src_physical.reserve(src->shape().rank());\n+  src_physical.reserve(src.shape().rank());\n   auto src_fragment_it = src_fragments_order.begin();\n-  for (int64_t dim_index : src->shape().layout().minor_to_major()) {\n-    const int64_t dim_size = src->shape().dimensions(dim_index);\n+  for (int64_t dim_index : src.shape().layout().minor_to_major()) {\n+    const int64_t dim_size = src.shape().dimensions(dim_index);\n     int64_t subdim_size_accumulator = 1;\n     std::vector<Fragment*> subdim_group;\n     do {\n@@ -792,21 +894,17 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n   std::vector<std::vector<Fragment*>> src_logical;\n   src_logical.resize(src_physical.size());\n   for (int i = 0; i < src_physical.size(); ++i) {\n-    src_logical[src->shape().layout().minor_to_major(i)] = src_physical[i];\n+    src_logical[src.shape().layout().minor_to_major(i)] = src_physical[i];\n   }\n \n-  HloInstruction::InstructionVector output;\n-  output.push_back(const_cast<HloInstruction*>(hlo));\n-  DimOrderUpdates result;\n-  for (const HloInstruction* dst :\n-       (direction == TransformDirection::kInputToOutput) ? output\n-                                                         : hlo->operands()) {\n+  DimOrderMap dst_dim_orders;\n+  for (const HloInstruction* dst : GetDestHlos(hlo, direction)) {\n     DimensionOrder& dst_dim_order =\n-        result.map.insert({dst, DimensionOrder()}).first->second;\n+        dst_dim_orders.insert({dst, DimensionOrder()}).first->second;\n     // Source logical -> destination logical.\n     std::vector<std::vector<Fragment*>> dst_logical;\n-    if (hlo->opcode() == HloOpcode::kTranspose) {\n-      const auto* transpose = Cast<HloTransposeInstruction>(hlo);\n+    if (hlo.opcode() == HloOpcode::kTranspose) {\n+      const auto* transpose = Cast<HloTransposeInstruction>(&hlo);\n       std::vector<int64_t> permutation(transpose->dimensions().cbegin(),\n                                        transpose->dimensions().cend());\n       if (direction == TransformDirection::kInputToOutput) {\n@@ -816,18 +914,18 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n       for (int i = 0; i < permutation.size(); ++i) {\n         dst_logical[permutation[i]] = src_logical[i];\n       }\n-    } else if (hlo->opcode() == HloOpcode::kBroadcast) {\n-      const auto* broadcast = Cast<HloBroadcastInstruction>(hlo);\n+    } else if (hlo.opcode() == HloOpcode::kBroadcast) {\n+      const auto* broadcast = Cast<HloBroadcastInstruction>(&hlo);\n       dst_logical.resize(broadcast->dimensions().size());\n       for (int i = 0; i < broadcast->dimensions().size(); ++i) {\n         dst_logical[i] = src_logical[broadcast->dimensions()[i]];\n       }\n-    } else if (hlo->opcode() == HloOpcode::kReduce) {\n+    } else if (hlo.opcode() == HloOpcode::kReduce) {\n       // Operand 1 (the neutral value) has to be a scalar.\n-      if (dst != hlo && hlo->operand_index(dst) == 1) {\n+      if (dst != &hlo && hlo.operand_index(dst) == 1) {\n         continue;\n       }\n-      const auto* reduce = Cast<HloReduceInstruction>(hlo);\n+      const auto* reduce = Cast<HloReduceInstruction>(&hlo);\n       dst_logical.resize(src_logical.size() + reduce->dimensions().size());\n       if (reduce->dimensions().size() != 1) {\n         return FusionDecision(\"Unsupported reduction.\");\n@@ -838,23 +936,23 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n           // softmax fusions with known patterns for now. Generally a reduction\n           // should create a new tiled dimension.\n           dst_logical[i] = {&new_fragments.emplace_back(\n-              std::get<SoftmaxProperties>(properties_)\n+              std::get<SoftmaxProperties>(properties)\n                   .softmax_reduction_dimension,\n               reduce->operand(0)->shape().dimensions(i))};\n         } else {\n           dst_logical[i] = src_logical[i];\n         }\n       }\n-    } else if (hlo->opcode() == HloOpcode::kCopy) {\n+    } else if (hlo.opcode() == HloOpcode::kCopy) {\n       // Copy preserves the logical shape, just permutes the layout.\n-      CHECK(ShapeUtil::SameDimensions(src->shape(), dst->shape()));\n+      CHECK(ShapeUtil::SameDimensions(src.shape(), dst->shape()));\n       dst_logical = src_logical;\n-    } else if (hlo->opcode() == HloOpcode::kPad) {\n+    } else if (hlo.opcode() == HloOpcode::kPad) {\n       // Operand 1 (the padding value) has to be a scalar.\n-      if (dst != hlo && hlo->operand_index(dst) == 1) {\n+      if (dst != &hlo && hlo.operand_index(dst) == 1) {\n         continue;\n       }\n-      const auto* pad = Cast<HloPadInstruction>(hlo);\n+      const auto* pad = Cast<HloPadInstruction>(&hlo);\n       dst_logical.resize(src_logical.size());\n       for (int i = 0; i < src_logical.size(); ++i) {\n         // This only handles the padding added by\n@@ -883,8 +981,8 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n           dst_logical[i] = {&new_fragments.back()};\n         }\n       }\n-    } else if (hlo->opcode() == HloOpcode::kSlice) {\n-      const auto slice = Cast<HloSliceInstruction>(hlo);\n+    } else if (hlo.opcode() == HloOpcode::kSlice) {\n+      const auto slice = Cast<HloSliceInstruction>(&hlo);\n       dst_logical.resize(src_logical.size());\n       for (int dim = 0; dim < src_logical.size(); ++dim) {\n         dst_logical[dim] = src_logical[dim];\n@@ -923,11 +1021,11 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n       }\n     }\n     for (const auto& [dim_index, dim_sequence] :\n-         dim_orders.at(src).DimFragmentsOrders()) {\n+         src_dim_order.DimFragmentsOrders()) {\n       for (const int fragment_number : dim_sequence) {\n         const auto it = src_to_dst.find(&src_fragments_order[fragment_number]);\n         if (it == src_to_dst.cend()) {\n-          if (hlo->opcode() == HloOpcode::kBroadcast &&\n+          if (hlo.opcode() == HloOpcode::kBroadcast &&\n               src_fragments_order[fragment_number].full_size() > 1 &&\n               dim_numbers_present_in_dst.contains(dim_index)) {\n             return FusionDecision(\"Unsupported broadcast\");\n@@ -938,55 +1036,63 @@ DimOrderUpdatesOrError FusionContext::HandleDimensionAlteringOp(\n       }\n     }\n   }\n-  return result;\n+  return dst_dim_orders;\n }\n \n // Infers DimensionOrders of all unknown sides (output, operands)\n // of `hlo` from the known ones.\n-DimOrderUpdatesOrError FusionContext::HandleInstruction(\n-    const HloInstruction* hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection direction) const {\n-  VLOG(7) << \"Analyzing \" << hlo->ToString();\n-  if (hlo->opcode() == HloOpcode::kParameter ||\n-      hlo_query::IsScalarConstant(hlo)) {\n-    return DimOrderUpdates{};\n-  } else if (hlo->opcode() == HloOpcode::kTranspose ||\n-             hlo->opcode() == HloOpcode::kCopy) {\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kBroadcast) {\n+/*static*/ DimOrderMapOrError FusionContext::GetPropagatedDimOrders(\n+    const HloInstruction& hlo, const TransformDirection direction,\n+    const DimensionOrder& src_dim_order, const HeroProperties& properties) {\n+  VLOG(7) << \"Analyzing \" << hlo.ToString();\n+  if (hlo.opcode() == HloOpcode::kParameter ||\n+      hlo_query::IsScalarConstant(&hlo)) {\n+    CHECK(direction == TransformDirection::kOutputToInput);\n+    return DimOrderMap{};\n+  } else if (hlo.opcode() == HloOpcode::kTranspose ||\n+             hlo.opcode() == HloOpcode::kCopy) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kBroadcast) {\n     if (direction != TransformDirection::kOutputToInput) {\n       return \"Unsupported broadcast direction.\";\n     }\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kReduce) {\n-    if (!std::holds_alternative<SoftmaxProperties>(properties_)) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kReduce) {\n+    if (!std::holds_alternative<SoftmaxProperties>(properties)) {\n       return \"Reductions are not supported in GEMM fusions yet.\";\n     }\n     if (direction != TransformDirection::kOutputToInput) {\n       return \"Unsupported direction of reduction.\";\n     }\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kPad) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kPad) {\n     if (direction != TransformDirection::kOutputToInput) {\n       return \"Unsupported pad direction.\";\n     }\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->operand_count() > 0 &&\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.operand_count() > 0 &&\n              IsTritonSupportedElementwise(\n-                 hlo->opcode(), hlo->operand(0)->shape().element_type())) {\n-    return HandleElementwise(hlo, dim_orders);\n-  } else if (hlo->opcode() == HloOpcode::kBitcast) {\n-    return HandleBitcast(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kSlice) {\n+                 hlo.opcode(), hlo.operand(0)->shape().element_type())) {\n+    return GetPropagatedDimOrdersForElementwise(hlo, direction, src_dim_order);\n+  } else if (hlo.opcode() == HloOpcode::kBitcast) {\n+    return GetPropagatedDimOrdersForBitcast(hlo, direction, src_dim_order,\n+                                            properties);\n+  } else if (hlo.opcode() == HloOpcode::kSlice) {\n     if (direction != TransformDirection::kOutputToInput) {\n       return \"Unsupported slice direction.\";\n     }\n-    return HandleDimensionAlteringOp(hlo, dim_orders, direction);\n-  } else if (hlo->opcode() == HloOpcode::kReshape) {\n-    if (!ShapeUtil::ReshapeIsBitcast(hlo->operand(0)->shape(), hlo->shape())) {\n+    return GetPropagatedDimOrdersForDimAlteringOp(hlo, direction, src_dim_order,\n+                                                  properties);\n+  } else if (hlo.opcode() == HloOpcode::kReshape) {\n+    if (!ShapeUtil::ReshapeIsBitcast(hlo.operand(0)->shape(), hlo.shape())) {\n       return \"Non-bitcast reshape.\";\n     }\n-    return HandleBitcast(hlo, dim_orders, direction);\n+    return GetPropagatedDimOrdersForBitcast(hlo, direction, src_dim_order,\n+                                            properties);\n   }\n   return \"Unimplemented instruction.\";\n }\n@@ -1033,34 +1139,36 @@ bool IsOutputWorthFusing(const HloInstruction& hlo) {\n          InputMinusOutputBytes(hlo) >= -kIoToleranceBytes;\n }\n \n-DimOrderUpdatesOrError FusionContext::RequireSupportedInstruction(\n-    const HloInstruction& hlo, const DimOrderMap& dim_orders,\n-    const TransformDirection transform_direction) const {\n-  auto result = HandleInstruction(&hlo, dim_orders, transform_direction);\n-  if (!std::holds_alternative<DimOrderUpdates>(result)) {\n-    return std::get<FusionDecision>(result);\n-  }\n-\n-  if (FusionDecision supported =\n-          RequireSupportedDimOrders(hlo, std::get<DimOrderUpdates>(result));\n-      !supported) {\n-    return supported;\n-  }\n-  return std::get<DimOrderUpdates>(result);\n+/*static*/ DimOrdersAndReqsOrError\n+FusionContext::GetPropagatedDimOrdersAndRequirements(\n+    const HloInstruction& hlo, const DimensionOrder& src_dim_order,\n+    TransformDirection direction, const HeroProperties& properties) {\n+  DimOrderMapOrError propagated_dim_orders_or_error =\n+      GetPropagatedDimOrders(hlo, direction, src_dim_order, properties);\n+  if (std::holds_alternative<FusionDecision>(propagated_dim_orders_or_error)) {\n+    return std::get<FusionDecision>(propagated_dim_orders_or_error);\n+  }\n+  DimOrderMap propagated_dim_orders =\n+      std::move(std::get<DimOrderMap>(propagated_dim_orders_or_error));\n+  RequirementsOrError requirements_or_error =\n+      GetRequirementsIfSupportedOrders(hlo, propagated_dim_orders, properties);\n+  if (std::holds_alternative<FusionDecision>(requirements_or_error)) {\n+    return std::get<FusionDecision>(requirements_or_error);\n+  }\n+  return DimOrdersAndReqs{propagated_dim_orders,\n+                          std::get<Requirements>(requirements_or_error)};\n }\n \n-DimOrderUpdatesOrError FusionContext::AnalyzeForFusion(\n-    const HloInstruction& hlo, const TransformDirection transform_direction,\n-    OldToNewHloMap& old_to_new_map,\n-    const se::GpuComputeCapability gpu_version) const {\n-  return AnalyzeForFusionImpl(hlo, transform_direction, old_to_new_map,\n-                              dim_orders_, gpu_version);\n-}\n+/*static*/ DimOrdersAndReqsOrError\n+FusionContext::GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+    const HloInstruction& hlo, TransformDirection transform_direction,\n+    const std::optional<int>& src_operand_index,\n+    const DimensionOrder& src_dim_order,\n+    const se::GpuComputeCapability& gpu_version,\n+    const HeroProperties& properties) {\n+  CHECK_EQ(transform_direction == TransformDirection::kInputToOutput,\n+           src_operand_index.has_value());\n \n-DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n-    const HloInstruction& hlo, const TransformDirection transform_direction,\n-    OldToNewHloMap& old_to_new_map, const DimOrderMap& dim_orders,\n-    const se::GpuComputeCapability gpu_version) const {\n   if (hlo.opcode() == HloOpcode::kTuple ||\n       hlo.opcode() == HloOpcode::kGetTupleElement) {\n     return \"Unsupported instruction.\";\n@@ -1080,11 +1188,14 @@ DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n   if (!IsTritonSupportedDataType(hlo.shape().element_type(), gpu_version)) {\n     return \"Unsupported output data type.\";\n   }\n-  DimOrderUpdatesOrError result =\n-      RequireSupportedInstruction(hlo, dim_orders, transform_direction);\n-  if (!std::holds_alternative<DimOrderUpdates>(result)) {\n-    return result;\n+  DimOrdersAndReqsOrError result_or_error =\n+      GetPropagatedDimOrdersAndRequirements(hlo, src_dim_order,\n+                                            transform_direction, properties);\n+  if (!std::holds_alternative<DimOrdersAndReqs>(result_or_error)) {\n+    return result_or_error;\n   }\n+  DimOrdersAndReqs dim_orders_and_requirements =\n+      std::move(std::get<DimOrdersAndReqs>(result_or_error));\n   int fusion_level =\n       hlo.GetModule()->config().debug_options().xla_gpu_triton_fusion_level();\n   if (!std::get<se::CudaComputeCapability>(gpu_version)\n@@ -1094,8 +1205,7 @@ DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n   if (transform_direction == TransformDirection::kOutputToInput) {\n     if (fusion_level < 2) {\n       if (hlo.opcode() == HloOpcode::kConvert) {\n-        if (FusionDecision decision =\n-                RequireTritonFusibleConvert(&hlo, gpu_version);\n+        if (FusionDecision decision = IsConversionWorthFusing(hlo, gpu_version);\n             !decision) {\n           return decision;\n         }\n@@ -1113,9 +1223,13 @@ DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n           if (operand->opcode() == HloOpcode::kBroadcast &&\n               (operand->operand(0)->opcode() == HloOpcode::kParameter ||\n                operand->operand(0)->opcode() == HloOpcode::kConstant) &&\n-              std::holds_alternative<DimOrderUpdates>(AnalyzeForFusionImpl(\n-                  *operand, transform_direction, old_to_new_map,\n-                  std::get<DimOrderUpdates>(result).map, gpu_version))) {\n+              std::holds_alternative<DimOrdersAndReqs>(\n+                  GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+                      *operand, TransformDirection::kOutputToInput,\n+                      /*src_operand_index=*/std::nullopt,\n+                      /*src_dim_order=*/\n+                      dim_orders_and_requirements.dim_orders.at(operand),\n+                      gpu_version, properties))) {\n             accepted = true;\n             break;\n           }\n@@ -1129,9 +1243,10 @@ DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n     if (fusion_level < 2) {\n       return \"Skipping fusing outputs at low fusion levels.\";\n     }\n-    for (const HloInstruction* operand : hlo.operands()) {\n-      // Skip already fused operands.\n-      if (old_to_new_map.contains(operand)) {\n+    for (int i = 0; i < hlo.operand_count(); ++i) {\n+      const HloInstruction* operand = hlo.operand(i);\n+      // Skip source operand.\n+      if (i == *src_operand_index) {\n         continue;\n       }\n       // Currently only broadcasts of scalar constants or parameters\n@@ -1147,7 +1262,7 @@ DimOrderUpdatesOrError FusionContext::AnalyzeForFusionImpl(\n       return \"Not obviously profitable to fuse as output.\";\n     }\n   }\n-  return std::get<DimOrderUpdates>(result);\n+  return dim_orders_and_requirements;\n }\n \n // Gets the fused HLO corresponding to `hlo` or adds a new parameter if not\n@@ -1223,21 +1338,24 @@ int64_t NumAddedParameters(const HloInstruction& hlo) {\n   return hlo.operand_count() - 1;\n }\n \n-bool FusionContext::MergeUpdates(const DimOrderUpdates& updates) {\n+bool FusionContext::CombineDimOrdersAndReqs(const DimOrdersAndReqs& update) {\n   // First check that all updates to insert are compatible to avoid\n   // incomplete merges.\n-  for (const auto& [key, value] : updates.map) {\n+  for (const auto& [key, value] : update.dim_orders) {\n     auto it = dim_orders_.find(key);\n     if (it != dim_orders_.cend() && !it->second.IsPhysicallyEquivalent(value)) {\n       return false;\n     }\n   }\n-  if (updates.splittable_dimension_major_part_size > 1 &&\n-      !SetSplittableDimensionMajorPartSize(\n-          updates.splittable_dimension_major_part_size)) {\n+\n+  RequirementsOrError requirements_or_error =\n+      CombineRequirements(requirements_, update.requirements);\n+  if (std::holds_alternative<FusionDecision>(requirements_or_error)) {\n     return false;\n   }\n-  dim_orders_.insert(updates.map.begin(), updates.map.end());\n+\n+  requirements_ = std::move(std::get<Requirements>(requirements_or_error));\n+  dim_orders_.insert(update.dim_orders.begin(), update.dim_orders.end());\n   return true;\n }\n \n@@ -1270,10 +1388,13 @@ void FusionContext::TryToFuseWithInputsRecursively(\n       continue;\n     }\n     num_requeued = 0;\n-    const DimOrderUpdatesOrError result = AnalyzeForFusion(\n-        *hlo, TransformDirection::kOutputToInput, old_to_new_map, gpu_version);\n-    if (!std::holds_alternative<DimOrderUpdates>(result) ||\n-        !MergeUpdates(std::get<DimOrderUpdates>(result))) {\n+    const DimOrdersAndReqsOrError result =\n+        GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+            *hlo, TransformDirection::kOutputToInput,\n+            /*src_operand_index=*/std::nullopt, dim_orders_.at(hlo),\n+            gpu_version, properties_);\n+    if (!std::holds_alternative<DimOrdersAndReqs>(result) ||\n+        !CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result))) {\n       continue;\n     }\n     if (hlo->opcode() != HloOpcode::kParameter) {\n@@ -1369,7 +1490,7 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n \n   // These describe _outputs_ of corresponding HLOs.\n   auto context = FusionContext::FromDotOutput(\n-      dot, /*split_k=*/1, lhs_context.SplittableDimensionMajorPartSize());\n+      dot, /*split_k=*/1, lhs_context.splittable_dimension_major_part_size());\n   HloInstruction* fusion_output = &dot;\n   bool output_changed = true;\n   while (output_changed) {\n@@ -1381,13 +1502,17 @@ StatusOr<FusionDecision> FuseDot(HloInstruction& dot,\n     if (!IsDistributiveOverAddition(*user)) {\n       break;\n     }\n-    auto result =\n-        context.AnalyzeForFusion(*user, TransformDirection::kInputToOutput,\n-                                 output_old_to_new_map, gpu_version);\n-    if (!std::holds_alternative<DimOrderUpdates>(result)) {\n+    DimOrdersAndReqsOrError result =\n+        FusionContext::GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible(\n+            *user, TransformDirection::kInputToOutput,\n+            user->operand_index(fusion_output),\n+            context.dim_orders().at(fusion_output), gpu_version,\n+            context.hero_properties());\n+    if (!std::holds_alternative<DimOrdersAndReqs>(result)) {\n       continue;\n     }\n-    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n+    TF_RET_CHECK(\n+        context.CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result)));\n     for (HloInstruction* operand : user->operands()) {\n       if (!output_old_to_new_map.contains(operand)) {\n         context.TryToFuseWithInputsRecursively(*operand, gpu_version,\n@@ -1509,12 +1634,11 @@ Status FusionContext::PropagateDimensionOrdersToParameters(\n       TF_RET_CHECK(parameters.insert(hlo).second);\n       VLOG(5) << hlo->ToString();\n     }\n-    auto result =\n-        HandleInstruction(hlo, dim_orders_, TransformDirection::kOutputToInput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n-    TF_RET_CHECK(\n-        RequireSupportedDimOrders(*hlo, std::get<DimOrderUpdates>(result)));\n-    TF_RET_CHECK(MergeUpdates(std::get<DimOrderUpdates>(result)));\n+    DimOrdersAndReqsOrError result = GetPropagatedDimOrdersAndRequirements(\n+        *hlo, dim_orders_.at(hlo), TransformDirection::kOutputToInput,\n+        properties_);\n+    TF_RET_CHECK(std::holds_alternative<DimOrdersAndReqs>(result));\n+    TF_RET_CHECK(CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result)));\n     iter_specs[hlo] = DimensionOrderToTensorIterationSpec(dim_orders_.at(hlo));\n     for (const HloInstruction* operand : hlo->operands()) {\n       if (!visited.insert(operand).second) {\n@@ -1531,7 +1655,7 @@ Status FusionContext::PropagateDimensionOrdersToParameters(\n   return OkStatus();\n }\n \n-}  // anonymous namespace\n+}  // namespace\n \n // Data types that are supported by the Triton emitters.\n bool IsTritonSupportedDataType(PrimitiveType type,\n@@ -1643,14 +1767,15 @@ Status TritonFusionAnalysis::ExecuteForSoftmaxFusion(\n \n Status TritonFusionAnalysis::ExecuteForDotFusion(const HloInstruction& dot,\n                                                  const int split_k) {\n-  int64_t lhs_nc_split_major_part_size = -1;\n+  int64_t lhs_nc_split_major_part_size = kNoSplitRequirement;\n   for (const Scope scope : {Scope::LHS, Scope::RHS}) {\n     const int operand_number = static_cast<int>(scope);\n     auto context = FusionContext::FromDotOperand(dot, operand_number, split_k);\n     TF_RETURN_IF_ERROR(context.PropagateDimensionOrdersToParameters(\n         *dot.operand(operand_number), parameters_[scope], iter_specs_[scope]));\n-    if (scope == Scope::LHS && context.SplittableDimensionMajorPartSize() > 1) {\n-      lhs_nc_split_major_part_size = context.SplittableDimensionMajorPartSize();\n+    if (scope == Scope::LHS) {\n+      lhs_nc_split_major_part_size =\n+          context.splittable_dimension_major_part_size();\n     }\n   }\n \n@@ -1661,17 +1786,19 @@ Status TritonFusionAnalysis::ExecuteForDotFusion(const HloInstruction& dot,\n   // Propagate dimension order from dot to root.\n   while (!output->IsRoot()) {\n     TF_RET_CHECK(output->user_count() == 1);\n+    const HloInstruction* input = output;\n     output = output->users()[0];\n-    auto result = context.HandleInstruction(output, context.DimOrders(),\n-                                            TransformDirection::kInputToOutput);\n-    TF_RET_CHECK(std::holds_alternative<DimOrderUpdates>(result));\n-    TF_RET_CHECK(context.RequireSupportedDimOrders(\n-        *output, std::get<DimOrderUpdates>(result)));\n-    TF_RET_CHECK(context.MergeUpdates(std::get<DimOrderUpdates>(result)));\n+    DimOrdersAndReqsOrError result =\n+        context.GetPropagatedDimOrdersAndRequirements(\n+            *output, context.dim_orders().at(input),\n+            TransformDirection::kInputToOutput, context.hero_properties());\n+    TF_RET_CHECK(std::holds_alternative<DimOrdersAndReqs>(result));\n+    TF_RET_CHECK(\n+        context.CombineDimOrdersAndReqs(std::get<DimOrdersAndReqs>(result)));\n   }\n   TF_RET_CHECK(iter_specs_[Scope::OUTPUT]\n                    .insert({output, DimensionOrderToTensorIterationSpec(\n-                                        context.DimOrders().at(output))})\n+                                        context.dim_orders().at(output))})\n                    .second);\n   if (output != &dot) {\n     // Propagate back to parameters of the output fusion."
        }
    ]
},
{
    "Id": 182,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/09b2e52e6dd1fd746503d8d457f16d09e272b03c",
    "date": "2024-01-25T04:30:59-08:00",
    "message": "Do not match `RewriteQuantizedDotGeneralOpToTflFullyConnectedOrBatchMatmulOp` when dot_general is not followed by a requantization.\n\nThe case without a following requantization is partially supported by the other pattern, `RewriteUpstreamQuantizedDotGeneralOpToTflFullyConnectedOp`, but this change guards against an edge case where the output is an i32 quantized tensor.\nWithout this change, the program will crash when a `dot_general` produces an `i32` quantized tensor but is not followed by a requantization.\n\nPiperOrigin-RevId: 601407496",
    "label": "YES",
    "changes": [
        {
            "name": "uniform-quantized-stablehlo-to-tfl.mlir",
            "path": "tensorflow/compiler/mlir/lite/stablehlo/tests/uniform-quantized-stablehlo-to-tfl.mlir",
            "patches": [
                {
                    "old_start": 519,
                    "old_length": 6,
                    "new_start": 519,
                    "new_length": 25,
                    "hunk": "@@ -519,6 +519,25 @@ func.func @dot_general_upstream_full_integer_per_axis_quantized_filter_with_mult\n \n // -----\n \n+// Test that a `stablehlo.dot_general` with an i32 output remains unchanged when\n+// it is not followed by a requantization (`stablehlo.quantize`).\n+\n+// CHECK-LABEL: dot_general_no_requantize\n+func.func @dot_general_no_requantize(%arg0: tensor<1x4xf32>) -> tensor<1x3xf32> {\n+  %0 = stablehlo.constant() {value = dense<5> : tensor<4x3xi8>} : () -> tensor<4x3x!quant.uniform<i8<-127:127>:f32, 2.000000e+00>>\n+  %1 = stablehlo.uniform_quantize %arg0 : (tensor<1x4xf32>) -> tensor<1x4x!quant.uniform<i8:f32, 3.000000e+00>>\n+  %2 = stablehlo.dot_general %1, %0, contracting_dims = [1] x [0] : (tensor<1x4x!quant.uniform<i8:f32, 3.000000e+00>>, tensor<4x3x!quant.uniform<i8<-127:127>:f32, 2.000000e+00>>) -> tensor<1x3x!quant.uniform<i32:f32, 5.000000e+00>>\n+  %3 = stablehlo.uniform_dequantize %2 : (tensor<1x3x!quant.uniform<i32:f32, 5.000000e+00>>) -> tensor<1x3xf32>\n+  return %3 : tensor<1x3xf32>\n+}\n+// CHECK: \"tfl.quantize\"\n+// CHECK: stablehlo.dot_general\n+// CHECK-NOT: tfl.fully_connected\n+// CHECK-NOT: tfl.batch_matmul\n+// CHECK: stablehlo.uniform_dequantize\n+\n+// -----\n+\n // Test that a quantized stablehlo.transpose is converted to tfl.transpose.\n \n // CHECK-LABEL: transpose\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// Test that a `stablehlo.dot_general` with an i32 output remains unchanged when\n+// it is not followed by a requantization (`stablehlo.quantize`).\n+\n+// CHECK-LABEL: dot_general_no_requantize\n+func.func @dot_general_no_requantize(%arg0: tensor<1x4xf32>) -> tensor<1x3xf32> {\n+  %0 = stablehlo.constant() {value = dense<5> : tensor<4x3xi8>} : () -> tensor<4x3x!quant.uniform<i8<-127:127>:f32, 2.000000e+00>>\n+  %1 = stablehlo.uniform_quantize %arg0 : (tensor<1x4xf32>) -> tensor<1x4x!quant.uniform<i8:f32, 3.000000e+00>>\n+  %2 = stablehlo.dot_general %1, %0, contracting_dims = [1] x [0] : (tensor<1x4x!quant.uniform<i8:f32, 3.000000e+00>>, tensor<4x3x!quant.uniform<i8<-127:127>:f32, 2.000000e+00>>) -> tensor<1x3x!quant.uniform<i32:f32, 5.000000e+00>>\n+  %3 = stablehlo.uniform_dequantize %2 : (tensor<1x3x!quant.uniform<i32:f32, 5.000000e+00>>) -> tensor<1x3xf32>\n+  return %3 : tensor<1x3xf32>\n+}\n+// CHECK: \"tfl.quantize\"\n+// CHECK: stablehlo.dot_general\n+// CHECK-NOT: tfl.fully_connected\n+// CHECK-NOT: tfl.batch_matmul\n+// CHECK: stablehlo.uniform_dequantize\n+\n+// -----\n+\n",
            "whole_hunk": "@@ -519,6 +519,25 @@ func.func @dot_general_upstream_full_integer_per_axis_quantized_filter_with_mult\n \n // -----\n \n+// Test that a `stablehlo.dot_general` with an i32 output remains unchanged when\n+// it is not followed by a requantization (`stablehlo.quantize`).\n+\n+// CHECK-LABEL: dot_general_no_requantize\n+func.func @dot_general_no_requantize(%arg0: tensor<1x4xf32>) -> tensor<1x3xf32> {\n+  %0 = stablehlo.constant() {value = dense<5> : tensor<4x3xi8>} : () -> tensor<4x3x!quant.uniform<i8<-127:127>:f32, 2.000000e+00>>\n+  %1 = stablehlo.uniform_quantize %arg0 : (tensor<1x4xf32>) -> tensor<1x4x!quant.uniform<i8:f32, 3.000000e+00>>\n+  %2 = stablehlo.dot_general %1, %0, contracting_dims = [1] x [0] : (tensor<1x4x!quant.uniform<i8:f32, 3.000000e+00>>, tensor<4x3x!quant.uniform<i8<-127:127>:f32, 2.000000e+00>>) -> tensor<1x3x!quant.uniform<i32:f32, 5.000000e+00>>\n+  %3 = stablehlo.uniform_dequantize %2 : (tensor<1x3x!quant.uniform<i32:f32, 5.000000e+00>>) -> tensor<1x3xf32>\n+  return %3 : tensor<1x3xf32>\n+}\n+// CHECK: \"tfl.quantize\"\n+// CHECK: stablehlo.dot_general\n+// CHECK-NOT: tfl.fully_connected\n+// CHECK-NOT: tfl.batch_matmul\n+// CHECK: stablehlo.uniform_dequantize\n+\n+// -----\n+\n // Test that a quantized stablehlo.transpose is converted to tfl.transpose.\n \n // CHECK-LABEL: transpose\n"
        },
        {
            "name": "uniform_quantized_stablehlo_to_tfl_pass.cc",
            "path": "tensorflow/compiler/mlir/lite/stablehlo/transforms/uniform_quantized_stablehlo_to_tfl_pass.cc",
            "patches": [
                {
                    "old_start": 1266,
                    "old_length": 6,
                    "new_start": 1266,
                    "new_length": 9,
                    "hunk": "@@ -1266,6 +1266,9 @@ class RewriteQuantizedDotGeneralOpToTflFullyConnectedOrBatchMatmulOp\n         LLVM_DEBUG(llvm::dbgs() << \"Expected a dequantize type.\\n\");\n         return failure();\n       }\n+    } else {\n+      // Op not followed by a requantization is not supported.\n+      return failure();\n     }\n     return success();\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    } else {\n+      // Op not followed by a requantization is not supported.\n+      return failure();\n",
            "whole_hunk": "@@ -1266,6 +1266,9 @@ class RewriteQuantizedDotGeneralOpToTflFullyConnectedOrBatchMatmulOp\n         LLVM_DEBUG(llvm::dbgs() << \"Expected a dequantize type.\\n\");\n         return failure();\n       }\n+    } else {\n+      // Op not followed by a requantization is not supported.\n+      return failure();\n     }\n     return success();\n   }"
        }
    ]
},
{
    "Id": 659,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b45878744c636d5f32cc7a48e348c3dbfbe6d160",
    "date": "2022-10-15T10:30:00-07:00",
    "message": "[xla:gml_st] Relax a check that refuses to tile parallel dimensions when op contains a reduction dimension\n\nThe user is responsible for making sure to not ask to generate parallel loops on reduction dimension.\n\nPiperOrigin-RevId: 481366310",
    "label": "NO",
    "changes": [
        {
            "name": "tiling.cc",
            "path": "tensorflow/compiler/xla/mlir_hlo/lib/Dialect/gml_st/transforms/tiling.cc",
            "patches": [
                {
                    "old_start": 254,
                    "old_length": 12,
                    "new_start": 254,
                    "new_length": 6,
                    "hunk": "@@ -254,12 +254,6 @@ FailureOr<TilingResult> tile(const TilingOptions &options,\n         op, \"missing tile size computation function\");\n   }\n \n-  // Implement adding accumulator to the gml_st.parallel terminator.\n-  if (options.distribute && llvm::count(op.getLoopIteratorTypes(),\n-                                        utils::IteratorType::reduction) > 0) {\n-    return failure();\n-  }\n-\n   // 1. Get the range of the loops that are represented by the operation.\n   SmallVector<Range> iterationDomain = op.getIterationDomain(rewriter);\n   size_t numLoops = iterationDomain.size();\n"
                }
            ],
            "whole_deleted": "-  // Implement adding accumulator to the gml_st.parallel terminator.\n-  if (options.distribute && llvm::count(op.getLoopIteratorTypes(),\n-                                        utils::IteratorType::reduction) > 0) {\n-    return failure();\n-  }\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -254,12 +254,6 @@ FailureOr<TilingResult> tile(const TilingOptions &options,\n         op, \"missing tile size computation function\");\n   }\n \n-  // Implement adding accumulator to the gml_st.parallel terminator.\n-  if (options.distribute && llvm::count(op.getLoopIteratorTypes(),\n-                                        utils::IteratorType::reduction) > 0) {\n-    return failure();\n-  }\n-\n   // 1. Get the range of the loops that are represented by the operation.\n   SmallVector<Range> iterationDomain = op.getIterationDomain(rewriter);\n   size_t numLoops = iterationDomain.size();\n"
        },
        {
            "name": "tiling.mlir",
            "path": "tensorflow/compiler/xla/mlir_hlo/tests/Dialect/gml_st/tiling.mlir",
            "patches": [
                {
                    "old_start": 235,
                    "old_length": 14,
                    "new_start": 235,
                    "new_length": 6,
                    "hunk": "@@ -235,14 +235,6 @@ func.func @reduce_row(%lhs: tensor<?x?xf32>,\n // CHECK-FOR:       return %[[FOR_0]]\n \n \n-// CHECK-PARALLEL-LABEL: @reduce_row\n-// CHECK-PARALLEL-SAME:  %[[LHS:.*]]: tensor<?x?xf32>, %[[RHS:.*]]: tensor<?x?xf32>\n-\n-// CHECK-PARALLEL-NOT:   gml_st.parallel\n-// CHECK-PARALLEL:       %[[RES:.*]] = linalg.generic\n-// CHECK-PARALLEL-NOT:   gml_st.parallel\n-// CHECK-PARALLEL:       return %[[RES]]\n-\n // -----\n \n func.func @thlo_reduction("
                }
            ],
            "whole_deleted": "-// CHECK-PARALLEL-LABEL: @reduce_row\n-// CHECK-PARALLEL-SAME:  %[[LHS:.*]]: tensor<?x?xf32>, %[[RHS:.*]]: tensor<?x?xf32>\n-\n-// CHECK-PARALLEL-NOT:   gml_st.parallel\n-// CHECK-PARALLEL:       %[[RES:.*]] = linalg.generic\n-// CHECK-PARALLEL-NOT:   gml_st.parallel\n-// CHECK-PARALLEL:       return %[[RES]]\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -235,14 +235,6 @@ func.func @reduce_row(%lhs: tensor<?x?xf32>,\n // CHECK-FOR:       return %[[FOR_0]]\n \n \n-// CHECK-PARALLEL-LABEL: @reduce_row\n-// CHECK-PARALLEL-SAME:  %[[LHS:.*]]: tensor<?x?xf32>, %[[RHS:.*]]: tensor<?x?xf32>\n-\n-// CHECK-PARALLEL-NOT:   gml_st.parallel\n-// CHECK-PARALLEL:       %[[RES:.*]] = linalg.generic\n-// CHECK-PARALLEL-NOT:   gml_st.parallel\n-// CHECK-PARALLEL:       return %[[RES]]\n-\n // -----\n \n func.func @thlo_reduction("
        }
    ]
},
{
    "Id": 163,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c510c1b8b1ef5be1d65971f5b9e21e61becd0bb3",
    "date": "2024-02-08T04:11:31-08:00",
    "message": "[XLA] Remove IsCalledComputation from HloComputation.\n\nThe function doesn't do what it seems. There are more types of called instruction that are not accounted in this check.\n\nPiperOrigin-RevId: 605270151",
    "label": "YES",
    "changes": [
        {
            "name": "hlo_computation.h",
            "path": "third_party/xla/xla/hlo/ir/hlo_computation.h",
            "patches": [
                {
                    "old_start": 807,
                    "old_length": 18,
                    "new_start": 807,
                    "new_length": 14,
                    "hunk": "@@ -807,18 +807,14 @@ class HloComputation {\n   HloInstruction* AsyncStart() const { return async_start_; }\n \n   void AddAsyncStart(HloInstruction* async_instruction) {\n-    CHECK(!IsCalledComputation());\n+    // TODO: Add instruction type for async instructions.\n+    CHECK(instruction_type() == InstructionType::kUnset);\n     CHECK(async_instruction->opcode() == HloOpcode::kAsyncStart);\n     async_start_ = async_instruction;\n   }\n \n   void RemoveAsyncStart() { async_start_ = nullptr; }\n \n-  // Returns if this computation is invoked by an Hlo instruction.\n-  bool IsCalledComputation() const {\n-    return IsFusionComputation() || IsCustomCallComputation();\n-  }\n-\n   // Clear the unique ID of the computation so that it can be re-assigned, such\n   // as for the purpose of compacting the unique IDs.\n   void ClearUniqueIdInternal() { unique_id_ = -1; }"
                }
            ],
            "whole_deleted": "-    CHECK(!IsCalledComputation());\n-  // Returns if this computation is invoked by an Hlo instruction.\n-  bool IsCalledComputation() const {\n-    return IsFusionComputation() || IsCustomCallComputation();\n-  }\n-\n",
            "whole_added": "+    // TODO: Add instruction type for async instructions.\n+    CHECK(instruction_type() == InstructionType::kUnset);\n",
            "whole_hunk": "@@ -807,18 +807,14 @@ class HloComputation {\n   HloInstruction* AsyncStart() const { return async_start_; }\n \n   void AddAsyncStart(HloInstruction* async_instruction) {\n-    CHECK(!IsCalledComputation());\n+    // TODO: Add instruction type for async instructions.\n+    CHECK(instruction_type() == InstructionType::kUnset);\n     CHECK(async_instruction->opcode() == HloOpcode::kAsyncStart);\n     async_start_ = async_instruction;\n   }\n \n   void RemoveAsyncStart() { async_start_ = nullptr; }\n \n-  // Returns if this computation is invoked by an Hlo instruction.\n-  bool IsCalledComputation() const {\n-    return IsFusionComputation() || IsCustomCallComputation();\n-  }\n-\n   // Clear the unique ID of the computation so that it can be re-assigned, such\n   // as for the purpose of compacting the unique IDs.\n   void ClearUniqueIdInternal() { unique_id_ = -1; }"
        }
    ]
},
{
    "Id": 291,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/74a206527e98a8da65a8c341a8ac182b2c8ee377",
    "date": "2023-09-13T11:47:13-07:00",
    "message": "[tflite-gpu] Push select_v2 dim check up from inference to parser.\n\nPiperOrigin-RevId: 565113938",
    "label": "NO",
    "changes": [
        {
            "name": "model_builder_test.cc",
            "path": "tensorflow/lite/delegates/gpu/common/model_builder_test.cc",
            "patches": [
                {
                    "old_start": 3282,
                    "old_length": 6,
                    "new_start": 3282,
                    "new_length": 15,
                    "hunk": "@@ -3282,6 +3282,15 @@ TEST(SelectV2OperationParserTest, TestIsSupported) {\n           ->IsSupported(context.get(), context->node(), context->registration())\n           .ok());\n \n+  TfLiteIntArrayFree(context->tensor(2)->dims);\n+  context->tensor(2)->dims = TfLiteIntArrayCreate(2);\n+  context->tensor(2)->dims->data[0] = 2;\n+  context->tensor(2)->dims->data[1] = 2;\n+  EXPECT_FALSE(\n+      parser\n+          ->IsSupported(context.get(), context->node(), context->registration())\n+          .ok());\n+\n   TfLiteIntArrayFree(context->tensor(2)->dims);\n   context->tensor(2)->dims = TfLiteIntArrayCreate(1);\n   context->tensor(2)->dims->data[0] = 1;\n"
                },
                {
                    "old_start": 3290,
                    "old_length": 6,
                    "new_start": 3299,
                    "new_length": 17,
                    "hunk": "@@ -3290,6 +3299,17 @@ TEST(SelectV2OperationParserTest, TestIsSupported) {\n           ->IsSupported(context.get(), context->node(), context->registration())\n           .ok());\n \n+  TfLiteIntArrayFree(context->tensor(3)->dims);\n+  context->tensor(3)->dims = TfLiteIntArrayCreate(2);\n+  context->tensor(3)->dims->data[0] = 2;\n+  context->tensor(3)->dims->data[1] = 2;\n+  EXPECT_FALSE(\n+      parser\n+          ->IsSupported(context.get(), context->node(), context->registration())\n+          .ok());\n+\n+  TfLiteIntArrayFree(context->tensor(3)->dims);\n+  context->tensor(3)->dims = TfLiteIntArrayCreate(4);\n   for (int i = 0; i < context->tensor(4)->dims->size; ++i) {\n     context->tensor(3)->dims->data[i] = context->tensor(4)->dims->data[i];\n   }\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  TfLiteIntArrayFree(context->tensor(2)->dims);\n+  context->tensor(2)->dims = TfLiteIntArrayCreate(2);\n+  context->tensor(2)->dims->data[0] = 2;\n+  context->tensor(2)->dims->data[1] = 2;\n+  EXPECT_FALSE(\n+      parser\n+          ->IsSupported(context.get(), context->node(), context->registration())\n+          .ok());\n+\n+  TfLiteIntArrayFree(context->tensor(3)->dims);\n+  context->tensor(3)->dims = TfLiteIntArrayCreate(2);\n+  context->tensor(3)->dims->data[0] = 2;\n+  context->tensor(3)->dims->data[1] = 2;\n+  EXPECT_FALSE(\n+      parser\n+          ->IsSupported(context.get(), context->node(), context->registration())\n+          .ok());\n+\n+  TfLiteIntArrayFree(context->tensor(3)->dims);\n+  context->tensor(3)->dims = TfLiteIntArrayCreate(4);\n",
            "whole_hunk": "@@ -3282,6 +3282,15 @@ TEST(SelectV2OperationParserTest, TestIsSupported) {\n           ->IsSupported(context.get(), context->node(), context->registration())\n           .ok());\n \n+  TfLiteIntArrayFree(context->tensor(2)->dims);\n+  context->tensor(2)->dims = TfLiteIntArrayCreate(2);\n+  context->tensor(2)->dims->data[0] = 2;\n+  context->tensor(2)->dims->data[1] = 2;\n+  EXPECT_FALSE(\n+      parser\n+          ->IsSupported(context.get(), context->node(), context->registration())\n+          .ok());\n+\n   TfLiteIntArrayFree(context->tensor(2)->dims);\n   context->tensor(2)->dims = TfLiteIntArrayCreate(1);\n   context->tensor(2)->dims->data[0] = 1;\n@@ -3290,6 +3299,17 @@ TEST(SelectV2OperationParserTest, TestIsSupported) {\n           ->IsSupported(context.get(), context->node(), context->registration())\n           .ok());\n \n+  TfLiteIntArrayFree(context->tensor(3)->dims);\n+  context->tensor(3)->dims = TfLiteIntArrayCreate(2);\n+  context->tensor(3)->dims->data[0] = 2;\n+  context->tensor(3)->dims->data[1] = 2;\n+  EXPECT_FALSE(\n+      parser\n+          ->IsSupported(context.get(), context->node(), context->registration())\n+          .ok());\n+\n+  TfLiteIntArrayFree(context->tensor(3)->dims);\n+  context->tensor(3)->dims = TfLiteIntArrayCreate(4);\n   for (int i = 0; i < context->tensor(4)->dims->size; ++i) {\n     context->tensor(3)->dims->data[i] = context->tensor(4)->dims->data[i];\n   }\n"
        },
        {
            "name": "gpu_compatibility.cc",
            "path": "tensorflow/lite/tools/versioning/gpu_compatibility.cc",
            "patches": [
                {
                    "old_start": 385,
                    "old_length": 12,
                    "new_start": 385,
                    "new_length": 20,
                    "hunk": "@@ -385,12 +385,20 @@ absl::Status CheckSelectV2GpuDelegateCompatibility(const OpSignature& op_sig) {\n        op_sig.inputs.at(1).dims[0] > 1)) {\n     return error;\n   }\n+  if (op_sig.inputs.at(1).is_const && op_sig.inputs.at(1).dims.size() == 2) {\n+    return absl::InvalidArgumentError(\n+        \"2-D if tensor only supported if constant.\");\n+  }\n   if (!op_sig.inputs.at(2).dims.empty() &&\n       (op_sig.inputs.at(2).dims != output_dims) &&\n       (op_sig.inputs.at(2).dims.size() > 1 ||\n        op_sig.inputs.at(2).dims[0] > 1)) {\n     return error;\n   }\n+  if (op_sig.inputs.at(2).is_const && op_sig.inputs.at(2).dims.size() == 2) {\n+    return absl::InvalidArgumentError(\n+        \"2-D else tensor only supported if constant.\");\n+  }\n   return absl::OkStatus();\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  if (op_sig.inputs.at(1).is_const && op_sig.inputs.at(1).dims.size() == 2) {\n+    return absl::InvalidArgumentError(\n+        \"2-D if tensor only supported if constant.\");\n+  }\n+  if (op_sig.inputs.at(2).is_const && op_sig.inputs.at(2).dims.size() == 2) {\n+    return absl::InvalidArgumentError(\n+        \"2-D else tensor only supported if constant.\");\n+  }\n",
            "whole_hunk": "@@ -385,12 +385,20 @@ absl::Status CheckSelectV2GpuDelegateCompatibility(const OpSignature& op_sig) {\n        op_sig.inputs.at(1).dims[0] > 1)) {\n     return error;\n   }\n+  if (op_sig.inputs.at(1).is_const && op_sig.inputs.at(1).dims.size() == 2) {\n+    return absl::InvalidArgumentError(\n+        \"2-D if tensor only supported if constant.\");\n+  }\n   if (!op_sig.inputs.at(2).dims.empty() &&\n       (op_sig.inputs.at(2).dims != output_dims) &&\n       (op_sig.inputs.at(2).dims.size() > 1 ||\n        op_sig.inputs.at(2).dims[0] > 1)) {\n     return error;\n   }\n+  if (op_sig.inputs.at(2).is_const && op_sig.inputs.at(2).dims.size() == 2) {\n+    return absl::InvalidArgumentError(\n+        \"2-D else tensor only supported if constant.\");\n+  }\n   return absl::OkStatus();\n }\n "
        }
    ]
},
{
    "Id": 616,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c9cb6d66bc77519202b98097f83fdd07241292a2",
    "date": "2022-12-06T13:18:28-08:00",
    "message": "[XLA:GPU] Remove an extraneous check which mistakenly prevents fusing a relu with a gemm when the inputs (bias included) are square.\n\nPiperOrigin-RevId: 493393214",
    "label": "NO",
    "changes": [
        {
            "name": "gemm_rewriter.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gemm_rewriter.cc",
            "patches": [
                {
                    "old_start": 574,
                    "old_length": 12,
                    "new_start": 574,
                    "new_length": 8,
                    "hunk": "@@ -574,12 +574,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     if (!SupportsEpilogueFusion(gemm->shape().element_type())) {\n       return OkStatus();\n     }\n-    bool valid_fusion_pattern =\n-        (gemm->operand_count() == 3)\n-            ? gemm->operand(0)->shape() != gemm->operand(2)->shape()\n-            : true;\n \n-    if (!valid_fusion_pattern || gemm->user_count() != 1) {\n+    if (gemm->user_count() != 1) {\n       return OkStatus();\n     }\n \n"
                }
            ],
            "whole_deleted": "-    bool valid_fusion_pattern =\n-        (gemm->operand_count() == 3)\n-            ? gemm->operand(0)->shape() != gemm->operand(2)->shape()\n-            : true;\n-    if (!valid_fusion_pattern || gemm->user_count() != 1) {\n",
            "whole_added": "+    if (gemm->user_count() != 1) {\n",
            "whole_hunk": "@@ -574,12 +574,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {\n     if (!SupportsEpilogueFusion(gemm->shape().element_type())) {\n       return OkStatus();\n     }\n-    bool valid_fusion_pattern =\n-        (gemm->operand_count() == 3)\n-            ? gemm->operand(0)->shape() != gemm->operand(2)->shape()\n-            : true;\n \n-    if (!valid_fusion_pattern || gemm->user_count() != 1) {\n+    if (gemm->user_count() != 1) {\n       return OkStatus();\n     }\n \n"
        },
        {
            "name": "gemm_rewrite_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/tests/gemm_rewrite_test.cc",
            "patches": [
                {
                    "old_start": 2501,
                    "old_length": 6,
                    "new_start": 2501,
                    "new_length": 51,
                    "hunk": "@@ -2501,6 +2501,51 @@ ENTRY test {\n       )\");\n }\n \n+TEST_F(CublasLtGemmRewriteTest, SquareMatrixBiasReluActivation) {\n+  const char* hlo_text = R\"(\n+HloModule test\n+\n+ENTRY test {\n+  x = f32[4,4] parameter(0)\n+  y = f32[4,4] parameter(1)\n+  z = f32[4,4] parameter(2)\n+  dot_a = f32[4,4] dot(x, y), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  add = f32[4,4] add(dot_a, z)\n+  c = f32[] constant(0)\n+  c_bcast = f32[4,4] broadcast(c), dimensions={}\n+  ROOT out = f32[4,4] maximum(add, c_bcast)\n+}\n+\n+)\";\n+\n+  EXPECT_TRUE(RunAndCompare(hlo_text, ErrorSpec{1e-5, 1e-5}));\n+  MatchOptimizedHlo(hlo_text,\n+                    R\"(\n+\n+; CHECK-LABEL: ENTRY %test (x: f32[4,4], y: f32[4,4], z: f32[4,4]) -> f32[4,4] {\n+; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[4,4]{1,0} parameter(0)\n+; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,4]{1,0} parameter(1)\n+; CHECK-NEXT:    [[P2:%[^ ]+]] = f32[4,4]{1,0} parameter(2)\n+; CHECK-NEXT:    ROOT [[OUT:%[^ ]+]] = f32[4,4]{1,0} custom-call([[P0]], [[P1]], [[P2]]),\n+; CHECK:           custom_call_target=\"__cublas$lt$matmul\",\n+; CHECK:           backend_config=\"{\n+; CHECK-DAG:         \\\"alpha_real\\\":1\n+; CHECK-DAG:         \\\"alpha_imag\\\":0\n+; CHECK-DAG:         \\\"beta\\\":1\n+; CHECK-DAG:         \\\"dot_dimension_numbers\\\":{\n+; CHECK-DAG:           \\\"lhs_contracting_dimensions\\\":[\\\"1\\\"]\n+; CHECK-DAG:           \\\"rhs_contracting_dimensions\\\":[\\\"0\\\"]\n+; CHECK-DAG:           \\\"lhs_batch_dimensions\\\":[]\n+; CHECK-DAG:           \\\"rhs_batch_dimensions\\\":[]\n+; CHECK-DAG:         }\n+; CHECK-DAG:         \\\"precision_config\\\":{\n+; CHECK-DAG:           \\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]\n+; CHECK-DAG:         }\n+; CHECK-DAG:         \\\"epilogue\\\":\\\"RELU\\\"\n+; CHECK:           }\"\n+      )\");\n+}\n+\n TEST_F(CublasLtGemmRewriteTest, VectorBiasReluActivation) {\n   const char* hlo_text = R\"(\n HloModule test"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(CublasLtGemmRewriteTest, SquareMatrixBiasReluActivation) {\n+  const char* hlo_text = R\"(\n+HloModule test\n+\n+ENTRY test {\n+  x = f32[4,4] parameter(0)\n+  y = f32[4,4] parameter(1)\n+  z = f32[4,4] parameter(2)\n+  dot_a = f32[4,4] dot(x, y), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  add = f32[4,4] add(dot_a, z)\n+  c = f32[] constant(0)\n+  c_bcast = f32[4,4] broadcast(c), dimensions={}\n+  ROOT out = f32[4,4] maximum(add, c_bcast)\n+}\n+\n+)\";\n+\n+  EXPECT_TRUE(RunAndCompare(hlo_text, ErrorSpec{1e-5, 1e-5}));\n+  MatchOptimizedHlo(hlo_text,\n+                    R\"(\n+\n+; CHECK-LABEL: ENTRY %test (x: f32[4,4], y: f32[4,4], z: f32[4,4]) -> f32[4,4] {\n+; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[4,4]{1,0} parameter(0)\n+; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,4]{1,0} parameter(1)\n+; CHECK-NEXT:    [[P2:%[^ ]+]] = f32[4,4]{1,0} parameter(2)\n+; CHECK-NEXT:    ROOT [[OUT:%[^ ]+]] = f32[4,4]{1,0} custom-call([[P0]], [[P1]], [[P2]]),\n+; CHECK:           custom_call_target=\"__cublas$lt$matmul\",\n+; CHECK:           backend_config=\"{\n+; CHECK-DAG:         \\\"alpha_real\\\":1\n+; CHECK-DAG:         \\\"alpha_imag\\\":0\n+; CHECK-DAG:         \\\"beta\\\":1\n+; CHECK-DAG:         \\\"dot_dimension_numbers\\\":{\n+; CHECK-DAG:           \\\"lhs_contracting_dimensions\\\":[\\\"1\\\"]\n+; CHECK-DAG:           \\\"rhs_contracting_dimensions\\\":[\\\"0\\\"]\n+; CHECK-DAG:           \\\"lhs_batch_dimensions\\\":[]\n+; CHECK-DAG:           \\\"rhs_batch_dimensions\\\":[]\n+; CHECK-DAG:         }\n+; CHECK-DAG:         \\\"precision_config\\\":{\n+; CHECK-DAG:           \\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]\n+; CHECK-DAG:         }\n+; CHECK-DAG:         \\\"epilogue\\\":\\\"RELU\\\"\n+; CHECK:           }\"\n+      )\");\n+}\n+\n",
            "whole_hunk": "@@ -2501,6 +2501,51 @@ ENTRY test {\n       )\");\n }\n \n+TEST_F(CublasLtGemmRewriteTest, SquareMatrixBiasReluActivation) {\n+  const char* hlo_text = R\"(\n+HloModule test\n+\n+ENTRY test {\n+  x = f32[4,4] parameter(0)\n+  y = f32[4,4] parameter(1)\n+  z = f32[4,4] parameter(2)\n+  dot_a = f32[4,4] dot(x, y), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  add = f32[4,4] add(dot_a, z)\n+  c = f32[] constant(0)\n+  c_bcast = f32[4,4] broadcast(c), dimensions={}\n+  ROOT out = f32[4,4] maximum(add, c_bcast)\n+}\n+\n+)\";\n+\n+  EXPECT_TRUE(RunAndCompare(hlo_text, ErrorSpec{1e-5, 1e-5}));\n+  MatchOptimizedHlo(hlo_text,\n+                    R\"(\n+\n+; CHECK-LABEL: ENTRY %test (x: f32[4,4], y: f32[4,4], z: f32[4,4]) -> f32[4,4] {\n+; CHECK-NEXT:    [[P0:%[^ ]+]] = f32[4,4]{1,0} parameter(0)\n+; CHECK-NEXT:    [[P1:%[^ ]+]] = f32[4,4]{1,0} parameter(1)\n+; CHECK-NEXT:    [[P2:%[^ ]+]] = f32[4,4]{1,0} parameter(2)\n+; CHECK-NEXT:    ROOT [[OUT:%[^ ]+]] = f32[4,4]{1,0} custom-call([[P0]], [[P1]], [[P2]]),\n+; CHECK:           custom_call_target=\"__cublas$lt$matmul\",\n+; CHECK:           backend_config=\"{\n+; CHECK-DAG:         \\\"alpha_real\\\":1\n+; CHECK-DAG:         \\\"alpha_imag\\\":0\n+; CHECK-DAG:         \\\"beta\\\":1\n+; CHECK-DAG:         \\\"dot_dimension_numbers\\\":{\n+; CHECK-DAG:           \\\"lhs_contracting_dimensions\\\":[\\\"1\\\"]\n+; CHECK-DAG:           \\\"rhs_contracting_dimensions\\\":[\\\"0\\\"]\n+; CHECK-DAG:           \\\"lhs_batch_dimensions\\\":[]\n+; CHECK-DAG:           \\\"rhs_batch_dimensions\\\":[]\n+; CHECK-DAG:         }\n+; CHECK-DAG:         \\\"precision_config\\\":{\n+; CHECK-DAG:           \\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]\n+; CHECK-DAG:         }\n+; CHECK-DAG:         \\\"epilogue\\\":\\\"RELU\\\"\n+; CHECK:           }\"\n+      )\");\n+}\n+\n TEST_F(CublasLtGemmRewriteTest, VectorBiasReluActivation) {\n   const char* hlo_text = R\"(\n HloModule test"
        }
    ]
},
{
    "Id": 445,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8a2e7deb21f02e4072d6b62cf7f447b9264afe01",
    "date": "2023-04-20T11:41:01-07:00",
    "message": "Adjust checks for `type(Tensor)` to isinstance or is_eager/is_symbolic_tensor.\n\nPiperOrigin-RevId: 525801792",
    "label": "YES",
    "changes": [
        {
            "name": "subscribe.py",
            "path": "tensorflow/python/framework/subscribe.py",
            "patches": [
                {
                    "old_start": 42,
                    "old_length": 7,
                    "new_start": 42,
                    "new_length": 7,
                    "hunk": "@@ -42,7 +42,7 @@ def _recursive_apply(tensors, apply_fn):\n     `TypeError` if undefined type in the tensors structure.\n   \"\"\"\n   tensors_type = type(tensors)\n-  if tensors_type is ops.Tensor:\n+  if isinstance(tensors, ops.Tensor):\n     return apply_fn(tensors)\n   elif isinstance(tensors, variables.Variable):\n     return apply_fn(tensors.value())"
                }
            ],
            "whole_deleted": "-  if tensors_type is ops.Tensor:\n",
            "whole_added": "+  if isinstance(tensors, ops.Tensor):\n",
            "whole_hunk": "@@ -42,7 +42,7 @@ def _recursive_apply(tensors, apply_fn):\n     `TypeError` if undefined type in the tensors structure.\n   \"\"\"\n   tensors_type = type(tensors)\n-  if tensors_type is ops.Tensor:\n+  if isinstance(tensors, ops.Tensor):\n     return apply_fn(tensors)\n   elif isinstance(tensors, variables.Variable):\n     return apply_fn(tensors.value())"
        }
    ]
},
{
    "Id": 352,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e1ad3b74ad44b883c7b3fdc3a19adcea1d28bfbc",
    "date": "2023-07-17T04:14:46-07:00",
    "message": "[XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast is an\neffective scalar. This short-circuit avoids crashing within last_dimension when\nattempting to match and either the operand or the result of the bitcast has a\nshape with rank 0.\n\nPiperOrigin-RevId: 548645429",
    "label": "YES",
    "changes": [
        {
            "name": "softmax_rewriter_triton.cc",
            "path": "tensorflow/compiler/xla/service/gpu/softmax_rewriter_triton.cc",
            "patches": [
                {
                    "old_start": 78,
                    "old_length": 7,
                    "new_start": 78,
                    "new_length": 7,
                    "hunk": "@@ -78,7 +78,7 @@ bool BitcastIsTilingNoop(HloInstruction* bitcast,\n                          const GpuVersion& gpu_version) {\n   CHECK_EQ(bitcast->opcode(), HloOpcode::kBitcast);\n \n-  if (bitcast->shape().rank() == 0) {\n+  if (ShapeUtil::IsEffectiveScalar(bitcast->shape())) {\n     return true;\n   }\n "
                }
            ],
            "whole_deleted": "-  if (bitcast->shape().rank() == 0) {\n",
            "whole_added": "+  if (ShapeUtil::IsEffectiveScalar(bitcast->shape())) {\n",
            "whole_hunk": "@@ -78,7 +78,7 @@ bool BitcastIsTilingNoop(HloInstruction* bitcast,\n                          const GpuVersion& gpu_version) {\n   CHECK_EQ(bitcast->opcode(), HloOpcode::kBitcast);\n \n-  if (bitcast->shape().rank() == 0) {\n+  if (ShapeUtil::IsEffectiveScalar(bitcast->shape())) {\n     return true;\n   }\n "
        }
    ]
},
{
    "Id": 399,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/bc27c5a90f3a009c63072a57495f6b7b8d9f1dc1",
    "date": "2023-06-13T00:21:25-07:00",
    "message": "Avoid forming multi-output fusions with incompatible non-trivial heroes.\n\nOne part of the fix is to run CSE before MultiOutputFusion, so that we can more\nreliably detect tiled transpose fusions. If we only run CSE after\nMultiOutputFusion, we would potentially detect more tiled transpose roots as\nbefore, some of them may be incompatible.\nThe second part of the fix is to add additional checks to FusionMerger, so that\nit does not merge two fusions with incompatible non-trivial heroes.\n\nPiperOrigin-RevId: 539875397",
    "label": "YES",
    "changes": [
        {
            "name": "fusion_merger.cc",
            "path": "tensorflow/compiler/xla/service/gpu/fusion_merger.cc",
            "patches": [
                {
                    "old_start": 212,
                    "old_length": 12,
                    "new_start": 212,
                    "new_length": 19,
                    "hunk": "@@ -212,12 +212,19 @@ FusionDecision FusionInstructionMerger::ShouldFuse(HloInstruction* producer) {\n     return \"not a loop fusion\";\n   }\n \n+  auto producer_hero = GetRealHeroForMultiOutputFusion(*producer);\n+\n   bool has_reduction_user = false;\n   for (const HloInstruction* user : producer->users()) {\n     if (user->opcode() == HloOpcode::kBitcast) {\n       ++num_fail_merge_all_users_;\n       return \"not fusing bitcast ops\";\n     }\n+    auto consumer_hero = GetRealHeroForMultiOutputFusion(*user);\n+    if (NoFusionPossible compatible =\n+            !FusionHeroesAreCompatible(producer_hero, consumer_hero)) {\n+      return !compatible;\n+    }\n     FusionDecision fusible = IsProducerConsumerFusible(*producer, *user);\n     if (!fusible) {\n       ++num_fail_merge_all_users_;\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  auto producer_hero = GetRealHeroForMultiOutputFusion(*producer);\n+\n+    auto consumer_hero = GetRealHeroForMultiOutputFusion(*user);\n+    if (NoFusionPossible compatible =\n+            !FusionHeroesAreCompatible(producer_hero, consumer_hero)) {\n+      return !compatible;\n+    }\n",
            "whole_hunk": "@@ -212,12 +212,19 @@ FusionDecision FusionInstructionMerger::ShouldFuse(HloInstruction* producer) {\n     return \"not a loop fusion\";\n   }\n \n+  auto producer_hero = GetRealHeroForMultiOutputFusion(*producer);\n+\n   bool has_reduction_user = false;\n   for (const HloInstruction* user : producer->users()) {\n     if (user->opcode() == HloOpcode::kBitcast) {\n       ++num_fail_merge_all_users_;\n       return \"not fusing bitcast ops\";\n     }\n+    auto consumer_hero = GetRealHeroForMultiOutputFusion(*user);\n+    if (NoFusionPossible compatible =\n+            !FusionHeroesAreCompatible(producer_hero, consumer_hero)) {\n+      return !compatible;\n+    }\n     FusionDecision fusible = IsProducerConsumerFusible(*producer, *user);\n     if (!fusible) {\n       ++num_fail_merge_all_users_;\n"
        },
        {
            "name": "fusion_merger_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/fusion_merger_test.cc",
            "patches": [
                {
                    "old_start": 1027,
                    "old_length": 6,
                    "new_start": 1027,
                    "new_length": 37,
                    "hunk": "@@ -1027,6 +1027,37 @@ TEST_F(FusionMergerTest, CommonElementwiseUsedParameter) {\n   EXPECT_TRUE(fusion_merger_.Run(module.get()).value());\n }\n \n+TEST_F(FusionMergerTest, IncompatibleNonTrivialHeroes) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    fused_computation {\n+      param_0.1 = f32[18,16,32]{2,1,0} parameter(0)\n+      param_1.1 = f32[32,16,18]{2,1,0} parameter(1)\n+      s.1 = f32[18,16,32]{2,1,0} sqrt(param_0.1)\n+      t.1 = f32[32,16,18]{2,1,0} transpose(s.1), dimensions={2,1,0}\n+      sub.1 = f32[32,16,18]{2,1,0} subtract(t.1, param_1.1)\n+      exp.1 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+      ROOT add.1 = f32[32,16,18]{2,1,0} add(exp.1, exp.1)\n+    }\n+\n+    fused_computation.2 {\n+      param_0.2 = f32[32,16,18]{2,1,0} parameter(0)\n+      s.2 = f32[32,16,18]{2,1,0} sqrt(param_0.2)\n+      ROOT t.2 = f32[32,18,16]{2,1,0} transpose(s.2), dimensions={0,2,1}\n+    }\n+\n+    ENTRY main {\n+      p = f32[18,16,32]{2,1,0} parameter(0)\n+      p2 = f32[32,16,18]{2,1,0} parameter(1)\n+      fusion = f32[32,16,18]{2,1,0} fusion(p, p2), kind=kLoop, calls=fused_computation\n+      ROOT fusion2 = f32[32,18,16]{2,1,0} fusion(fusion), kind=kInput, calls=fused_computation.2\n+    }\n+    )\")\n+                    .value();\n+  EXPECT_FALSE(fusion_merger_.Run(module.get()).value());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(FusionMergerTest, IncompatibleNonTrivialHeroes) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    fused_computation {\n+      param_0.1 = f32[18,16,32]{2,1,0} parameter(0)\n+      param_1.1 = f32[32,16,18]{2,1,0} parameter(1)\n+      s.1 = f32[18,16,32]{2,1,0} sqrt(param_0.1)\n+      t.1 = f32[32,16,18]{2,1,0} transpose(s.1), dimensions={2,1,0}\n+      sub.1 = f32[32,16,18]{2,1,0} subtract(t.1, param_1.1)\n+      exp.1 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+      ROOT add.1 = f32[32,16,18]{2,1,0} add(exp.1, exp.1)\n+    }\n+\n+    fused_computation.2 {\n+      param_0.2 = f32[32,16,18]{2,1,0} parameter(0)\n+      s.2 = f32[32,16,18]{2,1,0} sqrt(param_0.2)\n+      ROOT t.2 = f32[32,18,16]{2,1,0} transpose(s.2), dimensions={0,2,1}\n+    }\n+\n+    ENTRY main {\n+      p = f32[18,16,32]{2,1,0} parameter(0)\n+      p2 = f32[32,16,18]{2,1,0} parameter(1)\n+      fusion = f32[32,16,18]{2,1,0} fusion(p, p2), kind=kLoop, calls=fused_computation\n+      ROOT fusion2 = f32[32,18,16]{2,1,0} fusion(fusion), kind=kInput, calls=fused_computation.2\n+    }\n+    )\")\n+                    .value();\n+  EXPECT_FALSE(fusion_merger_.Run(module.get()).value());\n+}\n+\n",
            "whole_hunk": "@@ -1027,6 +1027,37 @@ TEST_F(FusionMergerTest, CommonElementwiseUsedParameter) {\n   EXPECT_TRUE(fusion_merger_.Run(module.get()).value());\n }\n \n+TEST_F(FusionMergerTest, IncompatibleNonTrivialHeroes) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+    HloModule module\n+\n+    fused_computation {\n+      param_0.1 = f32[18,16,32]{2,1,0} parameter(0)\n+      param_1.1 = f32[32,16,18]{2,1,0} parameter(1)\n+      s.1 = f32[18,16,32]{2,1,0} sqrt(param_0.1)\n+      t.1 = f32[32,16,18]{2,1,0} transpose(s.1), dimensions={2,1,0}\n+      sub.1 = f32[32,16,18]{2,1,0} subtract(t.1, param_1.1)\n+      exp.1 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+      ROOT add.1 = f32[32,16,18]{2,1,0} add(exp.1, exp.1)\n+    }\n+\n+    fused_computation.2 {\n+      param_0.2 = f32[32,16,18]{2,1,0} parameter(0)\n+      s.2 = f32[32,16,18]{2,1,0} sqrt(param_0.2)\n+      ROOT t.2 = f32[32,18,16]{2,1,0} transpose(s.2), dimensions={0,2,1}\n+    }\n+\n+    ENTRY main {\n+      p = f32[18,16,32]{2,1,0} parameter(0)\n+      p2 = f32[32,16,18]{2,1,0} parameter(1)\n+      fusion = f32[32,16,18]{2,1,0} fusion(p, p2), kind=kLoop, calls=fused_computation\n+      ROOT fusion2 = f32[32,18,16]{2,1,0} fusion(fusion), kind=kInput, calls=fused_computation.2\n+    }\n+    )\")\n+                    .value();\n+  EXPECT_FALSE(fusion_merger_.Run(module.get()).value());\n+}\n+\n }  // namespace\n }  // namespace gpu\n }  // namespace xla\n"
        },
        {
            "name": "gpu_compiler.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_compiler.cc",
            "patches": [
                {
                    "old_start": 692,
                    "old_length": 6,
                    "new_start": 692,
                    "new_length": 10,
                    "hunk": "@@ -692,6 +692,10 @@ Status GpuCompiler::OptimizeHloModule(HloModule* hlo_module,\n     fusion.AddPass<GpuInstructionFusion>(/*may_duplicate=*/true,\n                                          gpu_device_info);\n     fusion.AddPass<FusionMerger>(gpu_device_info, ShapeSizeBytesFunction());\n+    // Running CSE affects how many users an op has. This plays a role in what\n+    // we detect as a tiled transpose fusion.\n+    fusion.AddPass<HloCSE>(/*is_layout_sensitive=*/true,\n+                           /*only_fusion_computations=*/true);\n     fusion.AddPass<GpuMultiOutputFusion>(gpu_device_info,\n                                          ShapeSizeBytesFunction());\n     fusion.AddPass<HloCSE>(/*is_layout_sensitive=*/true,\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+    // Running CSE affects how many users an op has. This plays a role in what\n+    // we detect as a tiled transpose fusion.\n+    fusion.AddPass<HloCSE>(/*is_layout_sensitive=*/true,\n+                           /*only_fusion_computations=*/true);\n",
            "whole_hunk": "@@ -692,6 +692,10 @@ Status GpuCompiler::OptimizeHloModule(HloModule* hlo_module,\n     fusion.AddPass<GpuInstructionFusion>(/*may_duplicate=*/true,\n                                          gpu_device_info);\n     fusion.AddPass<FusionMerger>(gpu_device_info, ShapeSizeBytesFunction());\n+    // Running CSE affects how many users an op has. This plays a role in what\n+    // we detect as a tiled transpose fusion.\n+    fusion.AddPass<HloCSE>(/*is_layout_sensitive=*/true,\n+                           /*only_fusion_computations=*/true);\n     fusion.AddPass<GpuMultiOutputFusion>(gpu_device_info,\n                                          ShapeSizeBytesFunction());\n     fusion.AddPass<HloCSE>(/*is_layout_sensitive=*/true,\n"
        },
        {
            "name": "gpu_fusible.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible.cc",
            "patches": [
                {
                    "old_start": 166,
                    "old_length": 6,
                    "new_start": 166,
                    "new_length": 32,
                    "hunk": "@@ -166,6 +166,32 @@ static bool IsFusedReductionOutputConsistent(\n                            inst->shape().layout());\n }\n \n+FusionDecision FusionHeroesAreCompatible(const HloInstruction* hero1,\n+                                         const HloInstruction* hero2) {\n+  auto hero1_is_unnested_reduce =\n+      IsReductionFromOrToContiguousDimensions(*hero1);\n+  auto tiled_transpose_hero1 = FindAnyTiledTranspose(*hero1);\n+  bool hero1_is_unnested_transpose = tiled_transpose_hero1.has_value();\n+  bool hero2_is_unnested_reduce =\n+      IsReductionFromOrToContiguousDimensions(*hero2);\n+  auto tiled_transpose_hero2 = FindAnyTiledTranspose(*hero2);\n+  bool hero2_is_unnested_transpose = tiled_transpose_hero2.has_value();\n+\n+  if (hero1_is_unnested_reduce && hero2_is_unnested_reduce &&\n+      !IsFusedReductionOutputConsistent(hero2, hero1)) {\n+    return \"tiled reductions with different shapes\";\n+  } else if (hero1_is_unnested_transpose && hero2_is_unnested_transpose &&\n+             // After normalization to rank 3, the transposes should have the\n+             // same shape and permute the same dimensions.\n+             *tiled_transpose_hero1 != *tiled_transpose_hero2) {\n+    return \"tiled transposes with different shapes\";\n+  } else if ((hero1_is_unnested_transpose && hero2_is_unnested_reduce) ||\n+             (hero1_is_unnested_reduce && hero2_is_unnested_transpose)) {\n+    return \"MOF-fusion of a transpose and a reduction\";\n+  }\n+  return {};\n+}\n+\n FusionDecision ShapesCompatibleForMultiOutputFusion(\n     const HloInstruction& instr1, const HloInstruction& instr2) {\n   // Multi-output fusion kernels share a common parallel loop. The loop\n"
                },
                {
                    "old_start": 196,
                    "old_length": 17,
                    "new_start": 222,
                    "new_length": 9,
                    "hunk": "@@ -196,17 +222,9 @@ FusionDecision ShapesCompatibleForMultiOutputFusion(\n   auto tiled_transpose_hero2 = FindAnyTiledTranspose(*hero2);\n   bool hero2_is_unnested_transpose = tiled_transpose_hero2.has_value();\n \n-  if (hero1_is_unnested_reduce && hero2_is_unnested_reduce &&\n-      !IsFusedReductionOutputConsistent(hero2, hero1)) {\n-    return \"tiled reductions with different shapes\";\n-  } else if (hero1_is_unnested_transpose && hero2_is_unnested_transpose &&\n-             // After normalization to rank 3, the transposes should have the\n-             // same shape and permute the same dimensions.\n-             *tiled_transpose_hero1 != *tiled_transpose_hero2) {\n-    return \"tiled transposes with different shapes\";\n-  } else if ((hero1_is_unnested_transpose && hero2_is_unnested_reduce) ||\n-             (hero1_is_unnested_reduce && hero2_is_unnested_transpose)) {\n-    return \"MOF-fusion of a transpose and a reduction\";\n+  if (NoFusionPossible heroes_are_compatible =\n+          !FusionHeroesAreCompatible(hero1, hero2)) {\n+    return !heroes_are_compatible;\n   }\n \n   const Shape& l1 = get_loop_shape(hero1);\n"
                }
            ],
            "whole_deleted": "-  if (hero1_is_unnested_reduce && hero2_is_unnested_reduce &&\n-      !IsFusedReductionOutputConsistent(hero2, hero1)) {\n-    return \"tiled reductions with different shapes\";\n-  } else if (hero1_is_unnested_transpose && hero2_is_unnested_transpose &&\n-             // After normalization to rank 3, the transposes should have the\n-             // same shape and permute the same dimensions.\n-             *tiled_transpose_hero1 != *tiled_transpose_hero2) {\n-    return \"tiled transposes with different shapes\";\n-  } else if ((hero1_is_unnested_transpose && hero2_is_unnested_reduce) ||\n-             (hero1_is_unnested_reduce && hero2_is_unnested_transpose)) {\n-    return \"MOF-fusion of a transpose and a reduction\";\n",
            "whole_added": "+FusionDecision FusionHeroesAreCompatible(const HloInstruction* hero1,\n+                                         const HloInstruction* hero2) {\n+  auto hero1_is_unnested_reduce =\n+      IsReductionFromOrToContiguousDimensions(*hero1);\n+  auto tiled_transpose_hero1 = FindAnyTiledTranspose(*hero1);\n+  bool hero1_is_unnested_transpose = tiled_transpose_hero1.has_value();\n+  bool hero2_is_unnested_reduce =\n+      IsReductionFromOrToContiguousDimensions(*hero2);\n+  auto tiled_transpose_hero2 = FindAnyTiledTranspose(*hero2);\n+  bool hero2_is_unnested_transpose = tiled_transpose_hero2.has_value();\n+\n+  if (hero1_is_unnested_reduce && hero2_is_unnested_reduce &&\n+      !IsFusedReductionOutputConsistent(hero2, hero1)) {\n+    return \"tiled reductions with different shapes\";\n+  } else if (hero1_is_unnested_transpose && hero2_is_unnested_transpose &&\n+             // After normalization to rank 3, the transposes should have the\n+             // same shape and permute the same dimensions.\n+             *tiled_transpose_hero1 != *tiled_transpose_hero2) {\n+    return \"tiled transposes with different shapes\";\n+  } else if ((hero1_is_unnested_transpose && hero2_is_unnested_reduce) ||\n+             (hero1_is_unnested_reduce && hero2_is_unnested_transpose)) {\n+    return \"MOF-fusion of a transpose and a reduction\";\n+  }\n+  return {};\n+}\n+\n+  if (NoFusionPossible heroes_are_compatible =\n+          !FusionHeroesAreCompatible(hero1, hero2)) {\n+    return !heroes_are_compatible;\n",
            "whole_hunk": "@@ -166,6 +166,32 @@ static bool IsFusedReductionOutputConsistent(\n                            inst->shape().layout());\n }\n \n+FusionDecision FusionHeroesAreCompatible(const HloInstruction* hero1,\n+                                         const HloInstruction* hero2) {\n+  auto hero1_is_unnested_reduce =\n+      IsReductionFromOrToContiguousDimensions(*hero1);\n+  auto tiled_transpose_hero1 = FindAnyTiledTranspose(*hero1);\n+  bool hero1_is_unnested_transpose = tiled_transpose_hero1.has_value();\n+  bool hero2_is_unnested_reduce =\n+      IsReductionFromOrToContiguousDimensions(*hero2);\n+  auto tiled_transpose_hero2 = FindAnyTiledTranspose(*hero2);\n+  bool hero2_is_unnested_transpose = tiled_transpose_hero2.has_value();\n+\n+  if (hero1_is_unnested_reduce && hero2_is_unnested_reduce &&\n+      !IsFusedReductionOutputConsistent(hero2, hero1)) {\n+    return \"tiled reductions with different shapes\";\n+  } else if (hero1_is_unnested_transpose && hero2_is_unnested_transpose &&\n+             // After normalization to rank 3, the transposes should have the\n+             // same shape and permute the same dimensions.\n+             *tiled_transpose_hero1 != *tiled_transpose_hero2) {\n+    return \"tiled transposes with different shapes\";\n+  } else if ((hero1_is_unnested_transpose && hero2_is_unnested_reduce) ||\n+             (hero1_is_unnested_reduce && hero2_is_unnested_transpose)) {\n+    return \"MOF-fusion of a transpose and a reduction\";\n+  }\n+  return {};\n+}\n+\n FusionDecision ShapesCompatibleForMultiOutputFusion(\n     const HloInstruction& instr1, const HloInstruction& instr2) {\n   // Multi-output fusion kernels share a common parallel loop. The loop\n@@ -196,17 +222,9 @@ FusionDecision ShapesCompatibleForMultiOutputFusion(\n   auto tiled_transpose_hero2 = FindAnyTiledTranspose(*hero2);\n   bool hero2_is_unnested_transpose = tiled_transpose_hero2.has_value();\n \n-  if (hero1_is_unnested_reduce && hero2_is_unnested_reduce &&\n-      !IsFusedReductionOutputConsistent(hero2, hero1)) {\n-    return \"tiled reductions with different shapes\";\n-  } else if (hero1_is_unnested_transpose && hero2_is_unnested_transpose &&\n-             // After normalization to rank 3, the transposes should have the\n-             // same shape and permute the same dimensions.\n-             *tiled_transpose_hero1 != *tiled_transpose_hero2) {\n-    return \"tiled transposes with different shapes\";\n-  } else if ((hero1_is_unnested_transpose && hero2_is_unnested_reduce) ||\n-             (hero1_is_unnested_reduce && hero2_is_unnested_transpose)) {\n-    return \"MOF-fusion of a transpose and a reduction\";\n+  if (NoFusionPossible heroes_are_compatible =\n+          !FusionHeroesAreCompatible(hero1, hero2)) {\n+    return !heroes_are_compatible;\n   }\n \n   const Shape& l1 = get_loop_shape(hero1);\n"
        },
        {
            "name": "gpu_fusible.h",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible.h",
            "patches": [
                {
                    "old_start": 108,
                    "old_length": 6,
                    "new_start": 108,
                    "new_length": 12,
                    "hunk": "@@ -108,6 +108,12 @@ bool CreatesHeavyComputation(const HloInstruction& producer,\n const HloInstruction* GetRealHeroForMultiOutputFusion(\n     const HloInstruction& instr);\n \n+// Whether 'hero1' and 'hero2' are compatible if the two fusions containing\n+// 'hero1' and 'hero2' are merged together. For example merging two fusions with\n+// a reduction hero and a transpose here, respectively, does not work.\n+FusionDecision FusionHeroesAreCompatible(const HloInstruction* hero1,\n+                                         const HloInstruction* hero2);\n+\n // Whether instruction shapes are compatible for multi-output fusion, i.e.\n // whether the emitters support lowering the resulting fusion.\n // This function works for both, sibling and producer-consumer multi-output\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+// Whether 'hero1' and 'hero2' are compatible if the two fusions containing\n+// 'hero1' and 'hero2' are merged together. For example merging two fusions with\n+// a reduction hero and a transpose here, respectively, does not work.\n+FusionDecision FusionHeroesAreCompatible(const HloInstruction* hero1,\n+                                         const HloInstruction* hero2);\n+\n",
            "whole_hunk": "@@ -108,6 +108,12 @@ bool CreatesHeavyComputation(const HloInstruction& producer,\n const HloInstruction* GetRealHeroForMultiOutputFusion(\n     const HloInstruction& instr);\n \n+// Whether 'hero1' and 'hero2' are compatible if the two fusions containing\n+// 'hero1' and 'hero2' are merged together. For example merging two fusions with\n+// a reduction hero and a transpose here, respectively, does not work.\n+FusionDecision FusionHeroesAreCompatible(const HloInstruction* hero1,\n+                                         const HloInstruction* hero2);\n+\n // Whether instruction shapes are compatible for multi-output fusion, i.e.\n // whether the emitters support lowering the resulting fusion.\n // This function works for both, sibling and producer-consumer multi-output\n"
        },
        {
            "name": "gpu_fusible_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/gpu_fusible_test.cc",
            "patches": [
                {
                    "old_start": 600,
                    "old_length": 6,
                    "new_start": 600,
                    "new_length": 9,
                    "hunk": "@@ -600,6 +600,9 @@ TEST_F(\n       module->entry_computation()->root_instruction()->operand(0);\n   const HloInstruction* fusion_2 =\n       module->entry_computation()->root_instruction()->operand(1);\n+  EXPECT_FALSE(\n+      FusionHeroesAreCompatible(fusion_1->fused_expression_root()->operand(0),\n+                                fusion_2->fused_expression_root()->operand(0)));\n   EXPECT_FALSE(ShapesCompatibleForMultiOutputFusion(*fusion_1, *fusion_2));\n }\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  EXPECT_FALSE(\n+      FusionHeroesAreCompatible(fusion_1->fused_expression_root()->operand(0),\n+                                fusion_2->fused_expression_root()->operand(0)));\n",
            "whole_hunk": "@@ -600,6 +600,9 @@ TEST_F(\n       module->entry_computation()->root_instruction()->operand(0);\n   const HloInstruction* fusion_2 =\n       module->entry_computation()->root_instruction()->operand(1);\n+  EXPECT_FALSE(\n+      FusionHeroesAreCompatible(fusion_1->fused_expression_root()->operand(0),\n+                                fusion_2->fused_expression_root()->operand(0)));\n   EXPECT_FALSE(ShapesCompatibleForMultiOutputFusion(*fusion_1, *fusion_2));\n }\n \n"
        },
        {
            "name": "multi_output_fusion_test.cc",
            "path": "tensorflow/compiler/xla/service/gpu/multi_output_fusion_test.cc",
            "patches": [
                {
                    "old_start": 1705,
                    "old_length": 7,
                    "new_start": 1705,
                    "new_length": 88,
                    "hunk": "@@ -1705,7 +1705,88 @@ ENTRY main {\n   CheckGpuMultiOutputFusion(hlo, std::nullopt);\n }\n \n-TEST_F(TransposeMultiOutputFusionTest, MultipleCopiesAndInput) {\n+// Do not group incompatible transposes.\n+TEST_F(TransposeMultiOutputFusionTest, IncompatibleTransposes) {\n+  const char* hlo = R\"(\n+HloModule module\n+\n+fused_computation {\n+  param_0.1 = f32[18,16,32]{2,1,0} parameter(0)\n+  param_1.1 = f32[32,16,18]{2,1,0} parameter(1)\n+  s.1 = f32[18,16,32]{2,1,0} sqrt(param_0.1)\n+  t.1 = f32[32,16,18]{2,1,0} transpose(s.1), dimensions={2,1,0}\n+  sub.1 = f32[32,16,18]{2,1,0} subtract(t.1, param_1.1)\n+  exp.1 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+  ROOT add.1 = f32[32,16,18]{2,1,0} add(exp.1, exp.1)\n+}\n+\n+fused_computation.2 {\n+  param_0.2 = f32[18,16,32]{2,1,0} parameter(0)\n+  s.2 = f32[18,16,32]{2,1,0} sqrt(param_0.2)\n+  ROOT t.2 = f32[18,32,16]{2,1,0} transpose(s.2), dimensions={0,2,1}\n+}\n+\n+ENTRY main {\n+  p = f32[18,16,32]{2,1,0} parameter(0)\n+  p2 = f32[32,16,18]{2,1,0} parameter(1)\n+  fusion = f32[32,16,18]{2,1,0} fusion(p, p2), kind=kLoop, calls=fused_computation\n+  fusion2 = f32[18,32,16]{2,1,0} fusion(p), kind=kInput, calls=fused_computation.2\n+  ROOT t = (f32[32,16,18]{2,1,0}, f32[18,32,16]{2,1,0}) tuple(fusion, fusion2)\n+}\n+  )\";\n+\n+  CheckGpuMultiOutputFusion(hlo, std::nullopt);\n+}\n+\n+// A variation of the test above, where no CSE was run, so we don't detect\n+// 'fusion' as a transpose fusion.\n+TEST_F(TransposeMultiOutputFusionTest, IncompatibleTransposesNoCSE) {\n+  const char* hlo = R\"(\n+HloModule module\n+\n+fused_computation {\n+  param_0.1 = f32[18,16,32]{2,1,0} parameter(0)\n+  param_1.1 = f32[32,16,18]{2,1,0} parameter(1)\n+  s.1 = f32[18,16,32]{2,1,0} sqrt(param_0.1)\n+  t.1 = f32[32,16,18]{2,1,0} transpose(s.1), dimensions={2,1,0}\n+  sub.1 = f32[32,16,18]{2,1,0} subtract(t.1, param_1.1)\n+  exp.1 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+  exp.2 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+  ROOT add.1 = f32[32,16,18]{2,1,0} add(exp.1, exp.2)\n+}\n+\n+fused_computation.2 {\n+  param_0.2 = f32[18,16,32]{2,1,0} parameter(0)\n+  s.2 = f32[18,16,32]{2,1,0} sqrt(param_0.2)\n+  ROOT t.2 = f32[18,32,16]{2,1,0} transpose(s.2), dimensions={0,2,1}\n+}\n+\n+ENTRY main {\n+  p = f32[18,16,32]{2,1,0} parameter(0)\n+  p2 = f32[32,16,18]{2,1,0} parameter(1)\n+  fusion = f32[32,16,18]{2,1,0} fusion(p, p2), kind=kLoop, calls=fused_computation\n+  fusion2 = f32[18,32,16]{2,1,0} fusion(p), kind=kInput, calls=fused_computation.2\n+  ROOT t = (f32[32,16,18]{2,1,0}, f32[18,32,16]{2,1,0}) tuple(fusion, fusion2)\n+}\n+  )\";\n+\n+  CheckGpuMultiOutputFusion(hlo, R\"(\n+// CHECK: %fused_computation (param_0.1: f32[18,16,32], param_1.1: f32[32,16,18]) -> (f32[32,16,18], f32[18,32,16]) {\n+// CHECK-NEXT: [[param_0:%[^ ]+]] = f32[18,16,32]{2,1,0} parameter(0)\n+// CHECK-NEXT: [[s_1:%[^ ]+]] = f32[18,16,32]{2,1,0} sqrt([[param_0]])\n+// CHECK-NEXT: [[t_1:%[^ ]+]] = f32[32,16,18]{2,1,0} transpose([[s_1]]), dimensions={2,1,0}\n+// CHECK-NEXT: [[param_1:%[^ ]+]] = f32[32,16,18]{2,1,0} parameter(1)\n+// CHECK-NEXT: [[sub:%[^ ]+]] = f32[32,16,18]{2,1,0} subtract([[t_1]], [[param_1]])\n+// CHECK-NEXT: [[exp_1:%[^ ]+]] = f32[32,16,18]{2,1,0} exponential([[sub]])\n+// CHECK-NEXT: [[exp_2:%[^ ]+]] = f32[32,16,18]{2,1,0} exponential([[sub]])\n+// CHECK-NEXT: [[add:%[^ ]+]] = f32[32,16,18]{2,1,0} add([[exp_1]], [[exp_2]])\n+// CHECK-NEXT: [[s_2:%[^ ]+]] = f32[18,16,32]{2,1,0} sqrt([[param_0]])\n+// CHECK-NEXT: [[t_2:%[^ ]+]] = f32[18,32,16]{2,1,0} transpose([[s_2]]), dimensions={0,2,1}\n+// CHECK-NEXT: ROOT %{{.*}} = (f32[32,16,18]{2,1,0}, f32[18,32,16]{2,1,0}) tuple([[add]], [[t_2]])\n+})\");\n+}\n+\n+TEST_F(TransposeMultiOutputFusionTest, CopyAndInput) {\n   const char* hlo = R\"(\n HloModule module\n \n"
                },
                {
                    "old_start": 1735,
                    "old_length": 7,
                    "new_start": 1816,
                    "new_length": 7,
                    "hunk": "@@ -1735,7 +1816,7 @@ ENTRY main {\n )\");\n }\n \n-TEST_F(TransposeMultiOutputFusionTest, MultipleCopiesAndInputEpilogueFusion) {\n+TEST_F(TransposeMultiOutputFusionTest, CopyAndInputEpilogueFusion) {\n   const char* hlo = R\"(\n HloModule module\n "
                }
            ],
            "whole_deleted": "-TEST_F(TransposeMultiOutputFusionTest, MultipleCopiesAndInput) {\n-TEST_F(TransposeMultiOutputFusionTest, MultipleCopiesAndInputEpilogueFusion) {\n",
            "whole_added": "+// Do not group incompatible transposes.\n+TEST_F(TransposeMultiOutputFusionTest, IncompatibleTransposes) {\n+  const char* hlo = R\"(\n+HloModule module\n+\n+fused_computation {\n+  param_0.1 = f32[18,16,32]{2,1,0} parameter(0)\n+  param_1.1 = f32[32,16,18]{2,1,0} parameter(1)\n+  s.1 = f32[18,16,32]{2,1,0} sqrt(param_0.1)\n+  t.1 = f32[32,16,18]{2,1,0} transpose(s.1), dimensions={2,1,0}\n+  sub.1 = f32[32,16,18]{2,1,0} subtract(t.1, param_1.1)\n+  exp.1 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+  ROOT add.1 = f32[32,16,18]{2,1,0} add(exp.1, exp.1)\n+}\n+\n+fused_computation.2 {\n+  param_0.2 = f32[18,16,32]{2,1,0} parameter(0)\n+  s.2 = f32[18,16,32]{2,1,0} sqrt(param_0.2)\n+  ROOT t.2 = f32[18,32,16]{2,1,0} transpose(s.2), dimensions={0,2,1}\n+}\n+\n+ENTRY main {\n+  p = f32[18,16,32]{2,1,0} parameter(0)\n+  p2 = f32[32,16,18]{2,1,0} parameter(1)\n+  fusion = f32[32,16,18]{2,1,0} fusion(p, p2), kind=kLoop, calls=fused_computation\n+  fusion2 = f32[18,32,16]{2,1,0} fusion(p), kind=kInput, calls=fused_computation.2\n+  ROOT t = (f32[32,16,18]{2,1,0}, f32[18,32,16]{2,1,0}) tuple(fusion, fusion2)\n+}\n+  )\";\n+\n+  CheckGpuMultiOutputFusion(hlo, std::nullopt);\n+}\n+\n+// A variation of the test above, where no CSE was run, so we don't detect\n+// 'fusion' as a transpose fusion.\n+TEST_F(TransposeMultiOutputFusionTest, IncompatibleTransposesNoCSE) {\n+  const char* hlo = R\"(\n+HloModule module\n+\n+fused_computation {\n+  param_0.1 = f32[18,16,32]{2,1,0} parameter(0)\n+  param_1.1 = f32[32,16,18]{2,1,0} parameter(1)\n+  s.1 = f32[18,16,32]{2,1,0} sqrt(param_0.1)\n+  t.1 = f32[32,16,18]{2,1,0} transpose(s.1), dimensions={2,1,0}\n+  sub.1 = f32[32,16,18]{2,1,0} subtract(t.1, param_1.1)\n+  exp.1 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+  exp.2 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+  ROOT add.1 = f32[32,16,18]{2,1,0} add(exp.1, exp.2)\n+}\n+\n+fused_computation.2 {\n+  param_0.2 = f32[18,16,32]{2,1,0} parameter(0)\n+  s.2 = f32[18,16,32]{2,1,0} sqrt(param_0.2)\n+  ROOT t.2 = f32[18,32,16]{2,1,0} transpose(s.2), dimensions={0,2,1}\n+}\n+\n+ENTRY main {\n+  p = f32[18,16,32]{2,1,0} parameter(0)\n+  p2 = f32[32,16,18]{2,1,0} parameter(1)\n+  fusion = f32[32,16,18]{2,1,0} fusion(p, p2), kind=kLoop, calls=fused_computation\n+  fusion2 = f32[18,32,16]{2,1,0} fusion(p), kind=kInput, calls=fused_computation.2\n+  ROOT t = (f32[32,16,18]{2,1,0}, f32[18,32,16]{2,1,0}) tuple(fusion, fusion2)\n+}\n+  )\";\n+\n+  CheckGpuMultiOutputFusion(hlo, R\"(\n+// CHECK: %fused_computation (param_0.1: f32[18,16,32], param_1.1: f32[32,16,18]) -> (f32[32,16,18], f32[18,32,16]) {\n+// CHECK-NEXT: [[param_0:%[^ ]+]] = f32[18,16,32]{2,1,0} parameter(0)\n+// CHECK-NEXT: [[s_1:%[^ ]+]] = f32[18,16,32]{2,1,0} sqrt([[param_0]])\n+// CHECK-NEXT: [[t_1:%[^ ]+]] = f32[32,16,18]{2,1,0} transpose([[s_1]]), dimensions={2,1,0}\n+// CHECK-NEXT: [[param_1:%[^ ]+]] = f32[32,16,18]{2,1,0} parameter(1)\n+// CHECK-NEXT: [[sub:%[^ ]+]] = f32[32,16,18]{2,1,0} subtract([[t_1]], [[param_1]])\n+// CHECK-NEXT: [[exp_1:%[^ ]+]] = f32[32,16,18]{2,1,0} exponential([[sub]])\n+// CHECK-NEXT: [[exp_2:%[^ ]+]] = f32[32,16,18]{2,1,0} exponential([[sub]])\n+// CHECK-NEXT: [[add:%[^ ]+]] = f32[32,16,18]{2,1,0} add([[exp_1]], [[exp_2]])\n+// CHECK-NEXT: [[s_2:%[^ ]+]] = f32[18,16,32]{2,1,0} sqrt([[param_0]])\n+// CHECK-NEXT: [[t_2:%[^ ]+]] = f32[18,32,16]{2,1,0} transpose([[s_2]]), dimensions={0,2,1}\n+// CHECK-NEXT: ROOT %{{.*}} = (f32[32,16,18]{2,1,0}, f32[18,32,16]{2,1,0}) tuple([[add]], [[t_2]])\n+})\");\n+}\n+\n+TEST_F(TransposeMultiOutputFusionTest, CopyAndInput) {\n+TEST_F(TransposeMultiOutputFusionTest, CopyAndInputEpilogueFusion) {\n",
            "whole_hunk": "@@ -1705,7 +1705,88 @@ ENTRY main {\n   CheckGpuMultiOutputFusion(hlo, std::nullopt);\n }\n \n-TEST_F(TransposeMultiOutputFusionTest, MultipleCopiesAndInput) {\n+// Do not group incompatible transposes.\n+TEST_F(TransposeMultiOutputFusionTest, IncompatibleTransposes) {\n+  const char* hlo = R\"(\n+HloModule module\n+\n+fused_computation {\n+  param_0.1 = f32[18,16,32]{2,1,0} parameter(0)\n+  param_1.1 = f32[32,16,18]{2,1,0} parameter(1)\n+  s.1 = f32[18,16,32]{2,1,0} sqrt(param_0.1)\n+  t.1 = f32[32,16,18]{2,1,0} transpose(s.1), dimensions={2,1,0}\n+  sub.1 = f32[32,16,18]{2,1,0} subtract(t.1, param_1.1)\n+  exp.1 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+  ROOT add.1 = f32[32,16,18]{2,1,0} add(exp.1, exp.1)\n+}\n+\n+fused_computation.2 {\n+  param_0.2 = f32[18,16,32]{2,1,0} parameter(0)\n+  s.2 = f32[18,16,32]{2,1,0} sqrt(param_0.2)\n+  ROOT t.2 = f32[18,32,16]{2,1,0} transpose(s.2), dimensions={0,2,1}\n+}\n+\n+ENTRY main {\n+  p = f32[18,16,32]{2,1,0} parameter(0)\n+  p2 = f32[32,16,18]{2,1,0} parameter(1)\n+  fusion = f32[32,16,18]{2,1,0} fusion(p, p2), kind=kLoop, calls=fused_computation\n+  fusion2 = f32[18,32,16]{2,1,0} fusion(p), kind=kInput, calls=fused_computation.2\n+  ROOT t = (f32[32,16,18]{2,1,0}, f32[18,32,16]{2,1,0}) tuple(fusion, fusion2)\n+}\n+  )\";\n+\n+  CheckGpuMultiOutputFusion(hlo, std::nullopt);\n+}\n+\n+// A variation of the test above, where no CSE was run, so we don't detect\n+// 'fusion' as a transpose fusion.\n+TEST_F(TransposeMultiOutputFusionTest, IncompatibleTransposesNoCSE) {\n+  const char* hlo = R\"(\n+HloModule module\n+\n+fused_computation {\n+  param_0.1 = f32[18,16,32]{2,1,0} parameter(0)\n+  param_1.1 = f32[32,16,18]{2,1,0} parameter(1)\n+  s.1 = f32[18,16,32]{2,1,0} sqrt(param_0.1)\n+  t.1 = f32[32,16,18]{2,1,0} transpose(s.1), dimensions={2,1,0}\n+  sub.1 = f32[32,16,18]{2,1,0} subtract(t.1, param_1.1)\n+  exp.1 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+  exp.2 = f32[32,16,18]{2,1,0} exponential(sub.1)\n+  ROOT add.1 = f32[32,16,18]{2,1,0} add(exp.1, exp.2)\n+}\n+\n+fused_computation.2 {\n+  param_0.2 = f32[18,16,32]{2,1,0} parameter(0)\n+  s.2 = f32[18,16,32]{2,1,0} sqrt(param_0.2)\n+  ROOT t.2 = f32[18,32,16]{2,1,0} transpose(s.2), dimensions={0,2,1}\n+}\n+\n+ENTRY main {\n+  p = f32[18,16,32]{2,1,0} parameter(0)\n+  p2 = f32[32,16,18]{2,1,0} parameter(1)\n+  fusion = f32[32,16,18]{2,1,0} fusion(p, p2), kind=kLoop, calls=fused_computation\n+  fusion2 = f32[18,32,16]{2,1,0} fusion(p), kind=kInput, calls=fused_computation.2\n+  ROOT t = (f32[32,16,18]{2,1,0}, f32[18,32,16]{2,1,0}) tuple(fusion, fusion2)\n+}\n+  )\";\n+\n+  CheckGpuMultiOutputFusion(hlo, R\"(\n+// CHECK: %fused_computation (param_0.1: f32[18,16,32], param_1.1: f32[32,16,18]) -> (f32[32,16,18], f32[18,32,16]) {\n+// CHECK-NEXT: [[param_0:%[^ ]+]] = f32[18,16,32]{2,1,0} parameter(0)\n+// CHECK-NEXT: [[s_1:%[^ ]+]] = f32[18,16,32]{2,1,0} sqrt([[param_0]])\n+// CHECK-NEXT: [[t_1:%[^ ]+]] = f32[32,16,18]{2,1,0} transpose([[s_1]]), dimensions={2,1,0}\n+// CHECK-NEXT: [[param_1:%[^ ]+]] = f32[32,16,18]{2,1,0} parameter(1)\n+// CHECK-NEXT: [[sub:%[^ ]+]] = f32[32,16,18]{2,1,0} subtract([[t_1]], [[param_1]])\n+// CHECK-NEXT: [[exp_1:%[^ ]+]] = f32[32,16,18]{2,1,0} exponential([[sub]])\n+// CHECK-NEXT: [[exp_2:%[^ ]+]] = f32[32,16,18]{2,1,0} exponential([[sub]])\n+// CHECK-NEXT: [[add:%[^ ]+]] = f32[32,16,18]{2,1,0} add([[exp_1]], [[exp_2]])\n+// CHECK-NEXT: [[s_2:%[^ ]+]] = f32[18,16,32]{2,1,0} sqrt([[param_0]])\n+// CHECK-NEXT: [[t_2:%[^ ]+]] = f32[18,32,16]{2,1,0} transpose([[s_2]]), dimensions={0,2,1}\n+// CHECK-NEXT: ROOT %{{.*}} = (f32[32,16,18]{2,1,0}, f32[18,32,16]{2,1,0}) tuple([[add]], [[t_2]])\n+})\");\n+}\n+\n+TEST_F(TransposeMultiOutputFusionTest, CopyAndInput) {\n   const char* hlo = R\"(\n HloModule module\n \n@@ -1735,7 +1816,7 @@ ENTRY main {\n )\");\n }\n \n-TEST_F(TransposeMultiOutputFusionTest, MultipleCopiesAndInputEpilogueFusion) {\n+TEST_F(TransposeMultiOutputFusionTest, CopyAndInputEpilogueFusion) {\n   const char* hlo = R\"(\n HloModule module\n "
        }
    ]
},
{
    "Id": 104,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8e8ee3834146461a7472de086742752eb7aca245",
    "date": "2024-03-19T08:10:13-07:00",
    "message": "PR #9688: [XLA:CPU] Add F16 support for oneDNN matmul\n\nImported from GitHub PR https://github.com/openxla/xla/pull/9688\n\nThis PR enables F16 support for dot operation when onednn is enabled and adds tests. In the existing pipeline, F16 dot is converted to FP32 through the change-op-data-type pass. With the changes in this PR, if onednn is enabled and the conditions for rewriting dot to onednn matmul are met then F16 dot is not upcast to FP32.\nCopybara import of the project:\n\n--\n190e446d920fb82732054383c81545c528d6d9d9 by Kanvi Khanna <kanvi.khanna@intel.com>:\n\nAdd F16 support for matmul\n\n--\nf99f2bb30febc5c818c28c379232151085deebd6 by Kanvi Khanna <kanvi.khanna@intel.com>:\n\nfix formatting\n\n--\naef8cb1685539a5e1c4765262b8ed5f537613d58 by Kanvi Khanna <kanvi.khanna@intel.com>:\n\nfix for failing test\n\n--\n72dd379ce16eab6838ef568b20ab15e0e0ea0449 by Kanvi Khanna <kanvi.khanna@intel.com>:\n\nAdd check in ChangeOpDataType pass for F16 support as a separate pass for F16 support is not needed; undo changes for BMM;\n\nMerging this change closes #9688\n\nPiperOrigin-RevId: 617161505",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/BUILD",
            "patches": [
                {
                    "old_start": 7275,
                    "old_length": 6,
                    "new_start": 7275,
                    "new_length": 7,
                    "hunk": "@@ -7275,6 +7275,7 @@ cc_library(\n     deps = [\n         \":hlo_creation_utils\",\n         \":hlo_pass\",\n+        \"//xla/service/cpu:onednn_matmul_rewriter\",\n     ],\n )\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//xla/service/cpu:onednn_matmul_rewriter\",\n",
            "whole_hunk": "@@ -7275,6 +7275,7 @@ cc_library(\n     deps = [\n         \":hlo_creation_utils\",\n         \":hlo_pass\",\n+        \"//xla/service/cpu:onednn_matmul_rewriter\",\n     ],\n )\n \n"
        },
        {
            "name": "change_op_data_type.cc",
            "path": "third_party/xla/xla/service/change_op_data_type.cc",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 6,
                    "new_start": 18,
                    "new_length": 9,
                    "hunk": "@@ -18,6 +18,9 @@ limitations under the License.\n #include <optional>\n \n #include \"xla/service/hlo_creation_utils.h\"\n+#if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)\n+#include \"xla/service/cpu/onednn_matmul_rewriter.h\"\n+#endif  // INTEL_MKL && ENABLE_ONEDNN_V3\n \n namespace xla {\n namespace {\n"
                },
                {
                    "old_start": 59,
                    "old_length": 6,
                    "new_start": 62,
                    "new_length": 12,
                    "hunk": "@@ -59,6 +62,12 @@ absl::StatusOr<bool> ChangeOpDataType::Run(\n       if (it == to_type_map_.end()) {\n         continue;\n       }\n+#if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)\n+      if (instr->opcode == HloOpcode::kDot &&\n+          OneDnnMatMulRewriter::ShouldRewrite(instr)) {\n+        continue;\n+      }\n+#endif  // INTEL_MKL && ENABLE_ONEDNN_V3\n       const PrimitiveType to_type = it->second;\n       absl::InlinedVector<HloInstruction*, 8> new_operands;\n       for (HloInstruction* operand : instr->mutable_operands()) {\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)\n+#include \"xla/service/cpu/onednn_matmul_rewriter.h\"\n+#endif  // INTEL_MKL && ENABLE_ONEDNN_V3\n+#if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)\n+      if (instr->opcode == HloOpcode::kDot &&\n+          OneDnnMatMulRewriter::ShouldRewrite(instr)) {\n+        continue;\n+      }\n+#endif  // INTEL_MKL && ENABLE_ONEDNN_V3\n",
            "whole_hunk": "@@ -18,6 +18,9 @@ limitations under the License.\n #include <optional>\n \n #include \"xla/service/hlo_creation_utils.h\"\n+#if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)\n+#include \"xla/service/cpu/onednn_matmul_rewriter.h\"\n+#endif  // INTEL_MKL && ENABLE_ONEDNN_V3\n \n namespace xla {\n namespace {\n@@ -59,6 +62,12 @@ absl::StatusOr<bool> ChangeOpDataType::Run(\n       if (it == to_type_map_.end()) {\n         continue;\n       }\n+#if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)\n+      if (instr->opcode == HloOpcode::kDot &&\n+          OneDnnMatMulRewriter::ShouldRewrite(instr)) {\n+        continue;\n+      }\n+#endif  // INTEL_MKL && ENABLE_ONEDNN_V3\n       const PrimitiveType to_type = it->second;\n       absl::InlinedVector<HloInstruction*, 8> new_operands;\n       for (HloInstruction* operand : instr->mutable_operands()) {\n"
        },
        {
            "name": "onednn_matmul_rewriter.cc",
            "path": "third_party/xla/xla/service/cpu/onednn_matmul_rewriter.cc",
            "patches": [
                {
                    "old_start": 55,
                    "old_length": 16,
                    "new_start": 55,
                    "new_length": 6,
                    "hunk": "@@ -55,16 +55,6 @@ inline Status ValidateDotDimensionNumbers(\n   return OkStatus();\n }\n \n-// We also check if the convert instruction has only one use.\n-inline bool AllOperandsConvertedFromBF16ToF32(const HloInstruction* instr) {\n-  return absl::c_all_of(instr->operands(), [](HloInstruction* operand) {\n-    return Match(operand,\n-                 m::Convert(m::Op().WithElementType(PrimitiveType::BF16))\n-                     .WithElementType(PrimitiveType::F32)\n-                     .WithOneUse());\n-  });\n-}\n-\n template <typename Pattern>\n auto ElementwiseSafeIntermediate(HloInstruction** instr, Pattern pattern) {\n   return m::AnyOf<HloInstruction>(m::Broadcast(instr, pattern.WithOneUser()),\n"
                },
                {
                    "old_start": 254,
                    "old_length": 10,
                    "new_start": 244,
                    "new_length": 15,
                    "hunk": "@@ -254,10 +244,15 @@ inline bool IsRowMajor(const Shape& shape) {\n // TODO(intel-tf): Restict compatible types based on instruction kind.\n inline bool CompatibleElementType(const HloInstruction* instr) {\n   PrimitiveType element_type = instr->shape().element_type();\n-  return element_type == BF16 || element_type == F32;\n+  return element_type == BF16 || element_type == F32 || element_type == F16;\n+}\n+\n+inline bool LowPrecisionType(const HloInstruction* instr) {\n+  PrimitiveType element_type = instr->shape().element_type();\n+  return element_type == BF16 || element_type == F16;\n }\n \n-// Type conversion from and to any of BF16 and FP32.\n+// Type conversion from and to any of BF16, F16 and FP32.\n // TODO(intel-tf): Support more types when enabled.\n template <typename Pattern>\n inline auto SupportedConvert(Pattern pattern) {\n"
                },
                {
                    "old_start": 305,
                    "old_length": 14,
                    "new_start": 300,
                    "new_length": 13,
                    "hunk": "@@ -305,14 +300,13 @@ inline auto OptionalConvertAndBitcast(HloInstruction** optional_convert,\n   // Checks the presence of some intermediate operations that can be moved /\n   // folded to allow dot fusion with add.\n   // Try to match either of the following:\n-  //   1. pattern-root -> bf16-to-fp32 convert -> bitcast\n-  //   2. pattern-root -> bf16-to-fp32 convert\n+  //   1. pattern-root -> bf16/f16-to-fp32 convert -> bitcast\n+  //   2. pattern-root -> bf16/f16-to-fp32 convert\n   //   3. pattern-root -> bitcast\n   //   4. pattern-root\n   auto common =\n       m::AnyOf<HloInstruction>(\n           SupportedConvert(optional_convert, std::move(pattern).WithOneUser())\n-              .WithOperand(0, m::Op().WithElementType(PrimitiveType::BF16))\n               .WithElementType(PrimitiveType::F32),\n           std::move(pattern).WithOneUser())\n           .WithOneUser();\n"
                },
                {
                    "old_start": 413,
                    "old_length": 30,
                    "new_start": 408,
                    "new_length": 6,
                    "hunk": "@@ -413,30 +408,6 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n     return OkStatus();\n   }\n \n-  Status HandleConvert(HloInstruction* convert) override {\n-    HloInstruction* matmul_instr;\n-    auto pattern =\n-        m::Convert(m::CustomCall(&matmul_instr, {\"__onednn$matmul\"})\n-                       .WithOneUse()\n-                       .WithElementType(PrimitiveType::F32)\n-                       .WithPredicate(AllOperandsConvertedFromBF16ToF32))\n-            .WithElementType(PrimitiveType::BF16);\n-\n-    if (!Match(convert, pattern)) return OkStatus();\n-    if (!IsSupportedType(convert->shape().element_type())) return OkStatus();\n-\n-    // BFloat16 operands.\n-    std::vector<HloInstruction*> bf16_operands;\n-    for (auto operand : matmul_instr->operands()) {\n-      bf16_operands.push_back(operand->mutable_operand(0));\n-    }\n-\n-    HloInstruction* matmul_call = convert->AddInstruction(\n-        matmul_instr->CloneWithNewOperands(convert->shape(), bf16_operands));\n-    TF_RETURN_IF_ERROR(ReplaceInstruction(convert, matmul_call));\n-    return OkStatus();\n-  }\n-\n   Status HandleAdd(HloInstruction* instr) override {\n     // Try to do a fusion for Dot(onednn-matmul) + Add. However,\n     // HLO Add instruction might receive the addends after additional\n"
                },
                {
                    "old_start": 548,
                    "old_length": 11,
                    "new_start": 519,
                    "new_length": 11,
                    "hunk": "@@ -548,11 +519,11 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n       // for bf16 case to avoid datatype mismatch.\n       if (optional_dot_bitcast != nullptr &&\n           optional_dot_bitcast->opcode() == HloOpcode::kBitcast) {\n-        if (matmul_call->shape().element_type() == PrimitiveType::BF16) {\n+        if (LowPrecisionType(matmul_call)) {\n           auto bitcast_call =\n               matmul_call->AddInstruction(HloInstruction::CreateBitcast(\n-                  ShapeUtil::ChangeElementType(instr->shape(),\n-                                               PrimitiveType::BF16),\n+                  ShapeUtil::ChangeElementType(\n+                      instr->shape(), matmul_call->shape().element_type()),\n                   matmul_call));\n           new_instr =\n               bitcast_call->AddInstruction(HloInstruction::CreateConvert(\n"
                },
                {
                    "old_start": 564,
                    "old_length": 7,
                    "new_start": 535,
                    "new_length": 7,
                    "hunk": "@@ -564,7 +535,7 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n               HloInstruction::CreateBitcast(instr->shape(), matmul_call));\n         }\n       } else {\n-        if (matmul_call->shape().element_type() == PrimitiveType::BF16) {\n+        if (LowPrecisionType(matmul_call)) {\n           new_instr = matmul_call->AddInstruction(HloInstruction::CreateConvert(\n               ShapeUtil::ChangeElementType(matmul_call->shape(),\n                                            PrimitiveType::F32),\n"
                }
            ],
            "whole_deleted": "-// We also check if the convert instruction has only one use.\n-inline bool AllOperandsConvertedFromBF16ToF32(const HloInstruction* instr) {\n-  return absl::c_all_of(instr->operands(), [](HloInstruction* operand) {\n-    return Match(operand,\n-                 m::Convert(m::Op().WithElementType(PrimitiveType::BF16))\n-                     .WithElementType(PrimitiveType::F32)\n-                     .WithOneUse());\n-  });\n-}\n-\n-  return element_type == BF16 || element_type == F32;\n-// Type conversion from and to any of BF16 and FP32.\n-  //   1. pattern-root -> bf16-to-fp32 convert -> bitcast\n-  //   2. pattern-root -> bf16-to-fp32 convert\n-              .WithOperand(0, m::Op().WithElementType(PrimitiveType::BF16))\n-  Status HandleConvert(HloInstruction* convert) override {\n-    HloInstruction* matmul_instr;\n-    auto pattern =\n-        m::Convert(m::CustomCall(&matmul_instr, {\"__onednn$matmul\"})\n-                       .WithOneUse()\n-                       .WithElementType(PrimitiveType::F32)\n-                       .WithPredicate(AllOperandsConvertedFromBF16ToF32))\n-            .WithElementType(PrimitiveType::BF16);\n-\n-    if (!Match(convert, pattern)) return OkStatus();\n-    if (!IsSupportedType(convert->shape().element_type())) return OkStatus();\n-\n-    // BFloat16 operands.\n-    std::vector<HloInstruction*> bf16_operands;\n-    for (auto operand : matmul_instr->operands()) {\n-      bf16_operands.push_back(operand->mutable_operand(0));\n-    }\n-\n-    HloInstruction* matmul_call = convert->AddInstruction(\n-        matmul_instr->CloneWithNewOperands(convert->shape(), bf16_operands));\n-    TF_RETURN_IF_ERROR(ReplaceInstruction(convert, matmul_call));\n-    return OkStatus();\n-  }\n-\n-        if (matmul_call->shape().element_type() == PrimitiveType::BF16) {\n-                  ShapeUtil::ChangeElementType(instr->shape(),\n-                                               PrimitiveType::BF16),\n-        if (matmul_call->shape().element_type() == PrimitiveType::BF16) {\n",
            "whole_added": "+  return element_type == BF16 || element_type == F32 || element_type == F16;\n+}\n+\n+inline bool LowPrecisionType(const HloInstruction* instr) {\n+  PrimitiveType element_type = instr->shape().element_type();\n+  return element_type == BF16 || element_type == F16;\n+// Type conversion from and to any of BF16, F16 and FP32.\n+  //   1. pattern-root -> bf16/f16-to-fp32 convert -> bitcast\n+  //   2. pattern-root -> bf16/f16-to-fp32 convert\n+        if (LowPrecisionType(matmul_call)) {\n+                  ShapeUtil::ChangeElementType(\n+                      instr->shape(), matmul_call->shape().element_type()),\n+        if (LowPrecisionType(matmul_call)) {\n",
            "whole_hunk": "@@ -55,16 +55,6 @@ inline Status ValidateDotDimensionNumbers(\n   return OkStatus();\n }\n \n-// We also check if the convert instruction has only one use.\n-inline bool AllOperandsConvertedFromBF16ToF32(const HloInstruction* instr) {\n-  return absl::c_all_of(instr->operands(), [](HloInstruction* operand) {\n-    return Match(operand,\n-                 m::Convert(m::Op().WithElementType(PrimitiveType::BF16))\n-                     .WithElementType(PrimitiveType::F32)\n-                     .WithOneUse());\n-  });\n-}\n-\n template <typename Pattern>\n auto ElementwiseSafeIntermediate(HloInstruction** instr, Pattern pattern) {\n   return m::AnyOf<HloInstruction>(m::Broadcast(instr, pattern.WithOneUser()),\n@@ -254,10 +244,15 @@ inline bool IsRowMajor(const Shape& shape) {\n // TODO(intel-tf): Restict compatible types based on instruction kind.\n inline bool CompatibleElementType(const HloInstruction* instr) {\n   PrimitiveType element_type = instr->shape().element_type();\n-  return element_type == BF16 || element_type == F32;\n+  return element_type == BF16 || element_type == F32 || element_type == F16;\n+}\n+\n+inline bool LowPrecisionType(const HloInstruction* instr) {\n+  PrimitiveType element_type = instr->shape().element_type();\n+  return element_type == BF16 || element_type == F16;\n }\n \n-// Type conversion from and to any of BF16 and FP32.\n+// Type conversion from and to any of BF16, F16 and FP32.\n // TODO(intel-tf): Support more types when enabled.\n template <typename Pattern>\n inline auto SupportedConvert(Pattern pattern) {\n@@ -305,14 +300,13 @@ inline auto OptionalConvertAndBitcast(HloInstruction** optional_convert,\n   // Checks the presence of some intermediate operations that can be moved /\n   // folded to allow dot fusion with add.\n   // Try to match either of the following:\n-  //   1. pattern-root -> bf16-to-fp32 convert -> bitcast\n-  //   2. pattern-root -> bf16-to-fp32 convert\n+  //   1. pattern-root -> bf16/f16-to-fp32 convert -> bitcast\n+  //   2. pattern-root -> bf16/f16-to-fp32 convert\n   //   3. pattern-root -> bitcast\n   //   4. pattern-root\n   auto common =\n       m::AnyOf<HloInstruction>(\n           SupportedConvert(optional_convert, std::move(pattern).WithOneUser())\n-              .WithOperand(0, m::Op().WithElementType(PrimitiveType::BF16))\n               .WithElementType(PrimitiveType::F32),\n           std::move(pattern).WithOneUser())\n           .WithOneUser();\n@@ -413,30 +408,6 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n     return OkStatus();\n   }\n \n-  Status HandleConvert(HloInstruction* convert) override {\n-    HloInstruction* matmul_instr;\n-    auto pattern =\n-        m::Convert(m::CustomCall(&matmul_instr, {\"__onednn$matmul\"})\n-                       .WithOneUse()\n-                       .WithElementType(PrimitiveType::F32)\n-                       .WithPredicate(AllOperandsConvertedFromBF16ToF32))\n-            .WithElementType(PrimitiveType::BF16);\n-\n-    if (!Match(convert, pattern)) return OkStatus();\n-    if (!IsSupportedType(convert->shape().element_type())) return OkStatus();\n-\n-    // BFloat16 operands.\n-    std::vector<HloInstruction*> bf16_operands;\n-    for (auto operand : matmul_instr->operands()) {\n-      bf16_operands.push_back(operand->mutable_operand(0));\n-    }\n-\n-    HloInstruction* matmul_call = convert->AddInstruction(\n-        matmul_instr->CloneWithNewOperands(convert->shape(), bf16_operands));\n-    TF_RETURN_IF_ERROR(ReplaceInstruction(convert, matmul_call));\n-    return OkStatus();\n-  }\n-\n   Status HandleAdd(HloInstruction* instr) override {\n     // Try to do a fusion for Dot(onednn-matmul) + Add. However,\n     // HLO Add instruction might receive the addends after additional\n@@ -548,11 +519,11 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n       // for bf16 case to avoid datatype mismatch.\n       if (optional_dot_bitcast != nullptr &&\n           optional_dot_bitcast->opcode() == HloOpcode::kBitcast) {\n-        if (matmul_call->shape().element_type() == PrimitiveType::BF16) {\n+        if (LowPrecisionType(matmul_call)) {\n           auto bitcast_call =\n               matmul_call->AddInstruction(HloInstruction::CreateBitcast(\n-                  ShapeUtil::ChangeElementType(instr->shape(),\n-                                               PrimitiveType::BF16),\n+                  ShapeUtil::ChangeElementType(\n+                      instr->shape(), matmul_call->shape().element_type()),\n                   matmul_call));\n           new_instr =\n               bitcast_call->AddInstruction(HloInstruction::CreateConvert(\n@@ -564,7 +535,7 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n               HloInstruction::CreateBitcast(instr->shape(), matmul_call));\n         }\n       } else {\n-        if (matmul_call->shape().element_type() == PrimitiveType::BF16) {\n+        if (LowPrecisionType(matmul_call)) {\n           new_instr = matmul_call->AddInstruction(HloInstruction::CreateConvert(\n               ShapeUtil::ChangeElementType(matmul_call->shape(),\n                                            PrimitiveType::F32),\n"
        },
        {
            "name": "onednn_matmul_rewriter.h",
            "path": "third_party/xla/xla/service/cpu/onednn_matmul_rewriter.h",
            "patches": [
                {
                    "old_start": 37,
                    "old_length": 7,
                    "new_start": 37,
                    "new_length": 7,
                    "hunk": "@@ -37,7 +37,7 @@ class OneDnnMatMulRewriter : public HloModulePass {\n                        const tsl::thread::ThreadPool* compile_threadpool)\n       : intra_op_parallelism_(intra_op_parallelism),\n         compile_threadpool_(compile_threadpool) {}\n-\n+  OneDnnMatMulRewriter() = default;\n   absl::string_view name() const override { return \"onednn-matmul-rewriter\"; }\n \n   using HloPassInterface::Run;\n"
                }
            ],
            "whole_deleted": "-\n",
            "whole_added": "+  OneDnnMatMulRewriter() = default;\n",
            "whole_hunk": "@@ -37,7 +37,7 @@ class OneDnnMatMulRewriter : public HloModulePass {\n                        const tsl::thread::ThreadPool* compile_threadpool)\n       : intra_op_parallelism_(intra_op_parallelism),\n         compile_threadpool_(compile_threadpool) {}\n-\n+  OneDnnMatMulRewriter() = default;\n   absl::string_view name() const override { return \"onednn-matmul-rewriter\"; }\n \n   using HloPassInterface::Run;\n"
        },
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/tests/BUILD",
            "patches": [
                {
                    "old_start": 2867,
                    "old_length": 6,
                    "new_start": 2867,
                    "new_length": 7,
                    "hunk": "@@ -2867,6 +2867,7 @@ xla_test(\n         \"//xla:shape_util\",\n         \"//xla:test\",\n         \"//xla:test_helpers\",\n+        \"//xla/hlo/utils:hlo_matchers\",\n         \"//xla/service/cpu:onednn_util\",\n         \"@local_tsl//tsl/platform:platform_port\",\n     ],\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"//xla/hlo/utils:hlo_matchers\",\n",
            "whole_hunk": "@@ -2867,6 +2867,7 @@ xla_test(\n         \"//xla:shape_util\",\n         \"//xla:test\",\n         \"//xla:test_helpers\",\n+        \"//xla/hlo/utils:hlo_matchers\",\n         \"//xla/service/cpu:onednn_util\",\n         \"@local_tsl//tsl/platform:platform_port\",\n     ],\n"
        },
        {
            "name": "onednn_matmul_test.cc",
            "path": "third_party/xla/xla/tests/onednn_matmul_test.cc",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 15,
                    "new_start": 17,
                    "new_length": 20,
                    "hunk": "@@ -17,15 +17,20 @@ limitations under the License.\n \n #include <utility>\n \n+#include \"xla/hlo/utils/hlo_matchers.h\"\n #include \"xla/literal.h\"\n+#include \"xla/service/cpu/onednn_matmul_rewriter.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/test.h\"\n #include \"xla/test_helpers.h\"\n+#include \"xla/tests/filecheck.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"xla/tests/test_macros.h\"\n #include \"tsl/platform/cpu_info.h\"\n \n+namespace op = xla::testing::opcode_matchers;\n+\n namespace xla {\n namespace cpu {\n \n"
                },
                {
                    "old_start": 94,
                    "old_length": 6,
                    "new_start": 99,
                    "new_length": 23,
                    "hunk": "@@ -94,6 +99,23 @@ TEST_F(MatmulTest, SimpleTestBF16) {\n   MatchOptimizedHlo(matmul_module_str, matmul_rewrite_str_);\n }\n \n+TEST_F(MatmulTest, SimpleTestF16) {\n+  if (!IsSupportedType(PrimitiveType::F16)) {\n+    GTEST_SKIP() << \"CPU does not support F16.\";\n+  }\n+\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.test.f16, entry_computation_layout={(f16[32,8,128,64]{3,2,1,0},f16[32,8,64,128]{3,2,1,0})->f16[32,8,128,128]{3,2,1,0}}\n+  ENTRY matmul.test.f16 {\n+    arg.0 = f16[32,8,128,64]{3,2,1,0} parameter(0), parameter_replication={false}\n+    arg.1 = f16[32,8,64,128]{3,2,1,0} parameter(1), parameter_replication={false}\n+    ROOT onednn.matmul.0 = f16[32,8,128,128]{3,2,1,0} dot(arg.0, arg.1), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-2, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str, matmul_rewrite_str_);\n+}\n+\n TEST_F(MatmulTest, SimpleTestF32TransposeB) {\n   const char* matmul_module_str = R\"(\n   HloModule matmul.test.1, entry_computation_layout={(f32[32,8,128,64]{3,1,2,0},f32[32,8,128,64]{3,1,2,0})->f32[32,8,128,128]{3,2,1,0}}\n"
                },
                {
                    "old_start": 481,
                    "old_length": 6,
                    "new_start": 503,
                    "new_length": 55,
                    "hunk": "@@ -481,6 +503,55 @@ TEST_F(MatmulTest, DivisionByConstantWithEltwiseLinearF32) {\n   )\");\n }\n \n+TEST_F(MatmulTest, SimpleBiasTestFP16_PARAM_F32) {\n+  if (!IsSupportedType(PrimitiveType::F16)) {\n+    GTEST_SKIP() << \"CPU does not support F16.\";\n+  }\n+\n+  const char* matmul_module_str = R\"(\n+  HloModule jit_apply, entry_computation_layout={(f32[3072]{0}, f32[768,3072]{1,0}, f32[16,128,768]{2,1,0})->f16[16,128,3072]{2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n+  ENTRY matmul.test.f16 {\n+    Arg_2.3 = f32[16,128,768]{2,1,0} parameter(2), sharding={replicated}\n+    convert.4 = f16[16,128,768]{2,1,0} convert(Arg_2.3)\n+    Arg_1.2 = f32[768,3072]{1,0} parameter(1), sharding={replicated}\n+    convert.5 = f16[768,3072]{1,0} convert(Arg_1.2)\n+    dot.7 = f16[16,128,3072]{2,1,0} dot(convert.4, convert.5), lhs_contracting_dims={2}, rhs_contracting_dims={0}\n+    Arg_0.1 = f32[3072]{0} parameter(0), sharding={replicated}\n+    convert.6 = f16[3072]{0} convert(Arg_0.1)\n+    reshape.8 = f16[1,1,3072]{2,1,0} reshape(convert.6)\n+    broadcast.9 = f16[1,1,3072]{2,1,0} broadcast(reshape.8), dimensions={0,1,2}\n+    reshape.10 = f16[3072]{0} reshape(broadcast.9)\n+    broadcast.11 = f16[16,128,3072]{2,1,0} broadcast(reshape.10), dimensions={2}\n+    ROOT add.12 = f16[16,128,3072]{2,1,0} add(dot.7, broadcast.11)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-2, 1e-2}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_);\n+}\n+\n+TEST_F(MatmulTest, SimpleBiasTestFP16_PARAM_FP16) {\n+  if (!IsSupportedType(PrimitiveType::F16)) {\n+    GTEST_SKIP() << \"CPU does not support F16.\";\n+  }\n+  const char* matmul_module_str = R\"(\n+  HloModule jit_apply, entry_computation_layout={(f16[3072]{0}, f16[768,3072]{1,0}, f32[16,128,768]{2,1,0})->f16[16,128,3072]{2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n+  ENTRY matmul.test.f16 {\n+    Arg_2.3 = f32[16,128,768]{2,1,0} parameter(2), sharding={replicated}\n+    convert.4 = f16[16,128,768]{2,1,0} convert(Arg_2.3)\n+    Arg_1.2 = f16[768,3072]{1,0} parameter(1), sharding={replicated}\n+    dot.5 = f16[16,128,3072]{2,1,0} dot(convert.4, Arg_1.2), lhs_contracting_dims={2}, rhs_contracting_dims={0}\n+    Arg_0.1 = f16[3072]{0} parameter(0), sharding={replicated}\n+    reshape.6 = f16[1,1,3072]{2,1,0} reshape(Arg_0.1)\n+    broadcast.7 = f16[1,1,3072]{2,1,0} broadcast(reshape.6), dimensions={0,1,2}\n+    reshape.8 = f16[3072]{0} reshape(broadcast.7)\n+    broadcast.9 = f16[16,128,3072]{2,1,0} broadcast(reshape.8), dimensions={2}\n+    ROOT add.10 = f16[16,128,3072]{2,1,0} add(dot.5, broadcast.9)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-2, 1e-2}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_);\n+}\n+\n TEST_F(MatmulTest, TestF32NonConstantWeights) {\n   const char* matmul_module_str = R\"(\n   HloModule matmul.test.f32, entry_computation_layout={(f32[64,256,16]{2,1,0},f32[16,32]{1,0})->f32[64,256,32]{2,1,0}}"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"xla/hlo/utils/hlo_matchers.h\"\n+#include \"xla/service/cpu/onednn_matmul_rewriter.h\"\n+#include \"xla/tests/filecheck.h\"\n+namespace op = xla::testing::opcode_matchers;\n+\n+TEST_F(MatmulTest, SimpleTestF16) {\n+  if (!IsSupportedType(PrimitiveType::F16)) {\n+    GTEST_SKIP() << \"CPU does not support F16.\";\n+  }\n+\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.test.f16, entry_computation_layout={(f16[32,8,128,64]{3,2,1,0},f16[32,8,64,128]{3,2,1,0})->f16[32,8,128,128]{3,2,1,0}}\n+  ENTRY matmul.test.f16 {\n+    arg.0 = f16[32,8,128,64]{3,2,1,0} parameter(0), parameter_replication={false}\n+    arg.1 = f16[32,8,64,128]{3,2,1,0} parameter(1), parameter_replication={false}\n+    ROOT onednn.matmul.0 = f16[32,8,128,128]{3,2,1,0} dot(arg.0, arg.1), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-2, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str, matmul_rewrite_str_);\n+}\n+\n+TEST_F(MatmulTest, SimpleBiasTestFP16_PARAM_F32) {\n+  if (!IsSupportedType(PrimitiveType::F16)) {\n+    GTEST_SKIP() << \"CPU does not support F16.\";\n+  }\n+\n+  const char* matmul_module_str = R\"(\n+  HloModule jit_apply, entry_computation_layout={(f32[3072]{0}, f32[768,3072]{1,0}, f32[16,128,768]{2,1,0})->f16[16,128,3072]{2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n+  ENTRY matmul.test.f16 {\n+    Arg_2.3 = f32[16,128,768]{2,1,0} parameter(2), sharding={replicated}\n+    convert.4 = f16[16,128,768]{2,1,0} convert(Arg_2.3)\n+    Arg_1.2 = f32[768,3072]{1,0} parameter(1), sharding={replicated}\n+    convert.5 = f16[768,3072]{1,0} convert(Arg_1.2)\n+    dot.7 = f16[16,128,3072]{2,1,0} dot(convert.4, convert.5), lhs_contracting_dims={2}, rhs_contracting_dims={0}\n+    Arg_0.1 = f32[3072]{0} parameter(0), sharding={replicated}\n+    convert.6 = f16[3072]{0} convert(Arg_0.1)\n+    reshape.8 = f16[1,1,3072]{2,1,0} reshape(convert.6)\n+    broadcast.9 = f16[1,1,3072]{2,1,0} broadcast(reshape.8), dimensions={0,1,2}\n+    reshape.10 = f16[3072]{0} reshape(broadcast.9)\n+    broadcast.11 = f16[16,128,3072]{2,1,0} broadcast(reshape.10), dimensions={2}\n+    ROOT add.12 = f16[16,128,3072]{2,1,0} add(dot.7, broadcast.11)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-2, 1e-2}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_);\n+}\n+\n+TEST_F(MatmulTest, SimpleBiasTestFP16_PARAM_FP16) {\n+  if (!IsSupportedType(PrimitiveType::F16)) {\n+    GTEST_SKIP() << \"CPU does not support F16.\";\n+  }\n+  const char* matmul_module_str = R\"(\n+  HloModule jit_apply, entry_computation_layout={(f16[3072]{0}, f16[768,3072]{1,0}, f32[16,128,768]{2,1,0})->f16[16,128,3072]{2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n+  ENTRY matmul.test.f16 {\n+    Arg_2.3 = f32[16,128,768]{2,1,0} parameter(2), sharding={replicated}\n+    convert.4 = f16[16,128,768]{2,1,0} convert(Arg_2.3)\n+    Arg_1.2 = f16[768,3072]{1,0} parameter(1), sharding={replicated}\n+    dot.5 = f16[16,128,3072]{2,1,0} dot(convert.4, Arg_1.2), lhs_contracting_dims={2}, rhs_contracting_dims={0}\n+    Arg_0.1 = f16[3072]{0} parameter(0), sharding={replicated}\n+    reshape.6 = f16[1,1,3072]{2,1,0} reshape(Arg_0.1)\n+    broadcast.7 = f16[1,1,3072]{2,1,0} broadcast(reshape.6), dimensions={0,1,2}\n+    reshape.8 = f16[3072]{0} reshape(broadcast.7)\n+    broadcast.9 = f16[16,128,3072]{2,1,0} broadcast(reshape.8), dimensions={2}\n+    ROOT add.10 = f16[16,128,3072]{2,1,0} add(dot.5, broadcast.9)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-2, 1e-2}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_);\n+}\n+\n",
            "whole_hunk": "@@ -17,15 +17,20 @@ limitations under the License.\n \n #include <utility>\n \n+#include \"xla/hlo/utils/hlo_matchers.h\"\n #include \"xla/literal.h\"\n+#include \"xla/service/cpu/onednn_matmul_rewriter.h\"\n #include \"xla/service/cpu/onednn_util.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/test.h\"\n #include \"xla/test_helpers.h\"\n+#include \"xla/tests/filecheck.h\"\n #include \"xla/tests/hlo_test_base.h\"\n #include \"xla/tests/test_macros.h\"\n #include \"tsl/platform/cpu_info.h\"\n \n+namespace op = xla::testing::opcode_matchers;\n+\n namespace xla {\n namespace cpu {\n \n@@ -94,6 +99,23 @@ TEST_F(MatmulTest, SimpleTestBF16) {\n   MatchOptimizedHlo(matmul_module_str, matmul_rewrite_str_);\n }\n \n+TEST_F(MatmulTest, SimpleTestF16) {\n+  if (!IsSupportedType(PrimitiveType::F16)) {\n+    GTEST_SKIP() << \"CPU does not support F16.\";\n+  }\n+\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.test.f16, entry_computation_layout={(f16[32,8,128,64]{3,2,1,0},f16[32,8,64,128]{3,2,1,0})->f16[32,8,128,128]{3,2,1,0}}\n+  ENTRY matmul.test.f16 {\n+    arg.0 = f16[32,8,128,64]{3,2,1,0} parameter(0), parameter_replication={false}\n+    arg.1 = f16[32,8,64,128]{3,2,1,0} parameter(1), parameter_replication={false}\n+    ROOT onednn.matmul.0 = f16[32,8,128,128]{3,2,1,0} dot(arg.0, arg.1), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-2, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str, matmul_rewrite_str_);\n+}\n+\n TEST_F(MatmulTest, SimpleTestF32TransposeB) {\n   const char* matmul_module_str = R\"(\n   HloModule matmul.test.1, entry_computation_layout={(f32[32,8,128,64]{3,1,2,0},f32[32,8,128,64]{3,1,2,0})->f32[32,8,128,128]{3,2,1,0}}\n@@ -481,6 +503,55 @@ TEST_F(MatmulTest, DivisionByConstantWithEltwiseLinearF32) {\n   )\");\n }\n \n+TEST_F(MatmulTest, SimpleBiasTestFP16_PARAM_F32) {\n+  if (!IsSupportedType(PrimitiveType::F16)) {\n+    GTEST_SKIP() << \"CPU does not support F16.\";\n+  }\n+\n+  const char* matmul_module_str = R\"(\n+  HloModule jit_apply, entry_computation_layout={(f32[3072]{0}, f32[768,3072]{1,0}, f32[16,128,768]{2,1,0})->f16[16,128,3072]{2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n+  ENTRY matmul.test.f16 {\n+    Arg_2.3 = f32[16,128,768]{2,1,0} parameter(2), sharding={replicated}\n+    convert.4 = f16[16,128,768]{2,1,0} convert(Arg_2.3)\n+    Arg_1.2 = f32[768,3072]{1,0} parameter(1), sharding={replicated}\n+    convert.5 = f16[768,3072]{1,0} convert(Arg_1.2)\n+    dot.7 = f16[16,128,3072]{2,1,0} dot(convert.4, convert.5), lhs_contracting_dims={2}, rhs_contracting_dims={0}\n+    Arg_0.1 = f32[3072]{0} parameter(0), sharding={replicated}\n+    convert.6 = f16[3072]{0} convert(Arg_0.1)\n+    reshape.8 = f16[1,1,3072]{2,1,0} reshape(convert.6)\n+    broadcast.9 = f16[1,1,3072]{2,1,0} broadcast(reshape.8), dimensions={0,1,2}\n+    reshape.10 = f16[3072]{0} reshape(broadcast.9)\n+    broadcast.11 = f16[16,128,3072]{2,1,0} broadcast(reshape.10), dimensions={2}\n+    ROOT add.12 = f16[16,128,3072]{2,1,0} add(dot.7, broadcast.11)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-2, 1e-2}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_);\n+}\n+\n+TEST_F(MatmulTest, SimpleBiasTestFP16_PARAM_FP16) {\n+  if (!IsSupportedType(PrimitiveType::F16)) {\n+    GTEST_SKIP() << \"CPU does not support F16.\";\n+  }\n+  const char* matmul_module_str = R\"(\n+  HloModule jit_apply, entry_computation_layout={(f16[3072]{0}, f16[768,3072]{1,0}, f32[16,128,768]{2,1,0})->f16[16,128,3072]{2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n+  ENTRY matmul.test.f16 {\n+    Arg_2.3 = f32[16,128,768]{2,1,0} parameter(2), sharding={replicated}\n+    convert.4 = f16[16,128,768]{2,1,0} convert(Arg_2.3)\n+    Arg_1.2 = f16[768,3072]{1,0} parameter(1), sharding={replicated}\n+    dot.5 = f16[16,128,3072]{2,1,0} dot(convert.4, Arg_1.2), lhs_contracting_dims={2}, rhs_contracting_dims={0}\n+    Arg_0.1 = f16[3072]{0} parameter(0), sharding={replicated}\n+    reshape.6 = f16[1,1,3072]{2,1,0} reshape(Arg_0.1)\n+    broadcast.7 = f16[1,1,3072]{2,1,0} broadcast(reshape.6), dimensions={0,1,2}\n+    reshape.8 = f16[3072]{0} reshape(broadcast.7)\n+    broadcast.9 = f16[16,128,3072]{2,1,0} broadcast(reshape.8), dimensions={2}\n+    ROOT add.10 = f16[16,128,3072]{2,1,0} add(dot.5, broadcast.9)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-2, 1e-2}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_);\n+}\n+\n TEST_F(MatmulTest, TestF32NonConstantWeights) {\n   const char* matmul_module_str = R\"(\n   HloModule matmul.test.f32, entry_computation_layout={(f32[64,256,16]{2,1,0},f32[16,32]{1,0})->f32[64,256,32]{2,1,0}}"
        }
    ]
},
{
    "Id": 114,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2800ecbea2e747fcf2ccb79f63c5299221b1c1b6",
    "date": "2024-03-13T07:27:22-07:00",
    "message": "Remove shape checks on tensor shapes which may not yet be correctly set.\n\nSlice cannot accept negative sizes as the input shape may not be correctly set.\n\nPiperOrigin-RevId: 615396964",
    "label": "YES",
    "changes": [
        {
            "name": "xnnpack_delegate.cc",
            "path": "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc",
            "patches": [
                {
                    "old_start": 3190,
                    "old_length": 9,
                    "new_start": 3190,
                    "new_length": 6,
                    "hunk": "@@ -3190,9 +3190,6 @@ class Subgraph {\n       TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(\n           delegate, logging_context, input_tensor, node->inputs->data[i],\n           node_index));\n-\n-      TF_LITE_ENSURE_EQ(logging_context, NumDimensions(&input_tensor),\n-                        NumDimensions(&output_tensor));\n     }\n \n     if (subgraph != nullptr) {\n"
                },
                {
                    "old_start": 5583,
                    "old_length": 9,
                    "new_start": 5580,
                    "new_length": 6,
                    "hunk": "@@ -5583,9 +5580,6 @@ class Subgraph {\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorFloat32OrQUInt8Type(delegate, logging_context, input_tensor,\n                                        input_tensor_index, node_index));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor,\n-                                           num_dims, input_tensor_index,\n-                                           BuiltinOperator_SLICE, node_index));\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorNonDynamicAllocation(delegate, logging_context, input_tensor,\n                                         input_tensor_index, node_index));\n"
                },
                {
                    "old_start": 5593,
                    "old_length": 15,
                    "new_start": 5587,
                    "new_length": 10,
                    "hunk": "@@ -5593,15 +5587,10 @@ class Subgraph {\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorFloat32OrQUInt8Type(delegate, logging_context, output_tensor,\n                                        output_tensor_index, node_index));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor,\n-                                           num_dims, output_tensor_index,\n-                                           BuiltinOperator_SLICE, node_index));\n     TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(\n         delegate, logging_context, output_tensor, output_tensor_index,\n         node_index));\n \n-    const auto input_shape = input_tensor.dims;\n-    const auto output_shape = output_tensor.dims;\n     std::array<int64_t, XNN_MAX_TENSOR_DIMS> begin;\n     std::array<int64_t, XNN_MAX_TENSOR_DIMS> size;\n     CopyTensorDataInt32OrInt64(begin.data(), begin_tensor, num_dims);\n"
                },
                {
                    "old_start": 5609,
                    "old_length": 51,
                    "new_start": 5598,
                    "new_length": 18,
                    "hunk": "@@ -5609,51 +5598,18 @@ class Subgraph {\n \n     for (size_t i = 0; i < num_dims; i++) {\n       if (begin[i] < 0) {\n+        // TODO(b/329228576): Add support for negative begin.\n         TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                  \"begin %\" PRId64\n                                  \" must be greater than 0 in SLICE node #%d\",\n                                  begin[i], node_index);\n       }\n-      if (begin[i] >= input_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(\n-            logging_context,\n-            \"begin %\" PRId64\n-            \" must be less than input dimension %d in SLICE node #%d\",\n-            begin[i], input_shape->data[i], node_index);\n-      }\n       if (size[i] <= 0) {\n-        if (size[i] != -1) {\n-          TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                   \"size %\" PRId64\n-                                   \" must be positive or -1 in SLICE node #%d\",\n-                                   size[i], node_index);\n-          return kTfLiteError;\n-        }\n-        size[i] = input_shape->data[i] - begin[i];\n-      }\n-      if (size[i] > input_shape->data[i]) {\n+        // TODO(b/329228576): Add support for negative begin.\n         TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                  \"size %\" PRId64\n-                                 \" must be less than or equals to input \"\n-                                 \"dimension %d in SLICE node #%d\",\n-                                 size[i], input_shape->data[i], node_index);\n-        return kTfLiteError;\n-      }\n-      if (size[i] != output_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"size %\" PRId64\n-                                 \" does not match output shape %d at \"\n-                                 \"dimension %zu in SLICE node #%d\",\n-                                 size[i], output_shape->data[i], i, node_index);\n-        return kTfLiteError;\n-      }\n-      if (begin[i] + size[i] > input_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"begin + size (%\" PRId64 \" + %\" PRId64\n-                                 \") must not be greater than input \"\n-                                 \"dimension %d in SLICE node #%d\",\n-                                 begin[i], size[i], input_shape->data[i],\n-                                 node_index);\n+                                 \" must be positive in SLICE node #%d\",\n+                                 size[i], node_index);\n         return kTfLiteError;\n       }\n     }\n"
                },
                {
                    "old_start": 6097,
                    "old_length": 7,
                    "new_start": 6053,
                    "new_length": 6,
                    "hunk": "@@ -6097,7 +6053,6 @@ class Subgraph {\n       TfLiteContext* logging_context, int node_index, TfLiteNode* node,\n       const TfLiteTensor* tensors, const TfLiteStridedSliceParams* params,\n       const std::unordered_map<int, uint32_t>& input_output_tensors) {\n-    // return kTfLiteError;\n     // Only support strided slice with no ellipsis mask, no new axis mask, and\n     // no shrink_axis-mask.\n     if (params->ellipsis_mask != 0 || params->new_axis_mask != 0 ||\n"
                },
                {
                    "old_start": 6117,
                    "old_length": 7,
                    "new_start": 6072,
                    "new_length": 7,
                    "hunk": "@@ -6117,7 +6072,7 @@ class Subgraph {\n     TF_LITE_ENSURE_STATUS(CheckTensorInt32Type(\n         logging_context, stride_tensor, stride_tensor_index, node_index));\n \n-    const int num_dims = stride_tensor.dims->size;\n+    const int num_dims = stride_tensor.dims->data[0];\n     if (num_dims > XNN_MAX_TENSOR_DIMS) {\n       TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                \"number of dimensions %d must be less than %d \"\n"
                },
                {
                    "old_start": 6175,
                    "old_length": 9,
                    "new_start": 6130,
                    "new_length": 6,
                    "hunk": "@@ -6175,9 +6130,6 @@ class Subgraph {\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorFloat32OrQUInt8Type(delegate, logging_context, input_tensor,\n                                        input_tensor_index, node_index));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(\n-        logging_context, input_tensor, num_dims, input_tensor_index,\n-        BuiltinOperator_STRIDED_SLICE, node_index));\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorNonDynamicAllocation(delegate, logging_context, input_tensor,\n                                         input_tensor_index, node_index));\n"
                },
                {
                    "old_start": 6185,
                    "old_length": 9,
                    "new_start": 6137,
                    "new_length": 6,
                    "hunk": "@@ -6185,9 +6137,6 @@ class Subgraph {\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorFloat32OrQUInt8Type(delegate, logging_context, output_tensor,\n                                        output_tensor_index, node_index));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(\n-        logging_context, output_tensor, num_dims, output_tensor_index,\n-        BuiltinOperator_STRIDED_SLICE, node_index));\n     TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(\n         delegate, logging_context, output_tensor, output_tensor_index,\n         node_index));\n"
                },
                {
                    "old_start": 6199,
                    "old_length": 39,
                    "new_start": 6148,
                    "new_length": 44,
                    "hunk": "@@ -6199,39 +6148,44 @@ class Subgraph {\n     std::array<size_t, XNN_MAX_TENSOR_DIMS> sizes;\n     std::array<size_t, XNN_MAX_TENSOR_DIMS> ends;\n     for (size_t i = 0; i < num_dims; i++) {\n+      if (begin_data[i] < 0) {\n+        // TODO(b/329228576): Add support for negative begin.\n+        TF_LITE_MAYBE_KERNEL_LOG(\n+            logging_context,\n+            \"begin %d must be greater than or equal to zero \"\n+            \"in STRIDED_SLICE node #%d\",\n+            begin_data[i], node_index);\n+        return kTfLiteError;\n+      }\n       begins[i] = begin_data[i] < 0 ? input_shape->data[i] + begin_data[i]\n                                     : begin_data[i];\n       if ((params->begin_mask & (1 << i)) != 0) {\n         begins[i] = 0;\n       }\n \n-      if (begins[i] >= input_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"begin %zu must be less than input dimension \"\n-                                 \"%d in STRIDED_SLICE node #%d\",\n-                                 begins[i], input_shape->data[i], node_index);\n-      }\n-\n       int actual_end_data = end_data[i];\n       if (params->offset) {\n         actual_end_data += begin_data[i];\n       }\n       // If end is negative, we count from the back, -1 is the last element.\n       if (actual_end_data < 0) {\n-        ends[i] = actual_end_data + input_shape->data[i];\n+        // TODO(b/329228576): Add support for negative begin.\n+        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n+                                 \"end %d must be greater than or equal to zero \"\n+                                 \"in STRIDED_SLICE node #%d\",\n+                                 end_data[i], node_index);\n+        return kTfLiteError;\n       } else {\n         ends[i] = actual_end_data;\n       }\n \n       if ((params->end_mask & (1 << i)) != 0) {\n-        ends[i] = input_shape->data[i];\n-      }\n-\n-      if (ends[i] > input_shape->data[i]) {\n+        // TODO(b/329228576): Add support for negative begin.\n         TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"end %zu must be less than or equals to input \"\n-                                 \"dimension %d in STRIDED_SLICE node #%d\",\n-                                 ends[i], input_shape->data[i], node_index);\n+                                 \"non-zero end mask not supported \"\n+                                 \"in STRIDED_SLICE node #%d\",\n+                                 end_data[i], node_index);\n+        return kTfLiteError;\n       }\n \n       if (begins[i] >= ends[i]) {"
                }
            ],
            "whole_deleted": "-\n-      TF_LITE_ENSURE_EQ(logging_context, NumDimensions(&input_tensor),\n-                        NumDimensions(&output_tensor));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor,\n-                                           num_dims, input_tensor_index,\n-                                           BuiltinOperator_SLICE, node_index));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor,\n-                                           num_dims, output_tensor_index,\n-                                           BuiltinOperator_SLICE, node_index));\n-    const auto input_shape = input_tensor.dims;\n-    const auto output_shape = output_tensor.dims;\n-      if (begin[i] >= input_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(\n-            logging_context,\n-            \"begin %\" PRId64\n-            \" must be less than input dimension %d in SLICE node #%d\",\n-            begin[i], input_shape->data[i], node_index);\n-      }\n-        if (size[i] != -1) {\n-          TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                   \"size %\" PRId64\n-                                   \" must be positive or -1 in SLICE node #%d\",\n-                                   size[i], node_index);\n-          return kTfLiteError;\n-        }\n-        size[i] = input_shape->data[i] - begin[i];\n-      }\n-      if (size[i] > input_shape->data[i]) {\n-                                 \" must be less than or equals to input \"\n-                                 \"dimension %d in SLICE node #%d\",\n-                                 size[i], input_shape->data[i], node_index);\n-        return kTfLiteError;\n-      }\n-      if (size[i] != output_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"size %\" PRId64\n-                                 \" does not match output shape %d at \"\n-                                 \"dimension %zu in SLICE node #%d\",\n-                                 size[i], output_shape->data[i], i, node_index);\n-        return kTfLiteError;\n-      }\n-      if (begin[i] + size[i] > input_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"begin + size (%\" PRId64 \" + %\" PRId64\n-                                 \") must not be greater than input \"\n-                                 \"dimension %d in SLICE node #%d\",\n-                                 begin[i], size[i], input_shape->data[i],\n-                                 node_index);\n-    // return kTfLiteError;\n-    const int num_dims = stride_tensor.dims->size;\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(\n-        logging_context, input_tensor, num_dims, input_tensor_index,\n-        BuiltinOperator_STRIDED_SLICE, node_index));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(\n-        logging_context, output_tensor, num_dims, output_tensor_index,\n-        BuiltinOperator_STRIDED_SLICE, node_index));\n-      if (begins[i] >= input_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"begin %zu must be less than input dimension \"\n-                                 \"%d in STRIDED_SLICE node #%d\",\n-                                 begins[i], input_shape->data[i], node_index);\n-      }\n-\n-        ends[i] = actual_end_data + input_shape->data[i];\n-        ends[i] = input_shape->data[i];\n-      }\n-\n-      if (ends[i] > input_shape->data[i]) {\n-                                 \"end %zu must be less than or equals to input \"\n-                                 \"dimension %d in STRIDED_SLICE node #%d\",\n-                                 ends[i], input_shape->data[i], node_index);\n",
            "whole_added": "+        // TODO(b/329228576): Add support for negative begin.\n+        // TODO(b/329228576): Add support for negative begin.\n+                                 \" must be positive in SLICE node #%d\",\n+                                 size[i], node_index);\n+    const int num_dims = stride_tensor.dims->data[0];\n+      if (begin_data[i] < 0) {\n+        // TODO(b/329228576): Add support for negative begin.\n+        TF_LITE_MAYBE_KERNEL_LOG(\n+            logging_context,\n+            \"begin %d must be greater than or equal to zero \"\n+            \"in STRIDED_SLICE node #%d\",\n+            begin_data[i], node_index);\n+        return kTfLiteError;\n+      }\n+        // TODO(b/329228576): Add support for negative begin.\n+        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n+                                 \"end %d must be greater than or equal to zero \"\n+                                 \"in STRIDED_SLICE node #%d\",\n+                                 end_data[i], node_index);\n+        return kTfLiteError;\n+        // TODO(b/329228576): Add support for negative begin.\n+                                 \"non-zero end mask not supported \"\n+                                 \"in STRIDED_SLICE node #%d\",\n+                                 end_data[i], node_index);\n+        return kTfLiteError;\n",
            "whole_hunk": "@@ -3190,9 +3190,6 @@ class Subgraph {\n       TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(\n           delegate, logging_context, input_tensor, node->inputs->data[i],\n           node_index));\n-\n-      TF_LITE_ENSURE_EQ(logging_context, NumDimensions(&input_tensor),\n-                        NumDimensions(&output_tensor));\n     }\n \n     if (subgraph != nullptr) {\n@@ -5583,9 +5580,6 @@ class Subgraph {\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorFloat32OrQUInt8Type(delegate, logging_context, input_tensor,\n                                        input_tensor_index, node_index));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor,\n-                                           num_dims, input_tensor_index,\n-                                           BuiltinOperator_SLICE, node_index));\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorNonDynamicAllocation(delegate, logging_context, input_tensor,\n                                         input_tensor_index, node_index));\n@@ -5593,15 +5587,10 @@ class Subgraph {\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorFloat32OrQUInt8Type(delegate, logging_context, output_tensor,\n                                        output_tensor_index, node_index));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor,\n-                                           num_dims, output_tensor_index,\n-                                           BuiltinOperator_SLICE, node_index));\n     TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(\n         delegate, logging_context, output_tensor, output_tensor_index,\n         node_index));\n \n-    const auto input_shape = input_tensor.dims;\n-    const auto output_shape = output_tensor.dims;\n     std::array<int64_t, XNN_MAX_TENSOR_DIMS> begin;\n     std::array<int64_t, XNN_MAX_TENSOR_DIMS> size;\n     CopyTensorDataInt32OrInt64(begin.data(), begin_tensor, num_dims);\n@@ -5609,51 +5598,18 @@ class Subgraph {\n \n     for (size_t i = 0; i < num_dims; i++) {\n       if (begin[i] < 0) {\n+        // TODO(b/329228576): Add support for negative begin.\n         TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                  \"begin %\" PRId64\n                                  \" must be greater than 0 in SLICE node #%d\",\n                                  begin[i], node_index);\n       }\n-      if (begin[i] >= input_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(\n-            logging_context,\n-            \"begin %\" PRId64\n-            \" must be less than input dimension %d in SLICE node #%d\",\n-            begin[i], input_shape->data[i], node_index);\n-      }\n       if (size[i] <= 0) {\n-        if (size[i] != -1) {\n-          TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                   \"size %\" PRId64\n-                                   \" must be positive or -1 in SLICE node #%d\",\n-                                   size[i], node_index);\n-          return kTfLiteError;\n-        }\n-        size[i] = input_shape->data[i] - begin[i];\n-      }\n-      if (size[i] > input_shape->data[i]) {\n+        // TODO(b/329228576): Add support for negative begin.\n         TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                  \"size %\" PRId64\n-                                 \" must be less than or equals to input \"\n-                                 \"dimension %d in SLICE node #%d\",\n-                                 size[i], input_shape->data[i], node_index);\n-        return kTfLiteError;\n-      }\n-      if (size[i] != output_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"size %\" PRId64\n-                                 \" does not match output shape %d at \"\n-                                 \"dimension %zu in SLICE node #%d\",\n-                                 size[i], output_shape->data[i], i, node_index);\n-        return kTfLiteError;\n-      }\n-      if (begin[i] + size[i] > input_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"begin + size (%\" PRId64 \" + %\" PRId64\n-                                 \") must not be greater than input \"\n-                                 \"dimension %d in SLICE node #%d\",\n-                                 begin[i], size[i], input_shape->data[i],\n-                                 node_index);\n+                                 \" must be positive in SLICE node #%d\",\n+                                 size[i], node_index);\n         return kTfLiteError;\n       }\n     }\n@@ -6097,7 +6053,6 @@ class Subgraph {\n       TfLiteContext* logging_context, int node_index, TfLiteNode* node,\n       const TfLiteTensor* tensors, const TfLiteStridedSliceParams* params,\n       const std::unordered_map<int, uint32_t>& input_output_tensors) {\n-    // return kTfLiteError;\n     // Only support strided slice with no ellipsis mask, no new axis mask, and\n     // no shrink_axis-mask.\n     if (params->ellipsis_mask != 0 || params->new_axis_mask != 0 ||\n@@ -6117,7 +6072,7 @@ class Subgraph {\n     TF_LITE_ENSURE_STATUS(CheckTensorInt32Type(\n         logging_context, stride_tensor, stride_tensor_index, node_index));\n \n-    const int num_dims = stride_tensor.dims->size;\n+    const int num_dims = stride_tensor.dims->data[0];\n     if (num_dims > XNN_MAX_TENSOR_DIMS) {\n       TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                \"number of dimensions %d must be less than %d \"\n@@ -6175,9 +6130,6 @@ class Subgraph {\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorFloat32OrQUInt8Type(delegate, logging_context, input_tensor,\n                                        input_tensor_index, node_index));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(\n-        logging_context, input_tensor, num_dims, input_tensor_index,\n-        BuiltinOperator_STRIDED_SLICE, node_index));\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorNonDynamicAllocation(delegate, logging_context, input_tensor,\n                                         input_tensor_index, node_index));\n@@ -6185,9 +6137,6 @@ class Subgraph {\n     TF_LITE_ENSURE_STATUS(\n         CheckTensorFloat32OrQUInt8Type(delegate, logging_context, output_tensor,\n                                        output_tensor_index, node_index));\n-    TF_LITE_ENSURE_STATUS(CheckTensorShape(\n-        logging_context, output_tensor, num_dims, output_tensor_index,\n-        BuiltinOperator_STRIDED_SLICE, node_index));\n     TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(\n         delegate, logging_context, output_tensor, output_tensor_index,\n         node_index));\n@@ -6199,39 +6148,44 @@ class Subgraph {\n     std::array<size_t, XNN_MAX_TENSOR_DIMS> sizes;\n     std::array<size_t, XNN_MAX_TENSOR_DIMS> ends;\n     for (size_t i = 0; i < num_dims; i++) {\n+      if (begin_data[i] < 0) {\n+        // TODO(b/329228576): Add support for negative begin.\n+        TF_LITE_MAYBE_KERNEL_LOG(\n+            logging_context,\n+            \"begin %d must be greater than or equal to zero \"\n+            \"in STRIDED_SLICE node #%d\",\n+            begin_data[i], node_index);\n+        return kTfLiteError;\n+      }\n       begins[i] = begin_data[i] < 0 ? input_shape->data[i] + begin_data[i]\n                                     : begin_data[i];\n       if ((params->begin_mask & (1 << i)) != 0) {\n         begins[i] = 0;\n       }\n \n-      if (begins[i] >= input_shape->data[i]) {\n-        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"begin %zu must be less than input dimension \"\n-                                 \"%d in STRIDED_SLICE node #%d\",\n-                                 begins[i], input_shape->data[i], node_index);\n-      }\n-\n       int actual_end_data = end_data[i];\n       if (params->offset) {\n         actual_end_data += begin_data[i];\n       }\n       // If end is negative, we count from the back, -1 is the last element.\n       if (actual_end_data < 0) {\n-        ends[i] = actual_end_data + input_shape->data[i];\n+        // TODO(b/329228576): Add support for negative begin.\n+        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n+                                 \"end %d must be greater than or equal to zero \"\n+                                 \"in STRIDED_SLICE node #%d\",\n+                                 end_data[i], node_index);\n+        return kTfLiteError;\n       } else {\n         ends[i] = actual_end_data;\n       }\n \n       if ((params->end_mask & (1 << i)) != 0) {\n-        ends[i] = input_shape->data[i];\n-      }\n-\n-      if (ends[i] > input_shape->data[i]) {\n+        // TODO(b/329228576): Add support for negative begin.\n         TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n-                                 \"end %zu must be less than or equals to input \"\n-                                 \"dimension %d in STRIDED_SLICE node #%d\",\n-                                 ends[i], input_shape->data[i], node_index);\n+                                 \"non-zero end mask not supported \"\n+                                 \"in STRIDED_SLICE node #%d\",\n+                                 end_data[i], node_index);\n+        return kTfLiteError;\n       }\n \n       if (begins[i] >= ends[i]) {"
        }
    ]
},
{
    "Id": 631,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ef7c0b3680c6446fa83649184f50cb1edfbc3aef",
    "date": "2022-11-22T12:38:43-08:00",
    "message": "Fix multi-client D2H transfer when Coordination service is enabled.\n\nThe added test case fails without the fixes.\nBaseRendezvousMgr does not implement the heuristics for 'empty' src device, which is used by TPU multiclient to represent the local host device. This CL adds that\nknowledge to it.\n\nI think this Rendezvous path is triggered under coordination service because\nnow the device manager observes remote devices too. My theory is without coordination service the local path is always triggered, hence there were no need to check for 'empty' src device.\n\nPiperOrigin-RevId: 490307160",
    "label": "YES",
    "changes": [
        {
            "name": "base_rendezvous_mgr.cc",
            "path": "tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc",
            "patches": [
                {
                    "old_start": 137,
                    "old_length": 13,
                    "new_start": 137,
                    "new_length": 22,
                    "hunk": "@@ -137,13 +137,22 @@ BaseRemoteRendezvous::~BaseRemoteRendezvous() {\n }\n \n // Returns true if \"device_name\" is a valid full name of local device\n-// of the \"worker\".  This helper is purely based on the worker name\n+// of the \"worker\". This helper is purely based on the worker name\n // and device name and does no lookups in the worker->device_mgr.\n static bool IsLocalDevice(const StringPiece worker_name,\n                           const StringPiece device_name) {\n   return absl::StartsWith(device_name, worker_name);\n }\n \n+// Returns true if the parsed device name is empty. An empty src device\n+// is used to represent a Recv from the local host device when\n+// the host device name is not known at the time when the graph node is\n+// emitted.\n+static bool IsImplicitLocalDevice(\n+    const DeviceNameUtils::ParsedName parsed_device_name) {\n+  return !DeviceNameUtils::HasSomeDetails(parsed_device_name);\n+}\n+\n Status BaseRemoteRendezvous::Initialize(WorkerSession* session) {\n   CHECK_NE(session, nullptr) << \"session must not be null!\";\n   std::vector<DeferredCall> deferred_calls;\n"
                },
                {
                    "old_start": 191,
                    "old_length": 7,
                    "new_start": 200,
                    "new_length": 8,
                    "hunk": "@@ -191,7 +200,8 @@ Status BaseRemoteRendezvous::Send(const Rendezvous::ParsedKey& parsed,\n     sess = session_;\n   }\n \n-  if (!IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n+  if (!IsImplicitLocalDevice(parsed.src) &&\n+      !IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n     return errors::InvalidArgument(\n         \"Invalid rendezvous key (src): \", parsed.FullKey(), \" @ \",\n         sess->worker_name());\n"
                },
                {
                    "old_start": 214,
                    "old_length": 7,
                    "new_start": 224,
                    "new_length": 8,
                    "hunk": "@@ -214,7 +224,8 @@ Status BaseRemoteRendezvous::ValidateDevices(const ParsedKey& parsed,\n     }\n     sess = session_;\n   }\n-  if (is_src && !IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n+  if (is_src && !IsImplicitLocalDevice(parsed.src) &&\n+      !IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n     return errors::InvalidArgument(\n         \"Invalid rendezvous key (src): \", parsed.FullKey(), \" @ \",\n         sess->worker_name());\n"
                },
                {
                    "old_start": 321,
                    "old_length": 7,
                    "new_start": 332,
                    "new_length": 10,
                    "hunk": "@@ -321,7 +332,10 @@ void BaseRemoteRendezvous::RecvAsync(const ParsedKey& parsed,\n \n   profiler::ScopedMemoryDebugAnnotation op_annotation(\"RecvAsync\", step_id_);\n   // Are src and dst in the same worker?\n-  if (IsSameWorker(parsed.src, parsed.dst)) {\n+  // At this point parsed.dst must be a local device asserted by the previous\n+  // call to ValidateDevices.\n+  if (IsImplicitLocalDevice(parsed.src) ||\n+      IsSameWorker(parsed.src, parsed.dst)) {\n     // Recv the tensor from local_.\n     local_->RecvAsync(\n         parsed, recv_args,"
                }
            ],
            "whole_deleted": "-// of the \"worker\".  This helper is purely based on the worker name\n-  if (!IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n-  if (is_src && !IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n-  if (IsSameWorker(parsed.src, parsed.dst)) {\n",
            "whole_added": "+// of the \"worker\". This helper is purely based on the worker name\n+// Returns true if the parsed device name is empty. An empty src device\n+// is used to represent a Recv from the local host device when\n+// the host device name is not known at the time when the graph node is\n+// emitted.\n+static bool IsImplicitLocalDevice(\n+    const DeviceNameUtils::ParsedName parsed_device_name) {\n+  return !DeviceNameUtils::HasSomeDetails(parsed_device_name);\n+}\n+\n+  if (!IsImplicitLocalDevice(parsed.src) &&\n+      !IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n+  if (is_src && !IsImplicitLocalDevice(parsed.src) &&\n+      !IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n+  // At this point parsed.dst must be a local device asserted by the previous\n+  // call to ValidateDevices.\n+  if (IsImplicitLocalDevice(parsed.src) ||\n+      IsSameWorker(parsed.src, parsed.dst)) {\n",
            "whole_hunk": "@@ -137,13 +137,22 @@ BaseRemoteRendezvous::~BaseRemoteRendezvous() {\n }\n \n // Returns true if \"device_name\" is a valid full name of local device\n-// of the \"worker\".  This helper is purely based on the worker name\n+// of the \"worker\". This helper is purely based on the worker name\n // and device name and does no lookups in the worker->device_mgr.\n static bool IsLocalDevice(const StringPiece worker_name,\n                           const StringPiece device_name) {\n   return absl::StartsWith(device_name, worker_name);\n }\n \n+// Returns true if the parsed device name is empty. An empty src device\n+// is used to represent a Recv from the local host device when\n+// the host device name is not known at the time when the graph node is\n+// emitted.\n+static bool IsImplicitLocalDevice(\n+    const DeviceNameUtils::ParsedName parsed_device_name) {\n+  return !DeviceNameUtils::HasSomeDetails(parsed_device_name);\n+}\n+\n Status BaseRemoteRendezvous::Initialize(WorkerSession* session) {\n   CHECK_NE(session, nullptr) << \"session must not be null!\";\n   std::vector<DeferredCall> deferred_calls;\n@@ -191,7 +200,8 @@ Status BaseRemoteRendezvous::Send(const Rendezvous::ParsedKey& parsed,\n     sess = session_;\n   }\n \n-  if (!IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n+  if (!IsImplicitLocalDevice(parsed.src) &&\n+      !IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n     return errors::InvalidArgument(\n         \"Invalid rendezvous key (src): \", parsed.FullKey(), \" @ \",\n         sess->worker_name());\n@@ -214,7 +224,8 @@ Status BaseRemoteRendezvous::ValidateDevices(const ParsedKey& parsed,\n     }\n     sess = session_;\n   }\n-  if (is_src && !IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n+  if (is_src && !IsImplicitLocalDevice(parsed.src) &&\n+      !IsLocalDevice(sess->worker_name(), parsed.src_device)) {\n     return errors::InvalidArgument(\n         \"Invalid rendezvous key (src): \", parsed.FullKey(), \" @ \",\n         sess->worker_name());\n@@ -321,7 +332,10 @@ void BaseRemoteRendezvous::RecvAsync(const ParsedKey& parsed,\n \n   profiler::ScopedMemoryDebugAnnotation op_annotation(\"RecvAsync\", step_id_);\n   // Are src and dst in the same worker?\n-  if (IsSameWorker(parsed.src, parsed.dst)) {\n+  // At this point parsed.dst must be a local device asserted by the previous\n+  // call to ValidateDevices.\n+  if (IsImplicitLocalDevice(parsed.src) ||\n+      IsSameWorker(parsed.src, parsed.dst)) {\n     // Recv the tensor from local_.\n     local_->RecvAsync(\n         parsed, recv_args,"
        }
    ]
},
{
    "Id": 196,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/70b2702091d8842d0ca853d5d1cd8d1ad2c3a983",
    "date": "2024-01-17T04:18:09-08:00",
    "message": "Reland clean up wrong callers of FindNonTrivialHero.\n\nWe had several callers of FindNonTrivialHero which called it with a fusion\ninstruction. This was usually an indicator that the call was not actually needed\nat all and we should just use the instruction itself as the hero.\nAlso adjust ChooseFusionKind to check the producer, too, whether it is a kInput\nfusion.\n\nPiperOrigin-RevId: 599125417",
    "label": "NO",
    "changes": [
        {
            "name": "fusion_merger.cc",
            "path": "third_party/xla/xla/service/gpu/fusion_merger.cc",
            "patches": [
                {
                    "old_start": 105,
                    "old_length": 7,
                    "new_start": 105,
                    "new_length": 7,
                    "hunk": "@@ -105,7 +105,7 @@ absl::Status FusionInstructionMerger::FuseIntoAllUsers(\n     HloInstruction* consumer = user;\n     if (consumer->opcode() != HloOpcode::kFusion) {\n       consumer = computation_->AddInstruction(HloInstruction::CreateFusion(\n-          user->shape(), ChooseFusionKind(*user), user));\n+          user->shape(), ChooseFusionKind(*producer, *user), user));\n       TF_CHECK_OK(computation_->ReplaceInstruction(user, consumer));\n     }\n \n"
                }
            ],
            "whole_deleted": "-          user->shape(), ChooseFusionKind(*user), user));\n",
            "whole_added": "+          user->shape(), ChooseFusionKind(*producer, *user), user));\n",
            "whole_hunk": "@@ -105,7 +105,7 @@ absl::Status FusionInstructionMerger::FuseIntoAllUsers(\n     HloInstruction* consumer = user;\n     if (consumer->opcode() != HloOpcode::kFusion) {\n       consumer = computation_->AddInstruction(HloInstruction::CreateFusion(\n-          user->shape(), ChooseFusionKind(*user), user));\n+          user->shape(), ChooseFusionKind(*producer, *user), user));\n       TF_CHECK_OK(computation_->ReplaceInstruction(user, consumer));\n     }\n \n"
        },
        {
            "name": "fusion_wrapper.cc",
            "path": "third_party/xla/xla/service/gpu/fusion_wrapper.cc",
            "patches": [
                {
                    "old_start": 114,
                    "old_length": 8,
                    "new_start": 114,
                    "new_length": 8,
                    "hunk": "@@ -114,8 +114,8 @@ absl::StatusOr<bool> FusionWrapper::Run(\n         auto* computation = instruction->parent();\n         auto* fusion_instruction =\n             computation->AddInstruction(HloInstruction::CreateFusion(\n-                instruction->shape(), ChooseFusionKind(*instruction),\n-                instruction));\n+                instruction->shape(),\n+                ChooseFusionKind(*instruction, *instruction), instruction));\n         instruction->GetModule()->SetAndUniquifyInstrName(\n             fusion_instruction, absl::StrCat(\"wrapped_\", instruction->name()));\n         if (module->has_schedule()) {\n"
                }
            ],
            "whole_deleted": "-                instruction->shape(), ChooseFusionKind(*instruction),\n-                instruction));\n",
            "whole_added": "+                instruction->shape(),\n+                ChooseFusionKind(*instruction, *instruction), instruction));\n",
            "whole_hunk": "@@ -114,8 +114,8 @@ absl::StatusOr<bool> FusionWrapper::Run(\n         auto* computation = instruction->parent();\n         auto* fusion_instruction =\n             computation->AddInstruction(HloInstruction::CreateFusion(\n-                instruction->shape(), ChooseFusionKind(*instruction),\n-                instruction));\n+                instruction->shape(),\n+                ChooseFusionKind(*instruction, *instruction), instruction));\n         instruction->GetModule()->SetAndUniquifyInstrName(\n             fusion_instruction, absl::StrCat(\"wrapped_\", instruction->name()));\n         if (module->has_schedule()) {\n"
        },
        {
            "name": "gpu_fusible.cc",
            "path": "third_party/xla/xla/service/gpu/gpu_fusible.cc",
            "patches": [
                {
                    "old_start": 159,
                    "old_length": 8,
                    "new_start": 159,
                    "new_length": 7,
                    "hunk": "@@ -159,8 +159,7 @@ bool IsInputFusibleTranspose(const HloInstruction& instr) {\n   if (instr.opcode() == HloOpcode::kFusion) {\n     return HasAnyTiledTransposeRoot(*instr.fused_instructions_computation());\n   }\n-  auto& hero = FindNonTrivialHero(instr);\n-  return GetDescriptionForTiledTransposeEmitter(instr, hero).has_value();\n+  return GetDescriptionForTiledTransposeEmitter(instr, instr).has_value();\n }\n \n const HloInstruction* GetRealHeroForMultiOutputFusion(\n"
                },
                {
                    "old_start": 342,
                    "old_length": 8,
                    "new_start": 341,
                    "new_length": 7,
                    "hunk": "@@ -342,8 +341,7 @@ bool IsInputFusible(const HloInstruction& instr) {\n \n // Returns true if `instr` can be fused as a producer or as a consumer into a\n // kLoop fusion.\n-bool IsUniversallyLoopFusible(const HloInstruction& instr,\n-                              const HloInstruction& hero) {\n+bool IsUniversallyLoopFusible(const HloInstruction& instr) {\n   // NOTE: this check is done before the switch below, because a fusion instr\n   // can also be elementwise, even if it's not a kLoop.\n   if (instr.IsElementwise() && instr.operand_count() > 0 &&\n"
                },
                {
                    "old_start": 353,
                    "old_length": 7,
                    "new_start": 351,
                    "new_length": 7,
                    "hunk": "@@ -353,7 +351,7 @@ bool IsUniversallyLoopFusible(const HloInstruction& instr,\n \n   switch (instr.opcode()) {\n     case HloOpcode::kCopy:\n-      return !GetDescriptionForTiledTransposeEmitter(instr, hero).has_value();\n+      return !GetDescriptionForTiledTransposeEmitter(instr, instr).has_value();\n \n     case HloOpcode::kFusion:\n       return instr.fusion_kind() == HloInstruction::FusionKind::kLoop;\n"
                },
                {
                    "old_start": 377,
                    "old_length": 8,
                    "new_start": 375,
                    "new_length": 7,
                    "hunk": "@@ -377,8 +375,7 @@ bool IsUniversallyLoopFusible(const HloInstruction& instr,\n }\n \n // Returns true if `instr` can be fused as a consumer into a kLoop fusion.\n-bool IsLoopFusibleAsConsumer(const HloInstruction& instr,\n-                             const HloInstruction& hero) {\n+bool IsLoopFusibleAsConsumer(const HloInstruction& instr) {\n   // Instr should be fusible.\n   if (!instr.IsFusible()) return false;\n \n"
                },
                {
                    "old_start": 390,
                    "old_length": 12,
                    "new_start": 387,
                    "new_length": 19,
                    "hunk": "@@ -390,12 +387,19 @@ bool IsLoopFusibleAsConsumer(const HloInstruction& instr,\n   // Any reduction can be fused as a consumer.\n   if (instr.opcode() == HloOpcode::kReduce) return true;\n \n-  return IsUniversallyLoopFusible(instr, hero);\n+  // We may have input fusions which effectively have turned into loop\n+  // fusions. Those should still be considered as loop fusible consumers,\n+  // but they are not universally loop fusible.\n+  if (!IsInputFusible(instr) && instr.opcode() == HloOpcode::kFusion &&\n+      instr.fusion_kind() == HloInstruction::FusionKind::kInput) {\n+    return true;\n+  }\n+\n+  return IsUniversallyLoopFusible(instr);\n }\n \n // Returns true if `instr` can be fused as a producer into a kLoop fusion.\n-bool IsLoopFusibleAsProducer(const HloInstruction& instr,\n-                             const HloInstruction& hero) {\n+bool IsLoopFusibleAsProducer(const HloInstruction& instr) {\n   // Instr should be fusible.\n   if (!instr.IsFusible()) return false;\n \n"
                },
                {
                    "old_start": 407,
                    "old_length": 7,
                    "new_start": 411,
                    "new_length": 7,
                    "hunk": "@@ -407,7 +411,7 @@ bool IsLoopFusibleAsProducer(const HloInstruction& instr,\n       // Non-variadic reductions can be fused as producers.\n       return !instr.shape().IsTuple();\n     default:\n-      return IsUniversallyLoopFusible(instr, hero);\n+      return IsUniversallyLoopFusible(instr);\n   }\n }\n \n"
                },
                {
                    "old_start": 452,
                    "old_length": 12,
                    "new_start": 456,
                    "new_length": 8,
                    "hunk": "@@ -452,12 +456,8 @@ FusionDecision CanEmitInputFusedScatter(const HloInstruction& producer,\n \n FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n                                          const HloInstruction& consumer) {\n-  const auto& producer_hero = FindNonTrivialHero(producer);\n-  const auto& consumer_hero = FindNonTrivialHero(consumer);\n-  if (!IsLoopFusibleAsProducer(producer, producer_hero) &&\n-      !(GetDescriptionForTiledTransposeEmitter(producer, producer_hero)\n-            .has_value() &&\n-        &consumer_hero == &producer)) {\n+  if (!IsLoopFusibleAsProducer(producer) &&\n+      !IsInputFusibleTranspose(producer)) {\n     return \"the producer is not loop-fusible\";\n   }\n \n"
                },
                {
                    "old_start": 468,
                    "old_length": 12,
                    "new_start": 468,
                    "new_length": 10,
                    "hunk": "@@ -468,12 +468,10 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n              .xla_gpu_enable_reduction_epilogue_fusion()) {\n       return \"Reduction epilogue fusion is not enabled.\";\n     }\n-    // TODO(akuegel): Remove workaround when producer_hero is computed\n-    // correctly.\n     const HloInstruction& reduce_hero =\n-        producer_hero.opcode() == HloOpcode::kFusion\n-            ? FindNonTrivialHero(*producer_hero.fused_expression_root())\n-            : producer_hero;\n+        producer.opcode() == HloOpcode::kFusion\n+            ? FindNonTrivialHero(*producer.fused_expression_root())\n+            : producer;\n     if (!ReductionIsRaceFree(\n             reduce_hero.GetModule()->config(),\n             GetReductionKindAndContiguousComponents(reduce_hero))) {\n"
                },
                {
                    "old_start": 494,
                    "old_length": 8,
                    "new_start": 492,
                    "new_length": 7,
                    "hunk": "@@ -494,8 +492,7 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n     return can_fuse;\n   }\n \n-  if (!IsInputFusible(consumer) &&\n-      !IsLoopFusibleAsConsumer(consumer, consumer_hero)) {\n+  if (!IsInputFusible(consumer) && !IsLoopFusibleAsConsumer(consumer)) {\n     return \"the consumer is not input-fusible and not loop-fusible\";\n   }\n \n"
                },
                {
                    "old_start": 556,
                    "old_length": 7,
                    "new_start": 553,
                    "new_length": 7,
                    "hunk": "@@ -556,7 +553,7 @@ FusionDecision IsProducerMultiOutputFusible(const HloInstruction& producer) {\n     return \"In-place operations are present\";\n   }\n \n-  if (!IsLoopFusibleAsProducer(producer, FindNonTrivialHero(producer))) {\n+  if (!IsLoopFusibleAsProducer(producer)) {\n     return \"producer is not loop-fusible\";\n   }\n \n"
                },
                {
                    "old_start": 567,
                    "old_length": 11,
                    "new_start": 564,
                    "new_length": 18,
                    "hunk": "@@ -567,11 +564,18 @@ FusionDecision IsProducerMultiOutputFusible(const HloInstruction& producer) {\n   return {};\n }\n \n-// Returns shared memory usage for a given instruction in bytes.\n+// Returns an estimate of the shared memory usage for a given instruction in\n+// bytes.\n static int64_t SharedMemoryUsageNoCache(const HloInstruction& instr) {\n-  // For now we are only fusing reductions.\n-  if (instr.opcode() == HloOpcode::kReduce &&\n-      IsReductionFromOrToContiguousDimensions(instr)) {\n+  if (instr.opcode() == HloOpcode::kFusion) {\n+    int64_t sum = 0;\n+    for (const HloInstruction* hlo :\n+         instr.fused_instructions_computation()->instructions()) {\n+      sum += SharedMemoryUsageNoCache(*hlo);\n+    }\n+    return sum;\n+  } else if (instr.opcode() == HloOpcode::kReduce &&\n+             IsReductionFromOrToContiguousDimensions(instr)) {\n     ReductionDimensions reduction_info =\n         GetReductionKindAndContiguousComponents(instr);\n     int64_t primitive_size = ShapeUtil::ByteSizeOfPrimitiveType(\n"
                },
                {
                    "old_start": 586,
                    "old_length": 20,
                    "new_start": 590,
                    "new_length": 11,
                    "hunk": "@@ -586,20 +590,11 @@ static int64_t SharedMemoryUsageNoCache(const HloInstruction& instr) {\n       // from potential x-tiling).\n       return 2 * 32 * 33 * primitive_size * num_variadic;\n     }\n-  } else if (GetDescriptionForTiledTransposeEmitter(instr,\n-                                                    FindNonTrivialHero(instr))\n-                 .has_value()) {\n+  } else if (GetDescriptionForTiledTransposeEmitter(instr, instr).has_value()) {\n     // Tile size for transposition.\n     int64_t primitive_size =\n         ShapeUtil::ByteSizeOfPrimitiveType(instr.shape().element_type());\n     return 32 * 33 * primitive_size;\n-  } else if (instr.opcode() == HloOpcode::kFusion) {\n-    int64_t sum = 0;\n-    for (const HloInstruction* hlo :\n-         instr.fused_instructions_computation()->instructions()) {\n-      sum += SharedMemoryUsageNoCache(*hlo);\n-    }\n-    return sum;\n   }\n   // Other fused expressions for now don't need the shared memory budget.\n   return 0;\n"
                },
                {
                    "old_start": 867,
                    "old_length": 9,
                    "new_start": 862,
                    "new_length": 11,
                    "hunk": "@@ -867,9 +862,11 @@ bool IsFusibleAsMultiOutputFusionRoot(const HloInstruction& instr) {\n           instr.IsElementwise());\n }\n \n-HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& consumer) {\n-  return IsInputFusible(consumer) ? HloInstruction::FusionKind::kInput\n-                                  : HloInstruction::FusionKind::kLoop;\n+HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& producer,\n+                                            const HloInstruction& consumer) {\n+  return (IsInputFusible(consumer) || IsInputFusible(producer))\n+             ? HloInstruction::FusionKind::kInput\n+             : HloInstruction::FusionKind::kLoop;\n }\n \n bool IsConsumerTheOnlyNonRootUser(const HloInstruction& instr,\n"
                }
            ],
            "whole_deleted": "-  auto& hero = FindNonTrivialHero(instr);\n-  return GetDescriptionForTiledTransposeEmitter(instr, hero).has_value();\n-bool IsUniversallyLoopFusible(const HloInstruction& instr,\n-                              const HloInstruction& hero) {\n-      return !GetDescriptionForTiledTransposeEmitter(instr, hero).has_value();\n-bool IsLoopFusibleAsConsumer(const HloInstruction& instr,\n-                             const HloInstruction& hero) {\n-  return IsUniversallyLoopFusible(instr, hero);\n-bool IsLoopFusibleAsProducer(const HloInstruction& instr,\n-                             const HloInstruction& hero) {\n-      return IsUniversallyLoopFusible(instr, hero);\n-  const auto& producer_hero = FindNonTrivialHero(producer);\n-  const auto& consumer_hero = FindNonTrivialHero(consumer);\n-  if (!IsLoopFusibleAsProducer(producer, producer_hero) &&\n-      !(GetDescriptionForTiledTransposeEmitter(producer, producer_hero)\n-            .has_value() &&\n-        &consumer_hero == &producer)) {\n-    // TODO(akuegel): Remove workaround when producer_hero is computed\n-    // correctly.\n-        producer_hero.opcode() == HloOpcode::kFusion\n-            ? FindNonTrivialHero(*producer_hero.fused_expression_root())\n-            : producer_hero;\n-  if (!IsInputFusible(consumer) &&\n-      !IsLoopFusibleAsConsumer(consumer, consumer_hero)) {\n-  if (!IsLoopFusibleAsProducer(producer, FindNonTrivialHero(producer))) {\n-// Returns shared memory usage for a given instruction in bytes.\n-  // For now we are only fusing reductions.\n-  if (instr.opcode() == HloOpcode::kReduce &&\n-      IsReductionFromOrToContiguousDimensions(instr)) {\n-  } else if (GetDescriptionForTiledTransposeEmitter(instr,\n-                                                    FindNonTrivialHero(instr))\n-                 .has_value()) {\n-  } else if (instr.opcode() == HloOpcode::kFusion) {\n-    int64_t sum = 0;\n-    for (const HloInstruction* hlo :\n-         instr.fused_instructions_computation()->instructions()) {\n-      sum += SharedMemoryUsageNoCache(*hlo);\n-    }\n-    return sum;\n-HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& consumer) {\n-  return IsInputFusible(consumer) ? HloInstruction::FusionKind::kInput\n-                                  : HloInstruction::FusionKind::kLoop;\n",
            "whole_added": "+  return GetDescriptionForTiledTransposeEmitter(instr, instr).has_value();\n+bool IsUniversallyLoopFusible(const HloInstruction& instr) {\n+      return !GetDescriptionForTiledTransposeEmitter(instr, instr).has_value();\n+bool IsLoopFusibleAsConsumer(const HloInstruction& instr) {\n+  // We may have input fusions which effectively have turned into loop\n+  // fusions. Those should still be considered as loop fusible consumers,\n+  // but they are not universally loop fusible.\n+  if (!IsInputFusible(instr) && instr.opcode() == HloOpcode::kFusion &&\n+      instr.fusion_kind() == HloInstruction::FusionKind::kInput) {\n+    return true;\n+  }\n+\n+  return IsUniversallyLoopFusible(instr);\n+bool IsLoopFusibleAsProducer(const HloInstruction& instr) {\n+      return IsUniversallyLoopFusible(instr);\n+  if (!IsLoopFusibleAsProducer(producer) &&\n+      !IsInputFusibleTranspose(producer)) {\n+        producer.opcode() == HloOpcode::kFusion\n+            ? FindNonTrivialHero(*producer.fused_expression_root())\n+            : producer;\n+  if (!IsInputFusible(consumer) && !IsLoopFusibleAsConsumer(consumer)) {\n+  if (!IsLoopFusibleAsProducer(producer)) {\n+// Returns an estimate of the shared memory usage for a given instruction in\n+// bytes.\n+  if (instr.opcode() == HloOpcode::kFusion) {\n+    int64_t sum = 0;\n+    for (const HloInstruction* hlo :\n+         instr.fused_instructions_computation()->instructions()) {\n+      sum += SharedMemoryUsageNoCache(*hlo);\n+    }\n+    return sum;\n+  } else if (instr.opcode() == HloOpcode::kReduce &&\n+             IsReductionFromOrToContiguousDimensions(instr)) {\n+  } else if (GetDescriptionForTiledTransposeEmitter(instr, instr).has_value()) {\n+HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& producer,\n+                                            const HloInstruction& consumer) {\n+  return (IsInputFusible(consumer) || IsInputFusible(producer))\n+             ? HloInstruction::FusionKind::kInput\n+             : HloInstruction::FusionKind::kLoop;\n",
            "whole_hunk": "@@ -159,8 +159,7 @@ bool IsInputFusibleTranspose(const HloInstruction& instr) {\n   if (instr.opcode() == HloOpcode::kFusion) {\n     return HasAnyTiledTransposeRoot(*instr.fused_instructions_computation());\n   }\n-  auto& hero = FindNonTrivialHero(instr);\n-  return GetDescriptionForTiledTransposeEmitter(instr, hero).has_value();\n+  return GetDescriptionForTiledTransposeEmitter(instr, instr).has_value();\n }\n \n const HloInstruction* GetRealHeroForMultiOutputFusion(\n@@ -342,8 +341,7 @@ bool IsInputFusible(const HloInstruction& instr) {\n \n // Returns true if `instr` can be fused as a producer or as a consumer into a\n // kLoop fusion.\n-bool IsUniversallyLoopFusible(const HloInstruction& instr,\n-                              const HloInstruction& hero) {\n+bool IsUniversallyLoopFusible(const HloInstruction& instr) {\n   // NOTE: this check is done before the switch below, because a fusion instr\n   // can also be elementwise, even if it's not a kLoop.\n   if (instr.IsElementwise() && instr.operand_count() > 0 &&\n@@ -353,7 +351,7 @@ bool IsUniversallyLoopFusible(const HloInstruction& instr,\n \n   switch (instr.opcode()) {\n     case HloOpcode::kCopy:\n-      return !GetDescriptionForTiledTransposeEmitter(instr, hero).has_value();\n+      return !GetDescriptionForTiledTransposeEmitter(instr, instr).has_value();\n \n     case HloOpcode::kFusion:\n       return instr.fusion_kind() == HloInstruction::FusionKind::kLoop;\n@@ -377,8 +375,7 @@ bool IsUniversallyLoopFusible(const HloInstruction& instr,\n }\n \n // Returns true if `instr` can be fused as a consumer into a kLoop fusion.\n-bool IsLoopFusibleAsConsumer(const HloInstruction& instr,\n-                             const HloInstruction& hero) {\n+bool IsLoopFusibleAsConsumer(const HloInstruction& instr) {\n   // Instr should be fusible.\n   if (!instr.IsFusible()) return false;\n \n@@ -390,12 +387,19 @@ bool IsLoopFusibleAsConsumer(const HloInstruction& instr,\n   // Any reduction can be fused as a consumer.\n   if (instr.opcode() == HloOpcode::kReduce) return true;\n \n-  return IsUniversallyLoopFusible(instr, hero);\n+  // We may have input fusions which effectively have turned into loop\n+  // fusions. Those should still be considered as loop fusible consumers,\n+  // but they are not universally loop fusible.\n+  if (!IsInputFusible(instr) && instr.opcode() == HloOpcode::kFusion &&\n+      instr.fusion_kind() == HloInstruction::FusionKind::kInput) {\n+    return true;\n+  }\n+\n+  return IsUniversallyLoopFusible(instr);\n }\n \n // Returns true if `instr` can be fused as a producer into a kLoop fusion.\n-bool IsLoopFusibleAsProducer(const HloInstruction& instr,\n-                             const HloInstruction& hero) {\n+bool IsLoopFusibleAsProducer(const HloInstruction& instr) {\n   // Instr should be fusible.\n   if (!instr.IsFusible()) return false;\n \n@@ -407,7 +411,7 @@ bool IsLoopFusibleAsProducer(const HloInstruction& instr,\n       // Non-variadic reductions can be fused as producers.\n       return !instr.shape().IsTuple();\n     default:\n-      return IsUniversallyLoopFusible(instr, hero);\n+      return IsUniversallyLoopFusible(instr);\n   }\n }\n \n@@ -452,12 +456,8 @@ FusionDecision CanEmitInputFusedScatter(const HloInstruction& producer,\n \n FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n                                          const HloInstruction& consumer) {\n-  const auto& producer_hero = FindNonTrivialHero(producer);\n-  const auto& consumer_hero = FindNonTrivialHero(consumer);\n-  if (!IsLoopFusibleAsProducer(producer, producer_hero) &&\n-      !(GetDescriptionForTiledTransposeEmitter(producer, producer_hero)\n-            .has_value() &&\n-        &consumer_hero == &producer)) {\n+  if (!IsLoopFusibleAsProducer(producer) &&\n+      !IsInputFusibleTranspose(producer)) {\n     return \"the producer is not loop-fusible\";\n   }\n \n@@ -468,12 +468,10 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n              .xla_gpu_enable_reduction_epilogue_fusion()) {\n       return \"Reduction epilogue fusion is not enabled.\";\n     }\n-    // TODO(akuegel): Remove workaround when producer_hero is computed\n-    // correctly.\n     const HloInstruction& reduce_hero =\n-        producer_hero.opcode() == HloOpcode::kFusion\n-            ? FindNonTrivialHero(*producer_hero.fused_expression_root())\n-            : producer_hero;\n+        producer.opcode() == HloOpcode::kFusion\n+            ? FindNonTrivialHero(*producer.fused_expression_root())\n+            : producer;\n     if (!ReductionIsRaceFree(\n             reduce_hero.GetModule()->config(),\n             GetReductionKindAndContiguousComponents(reduce_hero))) {\n@@ -494,8 +492,7 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n     return can_fuse;\n   }\n \n-  if (!IsInputFusible(consumer) &&\n-      !IsLoopFusibleAsConsumer(consumer, consumer_hero)) {\n+  if (!IsInputFusible(consumer) && !IsLoopFusibleAsConsumer(consumer)) {\n     return \"the consumer is not input-fusible and not loop-fusible\";\n   }\n \n@@ -556,7 +553,7 @@ FusionDecision IsProducerMultiOutputFusible(const HloInstruction& producer) {\n     return \"In-place operations are present\";\n   }\n \n-  if (!IsLoopFusibleAsProducer(producer, FindNonTrivialHero(producer))) {\n+  if (!IsLoopFusibleAsProducer(producer)) {\n     return \"producer is not loop-fusible\";\n   }\n \n@@ -567,11 +564,18 @@ FusionDecision IsProducerMultiOutputFusible(const HloInstruction& producer) {\n   return {};\n }\n \n-// Returns shared memory usage for a given instruction in bytes.\n+// Returns an estimate of the shared memory usage for a given instruction in\n+// bytes.\n static int64_t SharedMemoryUsageNoCache(const HloInstruction& instr) {\n-  // For now we are only fusing reductions.\n-  if (instr.opcode() == HloOpcode::kReduce &&\n-      IsReductionFromOrToContiguousDimensions(instr)) {\n+  if (instr.opcode() == HloOpcode::kFusion) {\n+    int64_t sum = 0;\n+    for (const HloInstruction* hlo :\n+         instr.fused_instructions_computation()->instructions()) {\n+      sum += SharedMemoryUsageNoCache(*hlo);\n+    }\n+    return sum;\n+  } else if (instr.opcode() == HloOpcode::kReduce &&\n+             IsReductionFromOrToContiguousDimensions(instr)) {\n     ReductionDimensions reduction_info =\n         GetReductionKindAndContiguousComponents(instr);\n     int64_t primitive_size = ShapeUtil::ByteSizeOfPrimitiveType(\n@@ -586,20 +590,11 @@ static int64_t SharedMemoryUsageNoCache(const HloInstruction& instr) {\n       // from potential x-tiling).\n       return 2 * 32 * 33 * primitive_size * num_variadic;\n     }\n-  } else if (GetDescriptionForTiledTransposeEmitter(instr,\n-                                                    FindNonTrivialHero(instr))\n-                 .has_value()) {\n+  } else if (GetDescriptionForTiledTransposeEmitter(instr, instr).has_value()) {\n     // Tile size for transposition.\n     int64_t primitive_size =\n         ShapeUtil::ByteSizeOfPrimitiveType(instr.shape().element_type());\n     return 32 * 33 * primitive_size;\n-  } else if (instr.opcode() == HloOpcode::kFusion) {\n-    int64_t sum = 0;\n-    for (const HloInstruction* hlo :\n-         instr.fused_instructions_computation()->instructions()) {\n-      sum += SharedMemoryUsageNoCache(*hlo);\n-    }\n-    return sum;\n   }\n   // Other fused expressions for now don't need the shared memory budget.\n   return 0;\n@@ -867,9 +862,11 @@ bool IsFusibleAsMultiOutputFusionRoot(const HloInstruction& instr) {\n           instr.IsElementwise());\n }\n \n-HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& consumer) {\n-  return IsInputFusible(consumer) ? HloInstruction::FusionKind::kInput\n-                                  : HloInstruction::FusionKind::kLoop;\n+HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& producer,\n+                                            const HloInstruction& consumer) {\n+  return (IsInputFusible(consumer) || IsInputFusible(producer))\n+             ? HloInstruction::FusionKind::kInput\n+             : HloInstruction::FusionKind::kLoop;\n }\n \n bool IsConsumerTheOnlyNonRootUser(const HloInstruction& instr,\n"
        },
        {
            "name": "gpu_fusible.h",
            "path": "third_party/xla/xla/service/gpu/gpu_fusible.h",
            "patches": [
                {
                    "old_start": 170,
                    "old_length": 7,
                    "new_start": 170,
                    "new_length": 8,
                    "hunk": "@@ -170,7 +170,8 @@ FusionDecision IsProducerMultiOutputFusible(const HloInstruction& producer);\n bool IsFusibleAsMultiOutputFusionRoot(const HloInstruction& instr);\n \n // Determines the fusion kind to be used when fusing into `consumer`.\n-HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& consumer);\n+HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& producer,\n+                                            const HloInstruction& consumer);\n \n // Returns whether `consumer` is the only non-root user of `instr`.\n bool IsConsumerTheOnlyNonRootUser(const HloInstruction& instr,\n"
                }
            ],
            "whole_deleted": "-HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& consumer);\n",
            "whole_added": "+HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& producer,\n+                                            const HloInstruction& consumer);\n",
            "whole_hunk": "@@ -170,7 +170,8 @@ FusionDecision IsProducerMultiOutputFusible(const HloInstruction& producer);\n bool IsFusibleAsMultiOutputFusionRoot(const HloInstruction& instr);\n \n // Determines the fusion kind to be used when fusing into `consumer`.\n-HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& consumer);\n+HloInstruction::FusionKind ChooseFusionKind(const HloInstruction& producer,\n+                                            const HloInstruction& consumer);\n \n // Returns whether `consumer` is the only non-root user of `instr`.\n bool IsConsumerTheOnlyNonRootUser(const HloInstruction& instr,\n"
        },
        {
            "name": "gpu_fusible_test.cc",
            "path": "third_party/xla/xla/service/gpu/gpu_fusible_test.cc",
            "patches": [
                {
                    "old_start": 1001,
                    "old_length": 6,
                    "new_start": 1001,
                    "new_length": 51,
                    "hunk": "@@ -1001,6 +1001,51 @@ TEST_F(GpuFusibleTest, ProducerConsumerFusionElementwiseAndReduce) {\n   EXPECT_TRUE(ShapesCompatibleForMultiOutputFusion(*producer, *consumer));\n }\n \n+TEST_F(GpuFusibleTest, ProducerConsumerFusionTransposeAndLoopFusion) {\n+  auto module = ParseAndReturnVerifiedModule(absl::StrCat(kModulePrefix, R\"(\n+    fused_add {\n+      p0.1 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1.1 = f32[32,31,30]{2,1,0} parameter(1)\n+      neg = f32[32,31,30]{2,1,0} negate(p0.1)\n+      ROOT add = f32[32,31,30]{2,1,0} add(neg, p1.1)\n+    }\n+\n+    ENTRY reduce {\n+      p0 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1 = f32[32,30,31]{2,1,0} parameter(1)\n+      transpose =  f32[32,31,30]{2,1,0} transpose(p1), dimensions={0,2,1}\n+      ROOT add = f32[32,31,30]{2,1,0} fusion(p0, transpose), kind=kLoop, calls=fused_add\n+    })\"))\n+                    .value();\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  const HloInstruction* consumer = root;\n+  const HloInstruction* producer = root->operand(1);\n+  EXPECT_TRUE(IsProducerConsumerFusible(*producer, *consumer));\n+}\n+\n+TEST_F(GpuFusibleTest, ProducerConsumerFusionReduceAndLoopFusion) {\n+  auto module = ParseAndReturnVerifiedModule(absl::StrCat(kModulePrefix, R\"(\n+    fused_add {\n+      p0.1 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1.1 = f32[32,31,30]{2,1,0} parameter(1)\n+      neg = f32[32,31,30]{2,1,0} negate(p0.1)\n+      ROOT add = f32[32,31,30]{2,1,0} add(neg, p1.1)\n+    }\n+\n+    ENTRY reduce {\n+      p0 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1 = f32[32,31,30,29]{3,2,1,0} parameter(1)\n+      c0 = f32[] constant(0.0)\n+      reduce =  f32[32,31,30]{2,1,0} reduce(p1, c0), dimensions={3}, to_apply=scalar_add\n+      ROOT add = f32[32,31,30]{2,1,0} fusion(p0, reduce), kind=kLoop, calls=fused_add\n+    })\"))\n+                    .value();\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  const HloInstruction* consumer = root;\n+  const HloInstruction* producer = root->operand(1);\n+  EXPECT_TRUE(IsProducerConsumerFusible(*producer, *consumer));\n+}\n+\n TEST_F(GpuFusibleTest, ProducerConsumerFusionLoopFusionAndReduce) {\n   auto module = ParseAndReturnVerifiedModule(absl::StrCat(kModulePrefix, R\"(\n     fused_add {\n"
                },
                {
                    "old_start": 1403,
                    "old_length": 5,
                    "new_start": 1448,
                    "new_length": 22,
                    "hunk": "@@ -1403,5 +1448,22 @@ TEST_F(GpuFusibleTest, DoesNotCreateHeavyComputation_FusionInstr) {\n   EXPECT_FALSE(CreatesHeavyComputation(*producer, *consumer));\n }\n \n+TEST_F(GpuFusibleTest, ChooseFusionKind) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule module\n+\n+ENTRY computation {\n+    p = f32[5000,6000]{1,0} parameter(0)\n+    c = f32[6000,5000] transpose(p), dimensions={1,0}\n+    ROOT r = f32[300,20,5000] reshape(c)\n+}\n+)\")\n+                    .value();\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  const HloInstruction* producer = root->operand(0);\n+  EXPECT_EQ(ChooseFusionKind(*producer, *root),\n+            HloInstruction::FusionKind::kInput);\n+}\n+\n }  // namespace gpu\n }  // namespace xla\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+TEST_F(GpuFusibleTest, ProducerConsumerFusionTransposeAndLoopFusion) {\n+  auto module = ParseAndReturnVerifiedModule(absl::StrCat(kModulePrefix, R\"(\n+    fused_add {\n+      p0.1 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1.1 = f32[32,31,30]{2,1,0} parameter(1)\n+      neg = f32[32,31,30]{2,1,0} negate(p0.1)\n+      ROOT add = f32[32,31,30]{2,1,0} add(neg, p1.1)\n+    }\n+\n+    ENTRY reduce {\n+      p0 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1 = f32[32,30,31]{2,1,0} parameter(1)\n+      transpose =  f32[32,31,30]{2,1,0} transpose(p1), dimensions={0,2,1}\n+      ROOT add = f32[32,31,30]{2,1,0} fusion(p0, transpose), kind=kLoop, calls=fused_add\n+    })\"))\n+                    .value();\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  const HloInstruction* consumer = root;\n+  const HloInstruction* producer = root->operand(1);\n+  EXPECT_TRUE(IsProducerConsumerFusible(*producer, *consumer));\n+}\n+\n+TEST_F(GpuFusibleTest, ProducerConsumerFusionReduceAndLoopFusion) {\n+  auto module = ParseAndReturnVerifiedModule(absl::StrCat(kModulePrefix, R\"(\n+    fused_add {\n+      p0.1 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1.1 = f32[32,31,30]{2,1,0} parameter(1)\n+      neg = f32[32,31,30]{2,1,0} negate(p0.1)\n+      ROOT add = f32[32,31,30]{2,1,0} add(neg, p1.1)\n+    }\n+\n+    ENTRY reduce {\n+      p0 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1 = f32[32,31,30,29]{3,2,1,0} parameter(1)\n+      c0 = f32[] constant(0.0)\n+      reduce =  f32[32,31,30]{2,1,0} reduce(p1, c0), dimensions={3}, to_apply=scalar_add\n+      ROOT add = f32[32,31,30]{2,1,0} fusion(p0, reduce), kind=kLoop, calls=fused_add\n+    })\"))\n+                    .value();\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  const HloInstruction* consumer = root;\n+  const HloInstruction* producer = root->operand(1);\n+  EXPECT_TRUE(IsProducerConsumerFusible(*producer, *consumer));\n+}\n+\n+TEST_F(GpuFusibleTest, ChooseFusionKind) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule module\n+\n+ENTRY computation {\n+    p = f32[5000,6000]{1,0} parameter(0)\n+    c = f32[6000,5000] transpose(p), dimensions={1,0}\n+    ROOT r = f32[300,20,5000] reshape(c)\n+}\n+)\")\n+                    .value();\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  const HloInstruction* producer = root->operand(0);\n+  EXPECT_EQ(ChooseFusionKind(*producer, *root),\n+            HloInstruction::FusionKind::kInput);\n+}\n+\n",
            "whole_hunk": "@@ -1001,6 +1001,51 @@ TEST_F(GpuFusibleTest, ProducerConsumerFusionElementwiseAndReduce) {\n   EXPECT_TRUE(ShapesCompatibleForMultiOutputFusion(*producer, *consumer));\n }\n \n+TEST_F(GpuFusibleTest, ProducerConsumerFusionTransposeAndLoopFusion) {\n+  auto module = ParseAndReturnVerifiedModule(absl::StrCat(kModulePrefix, R\"(\n+    fused_add {\n+      p0.1 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1.1 = f32[32,31,30]{2,1,0} parameter(1)\n+      neg = f32[32,31,30]{2,1,0} negate(p0.1)\n+      ROOT add = f32[32,31,30]{2,1,0} add(neg, p1.1)\n+    }\n+\n+    ENTRY reduce {\n+      p0 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1 = f32[32,30,31]{2,1,0} parameter(1)\n+      transpose =  f32[32,31,30]{2,1,0} transpose(p1), dimensions={0,2,1}\n+      ROOT add = f32[32,31,30]{2,1,0} fusion(p0, transpose), kind=kLoop, calls=fused_add\n+    })\"))\n+                    .value();\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  const HloInstruction* consumer = root;\n+  const HloInstruction* producer = root->operand(1);\n+  EXPECT_TRUE(IsProducerConsumerFusible(*producer, *consumer));\n+}\n+\n+TEST_F(GpuFusibleTest, ProducerConsumerFusionReduceAndLoopFusion) {\n+  auto module = ParseAndReturnVerifiedModule(absl::StrCat(kModulePrefix, R\"(\n+    fused_add {\n+      p0.1 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1.1 = f32[32,31,30]{2,1,0} parameter(1)\n+      neg = f32[32,31,30]{2,1,0} negate(p0.1)\n+      ROOT add = f32[32,31,30]{2,1,0} add(neg, p1.1)\n+    }\n+\n+    ENTRY reduce {\n+      p0 = f32[32,31,30]{2,1,0} parameter(0)\n+      p1 = f32[32,31,30,29]{3,2,1,0} parameter(1)\n+      c0 = f32[] constant(0.0)\n+      reduce =  f32[32,31,30]{2,1,0} reduce(p1, c0), dimensions={3}, to_apply=scalar_add\n+      ROOT add = f32[32,31,30]{2,1,0} fusion(p0, reduce), kind=kLoop, calls=fused_add\n+    })\"))\n+                    .value();\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  const HloInstruction* consumer = root;\n+  const HloInstruction* producer = root->operand(1);\n+  EXPECT_TRUE(IsProducerConsumerFusible(*producer, *consumer));\n+}\n+\n TEST_F(GpuFusibleTest, ProducerConsumerFusionLoopFusionAndReduce) {\n   auto module = ParseAndReturnVerifiedModule(absl::StrCat(kModulePrefix, R\"(\n     fused_add {\n@@ -1403,5 +1448,22 @@ TEST_F(GpuFusibleTest, DoesNotCreateHeavyComputation_FusionInstr) {\n   EXPECT_FALSE(CreatesHeavyComputation(*producer, *consumer));\n }\n \n+TEST_F(GpuFusibleTest, ChooseFusionKind) {\n+  auto module = ParseAndReturnVerifiedModule(R\"(\n+HloModule module\n+\n+ENTRY computation {\n+    p = f32[5000,6000]{1,0} parameter(0)\n+    c = f32[6000,5000] transpose(p), dimensions={1,0}\n+    ROOT r = f32[300,20,5000] reshape(c)\n+}\n+)\")\n+                    .value();\n+  const HloInstruction* root = module->entry_computation()->root_instruction();\n+  const HloInstruction* producer = root->operand(0);\n+  EXPECT_EQ(ChooseFusionKind(*producer, *root),\n+            HloInstruction::FusionKind::kInput);\n+}\n+\n }  // namespace gpu\n }  // namespace xla\n"
        },
        {
            "name": "instruction_fusion.cc",
            "path": "third_party/xla/xla/service/gpu/instruction_fusion.cc",
            "patches": [
                {
                    "old_start": 130,
                    "old_length": 7,
                    "new_start": 130,
                    "new_length": 7,
                    "hunk": "@@ -130,7 +130,7 @@ FusionDecision GpuInstructionFusion::ShouldFuse(HloInstruction* consumer,\n \n HloInstruction::FusionKind GpuInstructionFusion::ChooseKind(\n     const HloInstruction* producer, const HloInstruction* consumer) {\n-  return ChooseFusionKind(*consumer);\n+  return ChooseFusionKind(*producer, *consumer);\n }\n \n HloInstruction* GpuInstructionFusion::FuseInstruction(\n"
                }
            ],
            "whole_deleted": "-  return ChooseFusionKind(*consumer);\n",
            "whole_added": "+  return ChooseFusionKind(*producer, *consumer);\n",
            "whole_hunk": "@@ -130,7 +130,7 @@ FusionDecision GpuInstructionFusion::ShouldFuse(HloInstruction* consumer,\n \n HloInstruction::FusionKind GpuInstructionFusion::ChooseKind(\n     const HloInstruction* producer, const HloInstruction* consumer) {\n-  return ChooseFusionKind(*consumer);\n+  return ChooseFusionKind(*producer, *consumer);\n }\n \n HloInstruction* GpuInstructionFusion::FuseInstruction(\n"
        },
        {
            "name": "ir_emission_utils.cc",
            "path": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "patches": [
                {
                    "old_start": 1084,
                    "old_length": 11,
                    "new_start": 1084,
                    "new_length": 7,
                    "hunk": "@@ -1084,11 +1084,7 @@ const HloInstruction& FindNonTrivialHero(const HloInstruction& instr,\n }\n \n const HloInstruction& FindNonTrivialHero(const HloInstruction& instr) {\n-  // It doesn't really make sense to call this function with a fusion, but it\n-  // happens. Return the fusion itself for historical reasons.\n-  // TODO(jreiffers): Clean this up.\n-  if (instr.opcode() == HloOpcode::kFusion) return instr;\n-\n+  CHECK_NE(instr.opcode(), HloOpcode::kFusion);\n   return FindNonTrivialHero(instr,\n                             *HloFusionAdaptor::ForComputation(instr.parent()));\n }\n"
                }
            ],
            "whole_deleted": "-  // It doesn't really make sense to call this function with a fusion, but it\n-  // happens. Return the fusion itself for historical reasons.\n-  // TODO(jreiffers): Clean this up.\n-  if (instr.opcode() == HloOpcode::kFusion) return instr;\n-\n",
            "whole_added": "+  CHECK_NE(instr.opcode(), HloOpcode::kFusion);\n",
            "whole_hunk": "@@ -1084,11 +1084,7 @@ const HloInstruction& FindNonTrivialHero(const HloInstruction& instr,\n }\n \n const HloInstruction& FindNonTrivialHero(const HloInstruction& instr) {\n-  // It doesn't really make sense to call this function with a fusion, but it\n-  // happens. Return the fusion itself for historical reasons.\n-  // TODO(jreiffers): Clean this up.\n-  if (instr.opcode() == HloOpcode::kFusion) return instr;\n-\n+  CHECK_NE(instr.opcode(), HloOpcode::kFusion);\n   return FindNonTrivialHero(instr,\n                             *HloFusionAdaptor::ForComputation(instr.parent()));\n }\n"
        },
        {
            "name": "multi_output_fusion.cc",
            "path": "third_party/xla/xla/service/gpu/multi_output_fusion.cc",
            "patches": [
                {
                    "old_start": 462,
                    "old_length": 7,
                    "new_start": 462,
                    "new_length": 8,
                    "hunk": "@@ -462,7 +462,8 @@ absl::StatusOr<bool> GpuMultiOutputFusion::DoMultiOutputFusion() {\n               << consumer_for_fusion->name();\n     } else {\n       input_fusion = computation_->AddInstruction(HloInstruction::CreateFusion(\n-          consumer_for_fusion->shape(), ChooseFusionKind(*consumer_for_fusion),\n+          consumer_for_fusion->shape(),\n+          ChooseFusionKind(*producer, *consumer_for_fusion),\n           consumer_for_fusion));\n       VLOG(2) << \"Fuse producer \" << producer->name() << \" and its consumer \"\n               << consumer_for_fusion->name() << \" into \"\n"
                }
            ],
            "whole_deleted": "-          consumer_for_fusion->shape(), ChooseFusionKind(*consumer_for_fusion),\n",
            "whole_added": "+          consumer_for_fusion->shape(),\n+          ChooseFusionKind(*producer, *consumer_for_fusion),\n",
            "whole_hunk": "@@ -462,7 +462,8 @@ absl::StatusOr<bool> GpuMultiOutputFusion::DoMultiOutputFusion() {\n               << consumer_for_fusion->name();\n     } else {\n       input_fusion = computation_->AddInstruction(HloInstruction::CreateFusion(\n-          consumer_for_fusion->shape(), ChooseFusionKind(*consumer_for_fusion),\n+          consumer_for_fusion->shape(),\n+          ChooseFusionKind(*producer, *consumer_for_fusion),\n           consumer_for_fusion));\n       VLOG(2) << \"Fuse producer \" << producer->name() << \" and its consumer \"\n               << consumer_for_fusion->name() << \" into \"\n"
        },
        {
            "name": "triton_autotuner.cc",
            "path": "third_party/xla/xla/service/gpu/triton_autotuner.cc",
            "patches": [
                {
                    "old_start": 484,
                    "old_length": 7,
                    "new_start": 484,
                    "new_length": 7,
                    "hunk": "@@ -484,7 +484,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n     if (root->opcode() == HloOpcode::kReduce) {\n       HloInstruction* fusion_instruction =\n           entry_computation->AddInstruction(HloInstruction::CreateFusion(\n-              root->shape(), ChooseFusionKind(*root), root));\n+              root->shape(), ChooseFusionKind(*root, *root), root));\n       HloInstruction* init_value = root->mutable_operand(1);\n       TF_CHECK_OK(\n           entry_computation->ReplaceInstruction(root, fusion_instruction));"
                }
            ],
            "whole_deleted": "-              root->shape(), ChooseFusionKind(*root), root));\n",
            "whole_added": "+              root->shape(), ChooseFusionKind(*root, *root), root));\n",
            "whole_hunk": "@@ -484,7 +484,7 @@ absl::StatusOr<std::unique_ptr<HloModule>> TritonGemmAutotuneExtractor(\n     if (root->opcode() == HloOpcode::kReduce) {\n       HloInstruction* fusion_instruction =\n           entry_computation->AddInstruction(HloInstruction::CreateFusion(\n-              root->shape(), ChooseFusionKind(*root), root));\n+              root->shape(), ChooseFusionKind(*root, *root), root));\n       HloInstruction* init_value = root->mutable_operand(1);\n       TF_CHECK_OK(\n           entry_computation->ReplaceInstruction(root, fusion_instruction));"
        }
    ]
},
{
    "Id": 483,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/6ecc07270e41eae2b7aaca1482bad9c3462c3c9c",
    "date": "2023-03-20T19:38:21+00:00",
    "message": "remove check",
    "label": "YES",
    "changes": [
        {
            "name": "cuda_blas.cc",
            "path": "tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc",
            "patches": [
                {
                    "old_start": 679,
                    "old_length": 8,
                    "new_start": 679,
                    "new_length": 6,
                    "hunk": "@@ -679,8 +679,6 @@ tsl::Status CUDABlas::DoBlasGemm(Stream *stream, blas::Transpose transa,\n       static_cast<int>(transa), static_cast<int>(transb), m, n, k, alpha,\n       a.opaque(), lda, b.opaque(), ldb, beta, c->opaque(), ldc);\n \n-  CHECK(a.opaque() != nullptr);\n-\n   switch (dtype) {\n     case blas::DataType::kHalf: {\n #if CUDA_VERSION < 7050"
                }
            ],
            "whole_deleted": "-  CHECK(a.opaque() != nullptr);\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -679,8 +679,6 @@ tsl::Status CUDABlas::DoBlasGemm(Stream *stream, blas::Transpose transa,\n       static_cast<int>(transa), static_cast<int>(transb), m, n, k, alpha,\n       a.opaque(), lda, b.opaque(), ldb, beta, c->opaque(), ldc);\n \n-  CHECK(a.opaque() != nullptr);\n-\n   switch (dtype) {\n     case blas::DataType::kHalf: {\n #if CUDA_VERSION < 7050"
        }
    ]
},
{
    "Id": 92,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/aa89782b20e7f4a9aa8ffbebb06ffb0232b69eee",
    "date": "2024-03-25T04:32:33-07:00",
    "message": "Add version checks to FindCudaExecutable\n\nCurrently we look for ptxas and nvlink in a few different places on the host machine, then we choose the first found binary without taking its version into account. If the chosen binary doesn't fulfill our version requirements we will later fail even if there was a suitable ptxas or nvlink in the search path in the first place.\n\nThis change makes it take the version of each binary into account when going through the search path. Unsuitable binaries will be discarded right away and the search continues until we are out of locations to check.\n\nThis should help with host environments that have multiple CUDA toolkits installed and should make ptxas and nvlink selection more robust.\n\nThe concreate changes:\n\n1. `FindCudaExecutable` now also takes a minimum version and a list of forbidden (think buggy) versions that are supposed to be skipped.\n2. `WarnIfBadPtxAsVersion` has been removed. It was checking for ptxas < 11.1 which is way older than our minimum supported version of 11.8 and was not doing anything given the check described in #3.\n3. There was another version check for `ptxas` in `NVPTXCompiler::ChooseLinkingMethod` which was checking for `version(ptxas)` < 11.8. This has also been removed/replace by the version check described in #4.\n4. Version checking for `ptxas` and `nvlink` has been consolidated into 2 methods `FindPtxAsExectuable` and `FindNvLinkExecutable`. These methods hard code the current minimum version (and the list of excluded versions) of each tool in one place. It's still not great but at least less spaghetti-like.\n\nPiperOrigin-RevId: 618797392",
    "label": "YES",
    "changes": [
        {
            "name": "nvptx_compiler.cc",
            "path": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 7,
                    "new_start": 20,
                    "new_length": 6,
                    "hunk": "@@ -20,7 +20,6 @@ limitations under the License.\n #include <fstream>\n #include <iterator>\n #include <memory>\n-#include <optional>\n #include <string>\n #include <tuple>\n #include <utility>\n"
                },
                {
                    "old_start": 672,
                    "old_length": 8,
                    "new_start": 671,
                    "new_length": 7,
                    "hunk": "@@ -672,8 +671,7 @@ NVPTXCompiler::CompileGpuAsmOrGetCachedResult(\n   return cache_value->maybe_cubin;\n }\n \n-static std::optional<std::array<int64_t, 3>> GetNvLinkVersion(\n-    const std::string& preferred_cuda_dir) {\n+static bool IsNvlinkEnabled() {\n   const bool use_nvlink_by_default =\n #ifdef TF_DISABLE_NVLINK_BY_DEFAULT\n       false;\n"
                },
                {
                    "old_start": 684,
                    "old_length": 24,
                    "new_start": 682,
                    "new_length": 7,
                    "hunk": "@@ -684,24 +682,7 @@ static std::optional<std::array<int64_t, 3>> GetNvLinkVersion(\n   TF_CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_USE_NVLINK_FOR_PARALLEL_COMPILATION\",\n                                       /*default_val=*/\n                                       use_nvlink_by_default, &use_nvlink));\n-\n-  if (!use_nvlink) {\n-    return std::nullopt;\n-  }\n-\n-  // Make sure nvlink exists and is executable.\n-  absl::StatusOr<std::string> bin_path =\n-      se::FindCudaExecutable(\"nvlink\", preferred_cuda_dir);\n-\n-  if (!bin_path.ok()) {\n-    return std::nullopt;\n-  }\n-\n-  auto version = se::GetToolVersion(bin_path.value());\n-  if (!version.ok()) {\n-    return std::nullopt;\n-  }\n-  return *version;\n+  return use_nvlink;\n }\n \n absl::StatusOr<NVPTXCompiler::LinkingMethod> ChooseLinkingMethodImpl(\n"
                },
                {
                    "old_start": 710,
                    "old_length": 16,
                    "new_start": 691,
                    "new_length": 9,
                    "hunk": "@@ -710,16 +691,9 @@ absl::StatusOr<NVPTXCompiler::LinkingMethod> ChooseLinkingMethodImpl(\n   TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n                       se::GetAsmCompilerVersion(preferred_cuda_dir));\n \n-  // ptxas versions prior to 11.8 are not supported anymore. We check this here,\n-  // since we are fetching the ptxas version anyway. Catching the error\n-  // elsewhere might introduce unnecessary overhead.\n-  if (ptxas_version_tuple < std::array<int64_t, 3>{11, 8, 0}) {\n-    return absl::InternalError(\"XLA requires ptxas version 11.8 or higher\");\n-  }\n-\n-  std::optional<std::array<int64_t, 3>> nvlink_version =\n-      GetNvLinkVersion(preferred_cuda_dir);\n-  if (nvlink_version && *nvlink_version >= ptxas_version_tuple) {\n+  auto nvlink_version = stream_executor::GetNvLinkVersion(preferred_cuda_dir);\n+  if (IsNvlinkEnabled() && nvlink_version.ok() &&\n+      nvlink_version.value() >= ptxas_version_tuple) {\n     return LinkingMethod::kNvLink;\n   }\n \n"
                }
            ],
            "whole_deleted": "-#include <optional>\n-static std::optional<std::array<int64_t, 3>> GetNvLinkVersion(\n-    const std::string& preferred_cuda_dir) {\n-\n-  if (!use_nvlink) {\n-    return std::nullopt;\n-  }\n-\n-  // Make sure nvlink exists and is executable.\n-  absl::StatusOr<std::string> bin_path =\n-      se::FindCudaExecutable(\"nvlink\", preferred_cuda_dir);\n-\n-  if (!bin_path.ok()) {\n-    return std::nullopt;\n-  }\n-\n-  auto version = se::GetToolVersion(bin_path.value());\n-  if (!version.ok()) {\n-    return std::nullopt;\n-  }\n-  return *version;\n-  // ptxas versions prior to 11.8 are not supported anymore. We check this here,\n-  // since we are fetching the ptxas version anyway. Catching the error\n-  // elsewhere might introduce unnecessary overhead.\n-  if (ptxas_version_tuple < std::array<int64_t, 3>{11, 8, 0}) {\n-    return absl::InternalError(\"XLA requires ptxas version 11.8 or higher\");\n-  }\n-\n-  std::optional<std::array<int64_t, 3>> nvlink_version =\n-      GetNvLinkVersion(preferred_cuda_dir);\n-  if (nvlink_version && *nvlink_version >= ptxas_version_tuple) {\n",
            "whole_added": "+static bool IsNvlinkEnabled() {\n+  return use_nvlink;\n+  auto nvlink_version = stream_executor::GetNvLinkVersion(preferred_cuda_dir);\n+  if (IsNvlinkEnabled() && nvlink_version.ok() &&\n+      nvlink_version.value() >= ptxas_version_tuple) {\n",
            "whole_hunk": "@@ -20,7 +20,6 @@ limitations under the License.\n #include <fstream>\n #include <iterator>\n #include <memory>\n-#include <optional>\n #include <string>\n #include <tuple>\n #include <utility>\n@@ -672,8 +671,7 @@ NVPTXCompiler::CompileGpuAsmOrGetCachedResult(\n   return cache_value->maybe_cubin;\n }\n \n-static std::optional<std::array<int64_t, 3>> GetNvLinkVersion(\n-    const std::string& preferred_cuda_dir) {\n+static bool IsNvlinkEnabled() {\n   const bool use_nvlink_by_default =\n #ifdef TF_DISABLE_NVLINK_BY_DEFAULT\n       false;\n@@ -684,24 +682,7 @@ static std::optional<std::array<int64_t, 3>> GetNvLinkVersion(\n   TF_CHECK_OK(tsl::ReadBoolFromEnvVar(\"TF_USE_NVLINK_FOR_PARALLEL_COMPILATION\",\n                                       /*default_val=*/\n                                       use_nvlink_by_default, &use_nvlink));\n-\n-  if (!use_nvlink) {\n-    return std::nullopt;\n-  }\n-\n-  // Make sure nvlink exists and is executable.\n-  absl::StatusOr<std::string> bin_path =\n-      se::FindCudaExecutable(\"nvlink\", preferred_cuda_dir);\n-\n-  if (!bin_path.ok()) {\n-    return std::nullopt;\n-  }\n-\n-  auto version = se::GetToolVersion(bin_path.value());\n-  if (!version.ok()) {\n-    return std::nullopt;\n-  }\n-  return *version;\n+  return use_nvlink;\n }\n \n absl::StatusOr<NVPTXCompiler::LinkingMethod> ChooseLinkingMethodImpl(\n@@ -710,16 +691,9 @@ absl::StatusOr<NVPTXCompiler::LinkingMethod> ChooseLinkingMethodImpl(\n   TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n                       se::GetAsmCompilerVersion(preferred_cuda_dir));\n \n-  // ptxas versions prior to 11.8 are not supported anymore. We check this here,\n-  // since we are fetching the ptxas version anyway. Catching the error\n-  // elsewhere might introduce unnecessary overhead.\n-  if (ptxas_version_tuple < std::array<int64_t, 3>{11, 8, 0}) {\n-    return absl::InternalError(\"XLA requires ptxas version 11.8 or higher\");\n-  }\n-\n-  std::optional<std::array<int64_t, 3>> nvlink_version =\n-      GetNvLinkVersion(preferred_cuda_dir);\n-  if (nvlink_version && *nvlink_version >= ptxas_version_tuple) {\n+  auto nvlink_version = stream_executor::GetNvLinkVersion(preferred_cuda_dir);\n+  if (IsNvlinkEnabled() && nvlink_version.ok() &&\n+      nvlink_version.value() >= ptxas_version_tuple) {\n     return LinkingMethod::kNvLink;\n   }\n \n"
        },
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "patches": [
                {
                    "old_start": 566,
                    "old_length": 10,
                    "new_start": 566,
                    "new_length": 7,
                    "hunk": "@@ -566,10 +566,7 @@ cuda_only_cc_library(\n         \"//xla:status_macros\",\n         \"//xla/stream_executor:stream_executor_headers\",\n         \"//xla/stream_executor/gpu:asm_compiler_header\",\n-        \"//xla/stream_executor/gpu:gpu_asm_opts\",\n         \"//xla/stream_executor/gpu:gpu_driver_header\",\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/log\",\n"
                },
                {
                    "old_start": 577,
                    "old_length": 6,
                    "new_start": 574,
                    "new_length": 7,
                    "hunk": "@@ -577,6 +574,7 @@ cuda_only_cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/types:span\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@local_tsl//tsl/platform:env\",\n         \"@local_tsl//tsl/platform:errors\",\n"
                }
            ],
            "whole_deleted": "-        \"//xla/stream_executor/gpu:gpu_asm_opts\",\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/base\",\n",
            "whole_added": "+        \"@com_google_absl//absl/types:span\",\n",
            "whole_hunk": "@@ -566,10 +566,7 @@ cuda_only_cc_library(\n         \"//xla:status_macros\",\n         \"//xla/stream_executor:stream_executor_headers\",\n         \"//xla/stream_executor/gpu:asm_compiler_header\",\n-        \"//xla/stream_executor/gpu:gpu_asm_opts\",\n         \"//xla/stream_executor/gpu:gpu_driver_header\",\n-        \"@com_google_absl//absl/algorithm:container\",\n-        \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/log\",\n@@ -577,6 +574,7 @@ cuda_only_cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/types:span\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@local_tsl//tsl/platform:env\",\n         \"@local_tsl//tsl/platform:errors\",\n"
        },
        {
            "name": "cuda_asm_compiler.cc",
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_asm_compiler.cc",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 7,
                    "new_start": 21,
                    "new_length": 6,
                    "hunk": "@@ -21,7 +21,6 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"absl/base/call_once.h\"\n #include \"absl/base/optimization.h\"\n #include \"absl/cleanup/cleanup.h\"\n #include \"absl/log/log.h\"\n"
                },
                {
                    "old_start": 31,
                    "old_length": 6,
                    "new_start": 30,
                    "new_length": 8,
                    "hunk": "@@ -31,6 +30,8 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"third_party/gpus/cuda/include/cuda.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/gpu/asm_compiler.h\"\n #include \"xla/stream_executor/gpu/gpu_driver.h\"\n"
                },
                {
                    "old_start": 56,
                    "old_length": 18,
                    "new_start": 57,
                    "new_length": 32,
                    "hunk": "@@ -56,18 +57,32 @@ namespace stream_executor {\n     }                                                                         \\\n   } while (false)\n \n+static absl::StatusOr<std::string> FindNvlinkExecutable(\n+    std::string_view preferred_cuda_dir) {\n+  static constexpr ToolVersion kMinimumNvlinkVersion{11, 8, 0};\n+  static constexpr absl::Span<const ToolVersion> kNoExcludedVersions{};\n+  static constexpr std::string_view kNvLinkBinaryName = \"nvlink\";\n+\n+  return FindCudaExecutable(kNvLinkBinaryName, preferred_cuda_dir,\n+                            kMinimumNvlinkVersion, kNoExcludedVersions);\n+}\n+\n+absl::StatusOr<ToolVersion> GetNvLinkVersion(\n+    std::string_view preferred_cuda_dir) {\n+  // Make sure nvlink exists and is executable.\n+  TF_ASSIGN_OR_RETURN(std::string bin_path,\n+                      FindNvlinkExecutable(preferred_cuda_dir));\n+\n+  return GetToolVersion(bin_path);\n+}\n+\n absl::StatusOr<std::vector<uint8_t>> LinkUsingNvlink(\n     absl::string_view preferred_cuda_dir, gpu::GpuContext* context,\n     std::vector<CubinOrPTXImage> images) {\n-  {\n-    static absl::once_flag log_once;\n-    absl::call_once(log_once,\n-                    [] { LOG(INFO) << \"Using nvlink for parallel linking\"; });\n-  }\n+  LOG_FIRST_N(INFO, 1) << \"Using nvlink for parallel linking\";\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::string bin_path,\n-      FindCudaExecutable(\"nvlink\", std::string(preferred_cuda_dir)));\n+  TF_ASSIGN_OR_RETURN(std::string bin_path,\n+                      FindNvlinkExecutable(preferred_cuda_dir));\n \n   if (images.empty()) {\n     return std::vector<uint8>();\n"
                }
            ],
            "whole_deleted": "-#include \"absl/base/call_once.h\"\n-  {\n-    static absl::once_flag log_once;\n-    absl::call_once(log_once,\n-                    [] { LOG(INFO) << \"Using nvlink for parallel linking\"; });\n-  }\n-  TF_ASSIGN_OR_RETURN(\n-      std::string bin_path,\n-      FindCudaExecutable(\"nvlink\", std::string(preferred_cuda_dir)));\n",
            "whole_added": "+#include \"absl/types/span.h\"\n+#include \"third_party/gpus/cuda/include/cuda.h\"\n+static absl::StatusOr<std::string> FindNvlinkExecutable(\n+    std::string_view preferred_cuda_dir) {\n+  static constexpr ToolVersion kMinimumNvlinkVersion{11, 8, 0};\n+  static constexpr absl::Span<const ToolVersion> kNoExcludedVersions{};\n+  static constexpr std::string_view kNvLinkBinaryName = \"nvlink\";\n+\n+  return FindCudaExecutable(kNvLinkBinaryName, preferred_cuda_dir,\n+                            kMinimumNvlinkVersion, kNoExcludedVersions);\n+}\n+\n+absl::StatusOr<ToolVersion> GetNvLinkVersion(\n+    std::string_view preferred_cuda_dir) {\n+  // Make sure nvlink exists and is executable.\n+  TF_ASSIGN_OR_RETURN(std::string bin_path,\n+                      FindNvlinkExecutable(preferred_cuda_dir));\n+\n+  return GetToolVersion(bin_path);\n+}\n+\n+  LOG_FIRST_N(INFO, 1) << \"Using nvlink for parallel linking\";\n+  TF_ASSIGN_OR_RETURN(std::string bin_path,\n+                      FindNvlinkExecutable(preferred_cuda_dir));\n",
            "whole_hunk": "@@ -21,7 +21,6 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"absl/base/call_once.h\"\n #include \"absl/base/optimization.h\"\n #include \"absl/cleanup/cleanup.h\"\n #include \"absl/log/log.h\"\n@@ -31,6 +30,8 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"third_party/gpus/cuda/include/cuda.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/stream_executor/gpu/asm_compiler.h\"\n #include \"xla/stream_executor/gpu/gpu_driver.h\"\n@@ -56,18 +57,32 @@ namespace stream_executor {\n     }                                                                         \\\n   } while (false)\n \n+static absl::StatusOr<std::string> FindNvlinkExecutable(\n+    std::string_view preferred_cuda_dir) {\n+  static constexpr ToolVersion kMinimumNvlinkVersion{11, 8, 0};\n+  static constexpr absl::Span<const ToolVersion> kNoExcludedVersions{};\n+  static constexpr std::string_view kNvLinkBinaryName = \"nvlink\";\n+\n+  return FindCudaExecutable(kNvLinkBinaryName, preferred_cuda_dir,\n+                            kMinimumNvlinkVersion, kNoExcludedVersions);\n+}\n+\n+absl::StatusOr<ToolVersion> GetNvLinkVersion(\n+    std::string_view preferred_cuda_dir) {\n+  // Make sure nvlink exists and is executable.\n+  TF_ASSIGN_OR_RETURN(std::string bin_path,\n+                      FindNvlinkExecutable(preferred_cuda_dir));\n+\n+  return GetToolVersion(bin_path);\n+}\n+\n absl::StatusOr<std::vector<uint8_t>> LinkUsingNvlink(\n     absl::string_view preferred_cuda_dir, gpu::GpuContext* context,\n     std::vector<CubinOrPTXImage> images) {\n-  {\n-    static absl::once_flag log_once;\n-    absl::call_once(log_once,\n-                    [] { LOG(INFO) << \"Using nvlink for parallel linking\"; });\n-  }\n+  LOG_FIRST_N(INFO, 1) << \"Using nvlink for parallel linking\";\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::string bin_path,\n-      FindCudaExecutable(\"nvlink\", std::string(preferred_cuda_dir)));\n+  TF_ASSIGN_OR_RETURN(std::string bin_path,\n+                      FindNvlinkExecutable(preferred_cuda_dir));\n \n   if (images.empty()) {\n     return std::vector<uint8>();\n"
        },
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "patches": [
                {
                    "old_start": 445,
                    "old_length": 6,
                    "new_start": 445,
                    "new_length": 7,
                    "hunk": "@@ -445,6 +445,7 @@ gpu_only_cc_library(\n         \"//xla/stream_executor/cuda:ptx_compiler\",\n         \"//xla/stream_executor/cuda:ptx_compiler_support\",\n         \"//xla/stream_executor/platform\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n"
                },
                {
                    "old_start": 468,
                    "old_length": 6,
                    "new_start": 469,
                    "new_length": 7,
                    "hunk": "@@ -468,6 +469,7 @@ gpu_only_cc_library(\n         \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n         \"@local_tsl//tsl/platform:subprocess\",\n+        \"@local_tsl//tsl/util:env_var\",\n     ] + if_cuda_is_configured([\n         \"//xla/stream_executor/cuda:cuda_asm_compiler\",\n         \"//xla/stream_executor/cuda:cuda_driver\",\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@local_tsl//tsl/util:env_var\",\n",
            "whole_hunk": "@@ -445,6 +445,7 @@ gpu_only_cc_library(\n         \"//xla/stream_executor/cuda:ptx_compiler\",\n         \"//xla/stream_executor/cuda:ptx_compiler_support\",\n         \"//xla/stream_executor/platform\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n@@ -468,6 +469,7 @@ gpu_only_cc_library(\n         \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:statusor\",\n         \"@local_tsl//tsl/platform:subprocess\",\n+        \"@local_tsl//tsl/util:env_var\",\n     ] + if_cuda_is_configured([\n         \"//xla/stream_executor/cuda:cuda_asm_compiler\",\n         \"//xla/stream_executor/cuda:cuda_driver\",\n"
        },
        {
            "name": "asm_compiler.cc",
            "path": "third_party/xla/xla/stream_executor/gpu/asm_compiler.cc",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 6,
                    "new_start": 26,
                    "new_length": 7,
                    "hunk": "@@ -26,6 +26,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/base/const_init.h\"\n #include \"absl/base/optimization.h\"\n #include \"absl/base/thread_annotations.h\"\n"
                },
                {
                    "old_start": 63,
                    "old_length": 7,
                    "new_start": 64,
                    "new_length": 7,
                    "hunk": "@@ -63,7 +64,7 @@ limitations under the License.\n namespace stream_executor {\n \n static absl::StatusOr<std::string> GetToolVersionString(\n-    absl::string_view binary_path) {\n+    std::string_view binary_path) {\n   // If binary_path doesn't exist, then tsl::SubProcess will log a bunch of\n   // error messages that have confused users in the past. Therefore we first\n   // check whether the binary_path exists and error out early if not.\n"
                },
                {
                    "old_start": 103,
                    "old_length": 7,
                    "new_start": 104,
                    "new_length": 7,
                    "hunk": "@@ -103,7 +104,7 @@ static absl::StatusOr<ToolVersion> GetToolVersionImpl(\n   }\n   static constexpr LazyRE2 kVersionRegex = {R\"(\\bV(\\d+)\\.(\\d+)\\.(\\d+)\\b)\"};\n   ToolVersion version{};\n-  absl::string_view vmaj_str, vmin_str, vdot_str;\n+  std::string_view vmaj_str, vmin_str, vdot_str;\n   if (!RE2::PartialMatch(tool_version.value(), *kVersionRegex, &vmaj_str,\n                          &vmin_str, &vdot_str) ||\n       !absl::SimpleAtoi(vmaj_str, &version[0]) ||\n"
                },
                {
                    "old_start": 134,
                    "old_length": 28,
                    "new_start": 135,
                    "new_length": 6,
                    "hunk": "@@ -134,28 +135,6 @@ absl::StatusOr<ToolVersion> GetToolVersion(std::string_view tool_path) {\n       .first->second;\n }\n \n-// Prints a warning if the ptxas at ptxas_path has known bugs.\n-//\n-// Only prints a warning the first time it's called for a particular value of\n-// ptxas_path.\n-//\n-// Locks on entry.\u02dd\n-static void WarnIfBadPtxasVersion(absl::string_view ptxas_path) {\n-  absl::StatusOr<std::array<int64_t, 3>> version = GetToolVersion(ptxas_path);\n-  if (!version.ok()) {\n-    LOG(WARNING) << \"Couldn't get ptxas version : \" << version.status();\n-    return;\n-  }\n-\n-  if (std::make_tuple((*version)[0], (*version)[1]) < std::make_tuple(11, 1)) {\n-    LOG(ERROR) << \"*** WARNING *** You are using ptxas \" << (*version)[0] << \".\"\n-               << (*version)[1] << \".\" << (*version)[2]\n-               << \", which is older than 11.1. ptxas before 11.1 is known to \"\n-                  \"miscompile XLA code, leading to incorrect results or \"\n-                  \"invalid-address errors.\\n\";\n-  }\n-}\n-\n absl::StatusOr<absl::Span<const uint8_t>> CompileGpuAsmOrGetCached(\n     int device_ordinal, const char* ptx, GpuAsmOpts compilation_options) {\n   using PtxCacheKey = std::tuple<int, std::string, GpuAsmOpts::PtxOptionsTuple>;\n"
                },
                {
                    "old_start": 201,
                    "old_length": 7,
                    "new_start": 180,
                    "new_length": 9,
                    "hunk": "@@ -201,7 +180,9 @@ absl::StatusOr<std::vector<uint8_t>> CompileGpuAsm(int device_ordinal,\n }\n \n absl::StatusOr<std::string> FindCudaExecutable(\n-    std::string_view binary_name, std::string_view preferred_cuda_dir) {\n+    std::string_view binary_name, std::string_view preferred_cuda_dir,\n+    ToolVersion minimum_version,\n+    absl::Span<const ToolVersion> excluded_versions) {\n #if defined(PLATFORM_WINDOWS)\n   const std::string binary_filename = std::string{binary_name} + \".exe\";\n #else\n"
                },
                {
                    "old_start": 234,
                    "old_length": 18,
                    "new_start": 215,
                    "new_length": 44,
                    "hunk": "@@ -234,18 +215,44 @@ absl::StatusOr<std::string> FindCudaExecutable(\n \n   for (const auto& candidate : candidates) {\n     VLOG(2) << \"Looking for \" << candidate;\n-    if (GetToolVersion(candidate).ok()) {\n-      VLOG(2) << \"Using \" << candidate;\n-      return candidate;\n+    auto candidate_version = GetToolVersion(candidate);\n+    if (!candidate_version.ok()) {\n+      continue;\n+    }\n+\n+    if (candidate_version.value() < minimum_version) {\n+      VLOG(2) << candidate << \" with version \"\n+              << absl::StrJoin(minimum_version, \".\") << \" is too old.\";\n+      continue;\n+    }\n+\n+    if (absl::c_find(excluded_versions, candidate_version.value()) !=\n+        excluded_versions.end()) {\n+      VLOG(2) << candidate << \" has version \"\n+              << absl::StrJoin(candidate_version.value(), \".\")\n+              << \" which was explicitly excluded.\";\n+      continue;\n     }\n+\n+    VLOG(2) << \"Using \" << candidate << \" with version \"\n+            << absl::StrJoin(candidate_version.value(), \".\");\n+    return candidate;\n   }\n \n   return absl::NotFoundError(\n-      absl::StrCat(\"Couldn't find \", binary_name,\n+      absl::StrCat(\"Couldn't find a suitable version of \", binary_name,\n                    \". The following locations were considered: \",\n                    absl::StrJoin(candidates, \", \")));\n }\n \n+absl::StatusOr<std::string> FindCudaExecutable(\n+    std::string_view binary_name, std::string_view preferred_cuda_dir) {\n+  static constexpr ToolVersion kNoMinimumVersion{0, 0, 0};\n+  static constexpr absl::Span<const ToolVersion> kNoExcludedVersions{};\n+  return FindCudaExecutable(binary_name, preferred_cuda_dir, kNoMinimumVersion,\n+                            kNoExcludedVersions);\n+}\n+\n static void LogPtxasTooOld(const std::string& ptxas_path, int cc_major,\n                            int cc_minor) {\n   using AlreadyLoggedSetTy =\n"
                },
                {
                    "old_start": 274,
                    "old_length": 29,
                    "new_start": 281,
                    "new_length": 28,
                    "hunk": "@@ -274,29 +281,28 @@ static void AppendArgsFromOptions(GpuAsmOpts options,\n               options.extra_flags.end());\n }\n \n-absl::StatusOr<std::array<int64_t, 3>> GetAsmCompilerVersion(\n-    const std::string& preferred_cuda_dir) {\n+static absl::StatusOr<std::string> FindPtxAsExecutable(\n+    std::string_view preferred_cuda_dir) {\n+  static constexpr ToolVersion kMinimumSupportedPtxAsVersion{11, 8, 0};\n+  static constexpr ToolVersion kBuggyPtxAsVersions[] = {{12, 3, 103}};\n+  static constexpr std::string_view kPtxAsBinaryName = \"ptxas\";\n+\n+  return FindCudaExecutable(kPtxAsBinaryName, preferred_cuda_dir,\n+                            kMinimumSupportedPtxAsVersion, kBuggyPtxAsVersions);\n+}\n+\n+absl::StatusOr<ToolVersion> GetAsmCompilerVersion(\n+    std::string_view preferred_cuda_dir) {\n   TF_ASSIGN_OR_RETURN(std::string ptxas_path,\n-                      FindCudaExecutable(\"ptxas\", preferred_cuda_dir));\n+                      FindPtxAsExecutable(preferred_cuda_dir));\n   return GetToolVersion(ptxas_path);\n }\n \n absl::StatusOr<std::vector<uint8_t>> CompileGpuAsmUsingPtxAs(\n     int cc_major, int cc_minor, const char* ptx_contents, GpuAsmOpts options,\n     bool cancel_if_reg_spill) {\n-  TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n-                      GetAsmCompilerVersion(options.preferred_cuda_dir));\n-  if (ptxas_version_tuple == std::array<int64_t, 3>{12, 3, 103}) {\n-    return absl::InternalError(absl::StrFormat(\n-        \"ptxas %d.%d.%d has a bug that we think can affect XLA. \"\n-        \"Please use a different version.\",\n-        std::get<0>(ptxas_version_tuple), std::get<1>(ptxas_version_tuple),\n-        std::get<2>(ptxas_version_tuple)));\n-  }\n   TF_ASSIGN_OR_RETURN(std::string ptxas_path,\n-                      FindCudaExecutable(\"ptxas\", options.preferred_cuda_dir));\n-\n-  WarnIfBadPtxasVersion(ptxas_path);\n+                      FindPtxAsExecutable(options.preferred_cuda_dir));\n \n   // Write ptx into a temporary file.\n   std::string ptx_path;\n"
                }
            ],
            "whole_deleted": "-    absl::string_view binary_path) {\n-  absl::string_view vmaj_str, vmin_str, vdot_str;\n-// Prints a warning if the ptxas at ptxas_path has known bugs.\n-//\n-// Only prints a warning the first time it's called for a particular value of\n-// ptxas_path.\n-//\n-// Locks on entry.\u02dd\n-static void WarnIfBadPtxasVersion(absl::string_view ptxas_path) {\n-  absl::StatusOr<std::array<int64_t, 3>> version = GetToolVersion(ptxas_path);\n-  if (!version.ok()) {\n-    LOG(WARNING) << \"Couldn't get ptxas version : \" << version.status();\n-    return;\n-  }\n-\n-  if (std::make_tuple((*version)[0], (*version)[1]) < std::make_tuple(11, 1)) {\n-    LOG(ERROR) << \"*** WARNING *** You are using ptxas \" << (*version)[0] << \".\"\n-               << (*version)[1] << \".\" << (*version)[2]\n-               << \", which is older than 11.1. ptxas before 11.1 is known to \"\n-                  \"miscompile XLA code, leading to incorrect results or \"\n-                  \"invalid-address errors.\\n\";\n-  }\n-}\n-\n-    std::string_view binary_name, std::string_view preferred_cuda_dir) {\n-    if (GetToolVersion(candidate).ok()) {\n-      VLOG(2) << \"Using \" << candidate;\n-      return candidate;\n-      absl::StrCat(\"Couldn't find \", binary_name,\n-absl::StatusOr<std::array<int64_t, 3>> GetAsmCompilerVersion(\n-    const std::string& preferred_cuda_dir) {\n-                      FindCudaExecutable(\"ptxas\", preferred_cuda_dir));\n-  TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n-                      GetAsmCompilerVersion(options.preferred_cuda_dir));\n-  if (ptxas_version_tuple == std::array<int64_t, 3>{12, 3, 103}) {\n-    return absl::InternalError(absl::StrFormat(\n-        \"ptxas %d.%d.%d has a bug that we think can affect XLA. \"\n-        \"Please use a different version.\",\n-        std::get<0>(ptxas_version_tuple), std::get<1>(ptxas_version_tuple),\n-        std::get<2>(ptxas_version_tuple)));\n-  }\n-                      FindCudaExecutable(\"ptxas\", options.preferred_cuda_dir));\n-\n-  WarnIfBadPtxasVersion(ptxas_path);\n",
            "whole_added": "+#include \"absl/algorithm/container.h\"\n+    std::string_view binary_path) {\n+  std::string_view vmaj_str, vmin_str, vdot_str;\n+    std::string_view binary_name, std::string_view preferred_cuda_dir,\n+    ToolVersion minimum_version,\n+    absl::Span<const ToolVersion> excluded_versions) {\n+    auto candidate_version = GetToolVersion(candidate);\n+    if (!candidate_version.ok()) {\n+      continue;\n+    }\n+\n+    if (candidate_version.value() < minimum_version) {\n+      VLOG(2) << candidate << \" with version \"\n+              << absl::StrJoin(minimum_version, \".\") << \" is too old.\";\n+      continue;\n+    }\n+\n+    if (absl::c_find(excluded_versions, candidate_version.value()) !=\n+        excluded_versions.end()) {\n+      VLOG(2) << candidate << \" has version \"\n+              << absl::StrJoin(candidate_version.value(), \".\")\n+              << \" which was explicitly excluded.\";\n+      continue;\n+\n+    VLOG(2) << \"Using \" << candidate << \" with version \"\n+            << absl::StrJoin(candidate_version.value(), \".\");\n+    return candidate;\n+      absl::StrCat(\"Couldn't find a suitable version of \", binary_name,\n+absl::StatusOr<std::string> FindCudaExecutable(\n+    std::string_view binary_name, std::string_view preferred_cuda_dir) {\n+  static constexpr ToolVersion kNoMinimumVersion{0, 0, 0};\n+  static constexpr absl::Span<const ToolVersion> kNoExcludedVersions{};\n+  return FindCudaExecutable(binary_name, preferred_cuda_dir, kNoMinimumVersion,\n+                            kNoExcludedVersions);\n+}\n+\n+static absl::StatusOr<std::string> FindPtxAsExecutable(\n+    std::string_view preferred_cuda_dir) {\n+  static constexpr ToolVersion kMinimumSupportedPtxAsVersion{11, 8, 0};\n+  static constexpr ToolVersion kBuggyPtxAsVersions[] = {{12, 3, 103}};\n+  static constexpr std::string_view kPtxAsBinaryName = \"ptxas\";\n+\n+  return FindCudaExecutable(kPtxAsBinaryName, preferred_cuda_dir,\n+                            kMinimumSupportedPtxAsVersion, kBuggyPtxAsVersions);\n+}\n+\n+absl::StatusOr<ToolVersion> GetAsmCompilerVersion(\n+    std::string_view preferred_cuda_dir) {\n+                      FindPtxAsExecutable(preferred_cuda_dir));\n+                      FindPtxAsExecutable(options.preferred_cuda_dir));\n",
            "whole_hunk": "@@ -26,6 +26,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n #include \"absl/base/const_init.h\"\n #include \"absl/base/optimization.h\"\n #include \"absl/base/thread_annotations.h\"\n@@ -63,7 +64,7 @@ limitations under the License.\n namespace stream_executor {\n \n static absl::StatusOr<std::string> GetToolVersionString(\n-    absl::string_view binary_path) {\n+    std::string_view binary_path) {\n   // If binary_path doesn't exist, then tsl::SubProcess will log a bunch of\n   // error messages that have confused users in the past. Therefore we first\n   // check whether the binary_path exists and error out early if not.\n@@ -103,7 +104,7 @@ static absl::StatusOr<ToolVersion> GetToolVersionImpl(\n   }\n   static constexpr LazyRE2 kVersionRegex = {R\"(\\bV(\\d+)\\.(\\d+)\\.(\\d+)\\b)\"};\n   ToolVersion version{};\n-  absl::string_view vmaj_str, vmin_str, vdot_str;\n+  std::string_view vmaj_str, vmin_str, vdot_str;\n   if (!RE2::PartialMatch(tool_version.value(), *kVersionRegex, &vmaj_str,\n                          &vmin_str, &vdot_str) ||\n       !absl::SimpleAtoi(vmaj_str, &version[0]) ||\n@@ -134,28 +135,6 @@ absl::StatusOr<ToolVersion> GetToolVersion(std::string_view tool_path) {\n       .first->second;\n }\n \n-// Prints a warning if the ptxas at ptxas_path has known bugs.\n-//\n-// Only prints a warning the first time it's called for a particular value of\n-// ptxas_path.\n-//\n-// Locks on entry.\u02dd\n-static void WarnIfBadPtxasVersion(absl::string_view ptxas_path) {\n-  absl::StatusOr<std::array<int64_t, 3>> version = GetToolVersion(ptxas_path);\n-  if (!version.ok()) {\n-    LOG(WARNING) << \"Couldn't get ptxas version : \" << version.status();\n-    return;\n-  }\n-\n-  if (std::make_tuple((*version)[0], (*version)[1]) < std::make_tuple(11, 1)) {\n-    LOG(ERROR) << \"*** WARNING *** You are using ptxas \" << (*version)[0] << \".\"\n-               << (*version)[1] << \".\" << (*version)[2]\n-               << \", which is older than 11.1. ptxas before 11.1 is known to \"\n-                  \"miscompile XLA code, leading to incorrect results or \"\n-                  \"invalid-address errors.\\n\";\n-  }\n-}\n-\n absl::StatusOr<absl::Span<const uint8_t>> CompileGpuAsmOrGetCached(\n     int device_ordinal, const char* ptx, GpuAsmOpts compilation_options) {\n   using PtxCacheKey = std::tuple<int, std::string, GpuAsmOpts::PtxOptionsTuple>;\n@@ -201,7 +180,9 @@ absl::StatusOr<std::vector<uint8_t>> CompileGpuAsm(int device_ordinal,\n }\n \n absl::StatusOr<std::string> FindCudaExecutable(\n-    std::string_view binary_name, std::string_view preferred_cuda_dir) {\n+    std::string_view binary_name, std::string_view preferred_cuda_dir,\n+    ToolVersion minimum_version,\n+    absl::Span<const ToolVersion> excluded_versions) {\n #if defined(PLATFORM_WINDOWS)\n   const std::string binary_filename = std::string{binary_name} + \".exe\";\n #else\n@@ -234,18 +215,44 @@ absl::StatusOr<std::string> FindCudaExecutable(\n \n   for (const auto& candidate : candidates) {\n     VLOG(2) << \"Looking for \" << candidate;\n-    if (GetToolVersion(candidate).ok()) {\n-      VLOG(2) << \"Using \" << candidate;\n-      return candidate;\n+    auto candidate_version = GetToolVersion(candidate);\n+    if (!candidate_version.ok()) {\n+      continue;\n+    }\n+\n+    if (candidate_version.value() < minimum_version) {\n+      VLOG(2) << candidate << \" with version \"\n+              << absl::StrJoin(minimum_version, \".\") << \" is too old.\";\n+      continue;\n+    }\n+\n+    if (absl::c_find(excluded_versions, candidate_version.value()) !=\n+        excluded_versions.end()) {\n+      VLOG(2) << candidate << \" has version \"\n+              << absl::StrJoin(candidate_version.value(), \".\")\n+              << \" which was explicitly excluded.\";\n+      continue;\n     }\n+\n+    VLOG(2) << \"Using \" << candidate << \" with version \"\n+            << absl::StrJoin(candidate_version.value(), \".\");\n+    return candidate;\n   }\n \n   return absl::NotFoundError(\n-      absl::StrCat(\"Couldn't find \", binary_name,\n+      absl::StrCat(\"Couldn't find a suitable version of \", binary_name,\n                    \". The following locations were considered: \",\n                    absl::StrJoin(candidates, \", \")));\n }\n \n+absl::StatusOr<std::string> FindCudaExecutable(\n+    std::string_view binary_name, std::string_view preferred_cuda_dir) {\n+  static constexpr ToolVersion kNoMinimumVersion{0, 0, 0};\n+  static constexpr absl::Span<const ToolVersion> kNoExcludedVersions{};\n+  return FindCudaExecutable(binary_name, preferred_cuda_dir, kNoMinimumVersion,\n+                            kNoExcludedVersions);\n+}\n+\n static void LogPtxasTooOld(const std::string& ptxas_path, int cc_major,\n                            int cc_minor) {\n   using AlreadyLoggedSetTy =\n@@ -274,29 +281,28 @@ static void AppendArgsFromOptions(GpuAsmOpts options,\n               options.extra_flags.end());\n }\n \n-absl::StatusOr<std::array<int64_t, 3>> GetAsmCompilerVersion(\n-    const std::string& preferred_cuda_dir) {\n+static absl::StatusOr<std::string> FindPtxAsExecutable(\n+    std::string_view preferred_cuda_dir) {\n+  static constexpr ToolVersion kMinimumSupportedPtxAsVersion{11, 8, 0};\n+  static constexpr ToolVersion kBuggyPtxAsVersions[] = {{12, 3, 103}};\n+  static constexpr std::string_view kPtxAsBinaryName = \"ptxas\";\n+\n+  return FindCudaExecutable(kPtxAsBinaryName, preferred_cuda_dir,\n+                            kMinimumSupportedPtxAsVersion, kBuggyPtxAsVersions);\n+}\n+\n+absl::StatusOr<ToolVersion> GetAsmCompilerVersion(\n+    std::string_view preferred_cuda_dir) {\n   TF_ASSIGN_OR_RETURN(std::string ptxas_path,\n-                      FindCudaExecutable(\"ptxas\", preferred_cuda_dir));\n+                      FindPtxAsExecutable(preferred_cuda_dir));\n   return GetToolVersion(ptxas_path);\n }\n \n absl::StatusOr<std::vector<uint8_t>> CompileGpuAsmUsingPtxAs(\n     int cc_major, int cc_minor, const char* ptx_contents, GpuAsmOpts options,\n     bool cancel_if_reg_spill) {\n-  TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n-                      GetAsmCompilerVersion(options.preferred_cuda_dir));\n-  if (ptxas_version_tuple == std::array<int64_t, 3>{12, 3, 103}) {\n-    return absl::InternalError(absl::StrFormat(\n-        \"ptxas %d.%d.%d has a bug that we think can affect XLA. \"\n-        \"Please use a different version.\",\n-        std::get<0>(ptxas_version_tuple), std::get<1>(ptxas_version_tuple),\n-        std::get<2>(ptxas_version_tuple)));\n-  }\n   TF_ASSIGN_OR_RETURN(std::string ptxas_path,\n-                      FindCudaExecutable(\"ptxas\", options.preferred_cuda_dir));\n-\n-  WarnIfBadPtxasVersion(ptxas_path);\n+                      FindPtxAsExecutable(options.preferred_cuda_dir));\n \n   // Write ptx into a temporary file.\n   std::string ptx_path;\n"
        },
        {
            "name": "asm_compiler.h",
            "path": "third_party/xla/xla/stream_executor/gpu/asm_compiler.h",
            "patches": [
                {
                    "old_start": 104,
                    "old_length": 16,
                    "new_start": 104,
                    "new_length": 25,
                    "hunk": "@@ -104,16 +104,25 @@ absl::StatusOr<std::vector<uint8_t>> LinkUsingNvlink(\n     absl::string_view preferred_cuda_dir, gpu::GpuContext* context,\n     std::vector<CubinOrPTXImage> images);\n \n+using ToolVersion = std::array<int64_t, 3>;\n+absl::StatusOr<std::string> FindCudaExecutable(\n+    std::string_view binary_name, std::string_view preferred_cuda_dir,\n+    ToolVersion minimum_version,\n+    absl::Span<const ToolVersion> excluded_versions);\n+\n absl::StatusOr<std::string> FindCudaExecutable(\n     std::string_view binary_name, std::string_view preferred_cuda_dir);\n \n // Runs tool --version and parses its version string.\n-using ToolVersion = std::array<int64_t, 3>;\n absl::StatusOr<ToolVersion> GetToolVersion(std::string_view tool_path);\n \n-// On NVIDIA GPUs, returns the CUDA toolkit version supported by the driver,\n+// On NVIDIA GPUs, returns the version of the ptxas command line tool.\n absl::StatusOr<ToolVersion> GetAsmCompilerVersion(\n-    const std::string& preferred_cuda_dir);\n+    std::string_view preferred_cuda_dir);\n+\n+// On NVIDIA GPUs, returns the version of the nvlink command line tool.\n+absl::StatusOr<ToolVersion> GetNvLinkVersion(\n+    std::string_view preferred_cuda_dir);\n \n #if GOOGLE_CUDA\n // Maintains a cache of pointers to loaded kernels"
                }
            ],
            "whole_deleted": "-using ToolVersion = std::array<int64_t, 3>;\n-// On NVIDIA GPUs, returns the CUDA toolkit version supported by the driver,\n-    const std::string& preferred_cuda_dir);\n",
            "whole_added": "+using ToolVersion = std::array<int64_t, 3>;\n+absl::StatusOr<std::string> FindCudaExecutable(\n+    std::string_view binary_name, std::string_view preferred_cuda_dir,\n+    ToolVersion minimum_version,\n+    absl::Span<const ToolVersion> excluded_versions);\n+\n+// On NVIDIA GPUs, returns the version of the ptxas command line tool.\n+    std::string_view preferred_cuda_dir);\n+\n+// On NVIDIA GPUs, returns the version of the nvlink command line tool.\n+absl::StatusOr<ToolVersion> GetNvLinkVersion(\n+    std::string_view preferred_cuda_dir);\n",
            "whole_hunk": "@@ -104,16 +104,25 @@ absl::StatusOr<std::vector<uint8_t>> LinkUsingNvlink(\n     absl::string_view preferred_cuda_dir, gpu::GpuContext* context,\n     std::vector<CubinOrPTXImage> images);\n \n+using ToolVersion = std::array<int64_t, 3>;\n+absl::StatusOr<std::string> FindCudaExecutable(\n+    std::string_view binary_name, std::string_view preferred_cuda_dir,\n+    ToolVersion minimum_version,\n+    absl::Span<const ToolVersion> excluded_versions);\n+\n absl::StatusOr<std::string> FindCudaExecutable(\n     std::string_view binary_name, std::string_view preferred_cuda_dir);\n \n // Runs tool --version and parses its version string.\n-using ToolVersion = std::array<int64_t, 3>;\n absl::StatusOr<ToolVersion> GetToolVersion(std::string_view tool_path);\n \n-// On NVIDIA GPUs, returns the CUDA toolkit version supported by the driver,\n+// On NVIDIA GPUs, returns the version of the ptxas command line tool.\n absl::StatusOr<ToolVersion> GetAsmCompilerVersion(\n-    const std::string& preferred_cuda_dir);\n+    std::string_view preferred_cuda_dir);\n+\n+// On NVIDIA GPUs, returns the version of the nvlink command line tool.\n+absl::StatusOr<ToolVersion> GetNvLinkVersion(\n+    std::string_view preferred_cuda_dir);\n \n #if GOOGLE_CUDA\n // Maintains a cache of pointers to loaded kernels"
        }
    ]
},
{
    "Id": 7,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/26715f1df5f4c16aad62474c6ecca28f2ece02de",
    "date": "2024-07-04T04:10:32-07:00",
    "message": "PR #13527: [XLA:CPU][oneDNN] Enable mm-bias-add fusion\n\nImported from GitHub PR https://github.com/openxla/xla/pull/13527\n\nThis PR enables matmul followed by bias-add + binary-add fusion and tests. It removes the check that was blocking this fusion.\nCopybara import of the project:\n\n--\n7b916f2c9ffc9bb703a4650be9be99a1c0cb1696 by Kanvi Khanna <kanvi.khanna@intel.com>:\n\nEnable mm-bias-add fusion\n\nMerging this change closes #13527\n\nPiperOrigin-RevId: 649355747",
    "label": "NO",
    "changes": [
        {
            "name": "onednn_matmul_rewriter.cc",
            "path": "third_party/xla/xla/service/cpu/onednn_matmul_rewriter.cc",
            "patches": [
                {
                    "old_start": 324,
                    "old_length": 6,
                    "new_start": 324,
                    "new_length": 20,
                    "hunk": "@@ -324,6 +324,20 @@ absl::StatusOr<Shape> AdjustBiasShape(const HloInstruction* broadcast_instr,\n   return new_shape;\n };\n \n+// Compute new shape for the binary operand when dot's outer dims\n+// are flattened/unflattened with respect to the binary operand dims.\n+// Adjusting the operand shape to the dot's shape enables fusion in oneDNN.\n+absl::StatusOr<Shape> AdjustBinaryOperandShape(\n+    const HloInstruction* operand_instr, const Shape& dot_shape) {\n+  if (ShapeUtil::ElementsIn(operand_instr->shape()) !=\n+      ShapeUtil::ElementsIn(dot_shape)) {\n+    return absl::CancelledError(\n+        \"Number of elements in operand and dot instruction do not match.\");\n+  }\n+  Shape new_shape = dot_shape;\n+  return new_shape;\n+};\n+\n inline bool IsOperandFusible(HloInstruction* operand, HloInstruction* dot) {\n   // Check if the operand's shape is compatible with matmul for fusion.\n   // An operand is fusable if\n"
                },
                {
                    "old_start": 353,
                    "old_length": 11,
                    "new_start": 367,
                    "new_length": 19,
                    "hunk": "@@ -353,11 +367,19 @@ inline auto OptionalConvertAndBitcast(HloInstruction** optional_convert,\n   //   1. pattern-root -> bf16/f16-to-fp32 convert -> bitcast\n   //   2. pattern-root -> bf16/f16-to-fp32 convert\n   //   3. pattern-root -> bitcast\n-  //   4. pattern-root\n+  //   4. pattern-root -> bitcast -> bf16-to-fp32 convert\n+  //   5. pattern-root\n   auto common = m::AnyOf<HloInstruction>(\n-      pu::SupportedConvert(optional_convert, std::move(pattern).WithOneUser())\n-          .WithElementType(PrimitiveType::F32),\n-      std::move(pattern).WithOneUser());\n+                    pu::SupportedConvert(optional_convert,\n+                                         std::move(pattern).WithOneUser())\n+                        .WithElementType(PrimitiveType::F32),\n+                    std::move(pattern).WithOneUser(),\n+                    pu::SupportedConvert(\n+                        optional_convert,\n+                        BitcastWithReshapeSemantics(\n+                            optional_bitcast, std::move(pattern).WithOneUser()))\n+                        .WithElementType(PrimitiveType::F32))\n+                    .WithOneUser();\n   return m::AnyOf<HloInstruction>(\n       BitcastWithReshapeSemantics(optional_bitcast, common), common);\n }\n"
                },
                {
                    "old_start": 499,
                    "old_length": 19,
                    "new_start": 521,
                    "new_length": 6,
                    "hunk": "@@ -499,19 +521,6 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n     if (Match(instr, pattern)) {\n       if (!IsSupportedType(dot->shape().element_type()))\n         return absl::OkStatus();\n-      // TODO(intel-tf): Remove the condition below when the fusion Dot +\n-      // Add(bias) + Add(e.g., residual) is enabled.\n-      if (!dot->backend_config<BackendConfig>()\n-               ->mutable_onednn_matmul_config()\n-               ->mutable_fusions()\n-               ->ops()\n-               .empty() &&\n-          dot->backend_config<BackendConfig>()\n-                  ->mutable_onednn_matmul_config()\n-                  ->mutable_fusions()\n-                  ->ops(0) == OneDnnFusionConfig::BIAS) {\n-        return absl::OkStatus();\n-      }\n       std::vector<HloInstruction*> new_operands;\n       for (auto operand : dot->operands()) {\n         new_operands.push_back(operand);\n"
                },
                {
                    "old_start": 550,
                    "old_length": 6,
                    "new_start": 559,
                    "new_length": 31,
                    "hunk": "@@ -550,6 +559,31 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n           return absl::OkStatus();\n         }\n       }\n+      // For cases where the dot is followed by a reshape, the binary operands\n+      // shape can be adjusted, making sure the number of elements match, to\n+      // enable the fusion. For example:\n+      //      dot = f32[6304,3072] dot(...)\n+      //      reshape = f32[32,197,3072] reshape(dot)\n+      //      constant = f32[32,197,3072] constant(..)\n+      //      add = f32[32,197,3072] add(reshape, constant)\n+      // can become\n+      //      dot = f32[6304,3072] dot(...)\n+      //      constant = f32[32,197,3072] constant(..)\n+      //      reshape1 = f32[6304,3072] reshape(constant)\n+      //      add = f32[6304,3072] add(dot, reshape1)\n+      // and be replaced with the fusion\n+      //      fused = f32[6304,3072] custom-call(..)\n+      //      bitcast = f32[32,197,3072] bitcast(fused)\n+      // clang-format on\n+      auto addend_dims = addend->shape().dimensions();\n+      auto dot_dims = dot->shape().dimensions();\n+      if (optional_dot_bitcast && addend_dims.size() != dot_dims.size()) {\n+        auto new_addend_shape = AdjustBinaryOperandShape(addend, dot->shape());\n+        if (new_addend_shape.ok()) {\n+          addend = addend->AddInstruction(\n+              HloInstruction::CreateBitcast(new_addend_shape.value(), addend));\n+        }\n+      }\n \n       // Validate addend for fusion.\n       if (IsSupportedType(addend->shape().element_type()) &&\n"
                }
            ],
            "whole_deleted": "-  //   4. pattern-root\n-      pu::SupportedConvert(optional_convert, std::move(pattern).WithOneUser())\n-          .WithElementType(PrimitiveType::F32),\n-      std::move(pattern).WithOneUser());\n-      // TODO(intel-tf): Remove the condition below when the fusion Dot +\n-      // Add(bias) + Add(e.g., residual) is enabled.\n-      if (!dot->backend_config<BackendConfig>()\n-               ->mutable_onednn_matmul_config()\n-               ->mutable_fusions()\n-               ->ops()\n-               .empty() &&\n-          dot->backend_config<BackendConfig>()\n-                  ->mutable_onednn_matmul_config()\n-                  ->mutable_fusions()\n-                  ->ops(0) == OneDnnFusionConfig::BIAS) {\n-        return absl::OkStatus();\n-      }\n",
            "whole_added": "+// Compute new shape for the binary operand when dot's outer dims\n+// are flattened/unflattened with respect to the binary operand dims.\n+// Adjusting the operand shape to the dot's shape enables fusion in oneDNN.\n+absl::StatusOr<Shape> AdjustBinaryOperandShape(\n+    const HloInstruction* operand_instr, const Shape& dot_shape) {\n+  if (ShapeUtil::ElementsIn(operand_instr->shape()) !=\n+      ShapeUtil::ElementsIn(dot_shape)) {\n+    return absl::CancelledError(\n+        \"Number of elements in operand and dot instruction do not match.\");\n+  }\n+  Shape new_shape = dot_shape;\n+  return new_shape;\n+};\n+\n+  //   4. pattern-root -> bitcast -> bf16-to-fp32 convert\n+  //   5. pattern-root\n+                    pu::SupportedConvert(optional_convert,\n+                                         std::move(pattern).WithOneUser())\n+                        .WithElementType(PrimitiveType::F32),\n+                    std::move(pattern).WithOneUser(),\n+                    pu::SupportedConvert(\n+                        optional_convert,\n+                        BitcastWithReshapeSemantics(\n+                            optional_bitcast, std::move(pattern).WithOneUser()))\n+                        .WithElementType(PrimitiveType::F32))\n+                    .WithOneUser();\n+      // For cases where the dot is followed by a reshape, the binary operands\n+      // shape can be adjusted, making sure the number of elements match, to\n+      // enable the fusion. For example:\n+      //      dot = f32[6304,3072] dot(...)\n+      //      reshape = f32[32,197,3072] reshape(dot)\n+      //      constant = f32[32,197,3072] constant(..)\n+      //      add = f32[32,197,3072] add(reshape, constant)\n+      // can become\n+      //      dot = f32[6304,3072] dot(...)\n+      //      constant = f32[32,197,3072] constant(..)\n+      //      reshape1 = f32[6304,3072] reshape(constant)\n+      //      add = f32[6304,3072] add(dot, reshape1)\n+      // and be replaced with the fusion\n+      //      fused = f32[6304,3072] custom-call(..)\n+      //      bitcast = f32[32,197,3072] bitcast(fused)\n+      // clang-format on\n+      auto addend_dims = addend->shape().dimensions();\n+      auto dot_dims = dot->shape().dimensions();\n+      if (optional_dot_bitcast && addend_dims.size() != dot_dims.size()) {\n+        auto new_addend_shape = AdjustBinaryOperandShape(addend, dot->shape());\n+        if (new_addend_shape.ok()) {\n+          addend = addend->AddInstruction(\n+              HloInstruction::CreateBitcast(new_addend_shape.value(), addend));\n+        }\n+      }\n",
            "whole_hunk": "@@ -324,6 +324,20 @@ absl::StatusOr<Shape> AdjustBiasShape(const HloInstruction* broadcast_instr,\n   return new_shape;\n };\n \n+// Compute new shape for the binary operand when dot's outer dims\n+// are flattened/unflattened with respect to the binary operand dims.\n+// Adjusting the operand shape to the dot's shape enables fusion in oneDNN.\n+absl::StatusOr<Shape> AdjustBinaryOperandShape(\n+    const HloInstruction* operand_instr, const Shape& dot_shape) {\n+  if (ShapeUtil::ElementsIn(operand_instr->shape()) !=\n+      ShapeUtil::ElementsIn(dot_shape)) {\n+    return absl::CancelledError(\n+        \"Number of elements in operand and dot instruction do not match.\");\n+  }\n+  Shape new_shape = dot_shape;\n+  return new_shape;\n+};\n+\n inline bool IsOperandFusible(HloInstruction* operand, HloInstruction* dot) {\n   // Check if the operand's shape is compatible with matmul for fusion.\n   // An operand is fusable if\n@@ -353,11 +367,19 @@ inline auto OptionalConvertAndBitcast(HloInstruction** optional_convert,\n   //   1. pattern-root -> bf16/f16-to-fp32 convert -> bitcast\n   //   2. pattern-root -> bf16/f16-to-fp32 convert\n   //   3. pattern-root -> bitcast\n-  //   4. pattern-root\n+  //   4. pattern-root -> bitcast -> bf16-to-fp32 convert\n+  //   5. pattern-root\n   auto common = m::AnyOf<HloInstruction>(\n-      pu::SupportedConvert(optional_convert, std::move(pattern).WithOneUser())\n-          .WithElementType(PrimitiveType::F32),\n-      std::move(pattern).WithOneUser());\n+                    pu::SupportedConvert(optional_convert,\n+                                         std::move(pattern).WithOneUser())\n+                        .WithElementType(PrimitiveType::F32),\n+                    std::move(pattern).WithOneUser(),\n+                    pu::SupportedConvert(\n+                        optional_convert,\n+                        BitcastWithReshapeSemantics(\n+                            optional_bitcast, std::move(pattern).WithOneUser()))\n+                        .WithElementType(PrimitiveType::F32))\n+                    .WithOneUser();\n   return m::AnyOf<HloInstruction>(\n       BitcastWithReshapeSemantics(optional_bitcast, common), common);\n }\n@@ -499,19 +521,6 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n     if (Match(instr, pattern)) {\n       if (!IsSupportedType(dot->shape().element_type()))\n         return absl::OkStatus();\n-      // TODO(intel-tf): Remove the condition below when the fusion Dot +\n-      // Add(bias) + Add(e.g., residual) is enabled.\n-      if (!dot->backend_config<BackendConfig>()\n-               ->mutable_onednn_matmul_config()\n-               ->mutable_fusions()\n-               ->ops()\n-               .empty() &&\n-          dot->backend_config<BackendConfig>()\n-                  ->mutable_onednn_matmul_config()\n-                  ->mutable_fusions()\n-                  ->ops(0) == OneDnnFusionConfig::BIAS) {\n-        return absl::OkStatus();\n-      }\n       std::vector<HloInstruction*> new_operands;\n       for (auto operand : dot->operands()) {\n         new_operands.push_back(operand);\n@@ -550,6 +559,31 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n           return absl::OkStatus();\n         }\n       }\n+      // For cases where the dot is followed by a reshape, the binary operands\n+      // shape can be adjusted, making sure the number of elements match, to\n+      // enable the fusion. For example:\n+      //      dot = f32[6304,3072] dot(...)\n+      //      reshape = f32[32,197,3072] reshape(dot)\n+      //      constant = f32[32,197,3072] constant(..)\n+      //      add = f32[32,197,3072] add(reshape, constant)\n+      // can become\n+      //      dot = f32[6304,3072] dot(...)\n+      //      constant = f32[32,197,3072] constant(..)\n+      //      reshape1 = f32[6304,3072] reshape(constant)\n+      //      add = f32[6304,3072] add(dot, reshape1)\n+      // and be replaced with the fusion\n+      //      fused = f32[6304,3072] custom-call(..)\n+      //      bitcast = f32[32,197,3072] bitcast(fused)\n+      // clang-format on\n+      auto addend_dims = addend->shape().dimensions();\n+      auto dot_dims = dot->shape().dimensions();\n+      if (optional_dot_bitcast && addend_dims.size() != dot_dims.size()) {\n+        auto new_addend_shape = AdjustBinaryOperandShape(addend, dot->shape());\n+        if (new_addend_shape.ok()) {\n+          addend = addend->AddInstruction(\n+              HloInstruction::CreateBitcast(new_addend_shape.value(), addend));\n+        }\n+      }\n \n       // Validate addend for fusion.\n       if (IsSupportedType(addend->shape().element_type()) &&\n"
        },
        {
            "name": "onednn_matmul_test.cc",
            "path": "third_party/xla/xla/tests/onednn_matmul_test.cc",
            "patches": [
                {
                    "old_start": 131,
                    "old_length": 6,
                    "new_start": 131,
                    "new_length": 15,
                    "hunk": "@@ -131,6 +131,15 @@ class MatmulTest : public HloTestBase {\n     ; CHECK-DAG:     }\n     ; CHECK:     }\n     )\";\n+  const char* fused_matmul_bias_add_str_ = R\"(\n+    ; CHECK:     custom_call_target=\"__onednn$matmul\",\n+    ; CHECK:       backend_config={\n+    ; CHECK-DAG:     \"outer_dimension_partitions\":[],\n+    ; CHECK-DAG:     \"onednn_matmul_config\":{\n+    ; CHECK-DAG:       \"fused_ops\":[\"BIAS\",\"BINARY_ADD\"]\n+    ; CHECK-DAG:   }\n+    ; CHECK:     }\n+    )\";\n };\n \n TEST_F(MatmulTest, SimpleTestF32) {\n"
                },
                {
                    "old_start": 1539,
                    "old_length": 6,
                    "new_start": 1548,
                    "new_length": 103,
                    "hunk": "@@ -1539,6 +1548,103 @@ TEST_F(MatmulTest, ConsecutiveBinaryAdd) {\n   EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n }\n \n+TEST_F(MatmulTest, SimpleTestF32WithBiasAndAddFusion) {\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.bias.add.test.f32\n+  ENTRY matmul.bias.add.test.f32 {\n+    arg0.1 = f32[32,32,40,30] parameter(0), parameter_replication={false}\n+    arg0.2 = f32[32,32,30,40]parameter(1), parameter_replication={false}\n+    dot.7 = f32[32,32,40,40] dot(arg0.1, arg0.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+    const.0 = f32[40] constant(15)\n+    bcast.1 = f32[32,32,40,40] broadcast(const.0), dimensions={3}\n+    add.0 = f32[32,32,40,40] add(dot.7,bcast.1)\n+    const.1 = f32[32,32,40,40] constant(0.65)\n+    add.1 = f32[32,32,40,40] add(add.0, const.1)\n+    tuple.12 = (f32[32,32,40,40]) tuple(add.1)\n+    ROOT get-tuple-element.13 = f32[32,32,40,40] get-tuple-element(tuple.12), index=0\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_add_str_);\n+}\n+\n+TEST_F(MatmulTest, SimpleTestF32WithBiasAndAddFusion2) {\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.test.f32\n+  ENTRY matmul.test.f32 {\n+    arg.0 = f32[6304,768] parameter(0), parameter_replication={false}\n+    arg.1 = f32[768,3072] parameter(1), parameter_replication={false}\n+    dot.378 = f32[6304,3072] dot(arg.0, arg.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    reshape.11 = f32[32,197,3072] reshape(dot.378)\n+    constant.381 = f32[3072] constant(0.3)\n+    broadcast.382 = f32[32,197,3072] broadcast(constant.381), dimensions={2}\n+    add.0 = f32[32,197,3072] add(reshape.11, broadcast.382)\n+    const.1 = f32[32,197,3072] constant(0.65)\n+    add.1 = f32[32,197,3072] add(add.0, const.1)\n+    ROOT out = f32[6304,3072] reshape(add.1)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_add_str_);\n+}\n+\n+TEST_F(MatmulTest, SimpleTestF32WithAddFusion) {\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.test.f32\n+  ENTRY matmul.test.f32 {\n+    arg.0 = f32[6304,768] parameter(0), parameter_replication={false}\n+    arg.1 = f32[768,3072] parameter(1), parameter_replication={false}\n+    dot.378 = f32[6304,3072] dot(arg.0, arg.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    reshape.11 = f32[32,197,3072] reshape(dot.378)\n+    const.1 = f32[32,197,3072] constant(0.65)\n+    add.1 = f32[32,197,3072] add(reshape.11, const.1)\n+    ROOT out = f32[6304,3072] reshape(add.1)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str,\n+                    R\"(\n+  ; CHECK:     custom_call_target=\"__onednn$matmul\",\n+  ; CHECK:       backend_config={\n+  ; CHECK-DAG:     \"outer_dimension_partitions\":[],\n+  ; CHECK-DAG:     \"onednn_matmul_config\":{\n+  ; CHECK-DAG:       \"fused_ops\":[\"BINARY_ADD\"]\n+  ; CHECK-DAG:   }\n+  ; CHECK:     }\n+  )\");\n+}\n+\n+TEST_F(MatmulTest, SimpleTestF32WithAddFusion_2) {\n+  // Only the first Bias should get fused as Bias\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.add.test.f32\n+  ENTRY matmul.add.test.f32 {\n+    arg0.1 = f32[32,32,40,30] parameter(0), parameter_replication={false}\n+    arg0.2 = f32[32,32,30,40]parameter(1), parameter_replication={false}\n+    dot.7 = f32[32,32,40,40] dot(arg0.1, arg0.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+    const.0 = f32[40] constant(15)\n+    bcast.1 = f32[32,32,40,40] broadcast(const.0), dimensions={3}\n+    add.0 = f32[32,32,40,40] add(dot.7,bcast.1)\n+    const.1 = f32[40] constant(0.65)\n+    bcast.2 = f32[32,32,40,40] broadcast(const.1), dimensions={3}\n+    add.1 = f32[32,32,40,40] add(add.0, bcast.2)\n+    tuple.12 = (f32[32,32,40,40]) tuple(add.1)\n+    ROOT get-tuple-element.13 = f32[32,32,40,40] get-tuple-element(tuple.12), index=0\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str,\n+                    R\"(\n+  ; CHECK:     custom_call_target=\"__onednn$matmul\",\n+  ; CHECK:       backend_config={\n+  ; CHECK-DAG:     \"outer_dimension_partitions\":[],\n+  ; CHECK-DAG:     \"onednn_matmul_config\":{\n+  ; CHECK-DAG:       \"fused_ops\":[\"BIAS\",\"BINARY_ADD\"]\n+  ; CHECK-DAG:   }\n+  ; CHECK:     }\n+  )\");\n+}\n+\n TEST_F(MatmulTest, BroadcastedAddAfterFusion) {\n   const char* matmul_module_str = R\"(\n   HloModule matmul.nonscalar.test.1"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  const char* fused_matmul_bias_add_str_ = R\"(\n+    ; CHECK:     custom_call_target=\"__onednn$matmul\",\n+    ; CHECK:       backend_config={\n+    ; CHECK-DAG:     \"outer_dimension_partitions\":[],\n+    ; CHECK-DAG:     \"onednn_matmul_config\":{\n+    ; CHECK-DAG:       \"fused_ops\":[\"BIAS\",\"BINARY_ADD\"]\n+    ; CHECK-DAG:   }\n+    ; CHECK:     }\n+    )\";\n+TEST_F(MatmulTest, SimpleTestF32WithBiasAndAddFusion) {\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.bias.add.test.f32\n+  ENTRY matmul.bias.add.test.f32 {\n+    arg0.1 = f32[32,32,40,30] parameter(0), parameter_replication={false}\n+    arg0.2 = f32[32,32,30,40]parameter(1), parameter_replication={false}\n+    dot.7 = f32[32,32,40,40] dot(arg0.1, arg0.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+    const.0 = f32[40] constant(15)\n+    bcast.1 = f32[32,32,40,40] broadcast(const.0), dimensions={3}\n+    add.0 = f32[32,32,40,40] add(dot.7,bcast.1)\n+    const.1 = f32[32,32,40,40] constant(0.65)\n+    add.1 = f32[32,32,40,40] add(add.0, const.1)\n+    tuple.12 = (f32[32,32,40,40]) tuple(add.1)\n+    ROOT get-tuple-element.13 = f32[32,32,40,40] get-tuple-element(tuple.12), index=0\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_add_str_);\n+}\n+\n+TEST_F(MatmulTest, SimpleTestF32WithBiasAndAddFusion2) {\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.test.f32\n+  ENTRY matmul.test.f32 {\n+    arg.0 = f32[6304,768] parameter(0), parameter_replication={false}\n+    arg.1 = f32[768,3072] parameter(1), parameter_replication={false}\n+    dot.378 = f32[6304,3072] dot(arg.0, arg.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    reshape.11 = f32[32,197,3072] reshape(dot.378)\n+    constant.381 = f32[3072] constant(0.3)\n+    broadcast.382 = f32[32,197,3072] broadcast(constant.381), dimensions={2}\n+    add.0 = f32[32,197,3072] add(reshape.11, broadcast.382)\n+    const.1 = f32[32,197,3072] constant(0.65)\n+    add.1 = f32[32,197,3072] add(add.0, const.1)\n+    ROOT out = f32[6304,3072] reshape(add.1)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_add_str_);\n+}\n+\n+TEST_F(MatmulTest, SimpleTestF32WithAddFusion) {\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.test.f32\n+  ENTRY matmul.test.f32 {\n+    arg.0 = f32[6304,768] parameter(0), parameter_replication={false}\n+    arg.1 = f32[768,3072] parameter(1), parameter_replication={false}\n+    dot.378 = f32[6304,3072] dot(arg.0, arg.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    reshape.11 = f32[32,197,3072] reshape(dot.378)\n+    const.1 = f32[32,197,3072] constant(0.65)\n+    add.1 = f32[32,197,3072] add(reshape.11, const.1)\n+    ROOT out = f32[6304,3072] reshape(add.1)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str,\n+                    R\"(\n+  ; CHECK:     custom_call_target=\"__onednn$matmul\",\n+  ; CHECK:       backend_config={\n+  ; CHECK-DAG:     \"outer_dimension_partitions\":[],\n+  ; CHECK-DAG:     \"onednn_matmul_config\":{\n+  ; CHECK-DAG:       \"fused_ops\":[\"BINARY_ADD\"]\n+  ; CHECK-DAG:   }\n+  ; CHECK:     }\n+  )\");\n+}\n+\n+TEST_F(MatmulTest, SimpleTestF32WithAddFusion_2) {\n+  // Only the first Bias should get fused as Bias\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.add.test.f32\n+  ENTRY matmul.add.test.f32 {\n+    arg0.1 = f32[32,32,40,30] parameter(0), parameter_replication={false}\n+    arg0.2 = f32[32,32,30,40]parameter(1), parameter_replication={false}\n+    dot.7 = f32[32,32,40,40] dot(arg0.1, arg0.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+    const.0 = f32[40] constant(15)\n+    bcast.1 = f32[32,32,40,40] broadcast(const.0), dimensions={3}\n+    add.0 = f32[32,32,40,40] add(dot.7,bcast.1)\n+    const.1 = f32[40] constant(0.65)\n+    bcast.2 = f32[32,32,40,40] broadcast(const.1), dimensions={3}\n+    add.1 = f32[32,32,40,40] add(add.0, bcast.2)\n+    tuple.12 = (f32[32,32,40,40]) tuple(add.1)\n+    ROOT get-tuple-element.13 = f32[32,32,40,40] get-tuple-element(tuple.12), index=0\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str,\n+                    R\"(\n+  ; CHECK:     custom_call_target=\"__onednn$matmul\",\n+  ; CHECK:       backend_config={\n+  ; CHECK-DAG:     \"outer_dimension_partitions\":[],\n+  ; CHECK-DAG:     \"onednn_matmul_config\":{\n+  ; CHECK-DAG:       \"fused_ops\":[\"BIAS\",\"BINARY_ADD\"]\n+  ; CHECK-DAG:   }\n+  ; CHECK:     }\n+  )\");\n+}\n+\n",
            "whole_hunk": "@@ -131,6 +131,15 @@ class MatmulTest : public HloTestBase {\n     ; CHECK-DAG:     }\n     ; CHECK:     }\n     )\";\n+  const char* fused_matmul_bias_add_str_ = R\"(\n+    ; CHECK:     custom_call_target=\"__onednn$matmul\",\n+    ; CHECK:       backend_config={\n+    ; CHECK-DAG:     \"outer_dimension_partitions\":[],\n+    ; CHECK-DAG:     \"onednn_matmul_config\":{\n+    ; CHECK-DAG:       \"fused_ops\":[\"BIAS\",\"BINARY_ADD\"]\n+    ; CHECK-DAG:   }\n+    ; CHECK:     }\n+    )\";\n };\n \n TEST_F(MatmulTest, SimpleTestF32) {\n@@ -1539,6 +1548,103 @@ TEST_F(MatmulTest, ConsecutiveBinaryAdd) {\n   EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n }\n \n+TEST_F(MatmulTest, SimpleTestF32WithBiasAndAddFusion) {\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.bias.add.test.f32\n+  ENTRY matmul.bias.add.test.f32 {\n+    arg0.1 = f32[32,32,40,30] parameter(0), parameter_replication={false}\n+    arg0.2 = f32[32,32,30,40]parameter(1), parameter_replication={false}\n+    dot.7 = f32[32,32,40,40] dot(arg0.1, arg0.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+    const.0 = f32[40] constant(15)\n+    bcast.1 = f32[32,32,40,40] broadcast(const.0), dimensions={3}\n+    add.0 = f32[32,32,40,40] add(dot.7,bcast.1)\n+    const.1 = f32[32,32,40,40] constant(0.65)\n+    add.1 = f32[32,32,40,40] add(add.0, const.1)\n+    tuple.12 = (f32[32,32,40,40]) tuple(add.1)\n+    ROOT get-tuple-element.13 = f32[32,32,40,40] get-tuple-element(tuple.12), index=0\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_add_str_);\n+}\n+\n+TEST_F(MatmulTest, SimpleTestF32WithBiasAndAddFusion2) {\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.test.f32\n+  ENTRY matmul.test.f32 {\n+    arg.0 = f32[6304,768] parameter(0), parameter_replication={false}\n+    arg.1 = f32[768,3072] parameter(1), parameter_replication={false}\n+    dot.378 = f32[6304,3072] dot(arg.0, arg.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    reshape.11 = f32[32,197,3072] reshape(dot.378)\n+    constant.381 = f32[3072] constant(0.3)\n+    broadcast.382 = f32[32,197,3072] broadcast(constant.381), dimensions={2}\n+    add.0 = f32[32,197,3072] add(reshape.11, broadcast.382)\n+    const.1 = f32[32,197,3072] constant(0.65)\n+    add.1 = f32[32,197,3072] add(add.0, const.1)\n+    ROOT out = f32[6304,3072] reshape(add.1)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str, fused_matmul_bias_add_str_);\n+}\n+\n+TEST_F(MatmulTest, SimpleTestF32WithAddFusion) {\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.test.f32\n+  ENTRY matmul.test.f32 {\n+    arg.0 = f32[6304,768] parameter(0), parameter_replication={false}\n+    arg.1 = f32[768,3072] parameter(1), parameter_replication={false}\n+    dot.378 = f32[6304,3072] dot(arg.0, arg.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+    reshape.11 = f32[32,197,3072] reshape(dot.378)\n+    const.1 = f32[32,197,3072] constant(0.65)\n+    add.1 = f32[32,197,3072] add(reshape.11, const.1)\n+    ROOT out = f32[6304,3072] reshape(add.1)\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str,\n+                    R\"(\n+  ; CHECK:     custom_call_target=\"__onednn$matmul\",\n+  ; CHECK:       backend_config={\n+  ; CHECK-DAG:     \"outer_dimension_partitions\":[],\n+  ; CHECK-DAG:     \"onednn_matmul_config\":{\n+  ; CHECK-DAG:       \"fused_ops\":[\"BINARY_ADD\"]\n+  ; CHECK-DAG:   }\n+  ; CHECK:     }\n+  )\");\n+}\n+\n+TEST_F(MatmulTest, SimpleTestF32WithAddFusion_2) {\n+  // Only the first Bias should get fused as Bias\n+  const char* matmul_module_str = R\"(\n+  HloModule matmul.add.test.f32\n+  ENTRY matmul.add.test.f32 {\n+    arg0.1 = f32[32,32,40,30] parameter(0), parameter_replication={false}\n+    arg0.2 = f32[32,32,30,40]parameter(1), parameter_replication={false}\n+    dot.7 = f32[32,32,40,40] dot(arg0.1, arg0.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n+    const.0 = f32[40] constant(15)\n+    bcast.1 = f32[32,32,40,40] broadcast(const.0), dimensions={3}\n+    add.0 = f32[32,32,40,40] add(dot.7,bcast.1)\n+    const.1 = f32[40] constant(0.65)\n+    bcast.2 = f32[32,32,40,40] broadcast(const.1), dimensions={3}\n+    add.1 = f32[32,32,40,40] add(add.0, bcast.2)\n+    tuple.12 = (f32[32,32,40,40]) tuple(add.1)\n+    ROOT get-tuple-element.13 = f32[32,32,40,40] get-tuple-element(tuple.12), index=0\n+  })\";\n+\n+  EXPECT_TRUE(RunAndCompare(matmul_module_str, ErrorSpec{1e-4, 1e-4}));\n+  MatchOptimizedHlo(matmul_module_str,\n+                    R\"(\n+  ; CHECK:     custom_call_target=\"__onednn$matmul\",\n+  ; CHECK:       backend_config={\n+  ; CHECK-DAG:     \"outer_dimension_partitions\":[],\n+  ; CHECK-DAG:     \"onednn_matmul_config\":{\n+  ; CHECK-DAG:       \"fused_ops\":[\"BIAS\",\"BINARY_ADD\"]\n+  ; CHECK-DAG:   }\n+  ; CHECK:     }\n+  )\");\n+}\n+\n TEST_F(MatmulTest, BroadcastedAddAfterFusion) {\n   const char* matmul_module_str = R\"(\n   HloModule matmul.nonscalar.test.1"
        }
    ]
},
{
    "Id": 89,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/48cc5bba3e1c6b32488020840e27c46ea3131ed8",
    "date": "2024-03-27T17:32:10-07:00",
    "message": "Remove check that the first thread pool is the default, as it is no longer needed.\n\nPiperOrigin-RevId: 619704113",
    "label": "NO",
    "changes": [
        {
            "name": "tfrt_session.cc",
            "path": "tensorflow/core/tfrt/tfrt_session/tfrt_session.cc",
            "patches": [
                {
                    "old_start": 604,
                    "old_length": 15,
                    "new_start": 604,
                    "new_length": 6,
                    "hunk": "@@ -604,15 +604,6 @@ class TfrtSessionFactory::ThreadPoolManager {\n         auto pool_index = it.index();\n         auto num_threads = pool_options.num_threads();\n \n-        // For the current use cases the first thread pool is always the default\n-        // thread pool. We add this check here to verify the assumption. We can\n-        // remove this check once the code stablizes, since it is semantically\n-        // meaningful to use non-default thread pool as the first thread pool.\n-        if (pool_index == 0 && num_threads != 0) {\n-          return errors::InvalidArgument(\n-              \"The first thread pool must have num_threads = 0\");\n-        }\n-\n         if (num_threads != 0) {\n           TF_ASSIGN_OR_RETURN(\n               auto* thread_pool,"
                }
            ],
            "whole_deleted": "-        // For the current use cases the first thread pool is always the default\n-        // thread pool. We add this check here to verify the assumption. We can\n-        // remove this check once the code stablizes, since it is semantically\n-        // meaningful to use non-default thread pool as the first thread pool.\n-        if (pool_index == 0 && num_threads != 0) {\n-          return errors::InvalidArgument(\n-              \"The first thread pool must have num_threads = 0\");\n-        }\n-\n",
            "whole_added": "",
            "whole_hunk": "@@ -604,15 +604,6 @@ class TfrtSessionFactory::ThreadPoolManager {\n         auto pool_index = it.index();\n         auto num_threads = pool_options.num_threads();\n \n-        // For the current use cases the first thread pool is always the default\n-        // thread pool. We add this check here to verify the assumption. We can\n-        // remove this check once the code stablizes, since it is semantically\n-        // meaningful to use non-default thread pool as the first thread pool.\n-        if (pool_index == 0 && num_threads != 0) {\n-          return errors::InvalidArgument(\n-              \"The first thread pool must have num_threads = 0\");\n-        }\n-\n         if (num_threads != 0) {\n           TF_ASSIGN_OR_RETURN(\n               auto* thread_pool,"
        }
    ]
},
{
    "Id": 532,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e99e31597c1b5cc9f0cbc8a3dea71674d81c20b1",
    "date": "2023-02-15T12:44:47-08:00",
    "message": "Fix GRUCellBlockOp message for invalid rank of x\n\nThe validation checks that x is a matrix, so rank must be 2. ff45913\nfixed the crash in #58261 but left this typo in an exception message.\n\nFixes #58261\n\nSigned-off-by: Reid Wahl <nrwahl@protonmail.com>",
    "label": "YES",
    "changes": [
        {
            "name": "gru_ops.cc",
            "path": "tensorflow/core/kernels/rnn/gru_ops.cc",
            "patches": [
                {
                    "old_start": 53,
                    "old_length": 8,
                    "new_start": 53,
                    "new_length": 8,
                    "hunk": "@@ -53,8 +53,8 @@ class GRUCellBlockOp : public OpKernel {\n \n     // Shape of 'x' must be [batch_size, input_size]\n     OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(x_tensor->shape()),\n-                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\n-                                        \" vs. 2\"));\n+                errors::InvalidArgument(\"Rank of x must be 2, got \",\n+                                        x_tensor->dims()));\n     const int64_t batch_size = x_tensor->dim_size(0);\n     const int64_t input_size = x_tensor->dim_size(1);\n "
                }
            ],
            "whole_deleted": "-                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\n-                                        \" vs. 2\"));\n",
            "whole_added": "+                errors::InvalidArgument(\"Rank of x must be 2, got \",\n+                                        x_tensor->dims()));\n",
            "whole_hunk": "@@ -53,8 +53,8 @@ class GRUCellBlockOp : public OpKernel {\n \n     // Shape of 'x' must be [batch_size, input_size]\n     OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(x_tensor->shape()),\n-                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\n-                                        \" vs. 2\"));\n+                errors::InvalidArgument(\"Rank of x must be 2, got \",\n+                                        x_tensor->dims()));\n     const int64_t batch_size = x_tensor->dim_size(0);\n     const int64_t input_size = x_tensor->dim_size(1);\n "
        }
    ]
},
{
    "Id": 639,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/258233804f2bc92b4bdb9714b396aed34b53ff0d",
    "date": "2022-11-10T09:37:36-08:00",
    "message": "sanity check of empty tensor on avgpool3d_grad",
    "label": "YES",
    "changes": [
        {
            "name": "mkl_avgpooling_op.cc",
            "path": "tensorflow/core/kernels/mkl/mkl_avgpooling_op.cc",
            "patches": [
                {
                    "old_start": 193,
                    "old_length": 6,
                    "new_start": 193,
                    "new_length": 11,
                    "hunk": "@@ -193,6 +193,11 @@ class MklAvgPoolingGradOp : public MklPoolingBackwardOpBase<T> {\n       const Tensor& grad_tensor =\n           MklGetInput(context, kInputTensorIndexInputGradient);\n \n+      // For empty tensor, avg_pool_3d_grad in oneDNN doesn't handle this case\n+      if (orig_input_tensor.NumElements() == 0 ||\n+          grad_tensor.NumElements() == 0)\n+        return;\n+      \n       MklDnnShape orig_input_mkl_shape, grad_mkl_shape;\n       GetMklShape(context, kInputTensorIndexInputShape, &orig_input_mkl_shape,\n                   this->native_format_);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+      // For empty tensor, avg_pool_3d_grad in oneDNN doesn't handle this case\n+      if (orig_input_tensor.NumElements() == 0 ||\n+          grad_tensor.NumElements() == 0)\n+        return;\n+      \n",
            "whole_hunk": "@@ -193,6 +193,11 @@ class MklAvgPoolingGradOp : public MklPoolingBackwardOpBase<T> {\n       const Tensor& grad_tensor =\n           MklGetInput(context, kInputTensorIndexInputGradient);\n \n+      // For empty tensor, avg_pool_3d_grad in oneDNN doesn't handle this case\n+      if (orig_input_tensor.NumElements() == 0 ||\n+          grad_tensor.NumElements() == 0)\n+        return;\n+      \n       MklDnnShape orig_input_mkl_shape, grad_mkl_shape;\n       GetMklShape(context, kInputTensorIndexInputShape, &orig_input_mkl_shape,\n                   this->native_format_);"
        }
    ]
},
{
    "Id": 525,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/d669ad1858a759cc2f916a5da02c081b62546d52",
    "date": "2023-02-17T02:15:30+01:00",
    "message": "Fix shape values profile handling\n\n- Recognize if input tensor changes size, and mark it as non-shape tensor\n- Do not check shape value profiles for tensors that are not shape value",
    "label": "YES",
    "changes": [
        {
            "name": "trt_shape_optimization_profiles.cc",
            "path": "tensorflow/compiler/tf2tensorrt/utils/trt_shape_optimization_profiles.cc",
            "patches": [
                {
                    "old_start": 94,
                    "old_length": 7,
                    "new_start": 94,
                    "new_length": 8,
                    "hunk": "@@ -94,7 +94,8 @@ Status ShapeProfileBinaryOp(std::vector<nvinfer1::Dims>* x,\n   for (int i = 0; i < x->size(); i++) {\n     if (x->at(i).nbDims != y[i].nbDims)\n       return errors::InvalidArgument(\n-          \"Number of input dimensions differ during profile creation\");\n+          \"Number of input dimensions differ during profile creation at dim \",\n+          i, \", values \", x->at(i).nbDims, y[i].nbDims);\n     for (int j = 0; j < x->at(i).nbDims; j++) {\n       x->at(i).d[j] = op(x->at(i).d[j], y[i].d[j]);\n     }\n"
                },
                {
                    "old_start": 165,
                    "old_length": 6,
                    "new_start": 166,
                    "new_length": 12,
                    "hunk": "@@ -165,6 +166,12 @@ Status TrtShapeOptimizationProfile::CollectShapeValues(OpKernelContext* ctx) {\n         is_shape_tensor_[i] = false;\n         continue;\n       }\n+      if (input_shape_values_.size() > 0 &&\n+          input_shape_values_[0][i].nbDims != ctx->input(i).NumElements()) {\n+        // Shape tensor dims should not change. It must be a value tensor.\n+        is_shape_tensor_[i] = false;\n+        continue;\n+      }\n       // We have to copy the shape values to the host, because TRT's\n       // ExecutionContext::setInputShapeBinding expects a host pointer.\n       n_shape_val++;\n"
                },
                {
                    "old_start": 271,
                    "old_length": 6,
                    "new_start": 278,
                    "new_length": 11,
                    "hunk": "@@ -271,6 +278,11 @@ void TrtShapeOptimizationProfile::InitProfiles(\n     auto shape_vec = input_shapes_[i];\n     VLOG(2) << \"Initprofiles, processing shape \" << i;\n     if (!shape_vec.empty()) {\n+      // Correct for values that are mistakenly used as shape values\n+      for (int k = 0; k < input_shape_values_[i].size(); k++) {\n+        if (!is_shape_tensor_[k])\n+          input_shape_values_[i][k] = nvinfer1::Dims{0, {}};\n+      }\n       std::vector<nvinfer1::Dims> dimvec = GetDimVec(shape_vec);\n       dimvec.insert(dimvec.end(), input_shape_values_[i].begin(),\n                     input_shape_values_[i].end());\n"
                },
                {
                    "old_start": 481,
                    "old_length": 12,
                    "new_start": 493,
                    "new_length": 13,
                    "hunk": "@@ -481,12 +493,13 @@ int TrtShapeOptimizationProfile::GetProfileNumber(\n   // TODO(tfeher): Return the best profile not just the first compatible.\n   for (int i = 0; i < profiles_.size(); i++) {\n     if (profiles_[i].IncludesShapes(shapes, HasShapeTensor(),\n-                                    actual_shape_values_, is_pruned_input_)) {\n+                                    actual_shape_values_, is_pruned_input_,\n+                                    is_shape_tensor_)) {\n       return i;\n     }\n   }\n-  VLOG(1) << \"Profile not found for input shapes \" << DebugString(shapes)\n-          << \".\";\n+  VLOG(1) << \"Profile not found for input shapes \" << DebugString(shapes);\n+  VLOG(2) << \"  and shape values \" << DebugString(actual_shape_values_);\n   return -1;\n }\n \n"
                }
            ],
            "whole_deleted": "-          \"Number of input dimensions differ during profile creation\");\n-                                    actual_shape_values_, is_pruned_input_)) {\n-  VLOG(1) << \"Profile not found for input shapes \" << DebugString(shapes)\n-          << \".\";\n",
            "whole_added": "+          \"Number of input dimensions differ during profile creation at dim \",\n+          i, \", values \", x->at(i).nbDims, y[i].nbDims);\n+      if (input_shape_values_.size() > 0 &&\n+          input_shape_values_[0][i].nbDims != ctx->input(i).NumElements()) {\n+        // Shape tensor dims should not change. It must be a value tensor.\n+        is_shape_tensor_[i] = false;\n+        continue;\n+      }\n+      // Correct for values that are mistakenly used as shape values\n+      for (int k = 0; k < input_shape_values_[i].size(); k++) {\n+        if (!is_shape_tensor_[k])\n+          input_shape_values_[i][k] = nvinfer1::Dims{0, {}};\n+      }\n+                                    actual_shape_values_, is_pruned_input_,\n+                                    is_shape_tensor_)) {\n+  VLOG(1) << \"Profile not found for input shapes \" << DebugString(shapes);\n+  VLOG(2) << \"  and shape values \" << DebugString(actual_shape_values_);\n",
            "whole_hunk": "@@ -94,7 +94,8 @@ Status ShapeProfileBinaryOp(std::vector<nvinfer1::Dims>* x,\n   for (int i = 0; i < x->size(); i++) {\n     if (x->at(i).nbDims != y[i].nbDims)\n       return errors::InvalidArgument(\n-          \"Number of input dimensions differ during profile creation\");\n+          \"Number of input dimensions differ during profile creation at dim \",\n+          i, \", values \", x->at(i).nbDims, y[i].nbDims);\n     for (int j = 0; j < x->at(i).nbDims; j++) {\n       x->at(i).d[j] = op(x->at(i).d[j], y[i].d[j]);\n     }\n@@ -165,6 +166,12 @@ Status TrtShapeOptimizationProfile::CollectShapeValues(OpKernelContext* ctx) {\n         is_shape_tensor_[i] = false;\n         continue;\n       }\n+      if (input_shape_values_.size() > 0 &&\n+          input_shape_values_[0][i].nbDims != ctx->input(i).NumElements()) {\n+        // Shape tensor dims should not change. It must be a value tensor.\n+        is_shape_tensor_[i] = false;\n+        continue;\n+      }\n       // We have to copy the shape values to the host, because TRT's\n       // ExecutionContext::setInputShapeBinding expects a host pointer.\n       n_shape_val++;\n@@ -271,6 +278,11 @@ void TrtShapeOptimizationProfile::InitProfiles(\n     auto shape_vec = input_shapes_[i];\n     VLOG(2) << \"Initprofiles, processing shape \" << i;\n     if (!shape_vec.empty()) {\n+      // Correct for values that are mistakenly used as shape values\n+      for (int k = 0; k < input_shape_values_[i].size(); k++) {\n+        if (!is_shape_tensor_[k])\n+          input_shape_values_[i][k] = nvinfer1::Dims{0, {}};\n+      }\n       std::vector<nvinfer1::Dims> dimvec = GetDimVec(shape_vec);\n       dimvec.insert(dimvec.end(), input_shape_values_[i].begin(),\n                     input_shape_values_[i].end());\n@@ -481,12 +493,13 @@ int TrtShapeOptimizationProfile::GetProfileNumber(\n   // TODO(tfeher): Return the best profile not just the first compatible.\n   for (int i = 0; i < profiles_.size(); i++) {\n     if (profiles_[i].IncludesShapes(shapes, HasShapeTensor(),\n-                                    actual_shape_values_, is_pruned_input_)) {\n+                                    actual_shape_values_, is_pruned_input_,\n+                                    is_shape_tensor_)) {\n       return i;\n     }\n   }\n-  VLOG(1) << \"Profile not found for input shapes \" << DebugString(shapes)\n-          << \".\";\n+  VLOG(1) << \"Profile not found for input shapes \" << DebugString(shapes);\n+  VLOG(2) << \"  and shape values \" << DebugString(actual_shape_values_);\n   return -1;\n }\n \n"
        },
        {
            "name": "trt_shape_optimization_profiles.h",
            "path": "tensorflow/compiler/tf2tensorrt/utils/trt_shape_optimization_profiles.h",
            "patches": [
                {
                    "old_start": 137,
                    "old_length": 7,
                    "new_start": 137,
                    "new_length": 8,
                    "hunk": "@@ -137,7 +137,8 @@ struct OptimizationProfileConfig {\n   bool IncludesShapes(const std::vector<TensorShape>& shapes,\n                       bool has_shape_tensor,\n                       const std::vector<nvinfer1::Dims>& shape_values,\n-                      const std::vector<bool>& is_pruned_input) const {\n+                      const std::vector<bool>& is_pruned_input,\n+                      const std::vector<bool>& is_shape_tensor) const {\n     // min, max, and opt must have the same size which is already verified in\n     // SetDimensions.\n     if (min.size() != shapes.size() * 2 ||\n"
                },
                {
                    "old_start": 169,
                    "old_length": 7,
                    "new_start": 170,
                    "new_length": 7,
                    "hunk": "@@ -169,7 +170,7 @@ struct OptimizationProfileConfig {\n     if (has_shape_tensor) {\n       int offset = shapes.size();\n       for (int i = 0; i < shape_values.size(); i++) {\n-        if (is_pruned_input[i]) {\n+        if (is_pruned_input[i] || !is_shape_tensor[i]) {\n           continue;\n         }\n         auto shape_val = shape_values[i];\n"
                }
            ],
            "whole_deleted": "-                      const std::vector<bool>& is_pruned_input) const {\n-        if (is_pruned_input[i]) {\n",
            "whole_added": "+                      const std::vector<bool>& is_pruned_input,\n+                      const std::vector<bool>& is_shape_tensor) const {\n+        if (is_pruned_input[i] || !is_shape_tensor[i]) {\n",
            "whole_hunk": "@@ -137,7 +137,8 @@ struct OptimizationProfileConfig {\n   bool IncludesShapes(const std::vector<TensorShape>& shapes,\n                       bool has_shape_tensor,\n                       const std::vector<nvinfer1::Dims>& shape_values,\n-                      const std::vector<bool>& is_pruned_input) const {\n+                      const std::vector<bool>& is_pruned_input,\n+                      const std::vector<bool>& is_shape_tensor) const {\n     // min, max, and opt must have the same size which is already verified in\n     // SetDimensions.\n     if (min.size() != shapes.size() * 2 ||\n@@ -169,7 +170,7 @@ struct OptimizationProfileConfig {\n     if (has_shape_tensor) {\n       int offset = shapes.size();\n       for (int i = 0; i < shape_values.size(); i++) {\n-        if (is_pruned_input[i]) {\n+        if (is_pruned_input[i] || !is_shape_tensor[i]) {\n           continue;\n         }\n         auto shape_val = shape_values[i];\n"
        },
        {
            "name": "shape_output_test.py",
            "path": "tensorflow/python/compiler/tensorrt/test/shape_output_test.py",
            "patches": [
                {
                    "old_start": 234,
                    "old_length": 5,
                    "new_start": 234,
                    "new_length": 41,
                    "hunk": "@@ -234,5 +234,41 @@ class ShapeValueMaskTest(trt_test.TfTrtIntegrationTestBase):\n             run_params.precision_mode != \"INT8\", \"no calibration dynamic shape\")\n \n \n+class InputProfile(trt_test.TfTrtIntegrationTestBase):\n+  \"\"\" The shape profiles has to fit values of shape tensors, but for regular\n+  tensors the values do not matter. Here we test shape profile managment with\n+  an INT32 input tensor that is not a shape tensor. The extra inputs with\n+  dim=10 would trigger an error if we mistakenly treat it as shape tensors.\"\"\"\n+\n+  def setUp(self):\n+    super().setUp()\n+\n+    self.DisableNonTrtOptimizers()\n+\n+  def GraphFn(self, x):\n+    z = x * x + x + 1\n+    z = array_ops.identity(z, name=\"output_0\")\n+    return z\n+\n+  def GetParams(self):\n+    return self.BuildParamsWithMask(\n+        self.GraphFn,\n+        dtypes.int32, [[4]], [[4]],\n+        extra_inputs=[[[5]], [[10]]],\n+        extra_outputs=[[[5]], [[10]]],\n+        input_mask=[[False]],\n+        output_mask=[[False]])\n+\n+  def ExpectedEnginesToBuild(self, run_params):\n+    \"\"\"Returns the expected engines to build.\"\"\"\n+    return [\"TRTEngineOp_000\"]\n+\n+  def ShouldRunTest(self, run_params):\n+    # Shape op is only converted in dynamic shape mode.\n+    return (run_params.dynamic_shape and\n+            run_params.is_v2 and\n+            not trt_test.IsQuantizationMode(run_params.precision_mode),\n+            \"Test v2 dynamic_shapes without INT8\")\n+\n if __name__ == \"__main__\":\n   test.main()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+class InputProfile(trt_test.TfTrtIntegrationTestBase):\n+  \"\"\" The shape profiles has to fit values of shape tensors, but for regular\n+  tensors the values do not matter. Here we test shape profile managment with\n+  an INT32 input tensor that is not a shape tensor. The extra inputs with\n+  dim=10 would trigger an error if we mistakenly treat it as shape tensors.\"\"\"\n+\n+  def setUp(self):\n+    super().setUp()\n+\n+    self.DisableNonTrtOptimizers()\n+\n+  def GraphFn(self, x):\n+    z = x * x + x + 1\n+    z = array_ops.identity(z, name=\"output_0\")\n+    return z\n+\n+  def GetParams(self):\n+    return self.BuildParamsWithMask(\n+        self.GraphFn,\n+        dtypes.int32, [[4]], [[4]],\n+        extra_inputs=[[[5]], [[10]]],\n+        extra_outputs=[[[5]], [[10]]],\n+        input_mask=[[False]],\n+        output_mask=[[False]])\n+\n+  def ExpectedEnginesToBuild(self, run_params):\n+    \"\"\"Returns the expected engines to build.\"\"\"\n+    return [\"TRTEngineOp_000\"]\n+\n+  def ShouldRunTest(self, run_params):\n+    # Shape op is only converted in dynamic shape mode.\n+    return (run_params.dynamic_shape and\n+            run_params.is_v2 and\n+            not trt_test.IsQuantizationMode(run_params.precision_mode),\n+            \"Test v2 dynamic_shapes without INT8\")\n+\n",
            "whole_hunk": "@@ -234,5 +234,41 @@ class ShapeValueMaskTest(trt_test.TfTrtIntegrationTestBase):\n             run_params.precision_mode != \"INT8\", \"no calibration dynamic shape\")\n \n \n+class InputProfile(trt_test.TfTrtIntegrationTestBase):\n+  \"\"\" The shape profiles has to fit values of shape tensors, but for regular\n+  tensors the values do not matter. Here we test shape profile managment with\n+  an INT32 input tensor that is not a shape tensor. The extra inputs with\n+  dim=10 would trigger an error if we mistakenly treat it as shape tensors.\"\"\"\n+\n+  def setUp(self):\n+    super().setUp()\n+\n+    self.DisableNonTrtOptimizers()\n+\n+  def GraphFn(self, x):\n+    z = x * x + x + 1\n+    z = array_ops.identity(z, name=\"output_0\")\n+    return z\n+\n+  def GetParams(self):\n+    return self.BuildParamsWithMask(\n+        self.GraphFn,\n+        dtypes.int32, [[4]], [[4]],\n+        extra_inputs=[[[5]], [[10]]],\n+        extra_outputs=[[[5]], [[10]]],\n+        input_mask=[[False]],\n+        output_mask=[[False]])\n+\n+  def ExpectedEnginesToBuild(self, run_params):\n+    \"\"\"Returns the expected engines to build.\"\"\"\n+    return [\"TRTEngineOp_000\"]\n+\n+  def ShouldRunTest(self, run_params):\n+    # Shape op is only converted in dynamic shape mode.\n+    return (run_params.dynamic_shape and\n+            run_params.is_v2 and\n+            not trt_test.IsQuantizationMode(run_params.precision_mode),\n+            \"Test v2 dynamic_shapes without INT8\")\n+\n if __name__ == \"__main__\":\n   test.main()"
        }
    ]
},
{
    "Id": 409,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b40420ef4c2064b7eb5e26f9d1a1c6abdc3db0f4",
    "date": "2023-06-05T21:28:31-07:00",
    "message": "Adds zero checks for groups and filters_per_group\n\nPiperOrigin-RevId: 538064566",
    "label": "YES",
    "changes": [
        {
            "name": "conv.h",
            "path": "tensorflow/lite/kernels/internal/reference/conv.h",
            "patches": [
                {
                    "old_start": 56,
                    "old_length": 8,
                    "new_start": 56,
                    "new_length": 10,
                    "hunk": "@@ -56,8 +56,10 @@ inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n   const int filter_width = filter_shape.Dims(2);\n   const int filter_input_depth = filter_shape.Dims(3);\n   const int groups = input_depth / filter_input_depth;\n+  TFLITE_DCHECK_NE(groups, 0);\n   TFLITE_DCHECK_EQ(input_depth % filter_input_depth, 0);\n   const int filters_per_group = output_depth / groups;\n+  TFLITE_DCHECK_NE(filters_per_group, 0);\n   const int output_height = output_shape.Dims(1);\n   const int output_width = output_shape.Dims(2);\n \n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  TFLITE_DCHECK_NE(groups, 0);\n+  TFLITE_DCHECK_NE(filters_per_group, 0);\n",
            "whole_hunk": "@@ -56,8 +56,10 @@ inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,\n   const int filter_width = filter_shape.Dims(2);\n   const int filter_input_depth = filter_shape.Dims(3);\n   const int groups = input_depth / filter_input_depth;\n+  TFLITE_DCHECK_NE(groups, 0);\n   TFLITE_DCHECK_EQ(input_depth % filter_input_depth, 0);\n   const int filters_per_group = output_depth / groups;\n+  TFLITE_DCHECK_NE(filters_per_group, 0);\n   const int output_height = output_shape.Dims(1);\n   const int output_width = output_shape.Dims(2);\n \n"
        },
        {
            "name": "conv.h",
            "path": "tensorflow/lite/kernels/internal/reference/integer_ops/conv.h",
            "patches": [
                {
                    "old_start": 63,
                    "old_length": 8,
                    "new_start": 63,
                    "new_length": 10,
                    "hunk": "@@ -63,8 +63,10 @@ inline void ConvPerChannel(\n   const int filter_width = filter_shape.Dims(2);\n   const int filter_input_depth = filter_shape.Dims(3);\n   const int groups = input_depth / filter_input_depth;\n+  TFLITE_DCHECK_NE(groups, 0);\n   TFLITE_DCHECK_EQ(input_depth % filter_input_depth, 0);\n   const int filters_per_group = output_depth / groups;\n+  TFLITE_DCHECK_NE(filters_per_group, 0);\n   const int output_height = output_shape.Dims(1);\n   const int output_width = output_shape.Dims(2);\n   for (int batch = 0; batch < batches; ++batch) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+  TFLITE_DCHECK_NE(groups, 0);\n+  TFLITE_DCHECK_NE(filters_per_group, 0);\n",
            "whole_hunk": "@@ -63,8 +63,10 @@ inline void ConvPerChannel(\n   const int filter_width = filter_shape.Dims(2);\n   const int filter_input_depth = filter_shape.Dims(3);\n   const int groups = input_depth / filter_input_depth;\n+  TFLITE_DCHECK_NE(groups, 0);\n   TFLITE_DCHECK_EQ(input_depth % filter_input_depth, 0);\n   const int filters_per_group = output_depth / groups;\n+  TFLITE_DCHECK_NE(filters_per_group, 0);\n   const int output_height = output_shape.Dims(1);\n   const int output_width = output_shape.Dims(2);\n   for (int batch = 0; batch < batches; ++batch) {"
        }
    ]
},
{
    "Id": 140,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/77531d7d6e887ece06177d28cfac76accdde3e85",
    "date": "2024-02-28T14:32:31-08:00",
    "message": "[xla:gpu] cudnn_fused_mha_rewriter_test: consistently enforce version checks\n\nAlso add comments to clarify why we enforce these checks.\n\nWhile at it, centralize the setting of skip conditions.\n\nPiperOrigin-RevId: 611229558",
    "label": "NO",
    "changes": [
        {
            "name": "BUILD",
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": [
                {
                    "old_start": 4611,
                    "old_length": 10,
                    "new_start": 4611,
                    "new_length": 16,
                    "hunk": "@@ -4611,10 +4611,16 @@ cc_library(\n     ],\n )\n \n-xla_cc_test(\n+xla_test(\n     name = \"cudnn_fused_mha_rewriter_test\",\n     srcs = [\"cudnn_fused_mha_rewriter_test.cc\"],\n-    tags = tf_cuda_tests_tags(),\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-nvidia\",\n+    ]},\n+    backends = [\n+        \"gpu\",\n+    ],\n+    local_defines = if_cuda_is_configured([\"GOOGLE_CUDA=1\"]),\n     deps = [\n         \":backend_configs_cc\",\n         \":cublas_cudnn\",\n"
                },
                {
                    "old_start": 4643,
                    "old_length": 7,
                    "new_start": 4649,
                    "new_length": 10,
                    "hunk": "@@ -4643,7 +4649,10 @@ xla_cc_test(\n         \"@local_tsl//tsl/lib/core:status_test_util\",\n         \"@local_tsl//tsl/platform:statusor\",\n         \"@local_tsl//tsl/platform:test_main\",\n-    ],\n+    ] + if_cuda_is_configured([\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@local_config_cuda//cuda:cudnn_header\",\n+    ]),\n )\n \n xla_test(\n"
                }
            ],
            "whole_deleted": "-xla_cc_test(\n-    tags = tf_cuda_tests_tags(),\n-    ],\n",
            "whole_added": "+xla_test(\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-nvidia\",\n+    ]},\n+    backends = [\n+        \"gpu\",\n+    ],\n+    local_defines = if_cuda_is_configured([\"GOOGLE_CUDA=1\"]),\n+    ] + if_cuda_is_configured([\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@local_config_cuda//cuda:cudnn_header\",\n+    ]),\n",
            "whole_hunk": "@@ -4611,10 +4611,16 @@ cc_library(\n     ],\n )\n \n-xla_cc_test(\n+xla_test(\n     name = \"cudnn_fused_mha_rewriter_test\",\n     srcs = [\"cudnn_fused_mha_rewriter_test.cc\"],\n-    tags = tf_cuda_tests_tags(),\n+    backend_tags = {\"gpu\": [\n+        \"requires-gpu-nvidia\",\n+    ]},\n+    backends = [\n+        \"gpu\",\n+    ],\n+    local_defines = if_cuda_is_configured([\"GOOGLE_CUDA=1\"]),\n     deps = [\n         \":backend_configs_cc\",\n         \":cublas_cudnn\",\n@@ -4643,7 +4649,10 @@ xla_cc_test(\n         \"@local_tsl//tsl/lib/core:status_test_util\",\n         \"@local_tsl//tsl/platform:statusor\",\n         \"@local_tsl//tsl/platform:test_main\",\n-    ],\n+    ] + if_cuda_is_configured([\n+        \"@local_config_cuda//cuda:cuda_headers\",\n+        \"@local_config_cuda//cuda:cudnn_header\",\n+    ]),\n )\n \n xla_test(\n"
        },
        {
            "name": "cudnn_fused_mha_rewriter_test.cc",
            "path": "third_party/xla/xla/service/gpu/cudnn_fused_mha_rewriter_test.cc",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 6,
                    "new_start": 16,
                    "new_length": 7,
                    "hunk": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/service/gpu/cudnn_fused_mha_rewriter.h\"\n \n #include <cstddef>\n+#include <optional>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n"
                },
                {
                    "old_start": 44,
                    "old_length": 6,
                    "new_start": 45,
                    "new_length": 11,
                    "hunk": "@@ -44,6 +45,11 @@ limitations under the License.\n #include \"tsl/lib/core/status_test_util.h\"\n #include \"tsl/platform/statusor.h\"\n \n+#if GOOGLE_CUDA\n+#include \"third_party/gpus/cuda/include/cuda.h\"\n+#include \"third_party/gpus/cudnn/cudnn.h\"  // IWYU pragma: keep\n+#endif\n+\n namespace xla {\n namespace gpu {\n namespace {\n"
                },
                {
                    "old_start": 59,
                    "old_length": 6,
                    "new_start": 65,
                    "new_length": 13,
                    "hunk": "@@ -59,6 +65,13 @@ class CudnnFusedMhaRewriterTestHloTest : public HloTestBase {\n     return se::CudaComputeCapability(8, 0);\n   }\n \n+  se::CudaComputeCapability GetRealCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n+\n   se::dnn::VersionInfo GetCudnnVersion() {\n     // Fake a supported compute capability to run tests,\n     // we don't run any kernels in these tests so they should be safe\n"
                },
                {
                    "old_start": 90,
                    "old_length": 7,
                    "new_start": 103,
                    "new_length": 12,
                    "hunk": "@@ -90,7 +103,12 @@ class CudnnFusedMhaRewriterTestHloTest : public HloTestBase {\n   CudnnFusedMhaRewriterTestHloTest()\n       : HloTestBase(/*verifier_layout_sensitive=*/false,\n                     /*allow_mixed_precision_in_hlo_verifier=*/false,\n-                    /*instruction_can_change_layout_func=*/{}) {}\n+                    /*instruction_can_change_layout_func=*/{}) {\n+#if !defined(GOOGLE_CUDA) || CUDA_VERSION < 12000\n+    skip_reason_ = \"cuDNN fused MHA requires CUDA 12 or later.\";\n+    return;\n+#endif\n+  }\n \n  protected:\n   size_t CountFusedAttentionCall(HloModule* module, bool is_backward = false) {\n"
                },
                {
                    "old_start": 118,
                    "old_length": 10,
                    "new_start": 136,
                    "new_length": 38,
                    "hunk": "@@ -118,10 +136,38 @@ class CudnnFusedMhaRewriterTestHloTest : public HloTestBase {\n     config_with_fmha.set_debug_options(debug_options);\n     return config_with_fmha;\n   }\n+\n+  // Centralize skip checks in the constructor. Unfortunately we cannot call\n+  // GTEST_SKIP from the constructor. Instead, we set (if needed) `skip_reason`,\n+  // and then check it from all test fixtures.\n+  // An alternative would be to use the SetUp() override, but for this to be\n+  // correct we'd have to ensure that all the parents' SetUp() methods are\n+  // called, which is error prone.\n+  std::optional<absl::string_view> skip_reason_;\n };\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2Pattern) {\n-  const char* module_str = R\"(\n+class CudnnFusedMhaRewriterPipelineTest\n+    : public CudnnFusedMhaRewriterTestHloTest {\n+ public:\n+  CudnnFusedMhaRewriterPipelineTest() {\n+    if (skip_reason_) return;  // the parent might have set it.\n+#if !defined(GOOGLE_CUDA) || CUDNN_VERSION < 8800  // NOLINT\n+    skip_reason_ = \"Pipeline test requires cuDNN 8.8.0 or later.\";\n+    return;\n+#endif\n+    stream_executor::CudaComputeCapability cc = GetRealCudaComputeCapability();\n+    // Enforce capability minor == 0 because hardware with a non-zero minor\n+    // number typically has insufficient shared memory for cuDNN FMHA.\n+    if (!cc.IsAtLeastAmpere() || cc.minor != 0) {\n+      skip_reason_ =\n+          \"Pipeline test requires Nvidia AMPERE+ GPUs with minor \"\n+          \"compute capability == 0.\";\n+      return;\n+    }\n+  }\n+};\n+\n+constexpr absl::string_view hlo_BF16Bmm1Bmm2Pattern = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n ENTRY main.6 {\n   Arg_2.3 = bf16[16,16,256,64]{3,2,1,0} parameter(2)\n"
                },
                {
                    "old_start": 129,
                    "old_length": 13,
                    "new_start": 175,
                    "new_length": 13,
                    "hunk": "@@ -129,13 +175,13 @@ ENTRY main.6 {\n   Arg_1.2 = bf16[16,16,256,64]{3,2,1,0} parameter(1)\n   dot.0 = bf16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = bf16[16,16,256,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n-}\n-\n-\n-)\";\n+})\";\n \n+TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      auto m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(hlo_BF16Bmm1Bmm2Pattern, GetModuleConfig()));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n"
                },
                {
                    "old_start": 152,
                    "old_length": 12,
                    "new_start": 198,
                    "new_length": 17,
                    "hunk": "@@ -152,12 +198,17 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_EQ(config.dropout_rate(), 0.0);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, BF16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(hlo_BF16Bmm1Bmm2Pattern, GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n+\n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n       optimized_module->entry_computation()->root_instruction(),\n"
                },
                {
                    "old_start": 169,
                    "old_length": 11,
                    "new_start": 220,
                    "new_length": 9,
                    "hunk": "@@ -169,11 +220,9 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_EQ(config.dropout_rate(), 0.0);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2UncanonicalizedPattern) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view hlo_BF16Bmm1Bmm2UncanonicalizedPattern = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,64,256]{3,2,1,0}}\n \n ENTRY main.6 {\n"
                },
                {
                    "old_start": 182,
                    "old_length": 12,
                    "new_start": 231,
                    "new_length": 12,
                    "hunk": "@@ -182,12 +231,12 @@ ENTRY main.6 {\n   Arg_1.2 = bf16[16,16,256,64]{3,2,1,0} parameter(1)\n   dot.0 = bf16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = bf16[16,16,64,256]{3,2,1,0} dot(Arg_2.3, dot.0), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n-}\n-\n-\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2UncanonicalizedPattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(\n+                                      hlo_BF16Bmm1Bmm2UncanonicalizedPattern));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n"
                },
                {
                    "old_start": 204,
                    "old_length": 12,
                    "new_start": 253,
                    "new_length": 16,
                    "hunk": "@@ -204,12 +253,16 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_EQ(config.dropout_rate(), 0.0);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, BF16Bmm1Bmm2UncanonicalizedPattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1Bmm2UncanonicalizedPattern, GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(optimized_module->entry_computation()->root_instruction(),\n"
                },
                {
                    "old_start": 222,
                    "old_length": 12,
                    "new_start": 275,
                    "new_length": 10,
                    "hunk": "@@ -222,12 +275,10 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_EQ(config.dropout_rate(), 0.0);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view\n+    hlo_BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n ENTRY main.6 {\n"
                },
                {
                    "old_start": 236,
                    "old_length": 10,
                    "new_start": 287,
                    "new_length": 15,
                    "hunk": "@@ -236,10 +287,15 @@ ENTRY main.6 {\n   Arg_1.2 = bf16[16,16,256,64]{2,3,1,0} parameter(1)\n   dot.0 = bf16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = bf16[16,16,256,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n-}\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK_AND_ASSIGN(bool result, RunHloPass(&fusedMhaRewriter, m.get()));\n"
                },
                {
                    "old_start": 257,
                    "old_length": 12,
                    "new_start": 313,
                    "new_length": 19,
                    "hunk": "@@ -257,12 +313,19 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.bmm1_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             2);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor,\n+          GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n"
                },
                {
                    "old_start": 275,
                    "old_length": 12,
                    "new_start": 338,
                    "new_length": 10,
                    "hunk": "@@ -275,12 +338,10 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.bmm1_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             2);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view\n+    hlo_BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n ENTRY main.6 {\n"
                },
                {
                    "old_start": 289,
                    "old_length": 10,
                    "new_start": 350,
                    "new_length": 15,
                    "hunk": "@@ -289,10 +350,15 @@ ENTRY main.6 {\n   Arg_1.2 = bf16[16,16,256,64]{2,3,1,0} parameter(1)\n   dot.0 = bf16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = bf16[16,16,256,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n-}\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK_AND_ASSIGN(bool result, RunHloPass(&fusedMhaRewriter, m.get()));\n"
                },
                {
                    "old_start": 312,
                    "old_length": 12,
                    "new_start": 378,
                    "new_length": 19,
                    "hunk": "@@ -312,12 +378,19 @@ ENTRY main.6 {\n             2);\n   EXPECT_EQ(config.bmm1_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             2);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor,\n+          GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n"
                },
                {
                    "old_start": 332,
                    "old_length": 12,
                    "new_start": 405,
                    "new_length": 10,
                    "hunk": "@@ -332,12 +405,10 @@ ENTRY main.6 {\n             2);\n   EXPECT_EQ(config.bmm1_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             2);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view\n+    hlo_BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n ENTRY main.6 {\n"
                },
                {
                    "old_start": 346,
                    "old_length": 10,
                    "new_start": 417,
                    "new_length": 15,
                    "hunk": "@@ -346,10 +417,15 @@ ENTRY main.6 {\n   Arg_1.2 = bf16[16,16,256,64]{2,3,1,0} parameter(1)\n   dot.0 = bf16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = bf16[16,16,256,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n-}\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK_AND_ASSIGN(bool result, RunHloPass(&fusedMhaRewriter, m.get()));\n"
                },
                {
                    "old_start": 369,
                    "old_length": 12,
                    "new_start": 445,
                    "new_length": 19,
                    "hunk": "@@ -369,12 +445,19 @@ ENTRY main.6 {\n             3);\n   EXPECT_EQ(config.bmm2_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             3);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor,\n+          GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n"
                },
                {
                    "old_start": 389,
                    "old_length": 11,
                    "new_start": 472,
                    "new_length": 9,
                    "hunk": "@@ -389,11 +472,9 @@ ENTRY main.6 {\n             3);\n   EXPECT_EQ(config.bmm2_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             3);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest, F16Bmm1Bmm2Pattern) {\n-  const char* module_str = R\"(\n+absl::string_view F16Bmm1Bmm2Pattern_str = R\"(\n HloModule fmha_test, entry_computation_layout={(f16[16,16,256,64]{3,2,1,0},f16[16,16,256,64]{3,2,1,0},f16[16,16,256,64]{3,2,1,0})->f16[16,16,256,64]{3,2,1,0}}\n ENTRY main.6 {\n   Arg_2.3 = f16[16,16,256,64]{3,2,1,0} parameter(2)\n"
                },
                {
                    "old_start": 401,
                    "old_length": 13,
                    "new_start": 482,
                    "new_length": 13,
                    "hunk": "@@ -401,13 +482,13 @@ ENTRY main.6 {\n   Arg_1.2 = f16[16,16,256,64]{3,2,1,0} parameter(1)\n   dot.0 = f16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = f16[16,16,256,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n-}\n-\n-\n-)\";\n+})\";\n \n+TEST_F(CudnnFusedMhaRewriterTestHloTest, F16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      auto m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(F16Bmm1Bmm2Pattern_str, GetModuleConfig()));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n"
                },
                {
                    "old_start": 424,
                    "old_length": 12,
                    "new_start": 505,
                    "new_length": 16,
                    "hunk": "@@ -424,12 +505,16 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_EQ(config.dropout_rate(), 0.0);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, F16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(F16Bmm1Bmm2Pattern_str, GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n"
                },
                {
                    "old_start": 442,
                    "old_length": 11,
                    "new_start": 527,
                    "new_length": 9,
                    "hunk": "@@ -442,11 +527,9 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1ScaleMaskSoftmaxBmm2Pattern) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view hlo_BF16Bmm1ScaleMaskSoftmaxBmm2Pattern = R\"(\n HloModule jit_bmm_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n region_0.14.clone {\n"
                },
                {
                    "old_start": 487,
                    "old_length": 11,
                    "new_start": 570,
                    "new_length": 12,
                    "hunk": "@@ -487,11 +570,12 @@ ENTRY main.38 {\n   convert.49 = bf16[16,16,256,256]{3,2,1,0} convert(divide.36)\n   Arg_2.3 = bf16[16,16,256,64]{3,2,1,0} parameter(2)\n   ROOT dot.37 = bf16[16,16,256,64]{3,2,1,0} dot(convert.49, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n-}\n-\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1ScaleMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(\n+                                      hlo_BF16Bmm1ScaleMaskSoftmaxBmm2Pattern));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n"
                },
                {
                    "old_start": 510,
                    "old_length": 13,
                    "new_start": 594,
                    "new_length": 16,
                    "hunk": "@@ -510,13 +594,16 @@ ENTRY main.38 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 2.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 4);\n+}\n \n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, BF16Bmm1ScaleMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1ScaleMaskSoftmaxBmm2Pattern, GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n"
                },
                {
                    "old_start": 531,
                    "old_length": 12,
                    "new_start": 618,
                    "new_length": 9,
                    "hunk": "@@ -531,12 +618,9 @@ ENTRY main.38 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 2.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 4);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view hlo_BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern = R\"(\n HloModule jit_bmm_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n region_0.17.clone {\n"
                },
                {
                    "old_start": 580,
                    "old_length": 11,
                    "new_start": 664,
                    "new_length": 14,
                    "hunk": "@@ -580,11 +664,14 @@ ENTRY main.41 {\n   convert.49 = bf16[16,16,256,256]{3,2,1,0} convert(divide.36)\n   Arg_2.3 = bf16[16,16,256,64]{3,2,1,0} parameter(2)\n   ROOT dot.37 = bf16[16,16,256,64]{3,2,1,0} dot(convert.49, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n-}\n-\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(auto m,\n+                          ParseAndReturnVerifiedModule(\n+                              hlo_BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n"
                },
                {
                    "old_start": 604,
                    "old_length": 13,
                    "new_start": 691,
                    "new_length": 18,
                    "hunk": "@@ -604,13 +691,18 @@ ENTRY main.41 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 3.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 5);\n+}\n \n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(hlo_BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern,\n+                                   GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n"
                },
                {
                    "old_start": 626,
                    "old_length": 12,
                    "new_start": 718,
                    "new_length": 10,
                    "hunk": "@@ -626,12 +718,10 @@ ENTRY main.41 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 3.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 5);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view\n+    hlo_BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern = R\"(\n HloModule jit_bmm_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n region_0.17.clone {\n"
                },
                {
                    "old_start": 676,
                    "old_length": 11,
                    "new_start": 766,
                    "new_length": 14,
                    "hunk": "@@ -676,11 +766,14 @@ ENTRY main.41 {\n   convert.49 = bf16[16,16,256,256]{3,2,1,0} convert(divide.36)\n   Arg_2.3 = bf16[16,16,256,64]{3,2,1,0} parameter(2)\n   ROOT dot.37 = bf16[16,16,256,64]{3,2,1,0} dot(convert.49, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n-}\n-\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n"
                },
                {
                    "old_start": 702,
                    "old_length": 13,
                    "new_start": 795,
                    "new_length": 18,
                    "hunk": "@@ -702,13 +795,18 @@ ENTRY main.41 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 3.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 5);\n+}\n \n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern,\n+                  GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n"
                },
                {
                    "old_start": 724,
                    "old_length": 10,
                    "new_start": 822,
                    "new_length": 10,
                    "hunk": "@@ -724,10 +822,10 @@ ENTRY main.41 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 3.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 5);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1CombinedMaskBiasSoftmaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_,\n entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0})->bf16[16,256,16,64]{3,2,1,0}}\n"
                },
                {
                    "old_start": 809,
                    "old_length": 6,
                    "new_start": 907,
                    "new_length": 7,
                    "hunk": "@@ -809,6 +907,7 @@ ENTRY main.61 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16Bmm1ScaleBiasMaskSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,40,64]{3,2,1,0},f16[2,6,64,40]{3,2,1,0},f16[2,6,40,64]{3,2,1,0})->f16[2,6,40,64]{3,2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n \n"
                },
                {
                    "old_start": 917,
                    "old_length": 6,
                    "new_start": 1016,
                    "new_length": 7,
                    "hunk": "@@ -917,6 +1016,7 @@ ENTRY main.83 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, F16Bmm1UnfusedSoftmaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,40,64]{3,2,1,0},f16[2,6,64,40]{3,2,1,0},f16[2,6,40,64]{3,2,1,0})->f16[2,6,40,64]{3,2,1,0}}\n \n"
                },
                {
                    "old_start": 973,
                    "old_length": 6,
                    "new_start": 1073,
                    "new_length": 7,
                    "hunk": "@@ -973,6 +1073,7 @@ ENTRY main.31 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16Bmm1UnfusedSoftmaxWithConvertF32ToReduceMaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[128,6,400,64]{3,2,1,0},f16[128,6,64,400]{3,2,1,0},f16[128,6,400,64]{3,2,1,0})->f16[128,6,400,64]{3,2,1,0}}\n \n"
                },
                {
                    "old_start": 1042,
                    "old_length": 6,
                    "new_start": 1143,
                    "new_length": 7,
                    "hunk": "@@ -1042,6 +1143,7 @@ ENTRY main.41 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1UnfusedScaleMaskBiasSoftmaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0})->bf16[16,256,16,64]{3,2,1,0}}\n \n"
                },
                {
                    "old_start": 1127,
                    "old_length": 6,
                    "new_start": 1229,
                    "new_length": 7,
                    "hunk": "@@ -1127,6 +1229,7 @@ ENTRY main.61 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1ConvertedMaskAddedAfterFirstGemmSoftmaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},pred[16,1,256,256]{3,2,1,0})->bf16[16,256,16,64]{3,2,1,0}}\n \n"
                },
                {
                    "old_start": 1203,
                    "old_length": 6,
                    "new_start": 1306,
                    "new_length": 7,
                    "hunk": "@@ -1203,6 +1306,7 @@ ENTRY main.56 {\n // negative test\n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1Bmm2Pattern_bmm1_contracting_dim_not_equal_64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,32]{3,2,1,0},bf16[16,16,256,32]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n ENTRY main.6 {\n"
                },
                {
                    "old_start": 1230,
                    "old_length": 6,
                    "new_start": 1334,
                    "new_length": 7,
                    "hunk": "@@ -1230,6 +1334,7 @@ ENTRY main.6 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1Bmm2Pattern_bmm1_non_contracting_dim_larger_than_512) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,1024,64]{3,2,1,0},bf16[16,16,1024,64]{3,2,1,0},bf16[16,16,1024,64]{3,2,1,0})->bf16[16,16,1024,64]{3,2,1,0}}\n ENTRY main.6 {\n"
                },
                {
                    "old_start": 1256,
                    "old_length": 6,
                    "new_start": 1361,
                    "new_length": 7,
                    "hunk": "@@ -1256,6 +1361,7 @@ ENTRY main.6 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1Bmm2Pattern_bmm2_rhs_non_contracting_dim_not_equal_64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,32]{3,2,1,0})->bf16[16,16,256,32]{3,2,1,0}}\n ENTRY main.6 {\n"
                },
                {
                    "old_start": 1283,
                    "old_length": 6,
                    "new_start": 1389,
                    "new_length": 7,
                    "hunk": "@@ -1283,6 +1389,7 @@ ENTRY main.6 {\n // check if MHA is unsupported, canonicalization will not kick in\n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1Bmm2PatternUncanonicalized_bmm1_contracting_dim_not_equal_64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,32]{3,2,1,0},bf16[16,16,256,32]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,64,256]{3,2,1,0}}\n \n"
                },
                {
                    "old_start": 1310,
                    "old_length": 6,
                    "new_start": 1417,
                    "new_length": 7,
                    "hunk": "@@ -1310,6 +1417,7 @@ ENTRY main.6 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1BiasSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[1,16,256,256]{3,2,1,0})->bf16[16,256,16,64]{3,2,1,0}}\n \n"
                },
                {
                    "old_start": 1422,
                    "old_length": 6,
                    "new_start": 1530,
                    "new_length": 7,
                    "hunk": "@@ -1422,6 +1530,7 @@ ENTRY main.82 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1ScaleBiasSoftmaxDropoutForm2Bmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[32,40,60,64]{3,2,1,0},bf16[32,40,60,64]{3,2,1,0},bf16[32,40,60,64]{3,2,1,0})->bf16[32,40,60,64]{3,2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n \n"
                },
                {
                    "old_start": 1531,
                    "old_length": 6,
                    "new_start": 1640,
                    "new_length": 7,
                    "hunk": "@@ -1531,6 +1640,7 @@ ENTRY main.79 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16TrainingBmm1Bmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0})->(bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0})}\n \n"
                },
                {
                    "old_start": 1584,
                    "old_length": 6,
                    "new_start": 1694,
                    "new_length": 7,
                    "hunk": "@@ -1584,6 +1694,7 @@ ENTRY main.17 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16TrainingBmm1ScaleBiasSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0})->(bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[1,16,256,256]{3,2,1,0})}\n \n"
                },
                {
                    "old_start": 1777,
                    "old_length": 6,
                    "new_start": 1888,
                    "new_length": 7,
                    "hunk": "@@ -1777,6 +1888,7 @@ ENTRY main.146 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[16,256,16,64]{3,2,1,0},f16[16,256,16,64]{3,2,1,0},f16[16,256,16,64]{3,2,1,0},f16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0},f16[16,256,16,64]{3,2,1,0})->(f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[1,16,256,256]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true,true}\n \n"
                },
                {
                    "old_start": 1968,
                    "old_length": 6,
                    "new_start": 2080,
                    "new_length": 7,
                    "hunk": "@@ -1968,6 +2080,7 @@ ENTRY main.146 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasSoftmaxDropoutBmm2WithTransposeFusion) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[16,256,16,64]{3,2,1,0},f16[16,256,16,64]{3,2,1,0},f16[16,256,16,64]{3,2,1,0},f16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0},f16[16,256,16,64]{3,2,1,0})->(f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[1,16,256,256]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true,true}\n \n"
                },
                {
                    "old_start": 2180,
                    "old_length": 6,
                    "new_start": 2293,
                    "new_length": 7,
                    "hunk": "@@ -2180,6 +2293,7 @@ ENTRY main.146 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16MiniT5xTest) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__lambda_, entry_computation_layout={(bf16[12,512,32,64]{3,2,1,0},bf16[12,512,2,32,64]{4,3,2,1,0},f32[12,512]{1,0},f32[12,512]{1,0})->(bf16[], bf16[12,512,32,64]{3,2,1,0}, bf16[12,512,2,32,64]{4,3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true}\n \n"
                },
                {
                    "old_start": 2329,
                    "old_length": 6,
                    "new_start": 2443,
                    "new_length": 7,
                    "hunk": "@@ -2329,6 +2443,7 @@ ENTRY main.129 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16TrainingBmm1ScaleBiasMaskSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,128,64]{3,2,1,0},bf16[2,6,64,128]{3,2,1,0},bf16[2,6,128,64]{3,2,1,0},bf16[2,6,128,64]{3,2,1,0})->(bf16[2,6,128,64]{3,2,1,0}, bf16[2,6,128,64]{3,2,1,0}, bf16[2,6,64,128]{3,2,1,0}, bf16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n"
                },
                {
                    "old_start": 2480,
                    "old_length": 6,
                    "new_start": 2595,
                    "new_length": 7,
                    "hunk": "@@ -2480,6 +2595,7 @@ ENTRY main.126 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasMaskSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,128,64]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n"
                },
                {
                    "old_start": 2632,
                    "old_length": 6,
                    "new_start": 2748,
                    "new_length": 7,
                    "hunk": "@@ -2632,6 +2748,7 @@ ENTRY main.126 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasMaskSoftmaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,128,64]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n"
                },
                {
                    "old_start": 2748,
                    "old_length": 6,
                    "new_start": 2865,
                    "new_length": 7,
                    "hunk": "@@ -2748,6 +2865,7 @@ ENTRY main.82 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16TrainingBmm1ScaleBiasSoftmaxDropoutBmm2DbiasShouldHaveUserShape) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0})->(bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[1,16,256,256]{3,2,1,0})}\n \n"
                },
                {
                    "old_start": 2943,
                    "old_length": 6,
                    "new_start": 3061,
                    "new_length": 7,
                    "hunk": "@@ -2943,6 +3061,7 @@ ENTRY main.146 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        ActivationHasMoreThan1UserShouldNotLower) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule test\n \n"
                },
                {
                    "old_start": 3001,
                    "old_length": 6,
                    "new_start": 3120,
                    "new_length": 7,
                    "hunk": "@@ -3001,6 +3120,7 @@ ENTRY main {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16InvalidTrainingBmm1ScaleBiasMaskSoftmaxBmm2ShouldNotBeLowered) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,128,64]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n"
                },
                {
                    "old_start": 3094,
                    "old_length": 6,
                    "new_start": 3214,
                    "new_length": 7,
                    "hunk": "@@ -3094,6 +3214,7 @@ ENTRY main.82 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16InvalidTrainingBmm1ScaleBiasMaskSoftmaxDropoutBmm2ShouldNotLower) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,128,64]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n"
                },
                {
                    "old_start": 3230,
                    "old_length": 6,
                    "new_start": 3351,
                    "new_length": 7,
                    "hunk": "@@ -3230,6 +3351,7 @@ ENTRY main.126 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasSoftmaxBmm2QTranspose) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,64,128]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n"
                },
                {
                    "old_start": 3339,
                    "old_length": 6,
                    "new_start": 3461,
                    "new_length": 7,
                    "hunk": "@@ -3339,6 +3461,7 @@ ENTRY main.82 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16Bmm1UnfusedSoftmaxBmm2IncorrectBmm1NumUsers) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,40,64]{3,2,1,0},f16[2,6,64,40]{3,2,1,0},f16[2,6,40,64]{3,2,1,0})->(f16[2,6,40,64]{3,2,1,0}, f16[2,6,40,40]{3,2,1,0})}\n \n"
                },
                {
                    "old_start": 3388,
                    "old_length": 6,
                    "new_start": 3511,
                    "new_length": 7,
                    "hunk": "@@ -3388,6 +3511,7 @@ ENTRY main.31 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16Bmm1UnfusedSoftmaxBmm2IncorrectSoftmaxNumUsers) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,40,64]{3,2,1,0},f16[2,6,64,40]{3,2,1,0},f16[2,6,40,64]{3,2,1,0})->(f16[2,6,40,64]{3,2,1,0}, f16[2,6,40,40]{3,2,1,0})}\n \n"
                },
                {
                    "old_start": 3437,
                    "old_length": 6,
                    "new_start": 3561,
                    "new_length": 7,
                    "hunk": "@@ -3437,6 +3561,7 @@ ENTRY main.31 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasSoftmaxBmm2IncorrectSoftmaxBwdNumUsers) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,64,128]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,128]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n"
                },
                {
                    "old_start": 3524,
                    "old_length": 6,
                    "new_start": 3649,
                    "new_length": 7,
                    "hunk": "@@ -3524,6 +3649,7 @@ ENTRY main.82 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, F16Bmm1SoftmaxBmm2IncorrectRank) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule reproducer, entry_computation_layout={(f16[1,8,16,5,128]{4,3,2,1,0}, f16[1,8,16,5,128]{4,3,2,1,0}, f16[1,8,16,5,128]{4,3,2,1,0}, f32[128,2,64]{2,1,0}, f32[2,64]{1,0}, /*index=5*/f32[128,2,64]{2,1,0}, f32[2,64]{1,0}, f32[128,2,64]{2,1,0}, f32[2,64]{1,0})->f16[8,16,2,5,64]{4,3,2,1,0}}\n \n"
                },
                {
                    "old_start": 3619,
                    "old_length": 6,
                    "new_start": 3745,
                    "new_length": 7,
                    "hunk": "@@ -3619,6 +3745,7 @@ ENTRY main {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasSoftmaxBmm2NonContractingDimNotDivisibleBy64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,64,100]{3,2,1,0},f16[2,6,64,100]{3,2,1,0},f16[2,6,100,64]{3,2,1,0},f16[2,6,100,64]{3,2,1,0})->(f16[2,6,100,64]{3,2,1,0}, f16[2,6,100,64]{3,2,1,0}, f16[2,6,64,100]{3,2,1,0}, f16[2,6,100,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n"
                },
                {
                    "old_start": 3706,
                    "old_length": 6,
                    "new_start": 3833,
                    "new_length": 7,
                    "hunk": "@@ -3706,6 +3833,7 @@ ENTRY main.82 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, F16TrainingBmm2Grad1IncorrectPattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,64,128]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,128]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n"
                },
                {
                    "old_start": 3795,
                    "old_length": 6,
                    "new_start": 3923,
                    "new_length": 7,
                    "hunk": "@@ -3795,6 +3923,7 @@ ENTRY main.82 {\n // flash attention\n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        FlashAttentionBF16TrainingBmm1CausalMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,128,2048]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0})->(bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,128,2048]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n region_0.32 {\n"
                },
                {
                    "old_start": 3907,
                    "old_length": 6,
                    "new_start": 4036,
                    "new_length": 7,
                    "hunk": "@@ -3907,6 +4036,7 @@ ENTRY main.92 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        FlashAttentionBF16TrainingBmm1BiasSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,128,2048]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,2048,2048]{3,2,1,0})->(bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,128,2048]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n region_0.32 {\n"
                },
                {
                    "old_start": 4014,
                    "old_length": 6,
                    "new_start": 4144,
                    "new_length": 7,
                    "hunk": "@@ -4014,6 +4144,7 @@ ENTRY main.92 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        FlashAttentionBF16TrainingBmm1SoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,128,2048]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0})->(bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,128,2048]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n region_0.32 {\n"
                },
                {
                    "old_start": 4116,
                    "old_length": 6,
                    "new_start": 4247,
                    "new_length": 7,
                    "hunk": "@@ -4116,6 +4247,7 @@ ENTRY main.92 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        FlashAttentionBF16TrainingBmm1ScaleMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,64]{3,2,1,0},bf16[2,6,64,2048]{3,2,1,0},bf16[2,6,2048,64]{3,2,1,0},bf16[2,6,2048,64]{3,2,1,0})->(bf16[2,6,2048,64]{3,2,1,0}, bf16[2,6,2048,64]{3,2,1,0}, bf16[2,6,64,2048]{3,2,1,0}, bf16[2,6,2048,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n"
                },
                {
                    "old_start": 4228,
                    "old_length": 6,
                    "new_start": 4360,
                    "new_length": 7,
                    "hunk": "@@ -4228,6 +4360,7 @@ ENTRY main.82 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        FlashAttentionF16Bmm1BiasSoftmaxBmm2PatternCrossAttention) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,2048,64]{3,2,1,0},f16[2,6,64,1024]{3,2,1,0},f16[2,6,1024,64]{3,2,1,0},f16[2,6,2048,1024]{3,2,1,0})->f16[2,6,2048,64]{3,2,1,0}}\n \n"
                },
                {
                    "old_start": 4294,
                    "old_length": 6,
                    "new_start": 4427,
                    "new_length": 7,
                    "hunk": "@@ -4294,6 +4427,7 @@ ENTRY main.31 {\n \n // GPT3 pattern\n TEST_F(CudnnFusedMhaRewriterTestHloTest, FlashAttentionBF16TrainingGPT3_5B) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={((s32[], bf16[32,2048,2048]{1,0,2}, bf16[24,8192]{1,0}, bf16[24,1024,8192]{2,1,0}, bf16[24,1024]{0,1}, /*index=5*/bf16[24,8192,1024]{1,2,0}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, /*index=10*/bf16[24,3,16,128]{3,2,1,0}, bf16[24,3,1024,16,128]{4,3,1,2,0}, bf16[24,1024]{1,0}, bf16[24,1024,16,128]{3,2,1,0}, bf16[24,8192]{1,0}, /*index=15*/bf16[24,1024,8192]{2,1,0}, bf16[24,8192,1024]{1,2,0}, bf16[24,2048]{1,0}, bf16[24,2048]{1,0}, bf16[24,2048]{1,0}, /*index=20*/bf16[24,2048]{1,0}, bf16[24,3,16,128]{3,2,1,0}, bf16[24,3,1024,16,128]{4,3,1,2,0}, bf16[24,1024]{1,0}, bf16[24,1024,16,128]{3,2,1,0}, /*index=25*/bf16[24,32,2048,2048]{2,1,3,0}, bf16[32,1,2048,2048]{3,2,0,1}, bf16[32,2048]{1,0}))->(s32[], bf16[32,2048,2048]{1,0,2}, bf16[24,8192]{1,0}, bf16[24,1024,8192]{2,1,0}, bf16[24,1024]{0,1}, /*index=5*/bf16[24,8192,1024]{1,2,0}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, /*index=10*/bf16[24,3,16,128]{3,2,1,0}, bf16[24,3,1024,16,128]{4,3,1,2,0}, bf16[24,1024]{1,0}, bf16[24,1024,16,128]{3,2,1,0}, bf16[24,8192]{1,0}, /*index=15*/bf16[24,1024,8192]{2,1,0}, bf16[24,8192,1024]{1,2,0}, bf16[24,2048]{1,0}, bf16[24,2048]{1,0}, bf16[24,2048]{1,0}, /*index=20*/bf16[24,2048]{1,0}, bf16[24,3,16,128]{3,2,1,0}, bf16[24,3,1024,16,128]{4,3,1,2,0}, bf16[24,1024]{1,0}, bf16[24,1024,16,128]{3,2,1,0}, /*index=25*/bf16[24,32,2048,2048]{2,1,3,0}, bf16[32,1,2048,2048]{3,2,0,1}, bf16[32,2048]{1,0})}\n add {\n"
                },
                {
                    "old_start": 4880,
                    "old_length": 6,
                    "new_start": 5014,
                    "new_length": 7,
                    "hunk": "@@ -4880,6 +5014,7 @@ main {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16TrainingBmm2CanonicalizationRestoreFwdGraph) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule pjit__unnamed_function_, entry_computation_layout={(bf16[2,256,4,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0}, bf16[2,4,256,256]{3,2,1,0})->(bf16[4,256,8,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={false,false,false,false}, num_partitions=4\n "
                }
            ],
            "whole_deleted": "-                    /*instruction_can_change_layout_func=*/{}) {}\n-TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2Pattern) {\n-  const char* module_str = R\"(\n-}\n-\n-\n-)\";\n-      auto m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2UncanonicalizedPattern) {\n-  const char* module_str = R\"(\n-}\n-\n-\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor) {\n-  const char* module_str = R\"(\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor) {\n-  const char* module_str = R\"(\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor) {\n-  const char* module_str = R\"(\n-}\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-TEST_F(CudnnFusedMhaRewriterTestHloTest, F16Bmm1Bmm2Pattern) {\n-  const char* module_str = R\"(\n-}\n-\n-\n-)\";\n-      auto m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1ScaleMaskSoftmaxBmm2Pattern) {\n-  const char* module_str = R\"(\n-}\n-\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern) {\n-  const char* module_str = R\"(\n-}\n-\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern) {\n-  const char* module_str = R\"(\n-}\n-\n-)\";\n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n",
            "whole_added": "+#include <optional>\n+#if GOOGLE_CUDA\n+#include \"third_party/gpus/cuda/include/cuda.h\"\n+#include \"third_party/gpus/cudnn/cudnn.h\"  // IWYU pragma: keep\n+#endif\n+\n+  se::CudaComputeCapability GetRealCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n+\n+                    /*instruction_can_change_layout_func=*/{}) {\n+#if !defined(GOOGLE_CUDA) || CUDA_VERSION < 12000\n+    skip_reason_ = \"cuDNN fused MHA requires CUDA 12 or later.\";\n+    return;\n+#endif\n+  }\n+\n+  // Centralize skip checks in the constructor. Unfortunately we cannot call\n+  // GTEST_SKIP from the constructor. Instead, we set (if needed) `skip_reason`,\n+  // and then check it from all test fixtures.\n+  // An alternative would be to use the SetUp() override, but for this to be\n+  // correct we'd have to ensure that all the parents' SetUp() methods are\n+  // called, which is error prone.\n+  std::optional<absl::string_view> skip_reason_;\n+class CudnnFusedMhaRewriterPipelineTest\n+    : public CudnnFusedMhaRewriterTestHloTest {\n+ public:\n+  CudnnFusedMhaRewriterPipelineTest() {\n+    if (skip_reason_) return;  // the parent might have set it.\n+#if !defined(GOOGLE_CUDA) || CUDNN_VERSION < 8800  // NOLINT\n+    skip_reason_ = \"Pipeline test requires cuDNN 8.8.0 or later.\";\n+    return;\n+#endif\n+    stream_executor::CudaComputeCapability cc = GetRealCudaComputeCapability();\n+    // Enforce capability minor == 0 because hardware with a non-zero minor\n+    // number typically has insufficient shared memory for cuDNN FMHA.\n+    if (!cc.IsAtLeastAmpere() || cc.minor != 0) {\n+      skip_reason_ =\n+          \"Pipeline test requires Nvidia AMPERE+ GPUs with minor \"\n+          \"compute capability == 0.\";\n+      return;\n+    }\n+  }\n+};\n+\n+constexpr absl::string_view hlo_BF16Bmm1Bmm2Pattern = R\"(\n+})\";\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m,\n+      ParseAndReturnVerifiedModule(hlo_BF16Bmm1Bmm2Pattern, GetModuleConfig()));\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, BF16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m,\n+      ParseAndReturnVerifiedModule(hlo_BF16Bmm1Bmm2Pattern, GetModuleConfig()));\n+  const HloInstruction* fmha;\n+\n+constexpr absl::string_view hlo_BF16Bmm1Bmm2UncanonicalizedPattern = R\"(\n+})\";\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2UncanonicalizedPattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(\n+                                      hlo_BF16Bmm1Bmm2UncanonicalizedPattern));\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, BF16Bmm1Bmm2UncanonicalizedPattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1Bmm2UncanonicalizedPattern, GetModuleConfig()));\n+  const HloInstruction* fmha;\n+constexpr absl::string_view\n+    hlo_BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor = R\"(\n+})\";\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor));\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor,\n+          GetModuleConfig()));\n+  const HloInstruction* fmha;\n+constexpr absl::string_view\n+    hlo_BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor = R\"(\n+})\";\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor));\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor,\n+          GetModuleConfig()));\n+  const HloInstruction* fmha;\n+constexpr absl::string_view\n+    hlo_BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor = R\"(\n+})\";\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor));\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor,\n+          GetModuleConfig()));\n+  const HloInstruction* fmha;\n+absl::string_view F16Bmm1Bmm2Pattern_str = R\"(\n+})\";\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, F16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m,\n+      ParseAndReturnVerifiedModule(F16Bmm1Bmm2Pattern_str, GetModuleConfig()));\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, F16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m,\n+      ParseAndReturnVerifiedModule(F16Bmm1Bmm2Pattern_str, GetModuleConfig()));\n+  const HloInstruction* fmha;\n+constexpr absl::string_view hlo_BF16Bmm1ScaleMaskSoftmaxBmm2Pattern = R\"(\n+})\";\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1ScaleMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(\n+                                      hlo_BF16Bmm1ScaleMaskSoftmaxBmm2Pattern));\n+}\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, BF16Bmm1ScaleMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1ScaleMaskSoftmaxBmm2Pattern, GetModuleConfig()));\n+  const HloInstruction* fmha;\n+constexpr absl::string_view hlo_BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern = R\"(\n+})\";\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(auto m,\n+                          ParseAndReturnVerifiedModule(\n+                              hlo_BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern));\n+}\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m,\n+      ParseAndReturnVerifiedModule(hlo_BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern,\n+                                   GetModuleConfig()));\n+  const HloInstruction* fmha;\n+constexpr absl::string_view\n+    hlo_BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern = R\"(\n+})\";\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern));\n+}\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern,\n+                  GetModuleConfig()));\n+  const HloInstruction* fmha;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n",
            "whole_hunk": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"xla/service/gpu/cudnn_fused_mha_rewriter.h\"\n \n #include <cstddef>\n+#include <optional>\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n@@ -44,6 +45,11 @@ limitations under the License.\n #include \"tsl/lib/core/status_test_util.h\"\n #include \"tsl/platform/statusor.h\"\n \n+#if GOOGLE_CUDA\n+#include \"third_party/gpus/cuda/include/cuda.h\"\n+#include \"third_party/gpus/cudnn/cudnn.h\"  // IWYU pragma: keep\n+#endif\n+\n namespace xla {\n namespace gpu {\n namespace {\n@@ -59,6 +65,13 @@ class CudnnFusedMhaRewriterTestHloTest : public HloTestBase {\n     return se::CudaComputeCapability(8, 0);\n   }\n \n+  se::CudaComputeCapability GetRealCudaComputeCapability() {\n+    return backend()\n+        .default_stream_executor()\n+        ->GetDeviceDescription()\n+        .cuda_compute_capability();\n+  }\n+\n   se::dnn::VersionInfo GetCudnnVersion() {\n     // Fake a supported compute capability to run tests,\n     // we don't run any kernels in these tests so they should be safe\n@@ -90,7 +103,12 @@ class CudnnFusedMhaRewriterTestHloTest : public HloTestBase {\n   CudnnFusedMhaRewriterTestHloTest()\n       : HloTestBase(/*verifier_layout_sensitive=*/false,\n                     /*allow_mixed_precision_in_hlo_verifier=*/false,\n-                    /*instruction_can_change_layout_func=*/{}) {}\n+                    /*instruction_can_change_layout_func=*/{}) {\n+#if !defined(GOOGLE_CUDA) || CUDA_VERSION < 12000\n+    skip_reason_ = \"cuDNN fused MHA requires CUDA 12 or later.\";\n+    return;\n+#endif\n+  }\n \n  protected:\n   size_t CountFusedAttentionCall(HloModule* module, bool is_backward = false) {\n@@ -118,10 +136,38 @@ class CudnnFusedMhaRewriterTestHloTest : public HloTestBase {\n     config_with_fmha.set_debug_options(debug_options);\n     return config_with_fmha;\n   }\n+\n+  // Centralize skip checks in the constructor. Unfortunately we cannot call\n+  // GTEST_SKIP from the constructor. Instead, we set (if needed) `skip_reason`,\n+  // and then check it from all test fixtures.\n+  // An alternative would be to use the SetUp() override, but for this to be\n+  // correct we'd have to ensure that all the parents' SetUp() methods are\n+  // called, which is error prone.\n+  std::optional<absl::string_view> skip_reason_;\n };\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2Pattern) {\n-  const char* module_str = R\"(\n+class CudnnFusedMhaRewriterPipelineTest\n+    : public CudnnFusedMhaRewriterTestHloTest {\n+ public:\n+  CudnnFusedMhaRewriterPipelineTest() {\n+    if (skip_reason_) return;  // the parent might have set it.\n+#if !defined(GOOGLE_CUDA) || CUDNN_VERSION < 8800  // NOLINT\n+    skip_reason_ = \"Pipeline test requires cuDNN 8.8.0 or later.\";\n+    return;\n+#endif\n+    stream_executor::CudaComputeCapability cc = GetRealCudaComputeCapability();\n+    // Enforce capability minor == 0 because hardware with a non-zero minor\n+    // number typically has insufficient shared memory for cuDNN FMHA.\n+    if (!cc.IsAtLeastAmpere() || cc.minor != 0) {\n+      skip_reason_ =\n+          \"Pipeline test requires Nvidia AMPERE+ GPUs with minor \"\n+          \"compute capability == 0.\";\n+      return;\n+    }\n+  }\n+};\n+\n+constexpr absl::string_view hlo_BF16Bmm1Bmm2Pattern = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n ENTRY main.6 {\n   Arg_2.3 = bf16[16,16,256,64]{3,2,1,0} parameter(2)\n@@ -129,13 +175,13 @@ ENTRY main.6 {\n   Arg_1.2 = bf16[16,16,256,64]{3,2,1,0} parameter(1)\n   dot.0 = bf16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = bf16[16,16,256,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n-}\n-\n-\n-)\";\n+})\";\n \n+TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      auto m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(hlo_BF16Bmm1Bmm2Pattern, GetModuleConfig()));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n@@ -152,12 +198,17 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_EQ(config.dropout_rate(), 0.0);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, BF16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(hlo_BF16Bmm1Bmm2Pattern, GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n+\n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n       optimized_module->entry_computation()->root_instruction(),\n@@ -169,11 +220,9 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_EQ(config.dropout_rate(), 0.0);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2UncanonicalizedPattern) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view hlo_BF16Bmm1Bmm2UncanonicalizedPattern = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,64,256]{3,2,1,0}}\n \n ENTRY main.6 {\n@@ -182,12 +231,12 @@ ENTRY main.6 {\n   Arg_1.2 = bf16[16,16,256,64]{3,2,1,0} parameter(1)\n   dot.0 = bf16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = bf16[16,16,64,256]{3,2,1,0} dot(Arg_2.3, dot.0), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n-}\n-\n-\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1Bmm2UncanonicalizedPattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(\n+                                      hlo_BF16Bmm1Bmm2UncanonicalizedPattern));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n@@ -204,12 +253,16 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_EQ(config.dropout_rate(), 0.0);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, BF16Bmm1Bmm2UncanonicalizedPattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1Bmm2UncanonicalizedPattern, GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(optimized_module->entry_computation()->root_instruction(),\n@@ -222,12 +275,10 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_EQ(config.dropout_rate(), 0.0);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view\n+    hlo_BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n ENTRY main.6 {\n@@ -236,10 +287,15 @@ ENTRY main.6 {\n   Arg_1.2 = bf16[16,16,256,64]{2,3,1,0} parameter(1)\n   dot.0 = bf16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = bf16[16,16,256,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n-}\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK_AND_ASSIGN(bool result, RunHloPass(&fusedMhaRewriter, m.get()));\n@@ -257,12 +313,19 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.bmm1_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             2);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_rhs_contracting_dim_not_most_minor,\n+          GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n@@ -275,12 +338,10 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.bmm1_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             2);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view\n+    hlo_BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n ENTRY main.6 {\n@@ -289,10 +350,15 @@ ENTRY main.6 {\n   Arg_1.2 = bf16[16,16,256,64]{2,3,1,0} parameter(1)\n   dot.0 = bf16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = bf16[16,16,256,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n-}\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK_AND_ASSIGN(bool result, RunHloPass(&fusedMhaRewriter, m.get()));\n@@ -312,12 +378,19 @@ ENTRY main.6 {\n             2);\n   EXPECT_EQ(config.bmm1_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             2);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm1_lhs_contracting_dim_not_most_minor,\n+          GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n@@ -332,12 +405,10 @@ ENTRY main.6 {\n             2);\n   EXPECT_EQ(config.bmm1_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             2);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view\n+    hlo_BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n ENTRY main.6 {\n@@ -346,10 +417,15 @@ ENTRY main.6 {\n   Arg_1.2 = bf16[16,16,256,64]{2,3,1,0} parameter(1)\n   dot.0 = bf16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = bf16[16,16,256,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n-}\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK_AND_ASSIGN(bool result, RunHloPass(&fusedMhaRewriter, m.get()));\n@@ -369,12 +445,19 @@ ENTRY main.6 {\n             3);\n   EXPECT_EQ(config.bmm2_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             3);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(\n+          hlo_BF16Bmm1Bmm2Pattern_bmm2_non_contracting_dim_not_most_minor,\n+          GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n@@ -389,11 +472,9 @@ ENTRY main.6 {\n             3);\n   EXPECT_EQ(config.bmm2_dot_dimension_numbers().rhs_contracting_dimensions()[0],\n             3);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest, F16Bmm1Bmm2Pattern) {\n-  const char* module_str = R\"(\n+absl::string_view F16Bmm1Bmm2Pattern_str = R\"(\n HloModule fmha_test, entry_computation_layout={(f16[16,16,256,64]{3,2,1,0},f16[16,16,256,64]{3,2,1,0},f16[16,16,256,64]{3,2,1,0})->f16[16,16,256,64]{3,2,1,0}}\n ENTRY main.6 {\n   Arg_2.3 = f16[16,16,256,64]{3,2,1,0} parameter(2)\n@@ -401,13 +482,13 @@ ENTRY main.6 {\n   Arg_1.2 = f16[16,16,256,64]{3,2,1,0} parameter(1)\n   dot.0 = f16[16,16,256,256]{3,2,1,0} dot(Arg_0.1, Arg_1.2), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={}\n   ROOT dot.1 = f16[16,16,256,64]{3,2,1,0} dot(dot.0, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={}\n-}\n-\n-\n-)\";\n+})\";\n \n+TEST_F(CudnnFusedMhaRewriterTestHloTest, F16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      auto m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(F16Bmm1Bmm2Pattern_str, GetModuleConfig()));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n@@ -424,12 +505,16 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_EQ(config.dropout_rate(), 0.0);\n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+}\n+\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, F16Bmm1Bmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(F16Bmm1Bmm2Pattern_str, GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n@@ -442,11 +527,9 @@ ENTRY main.6 {\n   const CudnnfMHABackendConfig& config = gpu_config.cudnn_fmha_backend_config();\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 1.0);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1ScaleMaskSoftmaxBmm2Pattern) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view hlo_BF16Bmm1ScaleMaskSoftmaxBmm2Pattern = R\"(\n HloModule jit_bmm_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n region_0.14.clone {\n@@ -487,11 +570,12 @@ ENTRY main.38 {\n   convert.49 = bf16[16,16,256,256]{3,2,1,0} convert(divide.36)\n   Arg_2.3 = bf16[16,16,256,64]{3,2,1,0} parameter(2)\n   ROOT dot.37 = bf16[16,16,256,64]{3,2,1,0} dot(convert.49, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n-}\n-\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1ScaleMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(\n+                                      hlo_BF16Bmm1ScaleMaskSoftmaxBmm2Pattern));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n@@ -510,13 +594,16 @@ ENTRY main.38 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 2.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 4);\n+}\n \n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+TEST_F(CudnnFusedMhaRewriterPipelineTest, BF16Bmm1ScaleMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1ScaleMaskSoftmaxBmm2Pattern, GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n@@ -531,12 +618,9 @@ ENTRY main.38 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 2.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 4);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view hlo_BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern = R\"(\n HloModule jit_bmm_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n region_0.17.clone {\n@@ -580,11 +664,14 @@ ENTRY main.41 {\n   convert.49 = bf16[16,16,256,256]{3,2,1,0} convert(divide.36)\n   Arg_2.3 = bf16[16,16,256,64]{3,2,1,0} parameter(2)\n   ROOT dot.37 = bf16[16,16,256,64]{3,2,1,0} dot(convert.49, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n-}\n-\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(auto m,\n+                          ParseAndReturnVerifiedModule(\n+                              hlo_BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n@@ -604,13 +691,18 @@ ENTRY main.41 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 3.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 5);\n+}\n \n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m,\n+      ParseAndReturnVerifiedModule(hlo_BF16Bmm1ScaleBiasMaskSoftmaxBmm2Pattern,\n+                                   GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n@@ -626,12 +718,10 @@ ENTRY main.41 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 3.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 5);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n-TEST_F(CudnnFusedMhaRewriterTestHloTest,\n-       BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern) {\n-  const char* module_str = R\"(\n+constexpr absl::string_view\n+    hlo_BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern = R\"(\n HloModule jit_bmm_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n \n region_0.17.clone {\n@@ -676,11 +766,14 @@ ENTRY main.41 {\n   convert.49 = bf16[16,16,256,256]{3,2,1,0} convert(divide.36)\n   Arg_2.3 = bf16[16,16,256,64]{3,2,1,0} parameter(2)\n   ROOT dot.37 = bf16[16,16,256,64]{3,2,1,0} dot(convert.49, Arg_2.3), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n-}\n-\n-)\";\n+})\";\n \n-  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));\n+TEST_F(CudnnFusedMhaRewriterTestHloTest,\n+       BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern));\n   CudnnFusedMHARewriter fusedMhaRewriter{GetCudaComputeCapability(),\n                                          GetCudnnVersion()};\n   TF_ASSERT_OK(RunHloPass(&fusedMhaRewriter, m.get()).status());\n@@ -702,13 +795,18 @@ ENTRY main.41 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 3.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 5);\n+}\n \n-#if GOOGLE_CUDA && CUDNN_VERSION >= 8800\n-  // run whole pipeline\n+TEST_F(CudnnFusedMhaRewriterPipelineTest,\n+       BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   TF_ASSERT_OK_AND_ASSIGN(\n-      m, ParseAndReturnVerifiedModule(module_str, GetModuleConfig()));\n+      auto m, ParseAndReturnVerifiedModule(\n+                  hlo_BF16Bmm1ScaleBiasNonConstantMaskSoftmaxBmm2Pattern,\n+                  GetModuleConfig()));\n   TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> optimized_module,\n                           GetOptimizedModule(std::move(m)));\n+  const HloInstruction* fmha;\n \n   SCOPED_TRACE(optimized_module->ToString());\n   EXPECT_THAT(\n@@ -724,10 +822,10 @@ ENTRY main.41 {\n   EXPECT_FLOAT_EQ(config.fmha_scale(), 3.1);\n   EXPECT_FLOAT_EQ(config.dropout_rate(), 0.0);\n   EXPECT_EQ(fmha->operands().size(), 5);\n-#endif  // GOOGLE_CUDA && CUDNN_VERSION >= 8800\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1CombinedMaskBiasSoftmaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_,\n entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0})->bf16[16,256,16,64]{3,2,1,0}}\n@@ -809,6 +907,7 @@ ENTRY main.61 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16Bmm1ScaleBiasMaskSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,40,64]{3,2,1,0},f16[2,6,64,40]{3,2,1,0},f16[2,6,40,64]{3,2,1,0})->f16[2,6,40,64]{3,2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n \n@@ -917,6 +1016,7 @@ ENTRY main.83 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, F16Bmm1UnfusedSoftmaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,40,64]{3,2,1,0},f16[2,6,64,40]{3,2,1,0},f16[2,6,40,64]{3,2,1,0})->f16[2,6,40,64]{3,2,1,0}}\n \n@@ -973,6 +1073,7 @@ ENTRY main.31 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16Bmm1UnfusedSoftmaxWithConvertF32ToReduceMaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[128,6,400,64]{3,2,1,0},f16[128,6,64,400]{3,2,1,0},f16[128,6,400,64]{3,2,1,0})->f16[128,6,400,64]{3,2,1,0}}\n \n@@ -1042,6 +1143,7 @@ ENTRY main.41 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1UnfusedScaleMaskBiasSoftmaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0})->bf16[16,256,16,64]{3,2,1,0}}\n \n@@ -1127,6 +1229,7 @@ ENTRY main.61 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1ConvertedMaskAddedAfterFirstGemmSoftmaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},pred[16,1,256,256]{3,2,1,0})->bf16[16,256,16,64]{3,2,1,0}}\n \n@@ -1203,6 +1306,7 @@ ENTRY main.56 {\n // negative test\n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1Bmm2Pattern_bmm1_contracting_dim_not_equal_64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,32]{3,2,1,0},bf16[16,16,256,32]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,256,64]{3,2,1,0}}\n ENTRY main.6 {\n@@ -1230,6 +1334,7 @@ ENTRY main.6 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1Bmm2Pattern_bmm1_non_contracting_dim_larger_than_512) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,1024,64]{3,2,1,0},bf16[16,16,1024,64]{3,2,1,0},bf16[16,16,1024,64]{3,2,1,0})->bf16[16,16,1024,64]{3,2,1,0}}\n ENTRY main.6 {\n@@ -1256,6 +1361,7 @@ ENTRY main.6 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1Bmm2Pattern_bmm2_rhs_non_contracting_dim_not_equal_64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0},bf16[16,16,256,32]{3,2,1,0})->bf16[16,16,256,32]{3,2,1,0}}\n ENTRY main.6 {\n@@ -1283,6 +1389,7 @@ ENTRY main.6 {\n // check if MHA is unsupported, canonicalization will not kick in\n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1Bmm2PatternUncanonicalized_bmm1_contracting_dim_not_equal_64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule fmha_test, entry_computation_layout={(bf16[16,16,256,32]{3,2,1,0},bf16[16,16,256,32]{3,2,1,0},bf16[16,16,256,64]{3,2,1,0})->bf16[16,16,64,256]{3,2,1,0}}\n \n@@ -1310,6 +1417,7 @@ ENTRY main.6 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16Bmm1BiasSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[1,16,256,256]{3,2,1,0})->bf16[16,256,16,64]{3,2,1,0}}\n \n@@ -1422,6 +1530,7 @@ ENTRY main.82 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16Bmm1ScaleBiasSoftmaxDropoutForm2Bmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[32,40,60,64]{3,2,1,0},bf16[32,40,60,64]{3,2,1,0},bf16[32,40,60,64]{3,2,1,0})->bf16[32,40,60,64]{3,2,1,0}}, allow_spmd_sharding_propagation_to_output={true}\n \n@@ -1531,6 +1640,7 @@ ENTRY main.79 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16TrainingBmm1Bmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0})->(bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0})}\n \n@@ -1584,6 +1694,7 @@ ENTRY main.17 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16TrainingBmm1ScaleBiasSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0})->(bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[1,16,256,256]{3,2,1,0})}\n \n@@ -1777,6 +1888,7 @@ ENTRY main.146 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[16,256,16,64]{3,2,1,0},f16[16,256,16,64]{3,2,1,0},f16[16,256,16,64]{3,2,1,0},f16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0},f16[16,256,16,64]{3,2,1,0})->(f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[1,16,256,256]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true,true}\n \n@@ -1968,6 +2080,7 @@ ENTRY main.146 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasSoftmaxDropoutBmm2WithTransposeFusion) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[16,256,16,64]{3,2,1,0},f16[16,256,16,64]{3,2,1,0},f16[16,256,16,64]{3,2,1,0},f16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0},f16[16,256,16,64]{3,2,1,0})->(f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[16,256,16,64]{3,2,1,0}, f16[1,16,256,256]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true,true}\n \n@@ -2180,6 +2293,7 @@ ENTRY main.146 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, BF16MiniT5xTest) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__lambda_, entry_computation_layout={(bf16[12,512,32,64]{3,2,1,0},bf16[12,512,2,32,64]{4,3,2,1,0},f32[12,512]{1,0},f32[12,512]{1,0})->(bf16[], bf16[12,512,32,64]{3,2,1,0}, bf16[12,512,2,32,64]{4,3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true}\n \n@@ -2329,6 +2443,7 @@ ENTRY main.129 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16TrainingBmm1ScaleBiasMaskSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,128,64]{3,2,1,0},bf16[2,6,64,128]{3,2,1,0},bf16[2,6,128,64]{3,2,1,0},bf16[2,6,128,64]{3,2,1,0})->(bf16[2,6,128,64]{3,2,1,0}, bf16[2,6,128,64]{3,2,1,0}, bf16[2,6,64,128]{3,2,1,0}, bf16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n@@ -2480,6 +2595,7 @@ ENTRY main.126 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasMaskSoftmaxDropoutBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,128,64]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n@@ -2632,6 +2748,7 @@ ENTRY main.126 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasMaskSoftmaxBmm2) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,128,64]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n@@ -2748,6 +2865,7 @@ ENTRY main.82 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16TrainingBmm1ScaleBiasSoftmaxDropoutBmm2DbiasShouldHaveUserShape) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0},bf16[1,16,256,256]{3,2,1,0},pred[16,1,256,256]{3,2,1,0},bf16[16,256,16,64]{3,2,1,0})->(bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[16,256,16,64]{3,2,1,0}, bf16[1,16,256,256]{3,2,1,0})}\n \n@@ -2943,6 +3061,7 @@ ENTRY main.146 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        ActivationHasMoreThan1UserShouldNotLower) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule test\n \n@@ -3001,6 +3120,7 @@ ENTRY main {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16InvalidTrainingBmm1ScaleBiasMaskSoftmaxBmm2ShouldNotBeLowered) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,128,64]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n@@ -3094,6 +3214,7 @@ ENTRY main.82 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16InvalidTrainingBmm1ScaleBiasMaskSoftmaxDropoutBmm2ShouldNotLower) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,128,64]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n@@ -3230,6 +3351,7 @@ ENTRY main.126 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasSoftmaxBmm2QTranspose) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,64,128]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n@@ -3339,6 +3461,7 @@ ENTRY main.82 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16Bmm1UnfusedSoftmaxBmm2IncorrectBmm1NumUsers) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,40,64]{3,2,1,0},f16[2,6,64,40]{3,2,1,0},f16[2,6,40,64]{3,2,1,0})->(f16[2,6,40,64]{3,2,1,0}, f16[2,6,40,40]{3,2,1,0})}\n \n@@ -3388,6 +3511,7 @@ ENTRY main.31 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16Bmm1UnfusedSoftmaxBmm2IncorrectSoftmaxNumUsers) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,40,64]{3,2,1,0},f16[2,6,64,40]{3,2,1,0},f16[2,6,40,64]{3,2,1,0})->(f16[2,6,40,64]{3,2,1,0}, f16[2,6,40,40]{3,2,1,0})}\n \n@@ -3437,6 +3561,7 @@ ENTRY main.31 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasSoftmaxBmm2IncorrectSoftmaxBwdNumUsers) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,64,128]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,128]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n@@ -3524,6 +3649,7 @@ ENTRY main.82 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, F16Bmm1SoftmaxBmm2IncorrectRank) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule reproducer, entry_computation_layout={(f16[1,8,16,5,128]{4,3,2,1,0}, f16[1,8,16,5,128]{4,3,2,1,0}, f16[1,8,16,5,128]{4,3,2,1,0}, f32[128,2,64]{2,1,0}, f32[2,64]{1,0}, /*index=5*/f32[128,2,64]{2,1,0}, f32[2,64]{1,0}, f32[128,2,64]{2,1,0}, f32[2,64]{1,0})->f16[8,16,2,5,64]{4,3,2,1,0}}\n \n@@ -3619,6 +3745,7 @@ ENTRY main {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        F16TrainingBmm1ScaleBiasSoftmaxBmm2NonContractingDimNotDivisibleBy64) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,64,100]{3,2,1,0},f16[2,6,64,100]{3,2,1,0},f16[2,6,100,64]{3,2,1,0},f16[2,6,100,64]{3,2,1,0})->(f16[2,6,100,64]{3,2,1,0}, f16[2,6,100,64]{3,2,1,0}, f16[2,6,64,100]{3,2,1,0}, f16[2,6,100,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n@@ -3706,6 +3833,7 @@ ENTRY main.82 {\n }\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest, F16TrainingBmm2Grad1IncorrectPattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,64,128]{3,2,1,0},f16[2,6,64,128]{3,2,1,0},f16[2,6,128,64]{3,2,1,0},f16[2,6,128,64]{3,2,1,0})->(f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,64,128]{3,2,1,0}, f16[2,6,128,64]{3,2,1,0}, f16[2,6,128,128]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n@@ -3795,6 +3923,7 @@ ENTRY main.82 {\n // flash attention\n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        FlashAttentionBF16TrainingBmm1CausalMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,128,2048]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0})->(bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,128,2048]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n region_0.32 {\n@@ -3907,6 +4036,7 @@ ENTRY main.92 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        FlashAttentionBF16TrainingBmm1BiasSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,128,2048]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,2048,2048]{3,2,1,0})->(bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,128,2048]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n region_0.32 {\n@@ -4014,6 +4144,7 @@ ENTRY main.92 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        FlashAttentionBF16TrainingBmm1SoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,128,2048]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0},bf16[2,6,2048,128]{3,2,1,0})->(bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0}, bf16[2,6,128,2048]{3,2,1,0}, bf16[2,6,2048,128]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n region_0.32 {\n@@ -4116,6 +4247,7 @@ ENTRY main.92 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        FlashAttentionBF16TrainingBmm1ScaleMaskSoftmaxBmm2Pattern) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(bf16[2,6,2048,64]{3,2,1,0},bf16[2,6,64,2048]{3,2,1,0},bf16[2,6,2048,64]{3,2,1,0},bf16[2,6,2048,64]{3,2,1,0})->(bf16[2,6,2048,64]{3,2,1,0}, bf16[2,6,2048,64]{3,2,1,0}, bf16[2,6,64,2048]{3,2,1,0}, bf16[2,6,2048,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={true,true,true,true}\n \n@@ -4228,6 +4360,7 @@ ENTRY main.82 {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        FlashAttentionF16Bmm1BiasSoftmaxBmm2PatternCrossAttention) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f16[2,6,2048,64]{3,2,1,0},f16[2,6,64,1024]{3,2,1,0},f16[2,6,1024,64]{3,2,1,0},f16[2,6,2048,1024]{3,2,1,0})->f16[2,6,2048,64]{3,2,1,0}}\n \n@@ -4294,6 +4427,7 @@ ENTRY main.31 {\n \n // GPT3 pattern\n TEST_F(CudnnFusedMhaRewriterTestHloTest, FlashAttentionBF16TrainingGPT3_5B) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule jit__unnamed_wrapped_function_, entry_computation_layout={((s32[], bf16[32,2048,2048]{1,0,2}, bf16[24,8192]{1,0}, bf16[24,1024,8192]{2,1,0}, bf16[24,1024]{0,1}, /*index=5*/bf16[24,8192,1024]{1,2,0}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, /*index=10*/bf16[24,3,16,128]{3,2,1,0}, bf16[24,3,1024,16,128]{4,3,1,2,0}, bf16[24,1024]{1,0}, bf16[24,1024,16,128]{3,2,1,0}, bf16[24,8192]{1,0}, /*index=15*/bf16[24,1024,8192]{2,1,0}, bf16[24,8192,1024]{1,2,0}, bf16[24,2048]{1,0}, bf16[24,2048]{1,0}, bf16[24,2048]{1,0}, /*index=20*/bf16[24,2048]{1,0}, bf16[24,3,16,128]{3,2,1,0}, bf16[24,3,1024,16,128]{4,3,1,2,0}, bf16[24,1024]{1,0}, bf16[24,1024,16,128]{3,2,1,0}, /*index=25*/bf16[24,32,2048,2048]{2,1,3,0}, bf16[32,1,2048,2048]{3,2,0,1}, bf16[32,2048]{1,0}))->(s32[], bf16[32,2048,2048]{1,0,2}, bf16[24,8192]{1,0}, bf16[24,1024,8192]{2,1,0}, bf16[24,1024]{0,1}, /*index=5*/bf16[24,8192,1024]{1,2,0}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, bf16[24,1024]{0,1}, /*index=10*/bf16[24,3,16,128]{3,2,1,0}, bf16[24,3,1024,16,128]{4,3,1,2,0}, bf16[24,1024]{1,0}, bf16[24,1024,16,128]{3,2,1,0}, bf16[24,8192]{1,0}, /*index=15*/bf16[24,1024,8192]{2,1,0}, bf16[24,8192,1024]{1,2,0}, bf16[24,2048]{1,0}, bf16[24,2048]{1,0}, bf16[24,2048]{1,0}, /*index=20*/bf16[24,2048]{1,0}, bf16[24,3,16,128]{3,2,1,0}, bf16[24,3,1024,16,128]{4,3,1,2,0}, bf16[24,1024]{1,0}, bf16[24,1024,16,128]{3,2,1,0}, /*index=25*/bf16[24,32,2048,2048]{2,1,3,0}, bf16[32,1,2048,2048]{3,2,0,1}, bf16[32,2048]{1,0})}\n add {\n@@ -4880,6 +5014,7 @@ main {\n \n TEST_F(CudnnFusedMhaRewriterTestHloTest,\n        BF16TrainingBmm2CanonicalizationRestoreFwdGraph) {\n+  if (skip_reason_) GTEST_SKIP() << *skip_reason_;\n   const char* module_str = R\"(\n HloModule pjit__unnamed_function_, entry_computation_layout={(bf16[2,256,4,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0}, bf16[2,4,256,256]{3,2,1,0})->(bf16[4,256,8,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0}, bf16[2,256,4,64]{3,2,1,0})}, allow_spmd_sharding_propagation_to_output={false,false,false,false}, num_partitions=4\n "
        }
    ]
},
{
    "Id": 28,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/662e615ccfa2ba1aa20c52be122a81343558d9cf",
    "date": "2024-06-13T11:36:37-07:00",
    "message": "Remove broadcast checking early on in SupportedOpForPropagation for space-to-batch conversion. We check this in CanPropagate anyway.\n\nPiperOrigin-RevId: 643056256",
    "label": "NO",
    "changes": [
        {
            "name": "space_to_batch_converter.cc",
            "path": "third_party/xla/xla/service/space_to_batch_converter.cc",
            "patches": [
                {
                    "old_start": 1620,
                    "old_length": 15,
                    "new_start": 1620,
                    "new_length": 6,
                    "hunk": "@@ -1620,15 +1620,6 @@ bool ConvolutionVisitor::SupportedOpForPropagation(HloInstruction* consumer,\n   }\n \n   if (IsTrivialElementwise(consumer)) {\n-    for (int64_t i = 0; i < consumer->operand_count(); ++i) {\n-      if (consumer->operand(i)->opcode() == HloOpcode::kBroadcast) {\n-        if (!IsBroadcastPropagatable(consumer->mutable_operand(i), producer)) {\n-          VLOG(2) << \"Could not propagate through broadcast \"\n-                  << consumer->operand(i)->ToString();\n-          return false;\n-        }\n-      }\n-    }\n     return true;\n   }\n "
                }
            ],
            "whole_deleted": "-    for (int64_t i = 0; i < consumer->operand_count(); ++i) {\n-      if (consumer->operand(i)->opcode() == HloOpcode::kBroadcast) {\n-        if (!IsBroadcastPropagatable(consumer->mutable_operand(i), producer)) {\n-          VLOG(2) << \"Could not propagate through broadcast \"\n-                  << consumer->operand(i)->ToString();\n-          return false;\n-        }\n-      }\n-    }\n",
            "whole_added": "",
            "whole_hunk": "@@ -1620,15 +1620,6 @@ bool ConvolutionVisitor::SupportedOpForPropagation(HloInstruction* consumer,\n   }\n \n   if (IsTrivialElementwise(consumer)) {\n-    for (int64_t i = 0; i < consumer->operand_count(); ++i) {\n-      if (consumer->operand(i)->opcode() == HloOpcode::kBroadcast) {\n-        if (!IsBroadcastPropagatable(consumer->mutable_operand(i), producer)) {\n-          VLOG(2) << \"Could not propagate through broadcast \"\n-                  << consumer->operand(i)->ToString();\n-          return false;\n-        }\n-      }\n-    }\n     return true;\n   }\n "
        }
    ]
},
{
    "Id": 43,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/6c1a8e4dcc8b6a05cfbb3c47bb85ef5f9e8d8800",
    "date": "2024-05-17T03:35:38-07:00",
    "message": "Followup to Scatter Layout normalization\n\nThe code already implicitly assumes that ScatterSimplifier has run before. We\ncannot normalize if there are more than 1 \"scatter\" (batch) dimensions. Add a\ncheck for that and adjust a test case that would in fact be incorrectly\nnormalized (but hidden from the verifier by running ScatterSimplifier\nafterwards).\n\nPiperOrigin-RevId: 634695331",
    "label": "YES",
    "changes": [
        {
            "name": "layout_normalization.cc",
            "path": "third_party/xla/xla/service/layout_normalization.cc",
            "patches": [
                {
                    "old_start": 404,
                    "old_length": 6,
                    "new_start": 404,
                    "new_length": 22,
                    "hunk": "@@ -404,6 +404,22 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {\n       TF_ASSIGN_OR_RETURN(auto normalized_update, GetNormalizedInput(operand));\n       normalized_updates.push_back(normalized_update);\n     }\n+\n+    // Since normalization might reorder the 'scatter_updates' operands\n+    // differently than the 'scatter_indices' update, we have no way to specify\n+    // the order of 'scatter' (batch) dimensions, as that is not an attribute in\n+    // ScatterDimensionNumbers. Scatter implicitly assumes that the 'scatter'\n+    // dimensions appear in the same order in 'scatter_updates' and\n+    // 'scatter_indices'. So we require that there is just a single\n+    // 'scatter' dimension. This is ensured by the ScatterSimplifier pass.\n+    const auto& dims = scatter->scatter_dimension_numbers();\n+    if (scatter->scatter_updates().front()->shape().rank() -\n+            dims.update_window_dims_size() >\n+        1) {\n+      return FailedPrecondition(\n+          \"There should be just a single scatter dimension. Make sure to run \"\n+          \"ScatterSimplifier before LayoutNormalization\");\n+    }\n     TF_ASSIGN_OR_RETURN(auto normalized_indices,\n                         GetNormalizedInput(scatter->scatter_indices()));\n \n"
                },
                {
                    "old_start": 423,
                    "old_length": 7,
                    "new_start": 439,
                    "new_length": 6,
                    "hunk": "@@ -423,7 +439,6 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {\n     // operand dimensions. scatter dimension i corresponds to\n     // scatter_dims_to_operand_dims[i] operand dimension.\n \n-    const auto& dims = scatter->scatter_dimension_numbers();\n     ScatterDimensionNumbers normalized_dims;\n     normalized_dims.set_index_vector_dim(\n         indices_permutation[dims.index_vector_dim()]);\n"
                }
            ],
            "whole_deleted": "-    const auto& dims = scatter->scatter_dimension_numbers();\n",
            "whole_added": "+\n+    // Since normalization might reorder the 'scatter_updates' operands\n+    // differently than the 'scatter_indices' update, we have no way to specify\n+    // the order of 'scatter' (batch) dimensions, as that is not an attribute in\n+    // ScatterDimensionNumbers. Scatter implicitly assumes that the 'scatter'\n+    // dimensions appear in the same order in 'scatter_updates' and\n+    // 'scatter_indices'. So we require that there is just a single\n+    // 'scatter' dimension. This is ensured by the ScatterSimplifier pass.\n+    const auto& dims = scatter->scatter_dimension_numbers();\n+    if (scatter->scatter_updates().front()->shape().rank() -\n+            dims.update_window_dims_size() >\n+        1) {\n+      return FailedPrecondition(\n+          \"There should be just a single scatter dimension. Make sure to run \"\n+          \"ScatterSimplifier before LayoutNormalization\");\n+    }\n",
            "whole_hunk": "@@ -404,6 +404,22 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {\n       TF_ASSIGN_OR_RETURN(auto normalized_update, GetNormalizedInput(operand));\n       normalized_updates.push_back(normalized_update);\n     }\n+\n+    // Since normalization might reorder the 'scatter_updates' operands\n+    // differently than the 'scatter_indices' update, we have no way to specify\n+    // the order of 'scatter' (batch) dimensions, as that is not an attribute in\n+    // ScatterDimensionNumbers. Scatter implicitly assumes that the 'scatter'\n+    // dimensions appear in the same order in 'scatter_updates' and\n+    // 'scatter_indices'. So we require that there is just a single\n+    // 'scatter' dimension. This is ensured by the ScatterSimplifier pass.\n+    const auto& dims = scatter->scatter_dimension_numbers();\n+    if (scatter->scatter_updates().front()->shape().rank() -\n+            dims.update_window_dims_size() >\n+        1) {\n+      return FailedPrecondition(\n+          \"There should be just a single scatter dimension. Make sure to run \"\n+          \"ScatterSimplifier before LayoutNormalization\");\n+    }\n     TF_ASSIGN_OR_RETURN(auto normalized_indices,\n                         GetNormalizedInput(scatter->scatter_indices()));\n \n@@ -423,7 +439,6 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {\n     // operand dimensions. scatter dimension i corresponds to\n     // scatter_dims_to_operand_dims[i] operand dimension.\n \n-    const auto& dims = scatter->scatter_dimension_numbers();\n     ScatterDimensionNumbers normalized_dims;\n     normalized_dims.set_index_vector_dim(\n         indices_permutation[dims.index_vector_dim()]);\n"
        },
        {
            "name": "layout_normalization_test.cc",
            "path": "third_party/xla/xla/service/layout_normalization_test.cc",
            "patches": [
                {
                    "old_start": 788,
                    "old_length": 16,
                    "new_start": 788,
                    "new_length": 16,
                    "hunk": "@@ -788,16 +788,16 @@ region_0.10 {\n \n ENTRY main.17 {\n   p0 = s16[3,2,2,14,16]{0,1,4,3,2} parameter(0)\n-  p1 = s32[6,11,8,2]{0,2,1,3} parameter(1)\n-  p2 = s16[6,11,8,3,5]{0,1,4,3,2} parameter(2)\n-  ROOT scatter = s16[3,2,2,14,16]{0,1,4,3,2} scatter(p0, p1, p2), update_window_dims={3,4}, inserted_window_dims={1,2,3}, scatter_dims_to_operand_dims={4,0}, index_vector_dim=3, to_apply=region_0.10\n+  p1 = s32[2,11]{0,1} parameter(1)\n+  p2 = s16[11,3,5]{2,0,1} parameter(2)\n+  ROOT scatter = s16[3,2,2,14,16]{0,1,4,3,2} scatter(p0, p1, p2), update_window_dims={1,2}, inserted_window_dims={1,2,3}, scatter_dims_to_operand_dims={4,0}, index_vector_dim=0, to_apply=region_0.10\n }\n )\";\n \n   CheckLayoutNormalization(\n       hlo, R\"(\n // CHECK: scatter({{.*}}),\n-// CHECK-SAME: update_window_dims={2,1}, inserted_window_dims={0,1,3}, scatter_dims_to_operand_dims={2,4}, index_vector_dim=0, to_apply=%region_0.10\n+// CHECK-SAME: update_window_dims={2,0}, inserted_window_dims={0,1,3}, scatter_dims_to_operand_dims={2,4}, index_vector_dim=1, to_apply=%region_0.10\n )\",\n       // Run the ScatterSimplifier afterwards, otherwise the verifier will\n       // complain!"
                }
            ],
            "whole_deleted": "-  p1 = s32[6,11,8,2]{0,2,1,3} parameter(1)\n-  p2 = s16[6,11,8,3,5]{0,1,4,3,2} parameter(2)\n-  ROOT scatter = s16[3,2,2,14,16]{0,1,4,3,2} scatter(p0, p1, p2), update_window_dims={3,4}, inserted_window_dims={1,2,3}, scatter_dims_to_operand_dims={4,0}, index_vector_dim=3, to_apply=region_0.10\n-// CHECK-SAME: update_window_dims={2,1}, inserted_window_dims={0,1,3}, scatter_dims_to_operand_dims={2,4}, index_vector_dim=0, to_apply=%region_0.10\n",
            "whole_added": "+  p1 = s32[2,11]{0,1} parameter(1)\n+  p2 = s16[11,3,5]{2,0,1} parameter(2)\n+  ROOT scatter = s16[3,2,2,14,16]{0,1,4,3,2} scatter(p0, p1, p2), update_window_dims={1,2}, inserted_window_dims={1,2,3}, scatter_dims_to_operand_dims={4,0}, index_vector_dim=0, to_apply=region_0.10\n+// CHECK-SAME: update_window_dims={2,0}, inserted_window_dims={0,1,3}, scatter_dims_to_operand_dims={2,4}, index_vector_dim=1, to_apply=%region_0.10\n",
            "whole_hunk": "@@ -788,16 +788,16 @@ region_0.10 {\n \n ENTRY main.17 {\n   p0 = s16[3,2,2,14,16]{0,1,4,3,2} parameter(0)\n-  p1 = s32[6,11,8,2]{0,2,1,3} parameter(1)\n-  p2 = s16[6,11,8,3,5]{0,1,4,3,2} parameter(2)\n-  ROOT scatter = s16[3,2,2,14,16]{0,1,4,3,2} scatter(p0, p1, p2), update_window_dims={3,4}, inserted_window_dims={1,2,3}, scatter_dims_to_operand_dims={4,0}, index_vector_dim=3, to_apply=region_0.10\n+  p1 = s32[2,11]{0,1} parameter(1)\n+  p2 = s16[11,3,5]{2,0,1} parameter(2)\n+  ROOT scatter = s16[3,2,2,14,16]{0,1,4,3,2} scatter(p0, p1, p2), update_window_dims={1,2}, inserted_window_dims={1,2,3}, scatter_dims_to_operand_dims={4,0}, index_vector_dim=0, to_apply=region_0.10\n }\n )\";\n \n   CheckLayoutNormalization(\n       hlo, R\"(\n // CHECK: scatter({{.*}}),\n-// CHECK-SAME: update_window_dims={2,1}, inserted_window_dims={0,1,3}, scatter_dims_to_operand_dims={2,4}, index_vector_dim=0, to_apply=%region_0.10\n+// CHECK-SAME: update_window_dims={2,0}, inserted_window_dims={0,1,3}, scatter_dims_to_operand_dims={2,4}, index_vector_dim=1, to_apply=%region_0.10\n )\",\n       // Run the ScatterSimplifier afterwards, otherwise the verifier will\n       // complain!"
        }
    ]
},
{
    "Id": 42,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/86f124fe7d01d8de95a9457f99b8a8d499dafc65",
    "date": "2024-05-21T10:17:10-07:00",
    "message": "[XLA:FFI] Fix annotating buffer `dimensions` memory\n\nThe last dimension of FFI buffers `dimensions` member variable was not annotated as initialized memory, because of an off-by-one code error.\nIt went unnoticed because of usage absl::c_accumulate algorithm in tests. Modified one test to directly access dimensions in a for loop, cause that triggers sanitizer check.\n\nPiperOrigin-RevId: 635840115",
    "label": "YES",
    "changes": [
        {
            "name": "runtime_handle_ffi_call.cc",
            "path": "third_party/xla/xla/service/cpu/runtime_handle_ffi_call.cc",
            "patches": [
                {
                    "old_start": 51,
                    "old_length": 9,
                    "new_start": 51,
                    "new_length": 8,
                    "hunk": "@@ -51,9 +51,8 @@ absl::Span<const int64_t> DecodeDims(int64_t* encoded_dims_data) {\n   // suppress false positives from msan sanitizer.\n   ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(encoded_dims_data, sizeof(int64_t));\n   auto dims_count = encoded_dims_data[0];\n-  ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(encoded_dims_data,\n-                                      dims_count * sizeof(int64_t));\n   auto dims_begin = encoded_dims_data + 1;\n+  ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(dims_begin, dims_count * sizeof(int64_t));\n   return absl::MakeSpan(dims_begin, dims_begin + dims_count);\n }\n \n"
                }
            ],
            "whole_deleted": "-  ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(encoded_dims_data,\n-                                      dims_count * sizeof(int64_t));\n",
            "whole_added": "+  ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(dims_begin, dims_count * sizeof(int64_t));\n",
            "whole_hunk": "@@ -51,9 +51,8 @@ absl::Span<const int64_t> DecodeDims(int64_t* encoded_dims_data) {\n   // suppress false positives from msan sanitizer.\n   ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(encoded_dims_data, sizeof(int64_t));\n   auto dims_count = encoded_dims_data[0];\n-  ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(encoded_dims_data,\n-                                      dims_count * sizeof(int64_t));\n   auto dims_begin = encoded_dims_data + 1;\n+  ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(dims_begin, dims_count * sizeof(int64_t));\n   return absl::MakeSpan(dims_begin, dims_begin + dims_count);\n }\n \n"
        },
        {
            "name": "custom_call_test.cc",
            "path": "third_party/xla/xla/tests/custom_call_test.cc",
            "patches": [
                {
                    "old_start": 583,
                    "old_length": 8,
                    "new_start": 583,
                    "new_length": 12,
                    "hunk": "@@ -583,8 +583,12 @@ static absl::Status FfiF32ReduceSum(F32Buffer in, R0F32ResultBuffer out) {\n   auto out_data = out->data.base();\n \n   // Calculate the total size of the vector\n-  const auto size =\n-      absl::c_accumulate(in.dimensions, 1, std::multiplies<int>());\n+  // Manual calculation is used here instead of absl::c_accumulate to trigger\n+  // sanitizer check for dimensions\n+  auto size = 1;\n+  for (auto dim : in.dimensions) {\n+    size *= dim;\n+  }\n \n   // Calculate the sum of the vector\n   *out_data = absl::c_accumulate(absl::MakeSpan(in_data, size), 0.0f);"
                }
            ],
            "whole_deleted": "-  const auto size =\n-      absl::c_accumulate(in.dimensions, 1, std::multiplies<int>());\n",
            "whole_added": "+  // Manual calculation is used here instead of absl::c_accumulate to trigger\n+  // sanitizer check for dimensions\n+  auto size = 1;\n+  for (auto dim : in.dimensions) {\n+    size *= dim;\n+  }\n",
            "whole_hunk": "@@ -583,8 +583,12 @@ static absl::Status FfiF32ReduceSum(F32Buffer in, R0F32ResultBuffer out) {\n   auto out_data = out->data.base();\n \n   // Calculate the total size of the vector\n-  const auto size =\n-      absl::c_accumulate(in.dimensions, 1, std::multiplies<int>());\n+  // Manual calculation is used here instead of absl::c_accumulate to trigger\n+  // sanitizer check for dimensions\n+  auto size = 1;\n+  for (auto dim : in.dimensions) {\n+    size *= dim;\n+  }\n \n   // Calculate the sum of the vector\n   *out_data = absl::c_accumulate(absl::MakeSpan(in_data, size), 0.0f);"
        }
    ]
},
{
    "Id": 73,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2f972a133a1fac16a73fd5e2df722664cf7c1523",
    "date": "2024-04-15T21:16:44-07:00",
    "message": "Remove a check for single user of an operand in convert-memory-placement-to-internal-annotations.\n\nPiperOrigin-RevId: 625183655",
    "label": "NO",
    "changes": [
        {
            "name": "convert_memory_placement_to_internal_annotations.cc",
            "path": "third_party/xla/xla/service/convert_memory_placement_to_internal_annotations.cc",
            "patches": [
                {
                    "old_start": 79,
                    "old_length": 10,
                    "new_start": 79,
                    "new_length": 6,
                    "hunk": "@@ -79,10 +79,6 @@ absl::StatusOr<bool> ConvertMemoryPlacementToInternalAnnotations::Run(\n         } else if (is_to_device_case) {\n           VLOG(1) << \"Process backward case: \" << instruction->ToString();\n           HloInstruction* custom_call_operand = instruction->mutable_operand(0);\n-          if (custom_call_operand->users().size() != 1) {\n-            VLOG(1) << \"Skip because operand is used by more than one user\";\n-            continue;\n-          }\n           HloInstruction* new_result =\n               c->AddInstruction(HloInstruction::CreateCustomCall(\n                   custom_call_operand->shape(), {custom_call_operand},"
                }
            ],
            "whole_deleted": "-          if (custom_call_operand->users().size() != 1) {\n-            VLOG(1) << \"Skip because operand is used by more than one user\";\n-            continue;\n-          }\n",
            "whole_added": "",
            "whole_hunk": "@@ -79,10 +79,6 @@ absl::StatusOr<bool> ConvertMemoryPlacementToInternalAnnotations::Run(\n         } else if (is_to_device_case) {\n           VLOG(1) << \"Process backward case: \" << instruction->ToString();\n           HloInstruction* custom_call_operand = instruction->mutable_operand(0);\n-          if (custom_call_operand->users().size() != 1) {\n-            VLOG(1) << \"Skip because operand is used by more than one user\";\n-            continue;\n-          }\n           HloInstruction* new_result =\n               c->AddInstruction(HloInstruction::CreateCustomCall(\n                   custom_call_operand->shape(), {custom_call_operand},"
        }
    ]
},
{
    "Id": 167,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/66d401491c7bd9380d9c87a21b4a0cb40fbad14a",
    "date": "2024-02-06T07:23:06-08:00",
    "message": "Relax the check for valid reduce fusions.\n\nWe can allow also combinations of reshape bitcast and transpose bitcast, which\nessentially means the shapes just need to have the same number of elements.\n\nPiperOrigin-RevId: 604638258",
    "label": "NO",
    "changes": [
        {
            "name": "hlo_fusion_analysis.cc",
            "path": "third_party/xla/xla/service/gpu/hlo_fusion_analysis.cc",
            "patches": [
                {
                    "old_start": 239,
                    "old_length": 10,
                    "new_start": 239,
                    "new_length": 10,
                    "hunk": "@@ -239,10 +239,10 @@ HloFusionAnalysis::EmitterFusionKind HloFusionAnalysis::GetEmitterFusionKind()\n         continue;\n       }\n       if (!IsRealReductionHero(*root, *hero)) {\n-        // Needs to have a compatible shape to the reduce operand.\n-        if (!ShapeUtil::IsReshapeOrTransposeBitcast(\n-                root->shape(), hero_operand_shape,\n-                /*ignore_element_type=*/true)) {\n+        // Needs to have a compatible shape to the reduce operand (compatible\n+        // meaning same number of elements).\n+        if (ShapeUtil::ElementsIn(root->shape()) !=\n+            ShapeUtil::ElementsIn(hero_operand_shape)) {\n           valid_shapes = false;\n           break;\n         }\n"
                }
            ],
            "whole_deleted": "-        // Needs to have a compatible shape to the reduce operand.\n-        if (!ShapeUtil::IsReshapeOrTransposeBitcast(\n-                root->shape(), hero_operand_shape,\n-                /*ignore_element_type=*/true)) {\n",
            "whole_added": "+        // Needs to have a compatible shape to the reduce operand (compatible\n+        // meaning same number of elements).\n+        if (ShapeUtil::ElementsIn(root->shape()) !=\n+            ShapeUtil::ElementsIn(hero_operand_shape)) {\n",
            "whole_hunk": "@@ -239,10 +239,10 @@ HloFusionAnalysis::EmitterFusionKind HloFusionAnalysis::GetEmitterFusionKind()\n         continue;\n       }\n       if (!IsRealReductionHero(*root, *hero)) {\n-        // Needs to have a compatible shape to the reduce operand.\n-        if (!ShapeUtil::IsReshapeOrTransposeBitcast(\n-                root->shape(), hero_operand_shape,\n-                /*ignore_element_type=*/true)) {\n+        // Needs to have a compatible shape to the reduce operand (compatible\n+        // meaning same number of elements).\n+        if (ShapeUtil::ElementsIn(root->shape()) !=\n+            ShapeUtil::ElementsIn(hero_operand_shape)) {\n           valid_shapes = false;\n           break;\n         }\n"
        },
        {
            "name": "multioutput_fusion_test.cc",
            "path": "third_party/xla/xla/tests/multioutput_fusion_test.cc",
            "patches": [
                {
                    "old_start": 494,
                    "old_length": 6,
                    "new_start": 494,
                    "new_length": 26,
                    "hunk": "@@ -494,6 +494,26 @@ ENTRY main {\n   EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));\n }\n \n+XLA_TEST_F(MultiOutputFusionTest, MultiOutputReduceGeneralBitcastCompatible) {\n+  const std::string testcase = absl::StrCat(kScalarOps, R\"(\n+fused_computation {\n+  param_0 = f32[64,128]{1,0} parameter(0)\n+  neg = f32[64,128]{1,0} negate(param_0)\n+  bitcast = f32[8,8,128]{2,1,0} bitcast(neg)\n+  bitcast2 = f32[128,64]{0,1} bitcast(neg)\n+  constant_0 = f32[] constant(0)\n+  reduce.1 = f32[128]{0} reduce(bitcast, constant_0), dimensions={0,1}, to_apply=Add\n+  ROOT tuple.12 = (f32[128]{0}, f32[64,128]{1,0}, f32[128,64]{0,1}) tuple(reduce.1, neg, bitcast2)\n+}\n+\n+ENTRY main {\n+  Arg_2.1 = f32[64,128]{1,0} parameter(0)\n+  ROOT fusion = (f32[128]{0}, f32[64,128]{1,0}, f32[128,64]{0,1}) fusion(Arg_2.1), kind=kInput, calls=fused_computation\n+})\");\n+  auto module = ParseAndReturnVerifiedModule(testcase).value();\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));\n+}\n+\n XLA_TEST_F(MultiOutputFusionTest, MultiOutputReduceWithEpilogue) {\n   const std::string testcase = absl::StrCat(kScalarOps, R\"(\n fused_computation {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+XLA_TEST_F(MultiOutputFusionTest, MultiOutputReduceGeneralBitcastCompatible) {\n+  const std::string testcase = absl::StrCat(kScalarOps, R\"(\n+fused_computation {\n+  param_0 = f32[64,128]{1,0} parameter(0)\n+  neg = f32[64,128]{1,0} negate(param_0)\n+  bitcast = f32[8,8,128]{2,1,0} bitcast(neg)\n+  bitcast2 = f32[128,64]{0,1} bitcast(neg)\n+  constant_0 = f32[] constant(0)\n+  reduce.1 = f32[128]{0} reduce(bitcast, constant_0), dimensions={0,1}, to_apply=Add\n+  ROOT tuple.12 = (f32[128]{0}, f32[64,128]{1,0}, f32[128,64]{0,1}) tuple(reduce.1, neg, bitcast2)\n+}\n+\n+ENTRY main {\n+  Arg_2.1 = f32[64,128]{1,0} parameter(0)\n+  ROOT fusion = (f32[128]{0}, f32[64,128]{1,0}, f32[128,64]{0,1}) fusion(Arg_2.1), kind=kInput, calls=fused_computation\n+})\");\n+  auto module = ParseAndReturnVerifiedModule(testcase).value();\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));\n+}\n+\n",
            "whole_hunk": "@@ -494,6 +494,26 @@ ENTRY main {\n   EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));\n }\n \n+XLA_TEST_F(MultiOutputFusionTest, MultiOutputReduceGeneralBitcastCompatible) {\n+  const std::string testcase = absl::StrCat(kScalarOps, R\"(\n+fused_computation {\n+  param_0 = f32[64,128]{1,0} parameter(0)\n+  neg = f32[64,128]{1,0} negate(param_0)\n+  bitcast = f32[8,8,128]{2,1,0} bitcast(neg)\n+  bitcast2 = f32[128,64]{0,1} bitcast(neg)\n+  constant_0 = f32[] constant(0)\n+  reduce.1 = f32[128]{0} reduce(bitcast, constant_0), dimensions={0,1}, to_apply=Add\n+  ROOT tuple.12 = (f32[128]{0}, f32[64,128]{1,0}, f32[128,64]{0,1}) tuple(reduce.1, neg, bitcast2)\n+}\n+\n+ENTRY main {\n+  Arg_2.1 = f32[64,128]{1,0} parameter(0)\n+  ROOT fusion = (f32[128]{0}, f32[64,128]{1,0}, f32[128,64]{0,1}) fusion(Arg_2.1), kind=kInput, calls=fused_computation\n+})\");\n+  auto module = ParseAndReturnVerifiedModule(testcase).value();\n+  EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));\n+}\n+\n XLA_TEST_F(MultiOutputFusionTest, MultiOutputReduceWithEpilogue) {\n   const std::string testcase = absl::StrCat(kScalarOps, R\"(\n fused_computation {"
        }
    ]
},
{
    "Id": 363,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/dac9af0b3e861336b7db97171a19e83f6365ed51",
    "date": "2023-07-11T16:24:28-07:00",
    "message": "[TF:PJRT] Use ShapeUtil::Compatible when checking the compatibility between buffer on_device_shape and expected execution shape.\n\nBuffer on_device_shape may be dynamic. ShapeUtil::Equal will fail if it is dynamic. An alternative fix is to compare logical_on_device_shape. But getting logical_on_device_shape is blocking and may have performance impact.\n\nPiperOrigin-RevId: 547327689",
    "label": "YES",
    "changes": [
        {
            "name": "pjrt_stream_executor_client.cc",
            "path": "tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc",
            "patches": [
                {
                    "old_start": 1751,
                    "old_length": 29,
                    "new_start": 1751,
                    "new_length": 29,
                    "hunk": "@@ -1751,29 +1751,29 @@ struct TupleHandle {\n };\n \n Status CheckCompatibleShapes(bool strict_shape_checking,\n-                             const Shape& buffer_shape,\n+                             const Shape& buffer_on_device_shape,\n                              const Shape& execution_shape,\n                              const TransferManager& transfer_manager,\n                              int parameter_index) {\n   // TODO(misard) Support casting of tuple parameters.\n-  if (strict_shape_checking || buffer_shape.IsTuple()) {\n-    if (!ShapeUtil::Equal(buffer_shape, execution_shape)) {\n+  if (strict_shape_checking || buffer_on_device_shape.IsTuple()) {\n+    if (!ShapeUtil::Compatible(buffer_on_device_shape, execution_shape)) {\n       return InvalidArgument(\n           \"Executable expected shape %s for argument %d but got \"\n           \"incompatible \"\n           \"shape %s\",\n           ShapeUtil::HumanStringWithLayout(execution_shape), parameter_index,\n-          ShapeUtil::HumanStringWithLayout(buffer_shape));\n+          ShapeUtil::HumanStringWithLayout(buffer_on_device_shape));\n     }\n   } else {\n-    if (transfer_manager.GetByteSizeRequirement(buffer_shape) !=\n+    if (transfer_manager.GetByteSizeRequirement(buffer_on_device_shape) !=\n         transfer_manager.GetByteSizeRequirement(execution_shape)) {\n       return InvalidArgument(\n           \"Executable expected shape %s for argument %d but got \"\n           \"incompatible \"\n           \"shape %s\",\n           ShapeUtil::HumanStringWithLayout(execution_shape), parameter_index,\n-          ShapeUtil::HumanStringWithLayout(buffer_shape));\n+          ShapeUtil::HumanStringWithLayout(buffer_on_device_shape));\n     }\n   }\n   return OkStatus();"
                }
            ],
            "whole_deleted": "-                             const Shape& buffer_shape,\n-  if (strict_shape_checking || buffer_shape.IsTuple()) {\n-    if (!ShapeUtil::Equal(buffer_shape, execution_shape)) {\n-          ShapeUtil::HumanStringWithLayout(buffer_shape));\n-    if (transfer_manager.GetByteSizeRequirement(buffer_shape) !=\n-          ShapeUtil::HumanStringWithLayout(buffer_shape));\n",
            "whole_added": "+                             const Shape& buffer_on_device_shape,\n+  if (strict_shape_checking || buffer_on_device_shape.IsTuple()) {\n+    if (!ShapeUtil::Compatible(buffer_on_device_shape, execution_shape)) {\n+          ShapeUtil::HumanStringWithLayout(buffer_on_device_shape));\n+    if (transfer_manager.GetByteSizeRequirement(buffer_on_device_shape) !=\n+          ShapeUtil::HumanStringWithLayout(buffer_on_device_shape));\n",
            "whole_hunk": "@@ -1751,29 +1751,29 @@ struct TupleHandle {\n };\n \n Status CheckCompatibleShapes(bool strict_shape_checking,\n-                             const Shape& buffer_shape,\n+                             const Shape& buffer_on_device_shape,\n                              const Shape& execution_shape,\n                              const TransferManager& transfer_manager,\n                              int parameter_index) {\n   // TODO(misard) Support casting of tuple parameters.\n-  if (strict_shape_checking || buffer_shape.IsTuple()) {\n-    if (!ShapeUtil::Equal(buffer_shape, execution_shape)) {\n+  if (strict_shape_checking || buffer_on_device_shape.IsTuple()) {\n+    if (!ShapeUtil::Compatible(buffer_on_device_shape, execution_shape)) {\n       return InvalidArgument(\n           \"Executable expected shape %s for argument %d but got \"\n           \"incompatible \"\n           \"shape %s\",\n           ShapeUtil::HumanStringWithLayout(execution_shape), parameter_index,\n-          ShapeUtil::HumanStringWithLayout(buffer_shape));\n+          ShapeUtil::HumanStringWithLayout(buffer_on_device_shape));\n     }\n   } else {\n-    if (transfer_manager.GetByteSizeRequirement(buffer_shape) !=\n+    if (transfer_manager.GetByteSizeRequirement(buffer_on_device_shape) !=\n         transfer_manager.GetByteSizeRequirement(execution_shape)) {\n       return InvalidArgument(\n           \"Executable expected shape %s for argument %d but got \"\n           \"incompatible \"\n           \"shape %s\",\n           ShapeUtil::HumanStringWithLayout(execution_shape), parameter_index,\n-          ShapeUtil::HumanStringWithLayout(buffer_shape));\n+          ShapeUtil::HumanStringWithLayout(buffer_on_device_shape));\n     }\n   }\n   return OkStatus();"
        }
    ]
},
{
    "Id": 696,
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/660ce5a89eb6766834bdc303d2ab3902aef99d3d",
    "date": "2022-09-14T14:59:17-07:00",
    "message": "[Security] Add a check for empty variant tensor input to CompositeTensorVariantToComponents.\n\nSo an exception will be raised instead of segfault.\n\nPiperOrigin-RevId: 474397914",
    "label": "YES",
    "changes": [
        {
            "name": "composite_tensor_ops.cc",
            "path": "tensorflow/core/kernels/composite_tensor_ops.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/variant.h\"\n #include \"tensorflow/core/framework/variant_encode_decode.h\"\n #include \"tensorflow/core/kernels/composite_tensor_variant.h\"\n"
                },
                {
                    "old_start": 66,
                    "old_length": 6,
                    "new_start": 67,
                    "new_length": 11,
                    "hunk": "@@ -66,6 +67,11 @@ class CompositeTensorVariantToComponents : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     Tensor encoded_t = context->input(0);\n+    OP_REQUIRES(\n+        context, encoded_t.flat<Variant>().size() > 0,\n+        errors::InvalidArgument(\"Input `encoded` must not be an empty variant \"\n+                                \"tensor, but got \",\n+                                encoded_t.DebugString()));\n     auto* encoded = encoded_t.flat<Variant>()(0).get<CompositeTensorVariant>();\n \n     // Check that the encoded TypeSpec is compatible with the expected TypeSpec.\n"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+#include \"tensorflow/core/framework/op_requires.h\"\n+    OP_REQUIRES(\n+        context, encoded_t.flat<Variant>().size() > 0,\n+        errors::InvalidArgument(\"Input `encoded` must not be an empty variant \"\n+                                \"tensor, but got \",\n+                                encoded_t.DebugString()));\n",
            "whole_hunk": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/variant.h\"\n #include \"tensorflow/core/framework/variant_encode_decode.h\"\n #include \"tensorflow/core/kernels/composite_tensor_variant.h\"\n@@ -66,6 +67,11 @@ class CompositeTensorVariantToComponents : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     Tensor encoded_t = context->input(0);\n+    OP_REQUIRES(\n+        context, encoded_t.flat<Variant>().size() > 0,\n+        errors::InvalidArgument(\"Input `encoded` must not be an empty variant \"\n+                                \"tensor, but got \",\n+                                encoded_t.DebugString()));\n     auto* encoded = encoded_t.flat<Variant>()(0).get<CompositeTensorVariant>();\n \n     // Check that the encoded TypeSpec is compatible with the expected TypeSpec.\n"
        },
        {
            "name": "composite_tensor_ops_test.py",
            "path": "tensorflow/python/kernel_tests/composite_tensor_ops_test.py",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 11,
                    "new_start": 18,
                    "new_length": 13,
                    "hunk": "@@ -18,11 +18,13 @@ from absl.testing import parameterized\n \n from tensorflow.python.eager import backprop\n from tensorflow.python.eager import context\n+from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import errors\n from tensorflow.python.framework import sparse_tensor\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import composite_tensor_ops\n+from tensorflow.python.ops import gen_composite_tensor_ops\n from tensorflow.python.ops import gradients_impl\n from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import parsing_ops\n"
                },
                {
                    "old_start": 83,
                    "old_length": 6,
                    "new_start": 85,
                    "new_length": 18,
                    "hunk": "@@ -83,6 +85,18 @@ class ExtensionTypeTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, message):\n       composite_tensor_ops.composite_tensor_to_variants(value(), spec)\n \n+  def testDecodingEmptyNonScalarTensorError(self):\n+    if not context.executing_eagerly():\n+      # Creating a variant tensor of an empty list is not allowed in eager mode.\n+      return\n+\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                'must not be an empty variant tensor'):\n+      gen_composite_tensor_ops.CompositeTensorVariantToComponents(\n+          encoded=constant_op.constant([], dtype=dtypes.variant),\n+          metadata='',\n+          Tcomponents=[dtypes.int32])\n+\n   def testRoundTripThroughTensorProto(self):\n     value = ragged_factory_ops.constant([[1, 2], [3], [4, 5, 6]])\n     encoded = composite_tensor_ops.composite_tensor_to_variants(value)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "+from tensorflow.python.framework import constant_op\n+from tensorflow.python.ops import gen_composite_tensor_ops\n+  def testDecodingEmptyNonScalarTensorError(self):\n+    if not context.executing_eagerly():\n+      # Creating a variant tensor of an empty list is not allowed in eager mode.\n+      return\n+\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                'must not be an empty variant tensor'):\n+      gen_composite_tensor_ops.CompositeTensorVariantToComponents(\n+          encoded=constant_op.constant([], dtype=dtypes.variant),\n+          metadata='',\n+          Tcomponents=[dtypes.int32])\n+\n",
            "whole_hunk": "@@ -18,11 +18,13 @@ from absl.testing import parameterized\n \n from tensorflow.python.eager import backprop\n from tensorflow.python.eager import context\n+from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import errors\n from tensorflow.python.framework import sparse_tensor\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import composite_tensor_ops\n+from tensorflow.python.ops import gen_composite_tensor_ops\n from tensorflow.python.ops import gradients_impl\n from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import parsing_ops\n@@ -83,6 +85,18 @@ class ExtensionTypeTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, message):\n       composite_tensor_ops.composite_tensor_to_variants(value(), spec)\n \n+  def testDecodingEmptyNonScalarTensorError(self):\n+    if not context.executing_eagerly():\n+      # Creating a variant tensor of an empty list is not allowed in eager mode.\n+      return\n+\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                'must not be an empty variant tensor'):\n+      gen_composite_tensor_ops.CompositeTensorVariantToComponents(\n+          encoded=constant_op.constant([], dtype=dtypes.variant),\n+          metadata='',\n+          Tcomponents=[dtypes.int32])\n+\n   def testRoundTripThroughTensorProto(self):\n     value = ragged_factory_ops.constant([[1, 2], [3], [4, 5, 6]])\n     encoded = composite_tensor_ops.composite_tensor_to_variants(value)"
        }
    ]
}]