[{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c",
    "message": "Verify types in custom op schemas (#124520)\n\nBefore this PR, we didn't check that types in a schema were valid. This\nis because TorchScript treats unknown types as type variables.\n\nThis PR checks types in a schema for the TORCH_LIBRARY APIs. To do this,\nwe add an `allow_typevars` flag to parseSchema so that TorchScript can\nuse allow_typevars=True. We also add some error messages for common\nmistakes (e.g. using int64_t or double in schema).\n\nTest Plan:\n- new tests\n\nDifferential Revision: [D56432690](https://our.internmc.facebook.com/intern/diff/D56432690)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124520\nApproved by: https://github.com/albanD",
    "date": "2024-04-23T14:18:35+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/csrc/jit/frontend/function_schema_parser.cpp",
            "patches": []
        },
        {
            "path": "torch/csrc/jit/frontend/function_schema_parser.h",
            "patches": []
        },
        {
            "path": "torch/csrc/jit/frontend/schema_type_parser.cpp",
            "patches": []
        },
        {
            "path": "torch/csrc/jit/frontend/schema_type_parser.h",
            "patches": []
        },
        {
            "path": "torch/csrc/jit/ir/irparser.cpp",
            "patches": []
        },
        {
            "path": "torch/csrc/jit/python/init.cpp",
            "patches": []
        },
        {
            "path": "torch/csrc/jit/runtime/static/passes.cpp",
            "patches": []
        },
        {
            "path": "torch/library.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fbc7559ceb372d88b55c96ef6984accbaa0ec3ec",
    "message": "[custom ops] convert string type annotation to real type (#128809)\n\nFixes #105157\n\nBug source: `from __future__ import annotations` converts type annotation to strings to make forwards references easier. However, existing custom ops do not consider strings to be valid types.\n\nFix: We check if the argument and return type annotation is string type. If so, we try to use `eval` to convert it to a type.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128809\nApproved by: https://github.com/zou3519",
    "date": "2024-06-18T00:55:50+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_library/infer_schema.py",
            "patches": [
                {
                    "Id": 1,
                    "hunk size": 12,
                    "hunk": "@@ -38,13 +51,19 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n         if param.annotation is inspect.Parameter.empty:\n             error_fn(f\"Parameter {name} must have a type annotation.\")\n \n-        if param.annotation not in SUPPORTED_PARAM_TYPES.keys():\n+        # The annotation might be converted to a string by annotation,\n+        # we convert it to the actual type.\n+        annotation_type = param.annotation\n+        if type(annotation_type) == str:\n+            annotation_type = convert_type_string(annotation_type)\n+\n+        if annotation_type not in SUPPORTED_PARAM_TYPES.keys():\n             error_fn(\n                 f\"Parameter {name} has unsupported type {param.annotation}. \"\n                 f\"The valid types are: {SUPPORTED_PARAM_TYPES.keys()}.\"\n             )\n \n-        schema_type = SUPPORTED_PARAM_TYPES[param.annotation]\n+        schema_type = SUPPORTED_PARAM_TYPES[annotation_type]\n         if name in mutates_args:\n             if not schema_type.startswith(\"Tensor\"):\n                 error_fn(\n"
                },
                {
                    "Id": 2,
                    "hunk size": 7,
                    "hunk": "@@ -72,7 +91,10 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n             f\"mutates_args should contain the names of all args that the \"\n             f\"custom op mutates.\"\n         )\n-    ret = parse_return(sig.return_annotation, error_fn)\n+    return_annotation = sig.return_annotation\n+    if type(return_annotation) == str:\n+        return_annotation = convert_type_string(return_annotation)\n+    ret = parse_return(return_annotation, error_fn)\n     return f\"({', '.join(params)}) -> {ret}\"\n \n "
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ac768333be2ad7cd3435c1ae257bdaf297595714",
    "message": "[dynamo] fix prim lowering validation logic for dynamic shape args (#111208)\n\nFixes https://github.com/pytorch/pytorch/issues/111199\n\nFixes https://github.com/pytorch/pytorch/issues/111203\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111208\nApproved by: https://github.com/ezyang",
    "date": "2023-10-13T18:36:13+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6",
    "message": "[RFC] Introduce Checkpointable for DCP (#127540) (#127628)\n\nSummary:\n# Introduce Checkpointable interface for DCP to support arbitrary tensor subclasses for checkpointing\n\n**Authors:**\n* zainhuda\n\n## **Summary**\nThis diff adds a CheckpointableTensor interface to allow for future compatibility for any tensor subclass with DCP in a clean and maintainable way.\n\n## **Motivation**\nFor TorchRec sharding migration from ShardedTensor to DTensor, we create a tensor subclass that is stored by DTensor to support TorchRec's sharding schemes (ex, empty shards, multiple shards on a rank).\n\n## **Proposed Implementation**\nView the CheckpointableTensor interface implementation, in which, we introduce the minimal set of methods needed to be compatible with DCP. These methods are expected to implemented by any tensor subclasses and as such are then checkpointable by DCP.\n\n## **Drawbacks**\nNo drawbacks, it extends functionality in a clean and maintainable way.\n\n## **Alternatives**\nAlternative design was creating paths for checking for certain attributes in tensor subclasses which can get messy and hard to maintain/understand why it was there in the first place.\n\nTest Plan:\nSandcastle\n\ncc mrshenli pritamdamania87 zhaojuanmao satgera gqchen aazzolini osalpekar jiayisuse H-Huang kwen2501 awgu penguinwu fegin XilunWu wanchaol fduwjj wz337 tianyu-l wconstab yf225 chauhang d4l3k LucasLLC\n\nDifferential Revision: D57970603\n\nPulled By: iamzainhuda\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127628\nApproved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/fegin",
    "date": "2024-06-03T21:21:55+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/distributed/checkpoint/planner.py",
            "patches": []
        },
        {
            "path": "torch/distributed/checkpoint/planner_helpers.py",
            "patches": [
                {
                    "Id": 3,
                    "hunk size": 9,
                    "hunk": "@@ -217,7 +218,12 @@ def _create_default_metadata_only_plan(state_dict: STATE_DICT_TYPE) -> SavePlan:\n \n \n def _create_write_items(fqn: str, object: Any) -> List[WriteItem]:\n-    if isinstance(object, DTensor):\n+    if isinstance(object, _Checkpointable):\n+        return object._create_write_items(fqn, object)\n+    elif isinstance(object, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(object.to_local(), _Checkpointable):\n+            return object.to_local()._create_write_items(fqn, object)  # type: ignore[arg-type]\n         return [_create_write_items_for_dtensor(fqn, object)]\n     elif isinstance(object, ShardedTensor):\n         return [\n"
                },
                {
                    "Id": 4,
                    "hunk size": 9,
                    "hunk": "@@ -242,7 +248,12 @@ def _create_chunk_from_dtensor(tensor: DTensor) -> ChunkStorageMetadata:\n \n \n def _create_chunk_list(tensor: torch.Tensor) -> List[ChunkStorageMetadata]:\n-    if isinstance(tensor, DTensor):\n+    if isinstance(tensor, _Checkpointable):\n+        local_chunks = tensor._create_chunk_list(tensor)\n+    elif isinstance(tensor, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(tensor.to_local(), _Checkpointable):\n+            return tensor.to_local()._create_chunk_list(tensor)  # type: ignore[arg-type]\n         local_chunks = [_create_chunk_from_dtensor(tensor)]\n     elif isinstance(tensor, ShardedTensor):\n         local_chunks = [\n"
                }
            ]
        },
        {
            "path": "torch/distributed/checkpoint/utils.py",
            "patches": [
                {
                    "Id": 5,
                    "hunk size": 9,
                    "hunk": "@@ -301,7 +302,12 @@ def _find_shard(tensor: ShardedTensor, index: MetadataIndex) -> Shard:\n \n \n def find_tensor_shard(tensor: torch.Tensor, index: MetadataIndex) -> torch.Tensor:\n-    if isinstance(tensor, DTensor):\n+    if isinstance(tensor, _Checkpointable):\n+        return tensor._get_tensor_shard(tensor, index)\n+    elif isinstance(tensor, DTensor):\n+        # DTensor can contain a local tensor that is a tensor subclass\n+        if isinstance(tensor.to_local(), _Checkpointable):\n+            return tensor.to_local()._get_tensor_shard(tensor, index)  # type: ignore[arg-type]\n         return tensor.to_local()\n     if isinstance(tensor, ShardedTensor):\n         return _find_shard(tensor, index).tensor"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ff432c048d353ff6944ccf50591c171372a924f7",
    "message": "[easy] Remove duplicate exprs in produce_guards (#111270)\n\nSummary: We're checking the original guard.expr in the issued set instead of the simplified expr, leading to duplicate guards in cases where one expression simplifies to another.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111270\nApproved by: https://github.com/Chillee, https://github.com/ezyang",
    "date": "2023-10-16T18:31:38+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/c36b31d5302d31746f3f3bd64ed8d9acd8e36155",
    "message": "`torch::nn::AdaptiveLogSoftmaxWithLoss`: check length of `cutoffs` (#106777)\n\nFixes #106698\n\nAlso added a check for python API, because current error message\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/sehoon/pytorch-latest/torch/nn/modules/adaptive.py\", line 128, in __init__\n    or (min(cutoffs) <= 0) \\\nValueError: min() arg is an empty sequence\n```\nis not very comprehensible.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/106777\nApproved by: https://github.com/albanD",
    "date": "2023-10-05T05:35:47+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/c1d960aadd23bb261724c11f34d4a852ffaacfd6",
    "message": "[Quant] [Inductor] add input shape check for quantized conv binary lowering (#115247)\n\nAdd inputs shape check for quantized conv binary lowering, since qconv2d_pointwise.binary does not yet support the case of broadcasting shape inputs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115247\nApproved by: https://github.com/leslie-fang-intel, https://github.com/eellison",
    "date": "2023-12-21T01:36:49+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08",
    "message": "[pipelining] Validate stage input/output shape/dtype (#126732)\n\nAddress the classes of user errors stemming from (possibly)\nunintentional dynamic shapes usage or mismatch of configuration time and\nrun time data shapes/dtypes.\n\nThe goal is to ensure a clear error is raised rather than relying on some underlying\nerror to bubble up when a tensor shape is not compatible, or worse,\nhaving a silent correctness issue.\n\n**Classes of shape/dtype errors**\n* (a) error is thrown within the stage-module forward code, but may be\nhard to understand/trace back to an input issue\n* (b) silent correctness issue happens inside the stage-module forward,\nbut the correct output shape is still produced\nproduces the expected output shape\n* (c) the stage-module produces an output that is locally correct, but not\nmatching the expectation of the following stage, leading to a hang or\ncorrectness issue down the line\n\n**How validation helps**\n\nInput shape validation\n- improves debugability of case (a)\n- guards against case (b)\n- only needed on first stage, since subsequent stages use pre-allocated recv\n  buffers that can't change shape/size even if they wanted to\n\nOutput shape validation\n- guards against case (c)\n\nValidation of first stage input and all stages' outputs inductively verifies all shapes\n\nShape/dtype are most critical as they literally affect the number of\nbytes on the wire.  Strides and other tensor properties may also (?)\nmatter, and the validation function can be adjusted accordingly if needed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126732\nApproved by: https://github.com/kwen2501",
    "date": "2024-05-23T20:16:06+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/distributed/pipelining/PipelineStage.py",
            "patches": [
                {
                    "Id": 6,
                    "hunk size": 14,
                    "hunk": "@@ -763,9 +821,21 @@ class _PipelineStage(_PipelineStageBase):\n                 if dst_rank is not None:\n                     dsts.append(dst_rank)\n \n+        output_node = self._get_output_node()\n+        output_vals: Tuple[torch.Tensor] = tuple(\n+            v.meta[\"val\"] for v in flatten_args(output_node.args)\n+        )\n+        self._configure_outputs_meta(output_vals)\n+\n         logger.debug(f\"{self.log_prefix} \" f\"Send info: {act_send_info}\")  # noqa: G004\n         return act_send_info\n \n+    def _get_output_node(self):\n+        output_nodes = [node for node in self.submod.graph.nodes if node.op == \"output\"]\n+        assert len(output_nodes) == 1\n+        output_node = output_nodes[0]\n+        return output_node\n+\n     def _create_grad_recv_info(\n         self,\n         act_send_info: Dict,\n"
                },
                {
                    "Id": 7,
                    "hunk size": 7,
                    "hunk": "@@ -775,9 +845,8 @@ class _PipelineStage(_PipelineStageBase):\n         \"\"\"\n         # Dict[output_index, _RecvInfo]\n         grad_recv_info: Dict[int, _RecvInfo] = {}\n-        output_nodes = [node for node in self.submod.graph.nodes if node.op == \"output\"]\n-        assert len(output_nodes) == 1\n-        output_node = output_nodes[0]\n+        output_node = self._get_output_node()\n+\n         # The output node may take multiple args, meaning the submod having multiple output values.\n         output_vals = flatten_args(output_node.args)\n \n"
                }
            ]
        },
        {
            "path": "torch/distributed/pipelining/_utils.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/362bc6d7cbcac57466a52701fac3ba3bfb668000",
    "message": "Fixed a segfault issue when passing an empty kernel to quantized_max_\u2026 (#116342)\n\n\u2026pool1d.\n\nFixes #116323.\n\nReused the same check as for `max_pool1d`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116342\nApproved by: https://github.com/jerryzh168",
    "date": "2023-12-27T01:22:49+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/43069c460e87ba77fd69d2193635179779328e27",
    "message": "Correct check for Boolean list input type (#124899)\n\nSummary:\nThis diff fixes a bug in PyTorch where when creating a tensor from a List of booleans, PyTorch was throwing an error.\n\nThis fix resolves that issue. All credit goes to swolchok for identifying the root cause of the issue and suggesting this fix.\n\nTest Plan: Running our model end to end works as expected and no error occurs.\n\nDifferential Revision: D55990810\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124899\nApproved by: https://github.com/zhxchen17",
    "date": "2024-04-26T22:25:43+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/csrc/jit/runtime/register_special_ops.cpp",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253",
    "message": "[cuDNN] Graph-capturable cuDNN CTCLoss (#128271)\n\ncuDNN v8.x added a graph-capturable CTCLoss, which slots \"neatly\" into the `Tensor` variant\n\n~~WIP as cuDNN has a restriction on the max target length (255), but this is not checkable in the graph-capture case, so the UX around warnings/error-messages here might need to be tuned...~~\nCurrently checks restriction on max target length during warmup run(s), and bails out during capture if this constraint was violated during warmup.\n\nCC @ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128271\nApproved by: https://github.com/ezyang",
    "date": "2024-06-21T21:40:23+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "aten/src/ATen/cudnn/Descriptors.h",
            "patches": [
                {
                    "Id": 8,
                    "hunk size": 11,
                    "hunk": "@@ -357,6 +357,15 @@ struct TORCH_CUDA_CPP_API CTCLossDescriptor\n     AT_CUDNN_CHECK(\n         cudnnSetCTCLossDescriptorEx(mut_desc(), datatype, normMode, gradMode));\n   }\n+  void set_v9(\n+      cudnnDataType_t datatype,\n+      cudnnLossNormalizationMode_t normMode,\n+      cudnnCTCGradMode_t gradMode,\n+      int maxLabelLength) {\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v9(mut_desc(), datatype, normMode, gradMode, maxLabelLength));\n+  }\n+\n };\n \n struct TORCH_CUDA_CPP_API ActivationDescriptor\n"
                }
            ]
        },
        {
            "path": "aten/src/ATen/native/LossCTC.cpp",
            "patches": [
                {
                    "Id": 9,
                    "hunk size": 9,
                    "hunk": "@@ -539,8 +539,13 @@ Tensor ctc_loss(const Tensor& log_probs_, const Tensor& targets, IntArrayRef inp\n \n // Convenience function accepting Tensors\n Tensor ctc_loss(const Tensor& log_probs, const Tensor& targets, const Tensor& input_lengths, const Tensor& target_lengths, int64_t BLANK, int64_t reduction, bool zero_infinity) {\n+  // we don't want to convert to IntArrayRef if we can dispatch to cuDNN (this allows graph-capturable ctc_loss)\n+  bool use_cudnn =\n+      (log_probs.device().type() == at::kCUDA) &&\n+      at::_use_cudnn_ctc_loss(\n+          log_probs, targets, input_lengths, target_lengths, BLANK);\n   if (at::areAnyTensorSubclassLike(\n-          {log_probs, targets, input_lengths, target_lengths})) {\n+          {log_probs, targets, input_lengths, target_lengths}) || use_cudnn) {\n     // Composite Compliant path for TensorSubclasses\n     return ctc_loss_impl(log_probs, targets, input_lengths, target_lengths, BLANK, reduction, zero_infinity);\n   }\n"
                }
            ]
        },
        {
            "path": "aten/src/ATen/native/cudnn/LossCTC.cpp",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba",
    "message": "[minifier] Add config flag to ignore non-fp values (#123006)\n\nWhen minifying, the after-aot minifier ignores non-floating values by\ndefault but does check them when running the the initial graph dump step.\nThis means we may capture a graph that doesn't fail the tester and doesn't have\nany meaningful divergence.\n\nFor example, the derivative of `elu(x)` depends on `x > 0` so this value is\nsaved for backwards and so becomes a graph output. However, the difference\nbetween `FLT_MIN` and `0` in `x` is now enough to trigger an accuracy failure.\n\nI fix this by adding a config variable and environment variable to ignore these\nnon floating point values.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123006\nApproved by: https://github.com/ezyang\nghstack dependencies: #123005",
    "date": "2024-04-09T03:34:09+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_dynamo/config.py",
            "patches": []
        },
        {
            "path": "torch/_dynamo/repro/after_aot.py",
            "patches": [
                {
                    "Id": 10,
                    "hunk size": 10,
                    "hunk": "@@ -709,7 +710,13 @@ def repro_run(options, mod, load_args):\n     if options.accuracy != \"\":\n         # We don't really respect --accuracy vs --strict-accuracy here, it\n         # seems counterintuitive\n-        if not same_two_models(mod, compiled, args, only_fwd=True):\n+        if not same_two_models(\n+            mod,\n+            compiled,\n+            args,\n+            only_fwd=True,\n+            ignore_non_fp=config.repro_ignore_non_fp,\n+        ):\n             raise AccuracyError(\"Bad accuracy detected\")\n     else:\n         need_sync = False\n"
                }
            ]
        },
        {
            "path": "torch/_dynamo/repro/after_dynamo.py",
            "patches": [
                {
                    "Id": 11,
                    "hunk size": 4,
                    "hunk": "@@ -78,7 +88,7 @@ def wrap_backend_debug(unconfigured_compiler_fn, compiler_name: str):\n             if config.repro_level == 4:\n                 # Check Accuracy\n                 compiled_gm = compiler_fn(copy.deepcopy(gm), example_inputs)\n-                if backend_accuracy_fails(gm, example_inputs, compiler_fn):\n+                if _accuracy_fails(gm, example_inputs, compiler_fn):\n                     log.warning(\n                         \"Accuracy failed for the TorchDynamo produced graph. Creating script to minify the error.\"\n                     )\n"
                },
                {
                    "Id": 12,
                    "hunk size": 9,
                    "hunk": "@@ -304,17 +314,14 @@ def dynamo_accuracy_minifier_backend(gm, example_inputs, compiler_name):\n     gm.eval()\n \n     # Check Accuracy\n-    if backend_accuracy_fails(\n-        gm, example_inputs, compiler_fn, only_fwd=config.repro_forward_only\n-    ):\n+    if _accuracy_fails(gm, example_inputs, compiler_fn):\n         log.warning(\"Accuracy failed for the TorchDynamo produced graph\")\n         dump_state_fn = functools.partial(\n             dump_backend_state, compiler_name=compiler_name, check_accuracy=True\n         )\n         fails_fn = functools.partial(\n-            backend_accuracy_fails,\n+            _accuracy_fails,\n             compiler_fn=compiler_fn,\n-            only_fwd=config.repro_forward_only,\n         )\n         dump_state_fn(fx.GraphModule(gm, copy.deepcopy(gm.graph)), example_inputs)\n         minifier(\n"
                },
                {
                    "Id": 13,
                    "hunk size": 10,
                    "hunk": "@@ -424,7 +431,13 @@ def repro_run(options, mod, load_args):\n             # TODO: disable clone\n             args = run_load_args(options, mod, load_args)\n             assert same_two_models(mod, mod, args), \"Eager itself failed\"\n-            if not same_two_models(mod, opt_mod, args):\n+            if not same_two_models(\n+                mod,\n+                opt_mod,\n+                args,\n+                only_fwd=config.repro_forward_only,\n+                ignore_non_fp=config.repro_ignore_non_fp,\n+            ):\n                 raise AccuracyError(\"Dynamo failed\")\n     else:\n         with torch.cuda.amp.autocast(enabled=options.autocast):"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2ebf2c88baa4667d55eda92f4c8424db505af781",
    "message": "Add test to check that COW inputs are not materialized (#119507)\n\nPart of #97856\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119507\nApproved by: https://github.com/ezyang\nghstack dependencies: #120455",
    "date": "2024-02-28T00:37:33+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/f8d60e0e0a4420def0cf6b3bc0a0c0d46c93de1c",
    "message": "[Inductor][CPP] Fix Half data type cse cache issue for CPP Backend (#128498)\n\n**Summary**\nFixing issue: https://github.com/pytorch/pytorch/issues/128263. After https://github.com/pytorch/pytorch/issues/115260, we cached the higher precision cse variable to avoid duplicate casting between buffers. However, it failed to check the original data type. This means if we convert `int32` to `bf16` for `store` and then convert `bf16` back to `fp32` for `load`, it would incorrectly hit the cache and reuse the `int32` cse var. This PR fixes the issue.\n\n**Test Plan**\n```\npython -u -m pytest -s -v test/inductor/test_cpu_repro.py -k test_issue_128263\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128498\nApproved by: https://github.com/jgong5, https://github.com/zhuhaozhe, https://github.com/jerryzh168",
    "date": "2024-06-16T11:27:13+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/codegen/cpp.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7553c495147f3e21a1e27d392d277906a47768e7",
    "message": "[S382174] Fix distributed debug w/ non-equal split (#115483)\n\nSummary:\nIn collectives, it's possible to have non-equal split that has a different implementation and the output tensor size will be different, e.g. https://www.internalfb.com/code/fbsource/[460afb1172b5]/fbcode/caffe2/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp?lines=3104. However, TORCH_DISTRIBUTED_DEBUG=DETAIL will assume the output tensor size is the same and does the check and will fail the job if they don't: https://fburl.com/code/mhte9ty8. c10d code should handle this.\n\nIdeally we should check the input size across ranks and make sure they're the same. Maybe for next diff.\n\nTest Plan: Test torchrec's TWRW w/ non-even split and it's working now.\n\nReviewed By: zhangruiskyline\n\nDifferential Revision: D52010942\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115483\nApproved by: https://github.com/kwen2501, https://github.com/fegin, https://github.com/XilunWu",
    "date": "2023-12-12T18:02:05+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e14d1d10ef9d24bf43366ac1f05a5aa8b732707b",
    "message": "Unwrap Identity in prepare indexing (#130967)\n\nWe wrap indexing calculation in the concat kernel in `Identity` so that we do not expand int32 intermediates to int64. This was causing an issue where the index simplified to an integer and would not hit an intended [path](https://github.com/pytorch/pytorch/blob/752c81789829cfce94f9664db97cc45aaae8ce32/torch/_inductor/codegen/triton.py#L1554) which would do wrapping with tl.full.\n\nI couldn't generate a minimal repro to add as test but I have a repro you can check here: P1483831261 There is already a test that we dont expand the int32 intermediates to int64.\n\nDifferential Revision: [D59871850](https://our.internmc.facebook.com/intern/diff/D59871850)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130967\nApproved by: https://github.com/Chillee, https://github.com/jansel",
    "date": "2024-07-18T00:43:53+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/codegen/simd.py",
            "patches": [
                {
                    "Id": 14,
                    "hunk size": 13,
                    "hunk": "@@ -694,7 +694,16 @@ class SIMDKernel(Kernel):\n                     replacements = {a: V.graph.sizevars.lookup_precomputed_size(a)}\n                     index = sympy_subs(index, replacements)\n \n-        return self.codegen_indexing(self.simplify_indexing(index))\n+        simp_index = self.simplify_indexing(index)\n+\n+        # Now that we are done simplifying we can unwrap Identity so that downstream handling\n+        # for its contained expression will work. previously, tl.full wrapping of sympy.Integer\n+        # would not occur\n+        simp_index = (\n+            simp_index if not isinstance(simp_index, Identity) else simp_index.args[0]\n+        )\n+\n+        return self.codegen_indexing(simp_index)\n \n     def active_range_trees(self, reorder=False):\n         trees = ["
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/15745a52b0d7dc5c3d0ecc061b1604288064f641",
    "message": "Inductor: don't change the stride_order of FlexibleLayout if it's already the same as required (#122945)\n\n## Pitch\nFixes https://github.com/pytorch/pytorch/issues/122489.\nDon't change the `stride_order` of `FlexibleLayout` if it already has the stride with the order required.\n\n## Description\nFor a layout that's both contiguous and channels last contiguous (for example `size=[s0, 1, 28, 28]`, `stride=[784, 784, 28, 1]` where the `C` dim is `1`), the behavior of calling  [require_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4053) (where the order is specified as channels last) on it is different when it's a `FixedLayout` or a `FlexibleLayout`.\n\n- For a `FixedLayout`, the size and stride is unchanged after the call: `size=[s0, 1, 28, 28]`, `stride=[784, 784, 28, 1]`.\n- For a `FlexibleLayout`, it will become `size=[s0, 1, 28, 28]`, `stride=[784, 1, 28, 1])`.\n\nWhen weight is not prepacked (in dynamic shapes cases), the Conv extern kernel returns output in channels **first** for input with `size=[s0, 1, 28, 28]`, `stride=[784, 784, 28, 1]` but output in channels **last** for `size=[s0, 1, 28, 28]`, `stride=[784, 1, 28, 1])`.\n\nIn this PR, for a `FlexibleLayout`, we add a check to see if it already has the stride in the required order. If that's the case, we don't change its stride order when freezing it. This makes the behavior of  calling [require_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4053) aligned for `FixedLayout` and `FlexibleLayout`.\n\n## Additional context\nFor a `FixedLayout`, when calling  [require_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4053), it will firstly run into [x.get_layout().is_stride_ordered(order)](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4067-L4070) to check if it's already ordered as expected.\n\nIf it is a `FlexibleLayout`, when calling  [require_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4053),  it runs into [as_storage_and_layout](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L4063-L4065), which will always [freeze_layout_with_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L1805) and will always call [as_stride_order](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L2909), without checking if the default stride of this `FlexibleLayout` (which has been realized before) is already as expected ([link](https://github.com/pytorch/pytorch/blob/069270db60112704c6387cf994cf9b7298731699/torch/_inductor/ir.py#L2693-L2700)).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122945\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "date": "2024-04-09T10:00:30+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/ir.py",
            "patches": [
                {
                    "Id": 15,
                    "hunk size": 20,
                    "hunk": "@@ -4086,9 +4086,25 @@ class ExternKernel(InputsKernel):\n             while isinstance(x.get_layout(), NonOwningLayout):\n                 x = x.get_layout().view\n             if isinstance(x.get_layout(), FlexibleLayout):\n+                # If the the FlexibleLayout already has the size and stride in the required order,\n+                # freeze it to a FixedLayout by using its current size and stride.\n+                # The behavior of using its current size and stride or the given order can be different\n+                # if the size and stride has ambiguilty, for example for a 4D input where the iC = 1:\n+                # size=[s0, 1, 28, 28], stride=[784, 784, 28, 1]. If the required order is [3, 0, 2, 1] (channels last),\n+                # the current size and stride already satisfies this order.\n+                # However by freezing it to the required order, the layout will be changed to:\n+                # size=[s0, 1, 28, 28], stride=[784, 1, 28, 1]), which is not actually necessary.\n+\n                 # fix flexiblelayout to be FixedLayout with stride_order\n                 as_storage_and_layout(\n-                    x, freeze=True, want_contiguous=False, stride_order=order\n+                    x,\n+                    freeze=True,\n+                    want_contiguous=False,\n+                    stride_order=get_stride_order(\n+                        V.graph.sizevars.size_hints(x.get_layout().stride)\n+                    )\n+                    if is_stride_order_storage_and_layout(x, order)\n+                    else order,\n                 )\n                 return x\n             elif isinstance("
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244",
    "message": "Revert \"[Cmake] Check that gcc-9.4 or newer is used (#112858)\"\n\nThis reverts commit ad894cd0728e97c649cd9b33e1f98b18fa12a1da.\n\nReverted https://github.com/pytorch/pytorch/pull/112858 on behalf of https://github.com/PaliC due to breaking internal tests (check diff for test page) ([comment](https://github.com/pytorch/pytorch/pull/112858#issuecomment-1795485009))",
    "date": "2023-11-06T16:56:09+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0fef82b3dfb4a5da1555a3c046950594b83e2898",
    "message": "[dcp] fix fsdp state_dict to use run_check=False (#114995)\n\nfrom_local with replicate placement would run mesh_broadcast if\nrun_check=True, by default from_local have run_check=True, but for FSDP\nstate_dict case we are for sure that these are replica already, so we\ndon't need to check/force check it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114995\nApproved by: https://github.com/fegin, https://github.com/XilunWu, https://github.com/wz337",
    "date": "2023-12-02T04:16:37+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/9782439277fb545ec4e14340ecf75df86ddc4f0f",
    "message": "[Profiler] Do not emit a warning when using CPU profiler (#125654)\n\nThis fixes a logic regression introduced by https://github.com/pytorch/pytorch/pull/123247 where\n```python\nif self.use_device and self.use_device != _get_privateuse1_backend_name():\n```\nwas replaced with\n```python\n        VALID_DEVICE_OPTIONS = [\"cuda\", \"xpu\", \"privateuseone\"]\n        if self.use_device not in VALID_DEVICE_OPTIONS:\n```\n\nThat triggers a warning every time code is invoke with `self.use_device` set to None\n\nThis change also skips all the checks which are useless if `use_device` is None to begin with\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125654\nApproved by: https://github.com/aaronenyeshi",
    "date": "2024-05-07T16:56:17+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/autograd/profiler.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/bf998a2c5d549cf4856c7becfca4a169bf68b709",
    "message": "[quant][pt2e][be] Cleanup observer insertion logic (#111828)\n\nSummary:\natt, after SharedQuantizationSpec bug fix we are doing some checks before hand, this can simplify the logic when we insert observers\n\nTest Plan:\npython test/test_quantization.py TestQuantizePT2E\n\nCIs\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111828\nApproved by: https://github.com/kimishpatel\nghstack dependencies: #111827",
    "date": "2023-10-25T03:48:36+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb",
    "message": "[FSDP2] Computed grad divide factors at runtime (#125484)\n\n**Context**\nWe are interested in supporting the case where HSDP reduce-scatters but does not all-reduce in a microbatch backward. This saves communication while still saving memory. Only on the last microbatch do we need to both reduce-scatter and all-reduce. This is not implemented yet and will hopefully come in a future PR.\n\nThere is one notable part of doing this. On the last microbatch, we need to perform an accumulation step after reduce-scatter and before all-reduce. If not, then the preceding microbatch's gradients will not be contributed across the replica group. (In other words, we cannot simply accumulate _after_ all-reduce.)\n\nConsider 32 GPUs with 4-way replication and 8-way sharding and 2 microbatches, and focus on global rank 0.\n- After the first microbatch, rank 0 will have its shard of $\\frac{1}{8} \\sum_{i \\in S(0)} g_i^{(1)}$, where we define $S(0) = \\{0, 1, \\dots, 7\\}$ to be the ranks in its shard group and we define the $(1)$ superscript to denote the first microbatch.\n- Upon the second microbatch, rank 0 after its reduce-scatter will additionally have its shard of $\\frac{1}{8} \\sum_{i \\in S(0)} g_i^{(2)}$. If we only all-reduce this, then this second microbatch's gradients become $\\frac{1}{32} \\sum_{i=0, 1, \\dots, 31} g_i^{(2)}$, so in total, rank 0 has $\\frac{1}{8} \\sum_{i \\in S(0)} g_i^{(1)} + \\frac{1}{32} \\sum_{i=0, 1, \\dots, 31} g_i^{(2)}$, which is wrong.\n- Importantly, we must accumulate $\\frac{1}{8} \\sum_{i \\in S(0)} g_i^{(1)}  + \\frac{1}{8} \\sum_{i \\in S(0)} g_i^{(2)} = \\frac{1}{8}\\sum_{i \\in S(0)} (g_i^{(1)} + g_i^{(2)})$ first before all-reducing to get $\\frac{1}{32} \\sum_{i=0, 1, \\dots, 31} (g_i^{(1)} + g_i^{(2)})$.\n\nNow, note how under this approach, we want a factor of $\\frac{1}{8}$ only (i.e. reciprocal of the shard group size), not $\\frac{1}{32}$, for the first microbatch's gradients.\n- For bf16/fp32, since we use `ReduceOp.AVG` and we only reduce-scatter on the first microbatch, we correctly have a factor of $\\frac{1}{8}$ on the first microbatch.\n- For fp16, since we precompute the gradient divide factors at init time assuming always reducing over both shard and replica groups, we incorrectly have a factor of $\\frac{1}{32}$ on the first microbatch, deviating from the bf16/fp32 case.\n\nWe can address this issue by matching the bf16/fp32 vs. fp16 semantics by computing the divide factors at runtime based on which process groups were passed into the reduction function (`foreach_reduce`).\n\n**Additional Notes**\nHow to implement the HSDP reduce-scatter but no all-reduce is not entirely clear yet. (What is the cleanest way to do this?) We need to store the partial reduce-scatter output and check for it upon the next backward. We should also be sure to error if the set of parameters receiving gradients changes, in which case we cannot support this easily. Anyway, we will implement this in a follow-up.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125484\nApproved by: https://github.com/wanchaol\nghstack dependencies: #125431, #125479",
    "date": "2024-05-03T23:44:05+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/distributed/_composable/fsdp/_fsdp_collectives.py",
            "patches": [
                {
                    "Id": 16,
                    "hunk size": 18,
                    "hunk": "@@ -166,18 +167,22 @@ def foreach_reduce(\n             (reduce_scatter_output_numel,)\n         )\n         _div_if_needed(reduce_scatter_input, predivide_factor)\n-        _reduce_scatter(\n-            post_reduce_output,\n-            reduce_scatter_input,\n-            reduce_scatter_group,\n-            divide_factors,\n+        dist.reduce_scatter_tensor(\n+            output=post_reduce_output,\n+            input=reduce_scatter_input,\n+            group=reduce_scatter_group,\n+            op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,\n         )\n     view_out_stream = reduce_scatter_stream\n     if all_reduce_group is not None:\n         view_out_stream = all_reduce_stream\n         all_reduce_stream.wait_stream(reduce_scatter_stream)\n         with torch.cuda.stream(all_reduce_stream):\n-            _all_reduce(post_reduce_output, all_reduce_group, divide_factors)\n+            dist.all_reduce(\n+                post_reduce_output,\n+                group=all_reduce_group,\n+                op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,\n+            )\n     with torch.cuda.stream(view_out_stream):\n         _div_if_needed(post_reduce_output, postdivide_factor)\n         post_reduce_output = _to_dtype_if_needed(post_reduce_output, orig_dtype)\n"
                }
            ]
        },
        {
            "path": "torch/distributed/_composable/fsdp/_fsdp_param_group.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0f81473d7b4a1bf09246410712df22541be7caf3",
    "message": "Update fake tensor error checks for bool tensor subtraction (#128492)\n\nFixes #127003\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128492\nApproved by: https://github.com/soulitzer",
    "date": "2024-06-17T13:41:15+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_refs/__init__.py",
            "patches": [
                {
                    "Id": 17,
                    "hunk size": 11,
                    "hunk": "@@ -1712,6 +1712,15 @@ def sub(\n \n     a, b = _maybe_broadcast(a, b)\n \n+    if isinstance(a, TensorLike) and isinstance(b, TensorLike):\n+        torch._check(\n+            not utils.is_boolean_dtype(a.dtype) and not utils.is_boolean_dtype(b.dtype),\n+            lambda: (\n+                \"Subtraction, the `-` operator, with two bool tensors is not supported. \"\n+                \"Use the `^` or `logical_xor()` operator instead.\"\n+            ),\n+        )\n+\n     if alpha != 1:\n         dtype = a.dtype if isinstance(a, TensorLike) else b.dtype  # type: ignore[union-attr]\n         python_type = utils.dtype_to_type(dtype)"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/610f64d72a53c644591e83fbecdd8a8f1702f548",
    "message": "inductor: also check index_exp when  select tiling var (#106765)\n\nFor select tiling var, currently, we only consider load and store which do not consider index exp, and meet accuracy issues:\n\nbefore(the index exp ```i1-1``` can not be vectrized):\n```\ncpp_fused_constant_pad_nd_mul_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_xiaobing/i5/ci5uspp363v3ky6jkccllm3bxudy2fkdpqinkqhmpehfihejs7ko.h\"\nextern \"C\" void kernel(const float* in_ptr0,\n                       const float* in_ptr1,\n                       float* out_ptr0)\n{\n    #pragma omp parallel num_threads(40)\n    {\n        {\n            #pragma omp for\n            for(long i0=static_cast<long>(0L); i0<static_cast<long>(64L); i0+=static_cast<long>(1L))\n            {\n                #pragma GCC ivdep\n                for(long i1=static_cast<long>(0L); i1<static_cast<long>(3136L); i1+=static_cast<long>(16L))\n                {\n                    #pragma GCC ivdep\n                    for(long i2=static_cast<long>(0L); i2<static_cast<long>(8L); i2+=static_cast<long>(1L))\n                    {\n                        auto tmp0 = at::vec::Vectorized<int>(static_cast<int>((-1L) + i1));\n                        auto tmp1 = at::vec::Vectorized<int>(static_cast<int>(0));\n                        auto tmp2 = to_float_mask(tmp0 >= tmp1);\n                        auto tmp3 = [&]\n                        {\n                            auto tmp4 = ([&]() { __at_align__ float tmpbuf[16]; for (long i1_inner = 0; i1_inner < 16; i1_inner++) tmpbuf[i1_inner] = in_ptr0[static_cast<long>((-8L) + i2 + (8L*i1) + (8L*i1_inner) + (25088L*i0))]; return at::vec::Vectorized<float>::loadu(tmpbuf); })();\n                            auto tmp5 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>((-1L) + i1 + (3136L*i2) + (25088L*i0)));\n                            auto tmp6 = tmp4 * tmp5;\n                            return tmp6;\n                        }\n                        ;\n                        auto tmp7 = decltype(tmp3())::blendv(at::vec::Vectorized<float>(0.0), tmp3(), to_float_mask(tmp2));\n                        { __at_align__ float tmpbuf[16*sizeof(float)/sizeof(float)]; tmp7.store(tmpbuf); for (long i1_inner = 0; i1_inner < 16; i1_inner++) out_ptr0[static_cast<long>(i2 + (8L*i1) + (8L*i1_inner) + (25096L*i0))] = tmpbuf[i1_inner]; }\n                    }\n                }\n                #pragma GCC ivdep\n                for(long i1=static_cast<long>(3136L); i1<static_cast<long>(3137L); i1+=static_cast<long>(1L))\n                {\n                    #pragma GCC ivdep\n                    for(long i2=static_cast<long>(0L); i2<static_cast<long>(8L); i2+=static_cast<long>(1L))\n                    {\n                        auto tmp0 = static_cast<long>((-1L) + i1);\n                        auto tmp1 = static_cast<long>(0);\n                        auto tmp2 = tmp0 >= tmp1;\n                        auto tmp3 = [&]\n                        {\n                            auto tmp4 = in_ptr0[static_cast<long>((-8L) + i2 + (8L*i1) + (25088L*i0))];\n                            auto tmp5 = in_ptr1[static_cast<long>((-1L) + i1 + (3136L*i2) + (25088L*i0))];\n                            auto tmp6 = decltype(tmp4)(tmp4 * tmp5);\n                            return tmp6;\n                        }\n                        ;\n                        auto tmp7 = tmp2 ? tmp3() : static_cast<decltype(tmp3())>(0.0);\n                        out_ptr0[static_cast<long>(i2 + (8L*i1) + (25096L*i0))] = tmp7;\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\nafter:\n```\ncpp_fused_constant_pad_nd_mul_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_xiaobing/i5/ci5uspp363v3ky6jkccllm3bxudy2fkdpqinkqhmpehfihejs7ko.h\"\nextern \"C\" void kernel(const float* in_ptr0,\n                       const float* in_ptr1,\n                       float* out_ptr0)\n{\n    #pragma omp parallel num_threads(40)\n    {\n        {\n            #pragma omp for\n            for(long i0=static_cast<long>(0L); i0<static_cast<long>(64L); i0+=static_cast<long>(1L))\n            {\n                #pragma GCC ivdep\n                for(long i1=static_cast<long>(0L); i1<static_cast<long>(3137L); i1+=static_cast<long>(1L))\n                {\n                    #pragma omp simd simdlen(8)\n                    for(long i2=static_cast<long>(0L); i2<static_cast<long>(8L); i2+=static_cast<long>(1L))\n                    {\n                        auto tmp0 = static_cast<long>((-1L) + i1);\n                        auto tmp1 = static_cast<long>(0);\n                        auto tmp2 = tmp0 >= tmp1;\n                        auto tmp3 = [&]\n                        {\n                            auto tmp4 = in_ptr0[static_cast<long>((-8L) + i2 + (8L*i1) + (25088L*i0))];\n                            auto tmp5 = in_ptr1[static_cast<long>((-1L) + i1 + (3136L*i2) + (25088L*i0))];\n                            auto tmp6 = decltype(tmp4)(tmp4 * tmp5);\n                            return tmp6;\n                        }\n                        ;\n                        auto tmp7 = tmp2 ? tmp3() : static_cast<decltype(tmp3())>(0.0);\n                        out_ptr0[static_cast<long>(i2 + (8L*i1) + (25096L*i0))] = tmp7;\n                    }\n                }\n            }\n        }\n    }\n}\n''')\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/106765\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "date": "2023-08-23T07:16:14+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332",
    "message": "Fix mypy issues in fake_tensor.py (#124428)\n\nfake_tensor.py had mypy error ignored. That seems less than desirable.\n\nAlso added SafePyObjectT<T> which is a tagged wrapper around a SafePyObject but provides static type checking (with no other guarantees).\n\nUsed `SafePyObjectT<TorchDispatchModeKey>` on some of the TorchDispatchModeTLS API to ensure that we don't accidentally inject a different type than expected into the stack.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124428\nApproved by: https://github.com/malfet",
    "date": "2024-04-26T15:35:53+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "c10/core/SafePyObject.h",
            "patches": []
        },
        {
            "path": "c10/core/impl/TorchDispatchModeTLS.cpp",
            "patches": []
        },
        {
            "path": "c10/core/impl/TorchDispatchModeTLS.h",
            "patches": []
        },
        {
            "path": "torch/_C/__init__.pyi.in",
            "patches": []
        },
        {
            "path": "torch/_ops.py",
            "patches": []
        },
        {
            "path": "torch/_subclasses/fake_tensor.py",
            "patches": [
                {
                    "Id": 18,
                    "hunk size": 4,
                    "hunk": "@@ -991,7 +1011,7 @@ class FakeTensorMode(TorchDispatchMode):\n         except _BypassDispatchCache as e:\n             FakeTensorMode.cache_bypasses[e.reason] += 1\n \n-        if output is unassigned:\n+        if output is _UNASSIGNED:\n             output = self._dispatch_impl(func, types, args, kwargs)\n \n         return output\n"
                },
                {
                    "Id": 19,
                    "hunk size": 4,
                    "hunk": "@@ -1177,7 +1197,7 @@ class FakeTensorMode(TorchDispatchMode):\n \n         # Synthesize a new FakeTensor with the cached metadata.\n         metadata = entry.metadata\n-        assert not metadata.is_sparse\n+        assert metadata and not metadata.is_sparse\n \n         empty = torch.empty_strided(\n             metadata.shape,\n"
                }
            ]
        },
        {
            "path": "torch/csrc/autograd/init.cpp",
            "patches": []
        },
        {
            "path": "torch/csrc/utils/torch_dispatch_mode.h",
            "patches": []
        },
        {
            "path": "torch/utils/_python_dispatch.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/4240304da4ddc42335b0219bae11f072ca240fe5",
    "message": "[TorchElastic] Handle SystemExit with code == 0 (#119697)\n\nSummary:\nFix for a case where --run-path option fails to exit if the script exits with non-error status code.\nWhen there is an error exit code, run-path correctly detects an error and fails when calling spawn.join(). However for-non error case, current behavior is to check the return value of the operation and the fix is to return None so that our MP code detects an exit.\n\nTest Plan:\ncat /tmp/script.py\n~~~\nimport sys\ndef main():\n    exit_code = 1\n    if len(sys.argv) > 1:\n        exit_code = int(sys.argv[1])\n    sys.exit(exit_code)\n\nif __name__==\"__main__\":\n    main()\n~~~\n\nCase of exit code with 0 (prior behavior - never exits):\ntorchrun --run-path /tmp/script.py 0\n\n~~~\n[2024-02-12 09:20:57,523] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n[2024-02-12 09:20:58,980] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n(conda:pytorch) \u279c  workspace echo $?\n0\n~~~\n\nExisting behavior for non-zero exit code still works:\ntorchrun --run-path /tmp/script.py\n~~~\n(conda:pytorch) \u279c  workspace torchrun --run-path /tmp/script.py\n[2024-02-12 09:16:20,667] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n[2024-02-12 09:16:22,197] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 64668) of fn: run_script_path (start_method: spawn)\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR] Traceback (most recent call last):\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR]   File \"/Users/kurman/workspace/pytorch/torch/distributed/elastic/multiprocessing/api.py\", line 441, in _poll\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR]     self._pc.join(-1)\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR]   File \"/Users/kurman/workspace/pytorch/torch/multiprocessing/spawn.py\", line 177, in join\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR]     raise ProcessExitedException(\n[2024-02-12 09:16:25,795] torch.distributed.elastic.multiprocessing.api: [ERROR] torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1\nTraceback (most recent call last):\n  File \"/Users/kurman/miniconda3/envs/pytorch/bin/torchrun\", line 33, in <module>\n    sys.exit(load_entry_point('torch', 'console_scripts', 'torchrun')())\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kurman/workspace/pytorch/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/kurman/workspace/pytorch/torch/distributed/run.py\", line 812, in main\n    run(args)\n  File \"/Users/kurman/workspace/pytorch/torch/distributed/run.py\", line 803, in run\n    elastic_launch(\n  File \"/Users/kurman/workspace/pytorch/torch/distributed/launcher/api.py\", line 135, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kurman/workspace/pytorch/torch/distributed/launcher/api.py\", line 268, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n============================================================\nrun_script_path FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-02-12_09:16:25\n  host      : kurman-mbp.dhcp.thefacebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 64668)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n~~~\n\nDifferential Revision: D53653874\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119697\nApproved by: https://github.com/wconstab",
    "date": "2024-02-14T03:09:09+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/distributed/elastic/multiprocessing/errors/__init__.py",
            "patches": [
                {
                    "Id": 20,
                    "hunk size": 9,
                    "hunk": "@@ -345,6 +345,13 @@ def record(\n             error_handler.initialize()\n             try:\n                 return f(*args, **kwargs)\n+            except SystemExit as se:\n+                # For run_path based entrypoints, SystemExit with code = 0 will never exit.\n+                # Handling it here by returning a value:\n+                if se.code == 0:\n+                    return None\n+                else:\n+                    raise\n             except ChildFailedError as e:\n                 rank, failure = e.get_first_failure()\n                 if failure.error_file != _NOT_AVAILABLE:"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/9a95b4bc7bbc66f0513d01e1c2af8e1ead5c1077",
    "message": "[dtensor] quick fix to #109306 (#109428)\n\nLooks like the op argument schema type check is not reliable.. for\nthings like aten.div.Tensor(Tensor, Tensor), the second argument can still be\na float/scalar for some reason, switch to check with the instance type\ndirectly\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109428\nApproved by: https://github.com/awgu, https://github.com/fegin",
    "date": "2023-09-16T20:53:55+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/56ef200c2dc8a1f1e269861b7a6e02e99d3b56a1",
    "message": "When doing typed typecheck, also check signature with symint removed (#109727)\n\nSee the test case for what we didn't catch (SymInt vs const SymInt&\nmismatch.)\n\nIt's necessary to test for both, because we will fall back to the\nnon-SymInt signature if there is no SymInt unboxed kernel available.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109727\nApproved by: https://github.com/zou3519",
    "date": "2023-09-22T12:12:10+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7a4e4511845dbeefe4d16c321b2a93ac72b76d93",
    "message": "[Dynamo] Fix function overrides (#120885)\n\nTo check existence of `__torch_function__`, the code intended to iterate each element but got `TupleVariable` when the ordinary `has_torch_function()` was being used. Needs further unpack in this case\n\nFixes #120653\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120885\nApproved by: https://github.com/yanboliang",
    "date": "2024-03-11T02:18:43+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_dynamo/variables/torch.py",
            "patches": [
                {
                    "Id": 21,
                    "hunk size": 9,
                    "hunk": "@@ -399,8 +399,13 @@ class TorchInGraphFunctionVariable(BaseTorchVariable):\n             torch.overrides.has_torch_function_unary,\n         ):\n             assert not kwargs\n+            elems = (\n+                args[0].unpack_var_sequence(tx)\n+                if len(args) == 1 and isinstance(args[0], TupleVariable)\n+                else args\n+            )\n             return ConstantVariable.create(\n-                any(has_torch_function(a) for a in args),\n+                any(has_torch_function(x) for x in elems),\n             )\n         elif any(\n             self.value is method"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f",
    "message": "[AOTInductor] Improve validation for C++ wrapper codegen (#111102)\n\nIt's a reimplementation of #111089\n\n1. When using fake inputs make sure they are on the same device as the original inputs.\n2. Don't change the value of self.cpp_wrapper from True to False if can't generate a C++ wrapper, instead have a check and fail early to avoid producing Python code for C++ compiler.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111102\nApproved by: https://github.com/desertfire, https://github.com/jgong5, https://github.com/chunyuan-w",
    "date": "2023-10-13T08:46:17+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7564f04389478df3fce478547dbee0dce9434f1a",
    "message": "[generate_opcheck_tests] add type checking (#109638)\n\nTest Plan:\n- lintrunner\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109638\nApproved by: https://github.com/bdhirsh, https://github.com/soulitzer\nghstack dependencies: #109637",
    "date": "2023-09-20T06:33:37+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/73ba226d986711e2474da52ec67bf9719c902611",
    "message": "[inductor] Linear time dead node elimination (#129082)\n\nThe nodes are already topologically sorted by this point, so DCEing a chain of\nnodes will take one full iteration per node. Simply reversing the iteration\norder means all users will be removed before checking a node.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129082\nApproved by: https://github.com/lezcano",
    "date": "2024-06-22T12:38:17+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_inductor/scheduler.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e",
    "message": "[export] add SchemaCheckMode testing for pre-dispatch export, OpInfo (#125481)\n\nThis adds a new dispatch mode, PreDispatchSchemaCheckMode, built on top of SchemaCheckMode, used for verifying op schemas for functionalization for PreDispatch IR. More specifically, the mode runs in eager mode on concrete inputs, checking if op schemas incorrectly claim to be functional, but are aliasing or mutating. This mode is pushed to the pre-dispatch mode stack, and run before decompositions.\n\nCurrent testing is hooked up to OpInfo, containing 1103 tests on 600 unique ops. Below is a list of ops that fail testing. One caveat is we only raise errors on ops that claim to be functional - if an op schema admits aliasing or mutating but fails testing for the other, it still may decompose further and become functional.\n\nList of failed ops:\n```\naten.atleast_1d.default\naten.atleast_2d.default\naten.atleast_3d.default\naten.cartesian_prod.default\naten.conj_physical.default\naten.alpha_dropout.default\naten.feature_dropout.default\naten.feature_alpha_dropout.default\naten.unsafe_chunk.default\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125481\nApproved by: https://github.com/tugsbayasgalan",
    "date": "2024-05-14T21:07:21+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_ops.py",
            "patches": [
                {
                    "Id": 22,
                    "hunk size": 18,
                    "hunk": "@@ -430,28 +438,36 @@ class _ModeStackStateForPreDispatch:\n         return self.__infra_modes[index]\n \n     def count(self):\n-        return len([i for i in self.__infra_modes if i is not None])\n+        return len([i for i in self.__infra_modes if i is not None]) + int(\n+            self._schema_check_mode is not None\n+        )\n \n \n _mode_stack_state_for_pre_dispatch = _ModeStackStateForPreDispatch()\n \n \n-def unset_mode_pre_dispatch(mode_key):\n+def unset_mode_pre_dispatch(mode_key, schema_check=False):\n     current_mode_stack_pre_dispatch = mode_stack_state_for_pre_dispatch()\n-    assert mode_key in (\n+    assert mode_key is None or mode_key in (\n         torch._C._TorchDispatchModeKey.PROXY,\n         torch._C._TorchDispatchModeKey.FUNCTIONAL,\n     )\n+    if schema_check:\n+        assert mode_key is None\n \n     def _unset_mode():\n         if mode_key == torch._C._TorchDispatchModeKey.PROXY:\n             current_mode = current_mode_stack_pre_dispatch.get(0)\n             mode_stack_state_for_pre_dispatch().set(0, None)\n             return current_mode\n-        else:\n+        elif mode_key == torch._C._TorchDispatchModeKey.FUNCTIONAL:\n             current_mode = current_mode_stack_pre_dispatch.get(1)\n             mode_stack_state_for_pre_dispatch().set(1, None)\n             return current_mode\n+        else:\n+            current_mode = mode_stack_state_for_pre_dispatch()._schema_check_mode\n+            mode_stack_state_for_pre_dispatch()._schema_check_mode = None\n+            return current_mode\n \n     current_mode = _unset_mode()\n \n"
                },
                {
                    "Id": 23,
                    "hunk size": 5,
                    "hunk": "@@ -501,9 +532,10 @@ def _pop_mode_from_pre_dispatch():\n     if pre_dispatch_len == 0:\n         raise AssertionError(\"Trying to pop empty mode stack\")\n \n+    if mode_stack._schema_check_mode is not None:\n+        return unset_mode_pre_dispatch(None, schema_check=True)\n     if mode_stack.get(1) is not None:\n         return unset_mode_pre_dispatch(torch._C._TorchDispatchModeKey.FUNCTIONAL)\n-\n     if mode_stack.get(0) is not None:\n         return unset_mode_pre_dispatch(torch._C._TorchDispatchModeKey.PROXY)\n \n"
                }
            ]
        },
        {
            "path": "torch/_subclasses/schema_check_mode.py",
            "patches": []
        },
        {
            "path": "torch/utils/_python_dispatch.py",
            "patches": [
                {
                    "Id": 24,
                    "hunk size": 5,
                    "hunk": "@@ -213,12 +214,15 @@ def _disable_current_modes():\n \n     has_proxy_mode_in_pre_dispatch = False\n     has_functional_mode_in_pre_dispatch = False\n+    has_schema_check_mode_in_pre_dispatch = False\n \n     for i in old_pre_dispatch_modes:\n         if isinstance(i, ProxyTorchDispatchMode):\n             has_proxy_mode_in_pre_dispatch = True\n         if isinstance(i, FunctionalTensorMode):\n             has_functional_mode_in_pre_dispatch = True\n+        if isinstance(i, SchemaCheckMode):\n+            has_schema_check_mode_in_pre_dispatch = True\n \n     mode_len = _len_torch_dispatch_stack()\n     old_modes = [_pop_mode() for _ in range(mode_len)]\n"
                },
                {
                    "Id": 25,
                    "hunk size": 9,
                    "hunk": "@@ -235,6 +239,13 @@ def _disable_current_modes():\n             raise AssertionError(\n                 \"Can't have ProxyTorchDispatchMode available both in PreDispatch and Python Key\"\n             )\n+        if (\n+            isinstance(old, SchemaCheckMode)\n+            and has_schema_check_mode_in_pre_dispatch\n+        ):\n+            raise AssertionError(\n+                \"Can't have SchemaCheckMode available both in PreDispatch and Python Key\"\n+            )\n \n     # Manually disable proxy and fake modes, if any are active\n     try:"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d3e8b8bf47206c27b6c5fdc021f7c2c3a8009521",
    "message": "Remove cuda check in the CUDAGraph destructor (#127382)\n\nFixes #125804\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127382\nApproved by: https://github.com/eqy, https://github.com/eellison",
    "date": "2024-06-19T08:09:31+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "aten/src/ATen/cuda/CUDAGeneratorImpl.cpp",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e98135d1ad2f999fec649ecd21b35f3d5676be43",
    "message": "[aota] Needs autograd if an input requires_grad, agnostic to enable_grad (#128890)\n\nReland of:  https://github.com/pytorch/pytorch/pull/128016\n\nSummary from previous PR:\nWe assume only two possible mutually exclusive scenarios:\n\nRunning compiled region for training (Any of inputs has requires_grad)\n\nProduced differentiable outputs should have requires_grad.\nRunning compiled region for inference (None of inputs has requires_grad)\n\nAll outputs do not have requires_grad.\nEven if user runs the region under no_grad(), but has an input Tensor with requires_grad - we go Training scenario (1).\n\nWith current state that means:\n1/ needs_autograd should not check torch.is_grad_enabled(), only that any of inputs requires_grad\n2/ if needs_autograd => trace_joint (We are in training scenario 1.) => always run compiled region under with.enable_grad()\n\nChanges in partitioner?\n\nInference and Training graphs had difference in return container, list/tuple.\nThe changes in partitioner are done to unify and return always tuple.\nAs a result - some changes in test_aotdispatch.py for graph contents list -> tuple.\n\nWhy was revert?\n\nThere was a regression of hf_Reformer model on inference.\n```\nTORCHINDUCTOR_FX_GRAPH_CACHE=0 python benchmarks/dynamo/torchbench.py --performance --inference --bfloat16 --backend inductor --device cuda --only hf_Reformer --cold-start-latency --use-eval-mode\n```\n\nBecause one of the compiled graphs contained outputs, which are aliases to the inputs that are nn.Parameter(requires_grad=True).\n\nEven if inference bencharmsk torchbench runs inside with` torch.no_grad()` - alias (specifically for hf_Reformer - expand) ops preserve requires_grad.\n\nAs a result we started compiling training graph instead of inference.\n\nFix for view ops:\n\nIf we have outputs, that are aliases to inputs that requires_grad, those outputs requires grad is not a reason to generate training graph.\n\nThis is handled in aot_autograd.py, where output_and_mutation_safe are calculated.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128890\nApproved by: https://github.com/bdhirsh",
    "date": "2024-07-18T08:27:53+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_functorch/_aot_autograd/runtime_wrappers.py",
            "patches": [
                {
                    "Id": 26,
                    "hunk size": 10,
                    "hunk": "@@ -304,7 +304,13 @@ def _create_runtime_wrapper(\n             for idx in indices_of_inps_to_detach:\n                 if isinstance(args_[idx], torch.Tensor):\n                     args_[idx] = args_[idx].detach()\n-            with torch.autograd._force_original_view_tracking(True):\n+\n+            # It's possible to have trace_joint inside user specified with no_grad() region,\n+            # if there is a nested with enable_grad(), that forces some outputs to require gradients.\n+            # Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\n+            with torch.autograd._force_original_view_tracking(\n+                True\n+            ), torch.enable_grad():\n                 all_outs = call_func_at_runtime_with_args(\n                     compiled_fn, args_, disable_amp=disable_amp, steal_args=True\n                 )\n"
                }
            ]
        },
        {
            "path": "torch/_functorch/aot_autograd.py",
            "patches": [
                {
                    "Id": 27,
                    "hunk size": 7,
                    "hunk": "@@ -572,9 +572,8 @@ def create_aot_dispatcher_function(\n \n         fake_flat_args = process_inputs(flat_args)\n \n-        needs_autograd = (\n-            any(x.requires_grad for x in fake_flat_args if isinstance(x, Tensor))\n-            and torch.is_grad_enabled()\n+        needs_autograd = any(\n+            x.requires_grad for x in fake_flat_args if isinstance(x, Tensor)\n         )\n \n         with enable_python_dispatcher():\n"
                }
            ]
        },
        {
            "path": "torch/_functorch/partitioners.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0fd1fc17c3a53cb4cede1992d431d5384b2813f3",
    "message": "[MPS] Fix `abs` for complex types (#125662)\n\nBy calling `realPartOfTensor:` if input type is complex on Sonoma and fall back to `at::view_as_real` trick on Ventura.\n\nSplit `unary_op` template into `unary_op` and `unary_op_noresize`, which skips resize and empty checks\n\nMarked `abs`, `isclose` and `nn.functional.softsign` OpInfo tests as supported by complex types\n\nFixes https://github.com/pytorch/pytorch/issues/125135\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125662\nApproved by: https://github.com/kulinseth",
    "date": "2024-05-07T22:15:20+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "aten/src/ATen/native/mps/MPSGraphSonomaOps.h",
            "patches": []
        },
        {
            "path": "aten/src/ATen/native/mps/operations/UnaryOps.mm",
            "patches": [
                {
                    "Id": 28,
                    "hunk size": 17,
                    "hunk": "@@ -75,23 +75,10 @@ static bool is_empty_tensor(const Tensor& self) {\n   return self.numel() == 0;\n }\n \n-static void unary_op(const Tensor& self,\n-                     const Tensor& output_,\n-                     std::string op_name,\n-                     UnaryOpBlock unaryBlock,\n-                     is_noop_p is_noop = is_empty_tensor) {\n+static void unary_op_noresize(const Tensor& self, const Tensor& output_, std::string op_name, UnaryOpBlock unaryBlock) {\n   TORCH_CHECK(!(!is_macos_13_or_newer() && self.scalar_type() == ScalarType::Byte),\n               \"MPS support unary op with uint8 natively starting from macOS 13.0\");\n \n-  if (!output_.is_same_size(self)) {\n-    output_.resize_(self.sizes());\n-  }\n-\n-  if (is_noop(self)) {\n-    output_.copy_(self);\n-    return;\n-  }\n-\n   auto output = output_;\n   bool needsCopyToOutput = false;\n   if (output.storage_offset() || !output.is_contiguous()) {\n"
                },
                {
                    "Id": 29,
                    "hunk size": 19,
                    "hunk": "@@ -139,6 +126,23 @@ static void unary_op(const Tensor& self,\n   }\n }\n \n+static void unary_op(const Tensor& self,\n+                     const Tensor& output_,\n+                     std::string op_name,\n+                     UnaryOpBlock unaryBlock,\n+                     is_noop_p is_noop = is_empty_tensor) {\n+  if (!output_.is_same_size(self)) {\n+    output_.resize_(self.sizes());\n+  }\n+\n+  if (is_noop(self)) {\n+    output_.copy_(self);\n+    return;\n+  }\n+\n+  unary_op_noresize(self, output_, op_name, unaryBlock);\n+}\n+\n MPSGraphTensor* trunc_tensor(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {\n   // Rounding is a no-op for integral types, and also a reasonable workaround\n   // For MPSGraph bug on Apple Silicon, that throws `Function floorOp_i64 was not found in the library`\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7731c97e065cffe4efe82698980a2c8297816dbd",
    "message": "Revert \"Fix checking symbolic shapes inside torch._check (#113811)\"\n\nThis reverts commit 7f224f6714419f3d56e64a66079340b0e914a2ca.\n\nReverted https://github.com/pytorch/pytorch/pull/113811 on behalf of https://github.com/jeanschmidt due to Breaking inductor tests on main ([comment](https://github.com/pytorch/pytorch/pull/113811#issuecomment-1816024288))",
    "date": "2023-11-17T09:29:45+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314",
    "message": "Skip launching kernels with zero grid in AOT Inductor (#110312)\n\nSummary: with the grid computed in terms of unbacked `SymInt`s, it can happen that the grid is zero size. This causes CUDA error on `cuLaunchKernel` in the AOT Inductor codegen.\n\nIn this PR, when the grid contains unbacked `SymInt`s, a check is added around the `launchKernel` in the AOT Inductor's C++ wrapper codegen to make sure that the grid is not zero-size.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110312\nApproved by: https://github.com/chenyang78",
    "date": "2023-09-30T09:12:56+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/606c4f1367c7eb4a49aa4a9538dd2b1eb92485d6",
    "message": "[PT] [ST] fix test_sharded_tensor (#124103)\n\nSummary:\nhttps://github.com/pytorch/pytorch/pull/123230 formalizes the rank validation to support sub groups.\n\nIt broke a few UTs, some of which got fixed in https://github.com/pytorch/pytorch/pull/123778\n\nThis is to fix the remaining one reported by DanilBaibak\n\nTest Plan: CI\n\nDifferential Revision: D56155076\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124103\nApproved by: https://github.com/fegin",
    "date": "2024-04-16T21:18:22+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/distributed/_shard/sharded_tensor/api.py",
            "patches": []
        },
        {
            "path": "torch/distributed/_shard/sharded_tensor/utils.py",
            "patches": [
                {
                    "Id": 30,
                    "hunk size": 4,
                    "hunk": "@@ -23,7 +23,7 @@ def _parse_and_validate_remote_device(pg, remote_device):\n     device = remote_device.device()\n \n     # Validate rank, skip validation if rank is not part of process group.\n-    if not c10d._rank_not_in_group(pg):\n+    if rank is not None and not c10d._rank_not_in_group(pg):\n         pg_global_ranks = c10d.get_process_group_ranks(pg)\n         if rank not in pg_global_ranks:\n             raise ValueError("
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/4926146537df5a39846dae0e551d394001588b13",
    "message": "[Inductor] Fix Conv Binary Inplace Fusion issue (#115153)\n\n**Summary**\nTake this Pattern as example\n```\n  #      ReLU\n  #     /   \\\n  #  Conv1\n  #   /      \\\n  # Conv2\n  #   \\      /\n  #      Add\n```\nThe current `ConvBinaryInplace` check will fail to perform Inplace fusion (using outplace fusion instead) due to `ReLU` having 2 users. However, if all users of `ReLU` are ancestor nodes of `Conv2`, we should be able to proceed with the `ConvBinaryInplace` fusion. This diff relaxes the `ConvBinaryInplace` check accordingly.\n\n**TestPlan**\n```\npython -m pytest test_mkldnn_pattern_matcher.py -k test_conv2d_binary_inplace_fusion_pass_cpu\npython -m pytest test_mkldnn_pattern_matcher.py -k test_conv2d_binary_inplace_fusion_failed_cpu\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115153\nApproved by: https://github.com/CaoE, https://github.com/jgong5",
    "date": "2024-01-04T01:06:27+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/fx_passes/mkldnn_fusion.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/1877b7896c237567285804ecc138bc86180a7ced",
    "message": "[checkpoint] Clean up selective activation checkpoint and make public (#125795)\n\n### bc-breaking for existing users of the private API:\n- Existing policy functions must now change their return value to be [CheckpointPolicy](https://github.com/pytorch/pytorch/blob/c0b40ab42e38a208351911496b7153511304f8da/torch/utils/checkpoint.py#L1204-L1230)  Enum instead of bool.\n   - To restore previous behavior, return `PREFER_RECOMPUTE` instead of `False` and `{PREFER,MUST}_SAVE` instead of `True` depending whether you prefer the compiler to override your policy.\n- Policy function now accepts a `ctx` object instead of `mode` for its first argument.\n   - To restore previous behavior, `mode = \"recompute\" if ctx.is_recompute else \"forward\"`.\n- Existing calls to `_pt2_selective_checkpoint_context_fn_gen` must be renamed to `create_selective_checkpoint_contexts `. The way you use the API remains the same. It would've been nice to do something different (not make the user have to use functools.partial?), but this was the easiest to compile (idk if this should actually be a constraint).\n\nRelated doc: https://docs.google.com/document/d/1BKyizkZPdri9mHqdDOLAUpkI7SbbKfLHRFVVpK9ZWqo/edit\n\nMemory considerations:\n- As with the existing SAC, cached values are cleared upon first use.\n- We error if the user wishes to backward a second time on a region forwarded with SAC enabled.\n\nIn-place:\n- We use version counting to enforce that if any cached tensor has been mutated. In-place operations not mutating cached tensors are allowed.\n- `allow_cache_entry_mutation=True` can be passed to disable this check (useful in the case of auto AC where the user is cleverly also saves the output of the in-place)\n\nRandomness, views\n- Currently in this PR, we don't do anything special for randomness or views, the author of the policy function is expected to handle them properly. (Would it would be beneficial to error? - we either want to save all or recompute all random tensors)\n\nTensor object preservation\n- ~We guarantee that if a tensor does not requires grad, and it is saved, then what you get out is the same tensor object.~ UPDATE: We guarantee that if a tensor is of non-differentiable dtype AND it is not a view, and it is saved, then what you get out is the same tensor object. This is a nice guarantee for nested tensors which care about the object identity of of the offsets tensor.\n\nPolicy function\n- Enum values are `{MUST,PREFER}_{SAVE,RECOMPUTE}` (bikeshed welcome). Alternatively there was `{SAVE,RECOMPUTE}_{NON_,}OVERRIDABLE`. The former was preferred bc it seemed clearer that two `MUST` clashing should error, versus it is ambiguous whether two `NON_OVERRIDABLE` being stacked should silently ignore or error.\n- The usage of Enum today. There actually is NO API to stack SAC policies today. The only thing the Enum should matter for in the near term is the compiler. The stacking SAC policy would be useful if someone wants to implement something like simple FSDP, but it is not perfect because with a policy of `PREFER_SAVE` you are actually saving more than autograd would save normally (would be fixed with AC v3).\n- The number of times we call the policy_fn is something that should be documented as part of public API. We call the policy function for all ops except ~~detach~~ UPDATE :  metadata ops listed in `torch.utils.checkpoint.SAC_IGNORED_OPS`) because these ops may be called a different number of times by AC itself between forward and recompute.\n- The policy function can be a stateful object (we do NOT make separate copies of this object for forward/recompute, the user is expected to handle that via is_recompute see below).\nTensors guaranteed to be the same tensor as-is\n- Policy function signature takes ctx object as its first argument. The ctx function is an object encapsulating info that may be useful to the user, it currently only holds \"is_recompute\". Adding this indirection gives us flexibility to add more attrs later if necessary.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125795\nApproved by: https://github.com/Chillee, https://github.com/fmassa",
    "date": "2024-06-18T18:18:50+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "docs/source/checkpoint.rst",
            "patches": []
        },
        {
            "path": "torch/_higher_order_ops/wrap.py",
            "patches": []
        },
        {
            "path": "torch/utils/checkpoint.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a",
    "message": "[c10d] Add an option for NAN check on every collective (#125726)\n\nSummary:\nThe NAN CHECK is done through device side assert without copying needed\nfrom GPU to CPU\nTest Plan:\nUnit test for collectives that should experience run time error\n\n(sqzhang_1) [sqzhang@devgpu009.cln1 ~/pytorch (38f5143e)]$  python\ntest/distributed/test_c10d_nccl.py ProcessGroupNCCLTest.test_nan_assert\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [0,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [1,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [2,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [3,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [4,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [5,0,0] Assertion `!isnan(val)`\nfailed.\n[rank0]:[E507 17:31:56.885473996 Utils.cu:30] CUDA error during\ncheckForNan: device-side assert triggered\n\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [0,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [1,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [2,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [3,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [4,0,0] Assertion `!isnan(val)`\nfailed.\n/home/sqzhang/pytorch/torch/csrc/distributed/c10d/Utils.cu:15:\ncheckForNaN: block: [0,0,0], thread: [5,0,0] Assertion `!isnan(val)`\nfailed.\n[rank1]:[E507 17:31:56.128961534 Utils.cu:30] CUDA error during\ncheckForNan: device-side assert triggered\n\n.\n----------------------------------------------------------------------\nRan 1 test in 7.723s\n\nOK\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125726\nApproved by: https://github.com/kwen2501",
    "date": "2024-05-14T00:05:41+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "BUILD.bazel",
            "patches": []
        },
        {
            "path": "build_variables.bzl",
            "patches": []
        },
        {
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
            "patches": [
                {
                    "Id": 31,
                    "hunk size": 5,
                    "hunk": "@@ -2424,6 +2426,9 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::collective(\n     OpType opType,\n     const char* profilingTitle,\n     bool avoidRecordStreams) {\n+  if (enableNanCheck_) {\n+    checkForNan(input);\n+  }\n   // Environment setting by the user may add onto collective call's option\n   avoidRecordStreams |= avoidRecordStreams_;\n   c10::cuda::CaptureStatus capture_status =\n"
                },
                {
                    "Id": 32,
                    "hunk size": 5,
                    "hunk": "@@ -2779,6 +2784,9 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::pointToPoint(\n     PreProcess pre,\n     PostProcess post,\n     const char* profilingTitle) {\n+  if (enableNanCheck_) {\n+    checkForNan(tensor);\n+  }\n   // avoidRecordStreams_ note:\n   // send, recv, and irecv should be ok with avoidRecordStreams,\n   // However, for isend, I don't think the API requires the user\n"
                }
            ]
        },
        {
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp",
            "patches": []
        },
        {
            "path": "torch/csrc/distributed/c10d/Utils.cu",
            "patches": []
        },
        {
            "path": "torch/csrc/distributed/c10d/Utils.hpp",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ca15671c303b4d6fa37373ec65935f8b448c6155",
    "message": "Fix failing test_invalid_input_csr_large (#114940)\n\nThe test introduced in #102530 has a bug:\nConstruction of `crow_indices` raises an exception: \"value cannot be converted to type int32 without overflow\" which is obviously correct.\nThis makes the test fail which is supposed to check for an overflow in nnz.\nFix by making the construction of `crow_indices` pass although with an invalid value which would error later but triggers the correct check.\n\nGiven that I'm not sure it is even worth checking for an overflow in nnz:\n- `crow_indices[..., -1] == nnz` is already enforced\n- this can only hold if `crow_indices` is able to hold `nnz` without overflow\n- `col_indices` has to be of the same type as `crow_indices`\n- Hence the type of `col_indices` has to be able to hold the value of `nnz`\n\nSo in conclusion: The situation being checked for cannot reasonably occur\n\nCC @pearu as the test author for additional insight\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114940\nApproved by: https://github.com/pearu, https://github.com/cpuhrsch",
    "date": "2023-12-08T11:55:21+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed",
    "message": "[ez][inductor][fx passes] quick fix for invalid nodes (#109234)\n\nSummary: As title.Need to check whether node is valid before fusion\n\nTest Plan: To add test\n\nDifferential Revision: D49241525\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109234\nApproved by: https://github.com/yanboliang",
    "date": "2023-09-14T01:40:49+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/175ccfc4c8443bcc65c87d9c942272d3ebf16b0b",
    "message": "Verify flatbuffer module fields are initialized (#109794)\n\nFixes #109793\n\nAdd validation on flatbuffer module field to prevent segfault\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109794\nApproved by: https://github.com/malfet",
    "date": "2023-09-21T23:19:17+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/3e382456c109820c836b104570104c3c5aff5632",
    "message": "Fix compiler check (#120492)\n\nFixes #119304\n\n1. Add try catch to handle the compiler version check.\n2. Retry to query compiler version info.\n3. Return False if can't get compiler info twice.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120492\nApproved by: https://github.com/ezyang",
    "date": "2024-02-25T02:41:20+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/utils/cpp_extension.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/50a084070fe782e898edc8ba42c9e5f945c9c149",
    "message": "[inductor][easy] Enable mypy checking for all inductor files that already pass (#109238)\n\nSummary: Let's just enable if mypy checking already passes. I checked all entries in the exclude list and enabled any that individually pass. Also needed one trivial change to a file already enabled.\n\nTest Plan: `lintrunner torch/_inductor/*.py torch/_inductor/*/*.py`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109238\nApproved by: https://github.com/eellison",
    "date": "2023-09-14T01:45:25+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab",
    "message": "[AOTI] Handle empty input args (#114682)\n\nSummary: When the model takes no inputs, AOTInductor relies on checking weights to figure out which device to compile the model into. Currently recording buffer device type happens too late, and this PR fixes that.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114682\nApproved by: https://github.com/chenyang78",
    "date": "2023-12-05T15:02:17+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451",
    "message": "[custom_op] support default dtype values (#129189)\n\nThis PR:\n- moves some of the dtype-string utilities into ScalarType.{h, cpp}\n- adds a new utility to get a mapping from dtype name to the C++ dtype\n- the perser now checks if the string is a dtype name; if it is then it\n  pulls the c++ dtype from the mapping.\n\nTest Plan:\n- new tests\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129189\nApproved by: https://github.com/albanD\nghstack dependencies: #129177, #129178, #129179",
    "date": "2024-06-23T00:13:23+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "c10/core/ScalarType.cpp",
            "patches": []
        },
        {
            "path": "c10/core/ScalarType.h",
            "patches": []
        },
        {
            "path": "torch/_library/infer_schema.py",
            "patches": [
                {
                    "Id": 33,
                    "hunk size": 7,
                    "hunk": "@@ -79,6 +79,11 @@ def infer_schema(prototype_function: typing.Callable, mutates_args=()) -> str:\n                 default_repr = str(param.default)\n             elif isinstance(param.default, str):\n                 default_repr = f'\"{param.default}\"'\n+            elif isinstance(param.default, torch.dtype):\n+                dtype_repr = str(param.default)\n+                torch_dot = \"torch.\"\n+                assert dtype_repr.startswith(torch_dot)\n+                default_repr = dtype_repr[len(torch_dot) :]\n             else:\n                 error_fn(\n                     f\"Parameter {name} has an unsupported default value type {type(param.default)}. \"\n"
                }
            ]
        },
        {
            "path": "torch/csrc/TypeInfo.cpp",
            "patches": []
        },
        {
            "path": "torch/csrc/jit/frontend/function_schema_parser.cpp",
            "patches": [
                {
                    "Id": 34,
                    "hunk size": 6,
                    "hunk": "@@ -231,6 +254,10 @@ struct SchemaParser {\n           return static_cast<int64_t>(at::Reduction::Mean);\n         } else if (\"contiguous_format\" == text) {\n           return static_cast<int64_t>(c10::MemoryFormat::Contiguous);\n+        } else if (\n+            isPossiblyOptionalScalarType(real_type) &&\n+            str2dtype.count(text) > 0) {\n+          return static_cast<int64_t>(str2dtype.at(text));\n         } else {\n           throw ErrorReport(L.cur().range) << \"invalid numeric default value\";\n         }\n"
                }
            ]
        },
        {
            "path": "torch/csrc/jit/frontend/schema_type_parser.cpp",
            "patches": []
        },
        {
            "path": "torch/csrc/utils/tensor_dtypes.cpp",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ff1e3ff5a503a520c1a310c8e72a383657f9a4bc",
    "message": "[reland] `_foreach_copy` with different src/dst dtypes (#123844)\n\nAttempt to reland https://github.com/pytorch/pytorch/pull/121717.\nThe change is the array bounds check.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123844\nApproved by: https://github.com/janeyx99",
    "date": "2024-04-16T02:20:58+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "aten/src/ATen/native/ForeachUtils.h",
            "patches": []
        },
        {
            "path": "aten/src/ATen/native/cuda/ForeachBinaryOpList.cu",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/16e539e0e68a9a7c100e6b4d7d4ba20d67fad2eb",
    "message": "Fix index range check (#116062)\n\nFixes incorrect range check when index is `std::numeric_limits<int64_t>::min()`, as result of unary minus operations for such values is undefined, but in practice is equal to self, see https://godbolt.org/z/Wxhh44ocr\n\nLower bound check was `size >= -index`, which was incorrect if `index` is `INT64_MIN`, with `-1 - index`, which for all int64_t values returns result that also fits into int64_t range. `- (index + 1)` is more readable and results in the identical optimized assembly, see https://godbolt.org/z/3vcnMYf9a , but its intermediate result for `INT64_MAX` is  outside of `int64_t` range, which leads to a similar problems as with `int64_min` in original example.\n\nAdded regression test.\n\nFixes https://github.com/pytorch/pytorch/issues/115415\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116062\nApproved by: https://github.com/Skylion007, https://github.com/albanD",
    "date": "2023-12-20T15:40:57+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/41902a6ebc1806e7f4d6ce1da604cc9921c6515e",
    "message": "[dynamo] Optimize is_tracing checks (#118474)\n\nbenchmarks/dynamo/microbenchmarks/overheads.py\n- before: 10.4us\n- after: 9.9us\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118474\nApproved by: https://github.com/yanboliang",
    "date": "2024-01-29T08:31:26+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_dynamo/eval_frame.py",
            "patches": [
                {
                    "Id": 35,
                    "hunk size": 13,
                    "hunk": "@@ -415,11 +419,16 @@ class _TorchDynamoContext:\n \n         callback = self.callback\n \n+        if isinstance(self, DisableContext):\n+            is_jit_tracing = always_false\n+            is_fx_tracing = always_false\n+        else:\n+            is_jit_tracing = torch._C._is_tracing\n+            is_fx_tracing = torch.fx._symbolic_trace.is_fx_tracing\n+\n         @functools.wraps(fn)\n         def _fn(*args, **kwargs):\n-            if torch.fx._symbolic_trace.is_fx_tracing() and not isinstance(\n-                self, DisableContext\n-            ):\n+            if is_fx_tracing():\n                 if config.error_on_nested_fx_trace:\n                     raise RuntimeError(\n                         \"Detected that you are using FX to symbolically trace \"\n"
                },
                {
                    "Id": 36,
                    "hunk size": 4,
                    "hunk": "@@ -428,7 +437,7 @@ class _TorchDynamoContext:\n                 else:\n                     return fn(*args, **kwargs)\n \n-            if torch.jit.is_tracing():\n+            if is_jit_tracing():\n                 if config.error_on_nested_jit_trace:\n                     raise RuntimeError(\n                         \"Detected that you are using FX to torch.jit.trace \""
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e385bf8ef8f84968c0ff6e47ca5e06d4948e8e62",
    "message": "Revert \"[halide-backend] Disable split reductions for Halide (#129320)\"\n\nThis reverts commit a18eb651d352e45860a96869abaf9fb7b215eac6.\n\nReverted https://github.com/pytorch/pytorch/pull/129320 on behalf of https://github.com/jeanschmidt due to This PR is breaking internal builds, please check comments on it D59204360 ([comment](https://github.com/pytorch/pytorch/pull/129320#issuecomment-2200351678))",
    "date": "2024-07-01T14:44:35+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/codegen/common.py",
            "patches": []
        },
        {
            "path": "torch/_inductor/codegen/cpp.py",
            "patches": []
        },
        {
            "path": "torch/_inductor/codegen/halide.py",
            "patches": []
        },
        {
            "path": "torch/_inductor/ir.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e31038d574712d383fdc4c2f1bb63fc82f256ed0",
    "message": "Check results dtype in index_out (#108167)\n\nThis logic exists for index_put and index_add, but for some reason not for `index.out`\nSkip testing, as this function is not technically exposed on the Python level.\n\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at c688cfd</samp>\n\n> _`index_out` checks types_\n> _avoiding errors in autumn_\n> _complex tensors work_\n\nFixes https://github.com/pytorch/pytorch/issues/107698\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108167\nApproved by: https://github.com/albanD",
    "date": "2023-08-30T14:55:18+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/3db0095ea2f351b9935de036a03f29e619093e16",
    "message": "[reland][quant][pt2e][be] Cleanup observer insertion logic (#111828) (#112453)\n\nSummary: att, after SharedQuantizationSpec bug fix we are doing some checks before hand, this can simplify the logic when we insert observers\n\nTest Plan:\ncontbuild & OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/bf998a2c5d549cf4856c7becfca4a169bf68b709\n\nTest plan from GitHub:\npython test/test_quantization.py TestQuantizePT2E\n\nCIs\n\nDifferential Revision: D50816224\n\nPulled By: jerryzh168\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112453\nApproved by: https://github.com/andrewor14",
    "date": "2023-10-31T17:33:24+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8629939a51813def63363ff3bdfe1a6e56c69e18",
    "message": "[torch/c10] Add C10_UBSAN_ENABLED macro and use it to disable SymInt_\u2026 (#127967)\n\nAdds `C10_UBSAN_ENABLED` macro and use it to disable `SymIntTest::Overflows` (fails under `signed-integer-overflow` UBSAN check).\n\nAlso cleans up UBSAN guard in `jit/test_misc.cpp` to use `C10_UBSAN_ENABLED`  and the existing `C10_ASAN_ENABLED` instead of locally defining `HAS_ASANUBSAN`.\n\n> NOTE: This should fix `SymIntTest::Overflows` failing under ubsan in fbcode too...\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127967\nApproved by: https://github.com/atalman, https://github.com/d4l3k, https://github.com/malfet",
    "date": "2024-06-14T16:01:12+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "c10/macros/Macros.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/db7a3cc436df016cf4d1b9edf7557621f5cdbf47",
    "message": "fix missing nvml in c10/cuda/driver_api.cpp issue (#112121)\n\nSince https://github.com/pytorch/pytorch/pull/99699 introduced a dependency on nvml for oom reporting in `c10/cuda/driver_api.h`, `c10/cuda/driver_api.cpp`, and `reportProcessMemoryInfo` from `c10/cuda/CUDACachingAllocator.cpp`, we've seen failures regarding cuda expandable segments and oom reporting in NVIDIA's internal CI, specifically on Jetson devices which don't have nvml support as it is incompatible with Jetson. Example failures using the latest upstream on Orin AGX node:\n\n`python test/test_cuda.py -k test_notifies_oom` generates\n\n```\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/pytorch/pytorch/test/test_cuda.py\", line 1643, in _worker\n    results[t] = torch.nn.functional.conv2d(results[t], weight, padding=0)\nRuntimeError: CUDA driver error: out of memory\n```\n\n`python test/test_cuda_expandable_segments.py` generates\n\n```\nTraceback (most recent call last):\n  File \"/opt/pytorch/pytorch/test/test_cuda_expandable_segments.py\", line 12, in <module>\n    exec(compile(open(filepath).read(), filepath, mode='exec'))\n  File \"/opt/pytorch/pytorch/test/test_cuda.py\", line 66, in <module>\n    class TestCuda(TestCase):\n  File \"/opt/pytorch/pytorch/test/test_cuda.py\", line 1609, in TestCuda\n    @unittest.skipIf(not TEST_CUDNN, 'CUDNN not available')\n  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_utils.py\", line 4628, in wrapped\n    self._value = self._cb()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_cuda.py\", line 20, in <lambda>\n    TEST_CUDNN = LazyVal(lambda: TEST_CUDA and torch.backends.cudnn.is_acceptable(torch.tensor(1., device=CUDA_DEVICE)))\nRuntimeError: handle_0 INTERNAL ASSERT FAILED at \"/opt/pytorch/pytorch/c10/cuda/driver_api.cpp\":15, please report a bug to PyTorch.\n```\n\nThis PR intends to fix this issue by adding various dlopen checks to make sure nvml actually exists, and safely fall back to using the older libcuda based features of cuda expandable segments and oom reporting if nvml is not found.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112121\nApproved by: https://github.com/eqy, https://github.com/ngimel, https://github.com/albanD",
    "date": "2023-11-02T21:28:05+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/bb89a9e48c869b3056a42d026f963c2ce4fb41a4",
    "message": "Skipped CUDA Flags if C++ Extension Name includes \"arch\" Substring (#111211)\n\nThe CUDA architecture flags from TORCH_CUDA_ARCH_LIST will be skipped if the TORCH_EXTENSION_NAME includes the substring \"arch\". A C++ Extension should be allowed to have any name. I just manually skip the TORCH_EXTENSION_NAME flag when checking if one of the flags is \"arch\". There is probably a better fix, but I'll leave this to experts.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111211\nApproved by: https://github.com/ezyang",
    "date": "2023-10-14T00:10:01+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/eb1d6ed9f9d9731401b04382f526a64e6d27b6e6",
    "message": "[Inductor] fix addmm fusion check (#121953)\n\nFixes #121253.\n\nTo avoid functional issue, disable pattern match for `addmm` when `beta!=1 or 0` or `alpha!=1`, as either `mkl_linear` or `mkldnn_linear` doesn't accept `beta` or `alpha` as parameters.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121953\nApproved by: https://github.com/jgong5, https://github.com/leslie-fang-intel, https://github.com/jansel",
    "date": "2024-03-20T09:22:51+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/fx_passes/mkldnn_fusion.py",
            "patches": [
                {
                    "Id": 37,
                    "hunk size": 8,
                    "hunk": "@@ -911,6 +911,12 @@ if torch._C._has_mkldnn:\n         Check if the node is supported for MKLDNN linear.\n         \"\"\"\n         linear_node = match.output_node()\n+        # mkldnn linear only supports beta=1or0 and alpha=1\n+        if linear_node.target == aten.addmm.default:\n+            alpha = linear_node.kwargs.get(\"alpha\", 1.0)\n+            beta = linear_node.kwargs.get(\"beta\", 1.0)\n+            if (beta != 0.0 and beta != 1.0) or alpha != 1.0:\n+                return False\n         # weight_idx is 1 for aten.mm and is 2 for aten.addmm\n         weight_idx = 2 if linear_node.target == aten.addmm.default else 1\n         if linear_node.args[weight_idx].op != \"get_attr\":\n"
                },
                {
                    "Id": 38,
                    "hunk size": 12,
                    "hunk": "@@ -1104,7 +1117,15 @@ if torch._C._has_mkldnn:\n             graph = match.graph\n             linear_node = match.output_node()\n             input = args[0] if linear_node.target == aten.mm.default else args[1]\n-            bias = None if linear_node.target == aten.mm.default else args[0]\n+            bias = (\n+                None\n+                if linear_node.target == aten.mm.default\n+                or (\n+                    linear_node.target == aten.addmm.default\n+                    and linear_node.kwargs.get(\"beta\", 1.0) == 0.0\n+                )\n+                else args[0]\n+            )\n             weight = args[1] if linear_node.target == aten.mm.default else args[2]\n             with graph.inserting_before(linear_node):\n                 transpose_weight_node = graph.create_node("
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fe3e6878c4bb2a6001045c179fd7fa9838242558",
    "message": "[cond] inlining into one of the branches when pred is a python constant (#128709)\n\nWhen the input predicate is a python constant, we specialize into one of the branches and warn users that torch.cond is not preserving the dynamism. The previous behavior is that we baked in True/False in the cond operator. This can be confusing. In this PR, we change it to be specializing into one of the branches when the inputs are constants.\n\nWe additionally change the naming of cond operator to default one without overriding its name. This allows better testing on de-serialized graph.\n\nTest Plan:\nThe predicate in some existing tests is the result of a shape comparison. When no dynamic shape is involved, the predicate is a python bool. To fix them, we either change the predicate to be some data-dependent tensor or change the test to check cond is specialized as one of the branches,\n\nDifferential Revision: [D59589709](https://our.internmc.facebook.com/intern/diff/D59589709)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128709\nApproved by: https://github.com/zou3519",
    "date": "2024-07-10T16:44:27+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_dynamo/variables/higher_order_ops.py",
            "patches": [
                {
                    "Id": 39,
                    "hunk size": 14,
                    "hunk": "@@ -632,6 +632,18 @@ class CondHigherOrderVariable(TorchHigherOrderOperatorVariable):\n                 f\"Expected 4 arguments but got {len(args)}.\\n\"\n                 f\"Usage: cond(pred, true_fn, false_fn, operands)\",\n             )\n+\n+        # Specialize into one of the branches since pred is constant\n+        if type(args[0]) is ConstantVariable:\n+            log.warning(\n+                \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+                \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+            )\n+            if args[0].as_python_constant():\n+                return args[1].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+            else:\n+                return args[2].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+\n         # predicate\n         if type(args[0]) not in (ConstantVariable, TensorVariable, SymNodeVariable):\n             unimplemented(\n"
                }
            ]
        },
        {
            "path": "torch/_higher_order_ops/cond.py",
            "patches": [
                {
                    "Id": 40,
                    "hunk size": 12,
                    "hunk": "@@ -107,6 +110,16 @@ def cond(pred, true_fn, false_fn, operands):\n     if torch.compiler.is_dynamo_compiling():\n         return cond_op(pred, true_fn, false_fn, operands)\n \n+    if isinstance(pred, (bool, int, float)):\n+        log.warning(\n+            \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+            \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+        )\n+        if pred:\n+            return true_fn(*operands)\n+        else:\n+            return false_fn(*operands)\n+\n     def _validate_input(pred, true_fn, false_fn, operands):\n         if not isinstance(pred, (bool, torch.Tensor, torch.SymBool)):\n             raise RuntimeError(f\"Expected pred to be bool or tensor, but got {pred}.\")\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/506eda538b85c9fec7a35a4c8684cdef05fc5e9d",
    "message": "Fix windows build error not propagating (#125306)\n\n* Fixes https://github.com/pytorch/pytorch/issues/124886\n* Kind of similar to https://github.com/pytorch/pytorch/pull/109393\n\nI think what happens is `exit` and `exit /b` propagate the errorlevel correctly, but `exit /b` only exists the currently running batch script and not the entire cmd.exe (or whatever program is running the batch script), so `exit /b` exits with errorlevel 1, but the the parent cmd exits with 0, and bash sees cmd's 0\n\nI think `goto fail` and `exit` are the same thing when the batch script is run from a bash script so either would work in this case?  But the `goto fail` method might be better if someone happens to run the script on cmdline\n\nI assumed that anywhere anyone was exiting after checking the error code, they did want to exit completely, and I'm pretty sure that being inside a parenthesis counts as being a different script, so I changed everything to goto fail just in case, this might be too aggressive?\n\nLogs after this change for a build failure on cuda:\nhttps://github.com/pytorch/pytorch/actions/runs/8912185834/job/24475087535?pr=125306\n```\n2 errors detected in the compilation of \"C:/actions-runner/_work/pytorch/pytorch/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu\".\nAdaptiveMaxPooling3d.cu\n[7599/8420] Linking CXX shared library bin\\torch_cpu.dll\nninja: build stopped: subcommand failed.\n-- Building version 2.4.0a0+git3171c11\ncmake -GNinja -DBUILD_ENVIRONMENT=win-vs2019-cuda11.8-py3 -DBUILD_PYTHON=True -DBUILD_TEST=True -DBUILD_TYPE=release -DBUILD_WHEEL=1 -DCMAKE_BUILD_TYPE=Release -DCMAKE_CUDA_COMPILER=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8/bin/nvcc.exe -DCMAKE_CUDA_COMPILER_LAUNCHER=C:/actions-runner/_work/pytorch/pytorch/build/win_tmp/bin/randomtemp.exe;C:/actions-runner/_work/pytorch/pytorch/build/win_tmp\\bin\\sccache.exe -DCMAKE_CXX_COMPILER_LAUNCHER=sccache -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_GENERATOR=Ninja -DCMAKE_INSTALL_PREFIX=C:\\actions-runner\\_work\\pytorch\\pytorch\\torch -DCMAKE_PREFIX_PATH=C:\\Jenkins\\Miniconda3\\Lib\\site-packages -DCUDA_NVCC_EXECUTABLE=C:/actions-runner/_work/pytorch/pytorch/build/win_tmp/bin/nvcc.bat -DCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\lib\\x64 -DNUMPY_INCLUDE_DIR=C:\\Jenkins\\Miniconda3\\lib\\site-packages\\numpy\\core\\include -DPYTHON_EXECUTABLE=C:\\Jenkins\\Miniconda3\\python.exe -DPYTHON_INCLUDE_DIR=C:\\Jenkins\\Miniconda3\\Include -DPYTHON_LIBRARY=C:\\Jenkins\\Miniconda3/libs/python39.lib -DTORCH_BUILD_VERSION=2.4.0a0+git3171c11 -DTORCH_CUDA_ARCH_LIST=8.6 -DUSE_CUDA=1 -DUSE_NUMPY=True C:\\actions-runner\\_work\\pytorch\\pytorch\ncmake --build . --target install --config Release -- -j 8\n\n(base) C:\\actions-runner\\_work\\pytorch\\pytorch>if errorlevel 1 goto fail\n\n(base) C:\\actions-runner\\_work\\pytorch\\pytorch>exit /b 1\nError: Process completed with exit code 1.\n```\n\nvs original\nhttps://github.com/pytorch/pytorch/actions/runs/8910674030/job/24470387612\n```\n2 errors detected in the compilation of \"C:/actions-runner/_work/pytorch/pytorch/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu\".\nAdaptiveMaxPooling3d.cu\n[7604/8420] Linking CXX shared library bin\\torch_cpu.dll\nninja: build stopped: subcommand failed.\n-- Building version 2.4.0a0+gite09f98c\ncmake -GNinja -DBUILD_ENVIRONMENT=win-vs2019-cuda11.8-py3 -DBUILD_PYTHON=True -DBUILD_TEST=True -DBUILD_TYPE=release -DBUILD_WHEEL=1 -DCMAKE_BUILD_TYPE=Release -DCMAKE_CUDA_COMPILER=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8/bin/nvcc.exe -DCMAKE_CUDA_COMPILER_LAUNCHER=C:/actions-runner/_work/pytorch/pytorch/build/win_tmp/bin/randomtemp.exe;C:/actions-runner/_work/pytorch/pytorch/build/win_tmp\\bin\\sccache.exe -DCMAKE_CXX_COMPILER_LAUNCHER=sccache -DCMAKE_C_COMPILER_LAUNCHER=sccache -DCMAKE_GENERATOR=Ninja -DCMAKE_INSTALL_PREFIX=C:\\actions-runner\\_work\\pytorch\\pytorch\\torch -DCMAKE_PREFIX_PATH=C:\\Jenkins\\Miniconda3\\Lib\\site-packages -DCUDA_NVCC_EXECUTABLE=C:/actions-runner/_work/pytorch/pytorch/build/win_tmp/bin/nvcc.bat -DCUDNN_LIBRARY=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\lib\\x64 -DNUMPY_INCLUDE_DIR=C:\\Jenkins\\Miniconda3\\lib\\site-packages\\numpy\\core\\include -DPYTHON_EXECUTABLE=C:\\Jenkins\\Miniconda3\\python.exe -DPYTHON_INCLUDE_DIR=C:\\Jenkins\\Miniconda3\\Include -DPYTHON_LIBRARY=C:\\Jenkins\\Miniconda3/libs/python39.lib -DTORCH_BUILD_VERSION=2.4.0a0+gite09f98c -DTORCH_CUDA_ARCH_LIST=8.6 -DUSE_CUDA=1 -DUSE_NUMPY=True C:\\actions-runner\\_work\\pytorch\\pytorch\ncmake --build . --target install --config Release -- -j 8\n\n(base) C:\\actions-runner\\_work\\pytorch\\pytorch>if errorlevel 1 exit /b\n+ assert_git_not_dirty\n+ [[ win-vs2019-cuda11.8-py3 != *rocm* ]]\n+ [[ win-vs2019-cuda11.8-py3 != *xla* ]]\n++ git status --porcelain\n++ grep -v '?? third_party'\n++ true\n+ git_status=\n+ [[ -n '' ]]\n+ echo 'BUILD PASSED'\nBUILD PASSED\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125306\nApproved by: https://github.com/ZainRizvi, https://github.com/huydhn, https://github.com/atalman",
    "date": "2024-05-01T22:06:47+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fba21edf5b9aa14babb9c0bc860dc9c597eb8010",
    "message": "[CI] Ensure inductor/test_cpu_cpp_wrapper is actually run in inductor_cpp_wrapper_abi_compatible (#126717)\n\n`inductor/test_cpu_cpp_wrapper` is not actually being run in `inductor_cpp_wrapper_abi_compatible` test config\n\nThe cpu device type gets removed in https://github.com/pytorch/pytorch/blob/d28868c7e8bcd41c9219f099aa5f7a5332c912fd/torch/testing/_internal/common_device_type.py#L733\n\nso https://github.com/pytorch/pytorch/blob/d28868c7e8bcd41c9219f099aa5f7a5332c912fd/test/inductor/test_cpu_cpp_wrapper.py#L396 returns false.\n\nFeel free to make a PR with a different way to do this (a better RUN_CPU check?)\n\nAdd a skip for a failing test.  I am not equipped to fix it\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126717\nApproved by: https://github.com/ZainRizvi",
    "date": "2024-06-06T18:23:52+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/490d72e4e6eb07ed7c65ee6f7c48bb393483c01c",
    "message": "CMake: Improve check and report of Magma (#117858)\n\n- Only search for magma if it is used (GPU builds)\n- Don't report it was not found when it isn't searched for\n- Don't report if magma is disabled (currently: \"MAGMA not found. Compiling without MAGMA support\" is reported)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117858\nApproved by: https://github.com/malfet",
    "date": "2024-05-15T17:18:22+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "cmake/Dependencies.cmake",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9",
    "message": "Re-enable type checking for distributed_c10d.py (#115223)\n\nRe-enable type checking for distributed_c10d.py\n\nType checking for distributed_c10d.py was inadvertently turned off in issues that have accumulated since.\n\nNote: the backwards compatibility linter does not like some of these changes.  But they were incorrect before.  This needs human verification, however.\n\n#suppress-api-compatibility-check\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115223\nApproved by: https://github.com/wconstab",
    "date": "2023-12-09T11:07:54+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7b442c2b0ae0d9c944a777d7352135f370837c15",
    "message": "limit fused kernel num args. (#113131)\n\nFixes #97361\n\nWhen fused kernel more than 1024 parameters, it should throw error from ctypes.\nLimit args number is should be a mechanism to protect stack memory. As we known, CPP is passing args via stack memory, and stack memory has size limitation.\n\nCode change:\n\n1. cpp backend will check the fused nodes' args number, if it is reach the limitation. It will status flush status to ready.\n2. scheduler will check `ready_to_flush` API and help backend flush codegen.\n3. Add `ready_to_flush` API to `BaseScheduling`, Triton backend will return False due to not support it yet.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113131\nApproved by: https://github.com/jgong5, https://github.com/mlazos",
    "date": "2023-11-18T03:55:52+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ede74940a150632bf28ed9d1728e929ee06d6d94",
    "message": "optimize vec isa check dispatch logical. (#128320)\n\nOptimize cpu vec isa check dispatch by archecture, it makes code easy to read and maintaince.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128320\nApproved by: https://github.com/jgong5, https://github.com/desertfire",
    "date": "2024-06-13T01:06:34+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "Id": 41,
                    "hunk size": 8,
                    "hunk": "@@ -1479,11 +1479,13 @@ def valid_vec_isa_list() -> List[VecISA]:\n     if sys.platform == \"darwin\" and platform.processor() == \"arm\":\n         return [VecNEON()]\n \n+    isa_list: List[VecISA] = []\n     cur_os = sys.platform\n     if cur_os != \"linux\" and cur_os != \"win32\":\n-        return []\n+        return isa_list\n \n-    if platform.machine() == \"s390x\":\n+    arch = platform.machine()\n+    if arch == \"s390x\":\n         with open(\"/proc/cpuinfo\") as _cpu_info:\n             while True:\n                 line = _cpu_info.readline()\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/17abbafdfc6935bcc133e5f43ba32d44914fe316",
    "message": "[inductor] Fix some windows cpp builder issue (#128765)\n\n1. fix some Windows build args.\n2. fix c++20 likely issue on Windows, reference: https://github.com/pytorch/pytorch/pull/124997.\n3. remove compiler return value check, different compilers return variant value, let's check exception to catch error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128765\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "date": "2024-06-18T03:25:20+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "Id": 42,
                    "hunk size": 4,
                    "hunk": "@@ -1376,8 +1376,6 @@ cdll.LoadLibrary(\"__lib_path__\")\n                 output_path = x86_isa_help_builder.get_target_file_path()\n                 if not os.path.isfile(output_path):\n                     status, target_file = x86_isa_help_builder.build()\n-                    if status:\n-                        return False\n \n                 # Check build result\n                 subprocess.check_call(\n"
                }
            ]
        },
        {
            "path": "torch/_inductor/cpp_builder.py",
            "patches": [
                {
                    "Id": 43,
                    "hunk size": 4,
                    "hunk": "@@ -567,7 +571,7 @@ def _get_torch_related_args(include_pytorch: bool, aot_mode: bool):\n     ]\n     libraries_dirs = [TORCH_LIB_PATH]\n     libraries = []\n-    if sys.platform == \"linux\" and not config.is_fbcode():\n+    if sys.platform != \"darwin\" and not config.is_fbcode():\n         libraries = [\"torch\", \"torch_cpu\"]\n         if not aot_mode:\n             libraries.append(\"torch_python\")\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d0c8e8240dc04ca85dd071d99be49454a9138de6",
    "message": "Revert \"When doing typed typecheck, also check signature with symint removed (#109727)\"\n\nThis reverts commit 56ef200c2dc8a1f1e269861b7a6e02e99d3b56a1.\n\nReverted https://github.com/pytorch/pytorch/pull/109727 on behalf of https://github.com/ezyang due to yolov3 problem ([comment](https://github.com/pytorch/pytorch/pull/109727#issuecomment-1731585002))",
    "date": "2023-09-22T15:11:27+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/05ac295177ba885165108b03f7a35155ceee6904",
    "message": "[export] Fix bug with user input mutations (#118942)\n\nWe hit an edge case where the graph exporting contains placeholder nodes whose names conflict with names from aot_export, we don't update the user_inputs_to_mutate in the graph signature correctly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118942\nApproved by: https://github.com/tugsbayasgalan, https://github.com/zhxchen17",
    "date": "2024-02-02T09:02:04+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/export/_trace.py",
            "patches": [
                {
                    "Id": 44,
                    "hunk size": 6,
                    "hunk": "@@ -368,7 +368,9 @@ def _lift_buffers_to_user_inputs(\n         i: b for i, b in graph_signature.inputs_to_buffers.items() if b not in names\n     }\n     user_inputs_to_mutate = {\n-        o: b for o, b in graph_signature.buffers_to_mutate.items() if b in names\n+        o: new_node_names[b]\n+        for o, b in graph_signature.buffers_to_mutate.items()\n+        if b in names\n     }\n     graph_signature.buffers_to_mutate = {\n         o: b for o, b in graph_signature.buffers_to_mutate.items() if b not in names"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/cdeb242fc977210e211fd77b217320205c9f4042",
    "message": "[inductor] fix mkldnn linear binary fusion check ut (#127296)\n\nIn this PR:\n\n\uff081\uff09Fix the unary fusion for bf16 conv/linear.\n    Previously we registered same fusion pattern for `bf16. fp16`. And we do not check the dtype while matching the pattern. This results the `fp16` case matched the `bf16` pattern but in later replacement, we found that we have a float16 here which is not expected, so we do not fuse them.  We fix it by checking dtypes to avoid `fp16` case matched `bf16` pattern.\n\n```\n  def _is_valid_computation_unary_fusion(computation_op, lowp_dtype=None):\n      def fn(match):\n          matched = _is_single_computation_op(computation_op, **lowp_dtype**)(match) # previously we do not check lowp_dtype here\n\n```\n\nIt is not exposed before because we only check the match count, and the match count is anyway correct because we matched the pattern. To address this, we add check on number of `generated_kernel`. If it is not fused, there will be an additional kernel to compute the post op.\n\n\uff082\uff09Previous the ut\n```\npython test/inductor/test_mkldnn_pattern_matcher.py -k test_linear_binary\n```\ndose not check the fusion status, fix it in this PR.\n\n\uff083\uff09Extend `test_conv_binary` to test with lp.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127296\nApproved by: https://github.com/leslie-fang-intel, https://github.com/jgong5, https://github.com/jansel",
    "date": "2024-05-30T12:29:36+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/fx_passes/mkldnn_fusion.py",
            "patches": [
                {
                    "Id": 45,
                    "hunk size": 10,
                    "hunk": "@@ -197,9 +197,15 @@ if torch._C._has_mkldnn:\n     def _binary_fusion_v2(computation_call, binary_fn):\n         return CallFunction(binary_fn, computation_call, KeywordArg(\"other\"))\n \n-    def _is_single_computation_op(computation_op):\n+    def _is_single_computation_op(computation_op, lowp_dtype=None):\n         def fn(match):\n             computation_nodes = filter_nodes(match.nodes, computation_op)\n+\n+            if lowp_dtype:\n+                output_node_meta = match.output_node().meta.get(\"val\")\n+                if output_node_meta.dtype != lowp_dtype:\n+                    return False\n+\n             if len(computation_nodes) < 1:\n                 return False\n             if any(n.args[-3] != \"none\" for n in computation_nodes):\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/f1ee3589a1277bdc9c580612868a92e7ba27e39a",
    "message": "[Inductor] Emit strided block pointer from ModularIndexing and FloorDiv (#127342)\n\n**Summary**\n\nInductor currently uses modulo and division to compute indices into certain multi-dimensional tensors, such as those arising from row padding. This PR matches on that indexing pattern, replacing it with an N-D block pointer. This should be more efficient than computing indices with division and modulo, and it can easily map to DMAs on non-GPU hardware targets.\n\nBecause the 1D block size needs to map to an integer block shape in ND, we need to know that the ND block size evenly divides the size of the iteration range. This PR only generates ND block pointers when it can guarantee that the iteration order and number of elements loaded are unchanged. This means that the number of elements in a slice of the iteration range must either be:\n  - Powers of 2. Since Triton block sizes are powers of 2, any integer power of 2 either divides the block size, or is greater than the block size. In the latter case, `CielDiv(x, y)` rounds up to 1.\n  - Multiples of the maximum block size. Since block sizes are powers of 2, the maximum block size is a multiple of every possible block size.\n\nNote that a *slice* of the iteration range does not include the leading dimension. Thus we can support arbitrary leading dimensions like `(5,8)`.\n\nFeature proposal and discussion: https://github.com/pytorch/pytorch/issues/125077\n\nExample kernel:\n```\ntriton.jit\ndef triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    tmp0 = tl.reshape(tl.load(tl.make_block_ptr(in_ptr0, shape=[32, 16, 8], strides=[1024, 32, 1], block_shape=[32 * (32 <= ((127 + XBLOCK) // 128)) + ((127 + XBLOCK) // 128) * (((127 + XBLOCK) // 128) < 32), 16 * (16 <= ((7 + XBLOCK) // 8)) + ((7 + XBLOCK) // 8) * (((7 + XBLOCK) // 8) < 16), 8 * (8 <= XBLOCK) + XBLOCK * (XBLOCK < 8)], order=[0, 1, 2], offsets=[(xoffset // 128), (xoffset // 8) % 16, xoffset % 8]), boundary_check=[0, 1, 2]), [XBLOCK])\n    tmp1 = tmp0 + tmp0\n    tl.store(tl.make_block_ptr(out_ptr0, shape=[4096], strides=[1], block_shape=[XBLOCK], order=[0], offsets=[xoffset]), tl.broadcast_to(tmp1, [XBLOCK]).to(tl.float32))\n''', device_str='cuda')\n```\n\n**Test Plan**\n\nThis PR adds a new CI test script to cover this feature. The tests can be grouped into a few main categories:\n  - Can we generate strided block pointers for the appropriate shapes?\n     - Powers of 2\n     - Non-power of 2, but multiple of the maximum block size\n     - Arbitrary leading dimensions, with power of 2 inner dimensions\n     - Weird strides and offsets\n     - Reductions\n     - Symbolic shapes that are multiples of the maximum block size (wasn't able to trace this through dynamo)\n     - Broadcasts (some variables are missing from the indexing expression)\n  - Do we still compile other cases correctly, even if we don't expect to be able to generate block pointers?\n     - Unsupported static shapes\n     - Unsupported symbolic shapes\n  - Mixing and matching these cases:\n     - Pointwise and reduction in the same kernel\n  - Sanity check the test harness\n     - Do we raise an exception if the expected number of block pointers and the actual number are different?\n\n**Follow-ups**\n\nThere are a few important cases which this PR can't handle. I'm hoping these can be deferred to follow-up PRs:\n  - Handle non-divisible shapes\n      - Change the tiling algorithm to generate a 2D (X,Y) blocking, if doing so enables block pointers to be emitted.\n      - Pad unsupported loads up to the nearest divisible size, then mask/slice out the extra elements? This is probably the best solution, but I'm not yet sure how to go about it in triton.\n - Take advantage of this analysis when `triton.use_block_ptr=False`. I'm guessing we can still avoid `%` and `/` without requiring block pointers. Maybe we could compute block indices with arange and broadcast instead?\n\nDifferential Revision: D56739375\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127342\nApproved by: https://github.com/jansel, https://github.com/shunting314",
    "date": "2024-06-16T07:35:57+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_inductor/codegen/triton.py",
            "patches": []
        },
        {
            "path": "torch/_inductor/runtime/runtime_utils.py",
            "patches": []
        },
        {
            "path": "torch/_inductor/sizevars.py",
            "patches": [
                {
                    "Id": 46,
                    "hunk size": 9,
                    "hunk": "@@ -354,6 +355,13 @@ class SizeVarAllocator:\n         expr = sympy.Eq(numerator % denominator, 0)\n         return self.is_expr_static_and_true(expr)  # type: ignore[arg-type]\n \n+    # See Note - [On Statically Known]\n+    def statically_known_power_of_2(self, expr: Expr) -> bool:\n+        \"\"\"\n+        Returns a bool indicating if x is known to be a power of 2.\n+        \"\"\"\n+        return isinstance(expr, sympy.Integer) and is_power_of_2(int(expr))\n+\n     # The guard functions require you to ALREADY KNOW that a particular\n     # condition holds.  If you don't know (you want to guard on an expression\n     # being a particular value, and then get access to that value), use\n"
                }
            ]
        },
        {
            "path": "torch/utils/_sympy/functions.py",
            "patches": [
                {
                    "Id": 47,
                    "hunk size": 20,
                    "hunk": "@@ -143,15 +143,15 @@ class FloorDiv(sympy.Function):\n         if isinstance(base, FloorDiv):\n             return FloorDiv(base.args[0], base.args[1] * divisor)\n \n-        # gcd in sympy is over polynomials, so you'll end up with rationals if\n-        # you do this.  Don't.\n-        \"\"\"\n-        if isinstance(base, sympy.Add):\n-            for a in base.args:\n-                gcd = sympy.gcd(a, divisor)\n-                if gcd == divisor:\n-                    return FloorDiv(base - a, divisor) + a / gcd\n-        \"\"\"\n+        # Expands (x + y) // b into x // b + y // b.\n+        # This only works if floor is an identity, i.e. x / b is an integer.\n+        for term in sympy.Add.make_args(base):\n+            quotient = term / divisor\n+            if quotient.is_integer and isinstance(divisor, sympy.Integer):\n+                # NB: this is correct even if the divisor is not an integer, but it\n+                # creates rational expressions that cause problems with dynamic\n+                # shapes.\n+                return FloorDiv(base - term, divisor) + quotient\n \n         try:\n             gcd = sympy.gcd(base, divisor)\n"
                }
            ]
        },
        {
            "path": "torch/utils/_sympy/symbol.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/1becd2c314f45bded8d3fbec91d785e7190b4afe",
    "message": "Align checks in `_use_cudnn_ctc_loss` with those in `_cudnn_ctc_loss` (#115617)\n\nThis PR is intended to fix the following problem:\n\nWhen using `CTCLoss`, there is a cudnn path gated by a call to [`_use_cudnn_ctc_loss`](\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/cudnn/LossCTC.cpp#L73-L101) which checks some conditions\n\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/LossCTC.cpp#L486-L496\n\nHowever, there are more checks in `_cudnn_ctc_loss`\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/cudnn/LossCTC.cpp#L122-L130\n\nsome of which are not present in `_use_cudnn_ctc_loss` (e.g. the check that `targets` is on CPU which will cause a RuntimeError after dispatching to `_cudnn_ctc_loss`). Instead, these checks should be in `_use_cudnn_ctc_loss` so that the normal `_ctc_loss` path will be used if the checks are not met)\n\ne.g. Before this PR\n\n```python\n>>> import torch\n>>> ctcloss = torch.nn.CTCLoss()\n>>> log_probs = torch.randn((50, 3, 15), device='cuda').log_softmax(2)\n>>> target = torch.randint(1, 15, (30 + 25 + 20,), dtype = torch.int)\n>>> input_lengths = torch.tensor((50, 50, 50), device='cuda')\n>>> target_lengths = torch.tensor((30, 25, 20), device='cuda')\n>>> ctcloss(log_probs, target, input_lengths, target_lengths)\ntensor(4.1172, device='cuda:0')\n>>> target = target.to('cuda')\n>>> ctcloss(log_probs, target, input_lengths, target_lengths)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/loss.py\", line 1779, in forward\n    return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n  File \"/data/users/mg1998/pytorch/torch/nn/functional.py\", line 2660, in ctc_loss\n    return torch.ctc_loss(\nRuntimeError: Expected tensor to have CPU Backend, but got tensor with CUDA Backend (while checking arguments for cudnn_ctc_loss)\n```\n\nAfter this PR the above snippet runs without error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115617\nApproved by: https://github.com/janeyx99",
    "date": "2023-12-12T22:20:20+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/20aa7cc6788ff10dee2d927057b10a81af638a32",
    "message": "Revert \"[c10d] Add an option for NAN check on every collective (#125726)\"\n\nThis reverts commit 6db32710074f0944305b2d1e4571bb4ce571bf6a.\n\nReverted https://github.com/pytorch/pytorch/pull/125726 on behalf of https://github.com/huydhn due to Sorry for reverting your change, but the new test is failing on both multigpu and rocm distributed, i.e. https://hud.pytorch.org/pytorch/pytorch/commit/c712b0f8a3e72feda9a90e22e9f36bd102b7d25e ([comment](https://github.com/pytorch/pytorch/pull/125726#issuecomment-2110646075))",
    "date": "2024-05-14T16:26:34+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "BUILD.bazel",
            "patches": []
        },
        {
            "path": "build_variables.bzl",
            "patches": []
        },
        {
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
            "patches": [
                {
                    "Id": 48,
                    "hunk size": 5,
                    "hunk": "@@ -2426,9 +2424,6 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::collective(\n     OpType opType,\n     const char* profilingTitle,\n     bool avoidRecordStreams) {\n-  if (enableNanCheck_) {\n-    checkForNan(input);\n-  }\n   // Environment setting by the user may add onto collective call's option\n   avoidRecordStreams |= avoidRecordStreams_;\n   c10::cuda::CaptureStatus capture_status =\n"
                },
                {
                    "Id": 49,
                    "hunk size": 5,
                    "hunk": "@@ -2784,9 +2779,6 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::pointToPoint(\n     PreProcess pre,\n     PostProcess post,\n     const char* profilingTitle) {\n-  if (enableNanCheck_) {\n-    checkForNan(tensor);\n-  }\n   // avoidRecordStreams_ note:\n   // send, recv, and irecv should be ok with avoidRecordStreams,\n   // However, for isend, I don't think the API requires the user\n"
                }
            ]
        },
        {
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp",
            "patches": []
        },
        {
            "path": "torch/csrc/distributed/c10d/Utils.cu",
            "patches": []
        },
        {
            "path": "torch/csrc/distributed/c10d/Utils.hpp",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/68b518c13e128997a0c7c9ab8ce9508cc4062e3a",
    "message": "Add check for out of range pointer. (#107510)\n\n### Summary\n\nHi! We've been fuzzing pytorch with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz) and found an error of accessing arbitary address while parsing flatbuffer format using `torch::load` function.\n\npytorch version: 18bcf62bbcf7ffd47e3bcf2596f72aa07a07d65f (the last commit at the moment of reporting the issue)\n\n### Details\nThe vulnerability appears while loading arbitrary user input using `torch::load` function. To detect the error the input must correspond to FlatbufferFileFormat, so the part of parsing flatbuffer in `import_ir_module` function must be executed.\n\nFirstly error can occur in `GetMutableRoot` in `module.h`, where we add pointer to input data buffer with the value, got from dereference of this pointer (which data fully depends on the user input and can be arbitrary). so the resulting `flatbuffer_module` address can be corrupted.\n\nMoreover, we can get the arbitrary address later at `flatbuffer_loader.cpp:305`, when we get `ival` pointer with `Get` method.\nThere in `IndirectHelper::Read` function we add pointer with the offset got from the dereference of this pointer, so the address can be corrupted again.\n\nThe corrupted `ival` pointer is dereferenced at `table.h` in flatbuffers project, where is used to get another address, which is later dereferenced again at `table.h` in flatbuffers project. The resulting corrupted address is written to `func` pointer at `flatbuffer_loader.cpp:274`, which is then used in `parseFunction`, where write access to the address occurs.\n\nTo fix the problem we can compute the end of memory area in `parse_and_initialize_mobile_module` function like this:\n```\nauto* end = static_cast<char*>(data) + size;\n```\nAnd then pass it to all the callees and insert corresponding checks.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107510\nApproved by: https://github.com/albanD",
    "date": "2023-08-29T18:25:11+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/469383755fe416eb1c41fa724762ad3eaecdff07",
    "message": "[inductor] add cpp builder code. (#124045)\n\nPrevious full PR https://github.com/pytorch/pytorch/pull/115248 is failed to merge due to fb_code is hard to debug.\nI also tried to submit them as two pieces, https://github.com/pytorch/pytorch/pull/118514 https://github.com/pytorch/pytorch/pull/118515. And they have passed PreCI at that time.\n\nNow I tried to split https://github.com/pytorch/pytorch/pull/115248 into smaller piece, and it is the first step of RFC https://github.com/pytorch/pytorch/issues/124245.\nChanges:\n1. Add cpp builder code, the new cpp_builder support Windows OS.\n2. Add CPU ISA checker which is cross OS and exported from backend cpuinfo.\n3. Switch compiler ISA checker to new cpp builder.\n4. CppCodeCache use the new ISA checker.\n5. Add temprary `test_new_cpp_build_logical` UT to help on transfer to new code.\n<img width=\"1853\" alt=\"Image\" src=\"https://github.com/pytorch/pytorch/assets/8433590/ce6519ab-ba92-4204-b1d6-7d15d2ba2cbe\">\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124045\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "date": "2024-05-08T05:27:15+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "aten/src/ATen/cpu/Utils.cpp",
            "patches": [
                {
                    "Id": 50,
                    "hunk size": 18,
                    "hunk": "@@ -5,6 +5,22 @@\n \n namespace at::cpu {\n \n+bool is_cpu_support_avx2() {\n+#if !defined(__s390x__) && !defined(__powerpc__)\n+  return cpuinfo_initialize() && cpuinfo_has_x86_avx2();\n+#else\n+  return false;\n+#endif\n+}\n+\n+bool is_cpu_support_avx512() {\n+#if !defined(__s390x__) && !defined(__powerpc__)\n+  return cpuinfo_initialize() && cpuinfo_has_x86_avx512f() && cpuinfo_has_x86_avx512vl() && cpuinfo_has_x86_avx512bw() && cpuinfo_has_x86_avx512dq();\n+#else\n+  return false;\n+#endif\n+}\n+\n bool is_cpu_support_vnni() {\n #if !defined(__s390x__) && !defined(__powerpc__)\n   return cpuinfo_initialize() && cpuinfo_has_x86_avx512vnni();\n"
                }
            ]
        },
        {
            "path": "aten/src/ATen/cpu/Utils.h",
            "patches": []
        },
        {
            "path": "torch/_C/_cpu.pyi",
            "patches": []
        },
        {
            "path": "torch/_dynamo/trace_rules.py",
            "patches": []
        },
        {
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "Id": 51,
                    "hunk size": 19,
                    "hunk": "@@ -1264,16 +1269,21 @@ cdll.LoadLibrary(\"__lib_path__\")\n         lock_dir = get_lock_dir()\n         lock = FileLock(os.path.join(lock_dir, key + \".lock\"), timeout=LOCK_TIMEOUT)\n         with lock:\n-            output_path = input_path[:-3] + \"so\"\n-            build_cmd = shlex.split(\n-                cpp_compile_command(\n-                    input_path, output_path, warning_all=False, vec_isa=self\n-                )\n+            output_dir = os.path.dirname(input_path)\n+            buid_options = CppTorchOptions(chosen_isa=self, warning_all=False)\n+            x86_isa_help_builder = CppBuilder(\n+                key,\n+                [input_path],\n+                buid_options,\n+                output_dir,\n             )\n             try:\n                 # Check if the output file exist, and compile when not.\n+                output_path = x86_isa_help_builder.get_target_file_path()\n                 if not os.path.isfile(output_path):\n-                    compile_file(input_path, output_path, build_cmd)\n+                    status, target_file = x86_isa_help_builder.build()\n+                    if status:\n+                        return False\n \n                 # Check build result\n                 subprocess.check_call(\n"
                },
                {
                    "Id": 52,
                    "hunk size": 10,
                    "hunk": "@@ -1307,8 +1317,12 @@ class VecNEON(VecISA):\n @dataclasses.dataclass\n class VecAVX512(VecISA):\n     _bit_width = 512\n-    _macro = \"-DCPU_CAPABILITY_AVX512\"\n-    _arch_flags = \"-mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma\"\n+    _macro = [\"CPU_CAPABILITY_AVX512\"]\n+    _arch_flags = (\n+        \"-mavx512f -mavx512dq -mavx512vl -mavx512bw -mfma\"\n+        if not _IS_WINDOWS\n+        else \"/arch:AVX512\"\n+    )  # TODO: use cflags\n     _dtype_nelements = {torch.float: 16, torch.bfloat16: 32, torch.float16: 32}\n \n     def __str__(self) -> str:\n"
                },
                {
                    "Id": 53,
                    "hunk size": 8,
                    "hunk": "@@ -1320,8 +1334,10 @@ class VecAVX512(VecISA):\n @dataclasses.dataclass\n class VecAVX2(VecISA):\n     _bit_width = 256\n-    _macro = \"-DCPU_CAPABILITY_AVX2\"\n-    _arch_flags = \"-mavx2 -mfma\"\n+    _macro = [\"CPU_CAPABILITY_AVX2\"]\n+    _arch_flags = (\n+        \"-mavx2 -mfma\" if not _IS_WINDOWS else \"/arch:AVX2\"\n+    )  # TODO: use cflags\n     _dtype_nelements = {torch.float: 8, torch.bfloat16: 16, torch.float16: 16}\n \n     def __str__(self) -> str:\n"
                },
                {
                    "Id": 54,
                    "hunk size": 5,
                    "hunk": "@@ -1370,7 +1415,8 @@ def valid_vec_isa_list() -> List[VecISA]:\n     if sys.platform == \"darwin\" and platform.processor() == \"arm\":\n         return [VecNEON()]\n \n-    if sys.platform != \"linux\":\n+    cur_os = sys.platform\n+    if cur_os != \"linux\" and cur_os != \"win32\":\n         return []\n \n     if platform.machine() == \"s390x\":\n"
                },
                {
                    "Id": 55,
                    "hunk size": 13,
                    "hunk": "@@ -1388,12 +1434,11 @@ def valid_vec_isa_list() -> List[VecISA]:\n         return []\n \n     isa_list = []\n-    with open(\"/proc/cpuinfo\") as _cpu_info:\n-        _cpu_info_content = _cpu_info.read()\n-        for isa in supported_vec_isa_list:\n-            if str(isa) in _cpu_info_content and isa:\n-                isa_list.append(isa)\n-        return isa_list\n+    _cpu_supported_isa = x86_isa_checker()\n+    for isa in supported_vec_isa_list:\n+        if str(isa) in _cpu_supported_isa:\n+            isa_list.append(isa)\n+    return isa_list\n \n \n def pick_vec_isa() -> VecISA:\n"
                },
                {
                    "Id": 56,
                    "hunk size": 11,
                    "hunk": "@@ -1569,7 +1615,14 @@ def get_include_and_linking_paths(\n     _set_gpu_runtime_env()\n     from torch.utils import cpp_extension\n \n-    macros = vec_isa.build_macro() if vec_isa != invalid_vec_isa else \"\"\n+    # Remove below in the further\n+    # macros = \"-D {}\".format(vec_isa.build_macro()) if vec_isa != invalid_vec_isa else \"\"\n+    macros = \"\"\n+    if vec_isa != invalid_vec_isa:\n+        for x in vec_isa.build_macro():\n+            macros_def = f\"-D{x} \"\n+            macros += macros_def\n+\n     build_arch_flags = \"\"\n     if sys.platform == \"linux\" and (\n         include_pytorch\n"
                }
            ]
        },
        {
            "path": "torch/_inductor/cpp_builder.py",
            "patches": []
        },
        {
            "path": "torch/cpu/__init__.py",
            "patches": [
                {
                    "Id": 57,
                    "hunk size": 12,
                    "hunk": "@@ -27,6 +27,16 @@ __all__ = [\n _device_t = Union[_device, str, int, None]\n \n \n+def _is_cpu_support_avx2() -> bool:\n+    r\"\"\"Returns a bool indicating if CPU supports AVX2.\"\"\"\n+    return torch._C._cpu._is_cpu_support_avx2()\n+\n+\n+def _is_cpu_support_avx512() -> bool:\n+    r\"\"\"Returns a bool indicating if CPU supports AVX512.\"\"\"\n+    return torch._C._cpu._is_cpu_support_avx512()\n+\n+\n def _is_cpu_support_vnni() -> bool:\n     r\"\"\"Returns a bool indicating if CPU supports VNNI.\"\"\"\n     return torch._C._cpu._is_cpu_support_vnni()\n"
                }
            ]
        },
        {
            "path": "torch/csrc/cpu/Module.cpp",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7d33ff59ba4ed920f590cb3e8f3e1bd571c78f62",
    "message": "[Split Build]Use same package (#127934)\n\nThis PR removes the second separate package we were using for the libtorch wheel.\nIn terms of testing that this works we will look use the PRs above this in the stack.\n\nAs for sanity checking these are the wheels that are produced by running\n```\npython setup.py clean && BUILD_LIBTORCH_WHL=1 with-proxy python setup.py bdist_whee\nl && BUILD_PYTHON_ONLY=1 with-proxy python setup.py bdist_wheel --cmake\n```\n\n```\nsahanp@devgpu086 ~/pytorch ((5f15e171\u2026))> ls -al dist/                                                        (pytorch-3.10)\ntotal 677236\ndrwxr-xr-x 1 sahanp users       188 Jun  4 12:19 ./\ndrwxr-xr-x 1 sahanp users      1696 Jun  4 12:59 ../\n-rw-r--r-- 1 sahanp users  81405742 Jun  4 12:19 torch-2.4.0a0+gitca0a73c-cp310-cp310-linux_x86_64.whl\n-rw-r--r-- 1 sahanp users 612076919 Jun  4 12:19 libtorch-2.4.0a0+gitca0a73c-py3-none-any.whl\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127934\nApproved by: https://github.com/atalman",
    "date": "2024-06-19T15:57:21+00:00",
    "label": "NO",
    "changes": [
        {
            "path": ".ci/pytorch/build.sh",
            "patches": [
                {
                    "Id": 58,
                    "hunk size": 20,
                    "hunk": "@@ -284,12 +284,26 @@ else\n         # Which should be backward compatible with Numpy-1.X\n         python -mpip install --pre numpy==2.0.0rc1\n       fi\n-      WERROR=1 python setup.py bdist_wheel\n+\n+      WERROR=1 python setup.py clean\n+\n+      if [[ \"$USE_SPLIT_BUILD\" == \"true\" ]]; then\n+        BUILD_LIBTORCH_WHL=1 BUILD_PYTHON_ONLY=0 python setup.py bdist_wheel\n+        BUILD_LIBTORCH_WHL=0 BUILD_PYTHON_ONLY=1 python setup.py bdist_wheel --cmake\n+      else\n+        WERROR=1 python setup.py bdist_wheel\n+      fi\n     else\n+      python setup.py clean\n       if [[ \"$BUILD_ENVIRONMENT\" == *xla* ]]; then\n         source .ci/pytorch/install_cache_xla.sh\n       fi\n-      python setup.py bdist_wheel\n+      if [[ \"$USE_SPLIT_BUILD\" == \"true\" ]]; then\n+        echo \"USE_SPLIT_BUILD cannot be used with xla or rocm\"\n+        exit 1\n+      else\n+        python setup.py bdist_wheel\n+      fi\n     fi\n     pip_install_whl \"$(echo dist/*.whl)\"\n \n"
                }
            ]
        },
        {
            "path": "setup.py",
            "patches": [
                {
                    "Id": 59,
                    "hunk size": 15,
                    "hunk": "@@ -210,19 +209,6 @@ if sys.platform == \"win32\" and sys.maxsize.bit_length() == 31:\n \n import platform\n \n-\n-def _get_package_path(package_name):\n-    loader = pkgutil.find_loader(package_name)\n-    if loader:\n-        # The package might be a namespace package, so get_data may fail\n-        try:\n-            file_path = loader.get_filename()\n-            return os.path.dirname(file_path)\n-        except AttributeError:\n-            pass\n-    return None\n-\n-\n BUILD_LIBTORCH_WHL = os.getenv(\"BUILD_LIBTORCH_WHL\", \"0\") == \"1\"\n BUILD_PYTHON_ONLY = os.getenv(\"BUILD_PYTHON_ONLY\", \"0\") == \"1\"\n \n"
                },
                {
                    "Id": 60,
                    "hunk size": 9,
                    "hunk": "@@ -347,9 +343,12 @@ cmake_python_include_dir = sysconfig.get_path(\"include\")\n # Version, create_version_file, and package_name\n ################################################################################\n \n-DEFAULT_PACKAGE_NAME = LIBTORCH_PKG_NAME if BUILD_LIBTORCH_WHL else \"torch\"\n+package_name = os.getenv(\"TORCH_PACKAGE_NAME\", \"torch\")\n+LIBTORCH_PKG_NAME = os.getenv(\"LIBTORCH_PACKAGE_NAME\", \"libtorch\")\n+if BUILD_LIBTORCH_WHL:\n+    package_name = LIBTORCH_PKG_NAME\n+\n \n-package_name = os.getenv(\"TORCH_PACKAGE_NAME\", DEFAULT_PACKAGE_NAME)\n package_type = os.getenv(\"PACKAGE_TYPE\", \"wheel\")\n version = get_torch_version()\n report(f\"Building wheel {package_name}-{version}\")\n"
                },
                {
                    "Id": 61,
                    "hunk size": 12,
                    "hunk": "@@ -1383,15 +1379,15 @@ def main():\n         \"utils/model_dump/*.mjs\",\n     ]\n \n-    if BUILD_PYTHON_ONLY:\n+    if not BUILD_LIBTORCH_WHL:\n         torch_package_data.extend(\n             [\n-                \"lib/libtorch_python*\",\n-                \"lib/*shm*\",\n-                \"lib/libtorch_global_deps*\",\n+                \"lib/libtorch_python.so\",\n+                \"lib/libtorch_python.dylib\",\n+                \"lib/libtorch_python.dll\",\n             ]\n         )\n-    else:\n+    if not BUILD_PYTHON_ONLY:\n         torch_package_data.extend(\n             [\n                 \"lib/*.so*\",\n"
                }
            ]
        },
        {
            "path": "tools/setup_helpers/env.py",
            "patches": []
        },
        {
            "path": "torch/CMakeLists.txt",
            "patches": []
        },
        {
            "path": "torch/__init__.py",
            "patches": [
                {
                    "Id": 62,
                    "hunk size": 7,
                    "hunk": "@@ -312,11 +280,6 @@ def _load_global_deps() -> None:\n     here = os.path.abspath(__file__)\n     global_deps_lib_path = os.path.join(os.path.dirname(here), \"lib\", lib_name)\n \n-    split_build_lib_name = LIBTORCH_PKG_NAME\n-    library_path = find_package_path(split_build_lib_name)\n-\n-    if library_path:\n-        global_deps_lib_path = os.path.join(library_path, \"lib\", lib_name)\n     try:\n         ctypes.CDLL(global_deps_lib_path, mode=ctypes.RTLD_GLOBAL)\n     except OSError as err:\n"
                },
                {
                    "Id": 63,
                    "hunk size": 6,
                    "hunk": "@@ -344,10 +307,6 @@ def _load_global_deps() -> None:\n             _preload_cuda_deps(lib_folder, lib_name)\n         ctypes.CDLL(global_deps_lib_path, mode=ctypes.RTLD_GLOBAL)\n \n-    if library_path:\n-        # loading libtorch_global_deps first due its special logic\n-        load_shared_libraries(library_path)\n-\n \n if (USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv(\"TORCH_USE_RTLD_GLOBAL\")) and (\n     _running_with_deploy() or platform.system() != \"Windows\""
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/978faf1fa29444f78a7ca805f8abc032cb29e0d8",
    "message": "Use an op counter to decide when to realize a kernel (#117030)\n\nInstead of checking the number of bytes in the string representation\nof the kernel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117030\nApproved by: https://github.com/lezcano, https://github.com/peterbell10",
    "date": "2024-01-27T05:28:46+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_inductor/config.py",
            "patches": []
        },
        {
            "path": "torch/_inductor/graph.py",
            "patches": [
                {
                    "Id": 64,
                    "hunk size": 4,
                    "hunk": "@@ -968,7 +968,7 @@ class GraphLowering(torch.fx.Interpreter):\n                 curr = result.data.data\n                 if isinstance(curr, Pointwise):\n                     # Use inner fn as a rough proxy. Good enough.\n-                    if curr.inner_fn_str_len() > config.realize_bytes_threshold:\n+                    if curr.has_large_inner_fn():\n                         result.realize()\n \n         # This is not complete, but it doesn't have to be: origin_node\n"
                }
            ]
        },
        {
            "path": "torch/_inductor/ir.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2367d0dacdded111d7859fdec23a6a3d87c19ca5",
    "message": "[AOTInductor] Add tensor_constantX to pass constant buffer update's check (#122562) (#122690)\n\nSummary:\n\nDuring tracing, some constants (tensor_constant{idx}) are being generated internally.\nThose constants are neither parameters or buffers, and users have zero control on them.\n\nTo accomodate this, we should allow users not passing in those constants generated internally but still be able the constants in the model.\n\nTest Plan:\nIncluded in commit.\n```\nbuild/bin/test_aot_inductor\n```\n\nReviewed By: zoranzhao\n\nDifferential Revision: D55354548\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122690\nApproved by: https://github.com/khabinov",
    "date": "2024-03-26T23:25:15+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/csrc/inductor/aoti_runtime/model_container.h",
            "patches": [
                {
                    "Id": 65,
                    "hunk size": 9,
                    "hunk": "@@ -243,6 +247,13 @@ class AOTInductorModelContainer {\n         auto constant_name = std::string(models_[0]->constant_name(idx));\n         auto it = constants_map.find(constant_name);\n         if (it == constants_map.end()) {\n+          if (_is_tensor_constant(constant_name)) {\n+            // tracing sometimes creates tensors that are non-existent in\n+            // original graph. We could skip those and do a direct copy.\n+            std::cerr << \"[WARNING] Found constant \" << constant_name\n+                      << \" in model, but not provided by user!\\n\";\n+            continue;\n+          }\n           throw std::runtime_error(\n               std::string(\"Cannot find constants \") + constant_name +\n               std::string(\" in constants_map!\"));\n"
                },
                {
                    "Id": 66,
                    "hunk size": 16,
                    "hunk": "@@ -253,17 +264,25 @@ class AOTInductorModelContainer {\n     for (size_t idx = 0; idx < num_constants; idx++) {\n       auto constant_name = std::string(models_[0]->constant_name(idx));\n       auto it = constants_map.find(constant_name);\n-      if (it == constants_map.end()) {\n+      if (it == constants_map.end() &&\n+          !(_is_tensor_constant(constant_name) && use_inactive)) {\n         continue;\n       }\n \n+      AtenTensorHandle tensor;\n+      if (_is_tensor_constant(constant_name) && use_inactive) {\n+        tensor = original_constants_map->find(constant_name)->second.get();\n+      } else {\n+        tensor = it->second;\n+      }\n+\n       // Move the data to container handled blob.\n       uint8_t* internal_constants_ptr =\n           constants_blob_ptr + constants_internal_offset_[idx];\n       void* user_constant_ptr;\n       int64_t constant_size;\n-      aoti_torch_get_data_ptr(it->second, &user_constant_ptr);\n-      aoti_torch_get_storage_size(it->second, &constant_size);\n+      aoti_torch_get_data_ptr(tensor, &user_constant_ptr);\n+      aoti_torch_get_storage_size(tensor, &constant_size);\n \n       AOTI_RUNTIME_DEVICE_CHECK(cudaMemcpy(\n           internal_constants_ptr,\n"
                },
                {
                    "Id": 67,
                    "hunk size": 6,
                    "hunk": "@@ -278,9 +297,9 @@ class AOTInductorModelContainer {\n       int64_t* stride;\n       int64_t offset;\n       int device_idx = models_[0]->get_device_idx();\n-      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(it->second, &stride));\n+      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(tensor, &stride));\n       AOTI_TORCH_ERROR_CODE_CHECK(\n-          aoti_torch_get_storage_offset(it->second, &offset));\n+          aoti_torch_get_storage_offset(tensor, &offset));\n       AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_create_tensor_from_blob(\n           internal_constants_ptr,\n           models_[0]->constant_ndim(idx),"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8bfd9e98154b83fc6bb0d4407e103d0eb65331ae",
    "message": "[cuDNN] Graph-capturable cuDNN CTCLoss (#128271)\n\ncuDNN v8.x added a graph-capturable CTCLoss, which slots \"neatly\" into the `Tensor` variant\n\n~~WIP as cuDNN has a restriction on the max target length (255), but this is not checkable in the graph-capture case, so the UX around warnings/error-messages here might need to be tuned...~~\nCurrently checks restriction on max target length during warmup run(s), and bails out during capture if this constraint was violated during warmup.\n\nCC @ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128271\nApproved by: https://github.com/ezyang, https://github.com/malfet",
    "date": "2024-06-25T06:01:50+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "aten/src/ATen/cudnn/Descriptors.h",
            "patches": [
                {
                    "Id": 68,
                    "hunk size": 20,
                    "hunk": "@@ -357,6 +357,24 @@ struct TORCH_CUDA_CPP_API CTCLossDescriptor\n     AT_CUDNN_CHECK(\n         cudnnSetCTCLossDescriptorEx(mut_desc(), datatype, normMode, gradMode));\n   }\n+  void set_v8_v9(\n+      cudnnDataType_t datatype,\n+      cudnnLossNormalizationMode_t normMode,\n+      cudnnNanPropagation_t gradMode,\n+      int maxLabelLength) {\n+#if defined(CUDNN_VERSION) && CUDNN_VERSION >= 90000\n+    auto gradModev9 = CUDNN_CTC_ZERO_OOB_GRADIENTS;\n+    if (gradMode == cudnnNanPropagation_t::CUDNN_PROPAGATE_NAN) {\n+      gradModev9 = CUDNN_CTC_SKIP_OOB_GRADIENTS;\n+    }\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v9(mut_desc(), datatype, normMode, gradModev9, maxLabelLength));\n+#else\n+    AT_CUDNN_CHECK(\n+        cudnnSetCTCLossDescriptor_v8(mut_desc(), datatype, normMode, gradMode, maxLabelLength));\n+#endif\n+  }\n+\n };\n \n struct TORCH_CUDA_CPP_API ActivationDescriptor\n"
                }
            ]
        },
        {
            "path": "aten/src/ATen/native/LossCTC.cpp",
            "patches": [
                {
                    "Id": 69,
                    "hunk size": 9,
                    "hunk": "@@ -539,8 +539,13 @@ Tensor ctc_loss(const Tensor& log_probs_, const Tensor& targets, IntArrayRef inp\n \n // Convenience function accepting Tensors\n Tensor ctc_loss(const Tensor& log_probs, const Tensor& targets, const Tensor& input_lengths, const Tensor& target_lengths, int64_t BLANK, int64_t reduction, bool zero_infinity) {\n+  // we don't want to convert to IntArrayRef if we can dispatch to cuDNN (this allows graph-capturable ctc_loss)\n+  bool use_cudnn =\n+      (log_probs.device().type() == at::kCUDA) &&\n+      at::_use_cudnn_ctc_loss(\n+          log_probs, targets, input_lengths, target_lengths, BLANK);\n   if (at::areAnyTensorSubclassLike(\n-          {log_probs, targets, input_lengths, target_lengths})) {\n+          {log_probs, targets, input_lengths, target_lengths}) || use_cudnn) {\n     // Composite Compliant path for TensorSubclasses\n     return ctc_loss_impl(log_probs, targets, input_lengths, target_lengths, BLANK, reduction, zero_infinity);\n   }\n"
                }
            ]
        },
        {
            "path": "aten/src/ATen/native/cudnn/LossCTC.cpp",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/31b044570221e577c0aa27fbc7153d98c8fba30b",
    "message": "Fix torch.compile with FakeTensor that has SymInt sizes (#107662)\n\n**Motivation:**\nWhen input FakeTensor to torch.compile has SymInt sizes (e.g. make_fx(opt_f, tracing_mode=\"symbolic\"):\n1. We cannot create a FakeTensor from that input in dynamo due to the SymInts.\n2. We cannot check input tensors in guard check function and will abort due to tensor check calls sizes/strides.\n\nFor 1, we specialize the FakeTensor's SymInts using their hints. This is mostly safe since inputs mostly have concrete shapes and not computed from some DynamicOutputShape ops. We'll throw a data dependent error if the symint is unbacked.\n\nFor 2, we replace size/stride calls with the sym_* variants in TENSOR_CHECK guards' check function.\n\n**Test Plan:**\nSee added tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107662\nApproved by: https://github.com/ezyang",
    "date": "2023-08-23T05:27:57+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/cf392d8a89a86f563b5223a889ef7f7027bd182e",
    "message": "[pytorch][cuda] Generate kernels for 5x5 filters on depth wise convolution backward (#129609)\n\nIn #125362 we improved the default implementation of depth wise convolution 2D forward pass by precomputing boundaries of accessed slices instead of doing expensive edge checks in the inner loops. We also generated kernels for 5x5 filters as this is a common problem size.\n\nIn this PR we tried to applied the same strategy for the backward kernel but we only saw good gains just by generating code for 5x5 filters. We could also write a fallback implementation that precomputes access boundaries when filter size and stride are not known at compile time may bring some speedup but that kernel would very rarely be called.\n\nThis PR also hints the thread count at compile time and leaves only the unroll directive that seems to help performance.\n\nBefore:\n\n```\n         B      C      iH      iW    kH    kW  conv2d-backward (cuda)  conv2d-fp16-backward (cuda)\n0      8.0   64.0  1024.0  1008.0   5.0   5.0               89.002686                    26.400480\n1      8.0   64.0  1008.0  1008.0   5.0   5.0               88.885025                    25.995296\n2      4.0   48.0   720.0   539.0   6.0   1.0                9.488832                     9.091136\n3      4.0  120.0   379.0   283.0   6.0   1.0                4.194640                     3.844432\n4      4.0   32.0   713.0   532.0   6.0   1.0                8.027296                     7.700064\n5      4.0    3.0   712.0   542.0  31.0  31.0               15.618095                    15.097760\n6      4.0  120.0   379.0   288.0   1.0   6.0                3.788224                     3.499648\n7   1024.0  384.0     1.0   928.0   1.0   3.0               18.988289                    14.152768\n8      4.0   24.0   687.0   512.0   6.0   1.0                6.902704                     6.685056\n9     96.0   96.0   112.0   112.0   5.0   5.0               15.672400                     4.953984\n10    96.0   80.0    56.0    56.0   5.0   5.0                3.261152                     1.250320\n11    64.0  128.0    64.0    84.0   3.0   3.0                3.172192                     1.515648\n12    16.0  960.0     7.0     7.0   5.0   5.0                0.197024                     0.072736\n13    16.0   64.0   112.0   112.0   3.0   3.0                1.126240                     0.650304\n```\n\nAfter\n```\nconv2d-performance:\n         B      C      iH      iW    kH    kW  conv2d-backward (cuda)  conv2d-fp16-backward (cuda)\n0      8.0   64.0  1024.0  1008.0   5.0   5.0               76.278656                    26.418720\n1      8.0   64.0  1008.0  1008.0   5.0   5.0               73.211617                    26.018433\n2      4.0   48.0   720.0   539.0   6.0   1.0                8.901312                     9.322912\n3      4.0  120.0   379.0   283.0   6.0   1.0                3.815616                     3.992208\n4      4.0   32.0   713.0   532.0   6.0   1.0                7.753024                     8.032433\n5      4.0    3.0   712.0   542.0  31.0  31.0               15.244144                    15.277296\n6      4.0  120.0   379.0   288.0   1.0   6.0                3.503264                     3.552976\n7   1024.0  384.0     1.0   928.0   1.0   3.0               16.682976                    14.167969\n8      4.0   24.0   687.0   512.0   6.0   1.0                6.802576                     7.019040\n9     96.0   96.0   112.0   112.0   5.0   5.0               12.713024                     4.958656\n10    96.0   80.0    56.0    56.0   5.0   5.0                2.648352                     1.254752\n11    64.0  128.0    64.0    84.0   3.0   3.0                3.213568                     1.517952\n12    16.0  960.0     7.0     7.0   5.0   5.0                0.182208                     0.076256\n13    16.0   64.0   112.0   112.0   3.0   3.0                1.139952                     0.652432\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129609\nApproved by: https://github.com/ezyang, https://github.com/eqy",
    "date": "2024-06-27T06:01:47+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "aten/src/ATen/native/cuda/DepthwiseConv2d.cu",
            "patches": [
                {
                    "Id": 70,
                    "hunk size": 5,
                    "hunk": "@@ -213,6 +213,9 @@ conv_depthwise2d_forward_kernel(\n }\n \n template <int kSize, int stride, typename scalar_t, typename index_t>\n+#if !defined(USE_ROCM)\n+C10_LAUNCH_BOUNDS_1(at::cuda::detail::CUDA_NUM_THREADS)\n+#endif\n __global__ void conv_depthwise2d_backward_kernel(\n     const PackedTensorAccessor32<const scalar_t, 4, DefaultPtrTraits> grad_output,\n     PackedTensorAccessor32<scalar_t, 4, DefaultPtrTraits> grad_input,\n"
                },
                {
                    "Id": 71,
                    "hunk size": 10,
                    "hunk": "@@ -245,17 +248,11 @@ __global__ void conv_depthwise2d_backward_kernel(\n \n     acc_t value(0);\n \n-#if !defined(USE_ROCM)\n-#pragma unroll\n-#endif\n     for (int multiplier = 0; multiplier < depthwiseMultiplier; ++multiplier) {\n       int och = (c * depthwiseMultiplier) + multiplier;\n       int weightOffset = och * kernelHeight * kernelWidth;\n-#if !defined(USE_ROCM)\n-#pragma unroll\n-#endif\n       for (int kh = 0; kh < KH_LIMIT; ++kh) {\n-#if defined(USE_ROCM)\n+#if !defined(USE_ROCM)\n #pragma unroll\n #endif\n         for (int kw = 0; kw < KW_LIMIT; ++kw) {\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/57a3d00b0659e4ac37c4a35a36c71f710e89197a",
    "message": "[AOTInductor] Add tensor_constantX to pass constant buffer update's check (#122562)\n\nSummary:\nDuring tracing, some constants (tensor_constant{idx}) are being generated internally.\nThose constants are neither parameters or buffers, and users have zero control on them.\n\nTo accomodate this, we should allow users not passing in those constants generated internally but still be able the constants in the model.\n\nTest Plan:\nIncluded in commit.\n```\nbuild/bin/test_aot_inductor\n```\n\nDifferential Revision: D55286634\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122562\nApproved by: https://github.com/chenyang78, https://github.com/khabinov",
    "date": "2024-03-25T22:05:20+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/csrc/inductor/aoti_runtime/model_container.h",
            "patches": [
                {
                    "Id": 72,
                    "hunk size": 9,
                    "hunk": "@@ -243,6 +247,13 @@ class AOTInductorModelContainer {\n         auto constant_name = std::string(models_[0]->constant_name(idx));\n         auto it = constants_map.find(constant_name);\n         if (it == constants_map.end()) {\n+          if (_is_tensor_constant(constant_name)) {\n+            // tracing sometimes create tensors that are non-existent in\n+            // original graph. We could skip those and do a direct copy.\n+            std::cerr << \"[WARNING] Found constant \" << constant_name\n+                      << \" in model, but not provided by user!\\n\";\n+            continue;\n+          }\n           throw std::runtime_error(\n               std::string(\"Cannot find constants \") + constant_name +\n               std::string(\" in constants_map!\"));\n"
                },
                {
                    "Id": 73,
                    "hunk size": 16,
                    "hunk": "@@ -253,17 +264,25 @@ class AOTInductorModelContainer {\n     for (size_t idx = 0; idx < num_constants; idx++) {\n       auto constant_name = std::string(models_[0]->constant_name(idx));\n       auto it = constants_map.find(constant_name);\n-      if (it == constants_map.end()) {\n+      if (it == constants_map.end() &&\n+          !(_is_tensor_constant(constant_name) && use_inactive)) {\n         continue;\n       }\n \n+      AtenTensorHandle tensor;\n+      if (_is_tensor_constant(constant_name) && use_inactive) {\n+        tensor = original_constants_map->find(constant_name)->second.get();\n+      } else {\n+        tensor = it->second;\n+      }\n+\n       // Move the data to container handled blob.\n       uint8_t* internal_constants_ptr =\n           constants_blob_ptr + constants_internal_offset_[idx];\n       void* user_constant_ptr;\n       int64_t constant_size;\n-      aoti_torch_get_data_ptr(it->second, &user_constant_ptr);\n-      aoti_torch_get_storage_size(it->second, &constant_size);\n+      aoti_torch_get_data_ptr(tensor, &user_constant_ptr);\n+      aoti_torch_get_storage_size(tensor, &constant_size);\n \n       AOTI_RUNTIME_DEVICE_CHECK(cudaMemcpy(\n           internal_constants_ptr,\n"
                },
                {
                    "Id": 74,
                    "hunk size": 6,
                    "hunk": "@@ -278,9 +297,9 @@ class AOTInductorModelContainer {\n       int64_t* stride;\n       int64_t offset;\n       int device_idx = models_[0]->get_device_idx();\n-      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(it->second, &stride));\n+      AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(tensor, &stride));\n       AOTI_TORCH_ERROR_CODE_CHECK(\n-          aoti_torch_get_storage_offset(it->second, &offset));\n+          aoti_torch_get_storage_offset(tensor, &offset));\n       AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_create_tensor_from_blob(\n           internal_constants_ptr,\n           models_[0]->constant_ndim(idx),"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fd6c993eeaacda7ef6b83f59ad3474aed0d52eaf",
    "message": "Add missing CUDA error check (#110368)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110368\nApproved by: https://github.com/Skylion007",
    "date": "2023-10-02T17:34:31+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d97332f8391e9d1f0c3fa019376d04b523bfb06c",
    "message": "Add cuda status checks to FA templates (#112229)\n\n# Summary\ncuda status checks were accidentely removed on latest update\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112229\nApproved by: https://github.com/Skylion007",
    "date": "2023-10-27T16:54:23+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d65e067baa2ef224444e180594c4882b5a55efde",
    "message": "Updates to attn_mask handiling in mem_eff (#109620)\n\n# Summar\nAlign internal changes to what is xformers: https://github.com/facebookresearch/xformers/commit/a67cd575315a6b59c16d735fe6dac66419d18e7b\n\nWe have actually already removed the bias 4d view so this is in theory is a no op and really just increased safety checks\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109620\nApproved by: https://github.com/cpuhrsch",
    "date": "2023-09-21T22:40:58+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/92cc52ab0e48a27d77becd37f1683fd442992120",
    "message": "[CPU SDP] Remove mem efficient attn checks in CPU (#112375)\n\nIt doesn't seem like memory efficient attention can be used on CPU, as we don't check for it when iterating backends in `select_sdp_backend_cpp`. So removing some of the logic around mem efficient attention selection.\n\nCreated from CodeHub with https://fburl.com/edit-in-codehub\n\nDifferential Revision: [D50775562](https://our.internmc.facebook.com/intern/diff/D50775562/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D50775562/)!\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112375\nApproved by: https://github.com/drisspg",
    "date": "2023-10-30T16:43:20+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/9912209743b7adbcbb82050829a47c00db2881c1",
    "message": "check if the input fx graph of aot_compile return tuple (#129824)\n\nFixes https://github.com/pytorch/pytorch/issues/129719\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129824\nApproved by: https://github.com/angelayi, https://github.com/yushangdi",
    "date": "2024-07-10T01:18:55+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/__init__.py",
            "patches": [
                {
                    "Id": 75,
                    "hunk size": 10,
                    "hunk": "@@ -48,7 +48,13 @@ def aot_compile(\n     Returns:\n         Path to the generated shared library\n     \"\"\"\n-    from .compile_fx import compile_fx_aot\n+    from .compile_fx import compile_fx_aot, graph_returns_tuple\n+\n+    assert graph_returns_tuple(gm), (\n+        \"Graph output must be a tuple(). This is so that we can avoid \"\n+        \"pytree processing of the outputs. Please change the module to \"\n+        \"have tuple outputs.\"\n+    )\n \n     # We will serialize the pytree info into the .so as constant strings\n     in_spec = None"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7ccca60927cdccde63d6a1d40480950f24e9877a",
    "message": "[BE] Remove stale CUDA version check from cpp_extension.py (#113447)\n\nAs at least CUDA-11.x is needed to build PyTorch on latest trunk\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113447\nApproved by: https://github.com/Skylion007, https://github.com/atalman, https://github.com/PaliC, https://github.com/huydhn",
    "date": "2023-11-10T18:54:19+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/516f38a144bf60f44c81342fe2327acce3829ece",
    "message": "[RelEng] Define `BUILD_BUNDLE_PTXAS` (#119750)\n\nThat would bundle PTXAS into a `bin` folder\n\nWhen compiling for Triton, define `TRITION_PTXAS_PATH` if `ptxas` is bundled with PyTorch Needed to make PyTorch compiled against CUDA-11.8 usable with 11.8 driver, as Triton is bundled with latest (CUDA-12.3 at time of PyTorch-2.2 release) ptxas\n\nNeeds https://github.com/pytorch/builder/commit/5c814e2527b3f5797488bf57d9d5425e63dcc1ac to produce valid binary builds\n\nTest plan:\n- Create dummy ptxas in `torch/bin` folder and observe `torch.compile` fail with backtrace in Triton module.\n- Run following script (to be added to binary tests ) against CUDA-11.8 wheel:\n```python\nimport torch\nimport triton\n\n@torch.compile\ndef foo(x: torch.Tensor) -> torch.Tensor:\n  return torch.sin(x) + torch.cos(x)\n\nx=torch.rand(3, 3, device=\"cuda\")\nprint(foo(x))\n# And check that CUDA versions match\ncuda_version = torch.version.cuda\nptxas_version = triton.backends.nvidia.compiler.get_ptxas_version().decode(\"ascii\")\nassert cuda_version in ptxas_version, f\"CUDA version mismatch: torch build with {cuda_version}, but Triton uses ptxs {ptxas_version}\"\n```\n\nFixes https://github.com/pytorch/pytorch/issues/119054\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119750\nApproved by: https://github.com/jansel, https://github.com/atalman",
    "date": "2024-02-15T02:08:57+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "CMakeLists.txt",
            "patches": [
                {
                    "Id": 76,
                    "hunk size": 11,
                    "hunk": "@@ -1241,3 +1243,12 @@ if(DEFINED USE_CUSTOM_DEBINFO)\n     set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -g\")\n     set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -g\")\n endif()\n+\n+# Bundle PTXAS if needed\n+if(BUILD_BUNDLE_PTXAS AND USE_CUDA)\n+   if(NOT EXISTS \"${PROJECT_SOURCE_DIR}/build/bin/ptxas\")\n+     message(STATUS \"Copying PTXAS into the bin folder\")\n+     file(COPY \"${CUDAToolkit_BIN_DIR}/ptxas\" DESTINATION \"${PROJECT_BINARY_DIR}\")\n+   endif()\n+   install(PROGRAMS \"${PROJECT_BINARY_DIR}/ptxas\" DESTINATION \"${CMAKE_INSTALL_BINDIR}\")\n+endif()\n"
                }
            ]
        },
        {
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "Id": 77,
                    "hunk size": 16,
                    "hunk": "@@ -2391,6 +2391,20 @@ def caching_device_properties():\n             device_interface.Worker.get_device_properties()\n \n \n+def _set_triton_ptxas_path() -> None:\n+    if os.environ.get(\"TRITON_PTXAS_PATH\") is not None:\n+        return\n+    ptxas_path = os.path.abspath(\n+        os.path.join(os.path.dirname(__file__), \"..\", \"bin\", \"ptxas\")\n+    )\n+    if not os.path.exists(ptxas_path):\n+        return\n+    if os.path.isfile(ptxas_path) and os.access(ptxas_path, os.X_OK):\n+        os.environ[\"TRITON_PTXAS_PATH\"] = ptxas_path\n+    else:\n+        warnings.warn(f\"{ptxas_path} exists but is not an executable\")\n+\n+\n def _worker_compile(\n     kernel_name: str, source_code: str, cc: int, device: torch.device\n ) -> None:\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/745324e487c451998e4e91d074677ebe417ca1d1",
    "message": "[export] turn on hybrid symints by default (#130775)\n\nSets `prefer_deferred_runtime_asserts_over_guards=True` for export, so any guards emitted from `SymNode.expect_true` (for example, guards that are implicitly required to be true for an op to succeed) won't lead to constraint violations. Instead these should appear in the graph as runtime asserts, or potentially as replacement expressions for placeholder shapes.\n\nFor example, this reshape op should emit s0 * s1 = s2, deferred as a runtime assert.\n```\nx = torch.randn(4, 8)  # [s0, s1]\ny = torch.randn(32)  # [s2]\nout = x.reshape(-1) + y\n# this emits Eq(s0 * s1, s2), and we represent y's shape as [s0*s1] in the graph.\n```\n\nHowever, other complex guards can still cause export to fail, for instance guards emitted from `SymNode.guard_bool/guard_size_oblivious` (e.g. explicit if-else conditions in user code or lower-level op implementations hit during tracing) can still raise constraint violations. These can be deferred with `allow_complex_guards_as_runtime_asserts=True`. We don't yet make this default, because while this makes export more likely to succeed, it results in non-trivial asserts being emitted that often represent specialization to a variant of the op, or checks related to 0/1 specialization.\n\nWe also remove forced specializations for export and kill the `_disable_forced_specializations` flag - now any guard we can't express with Dims/DerivedDims either are handled with Hybrid SymInts, or should be resolved with rewriting or deferring.\n\nFollow up:\nCurrently, `ShapeEnv._set_replacement()` is called for complex equality expressions (e.g. s2 -> s0*s1 in the example above), and the ExportedProgram stores `s0*s1` in the input placeholder. This isn't checked for validity when the program is run, so an option is to avoid replacement and/or runtime assert on equality.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130775\nApproved by: https://github.com/avikchaudhuri",
    "date": "2024-07-18T17:40:58+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_export/non_strict_utils.py",
            "patches": [
                {
                    "Id": 78,
                    "hunk size": 3,
                    "hunk": "@@ -254,7 +253,6 @@ def produce_guards_and_solve_constraints(\n     Additional inputs:\n         equalities_inputs: the equality constraints to use for guards\n         original_signature: the signature of the forward method\n-        _disable_forced_specializations: if True, avoids forced specializations\n     \"\"\"\n     shape_env = fake_mode.shape_env\n     assert shape_env is not None\n"
                }
            ]
        },
        {
            "path": "torch/_export/serde/serialize.py",
            "patches": [
                {
                    "Id": 79,
                    "hunk size": 6,
                    "hunk": "@@ -215,11 +218,11 @@ def deserialize_device(d: Device) -> torch.device:\n \n \n def serialize_sym_int(s: Union[int, torch.SymInt]) -> SymInt:\n-    if isinstance(s, (torch.SymInt, int)):\n+    if isinstance(s, (torch.SymInt, sympy.Symbol, int)):\n         if symbolic_shapes.is_concrete_int(s):\n             return SymInt.create(as_int=int(s))\n         else:\n-            assert isinstance(s, torch.SymInt)\n+            assert isinstance(s, (torch.SymInt, sympy.Symbol))\n             if s.node.hint is None:\n                 return SymInt.create(as_expr=SymExpr(str(s)))\n             else:\n"
                },
                {
                    "Id": 80,
                    "hunk size": 10,
                    "hunk": "@@ -487,9 +490,13 @@ class GraphModuleSerializer(metaclass=Final):\n         if node.target is operator.getitem:\n             return\n \n-        if node.target in _SYM_INT_OPS:\n+        meta_val = node.meta.get(\"val\")\n+        if (\n+            node.target in _SYM_INT_OPS\n+            or node.target in _SYM_BOOL_OPS\n+            or (meta_val is not None and isinstance(meta_val, (torch.SymInt, torch.SymBool)))\n+        ):\n             assert len(node.kwargs) == 0\n-            meta_val = node.meta[\"val\"]\n             ex_node = Node(\n                 target=self.serialize_operator(node.target),\n                 inputs=self.serialize_sym_op_inputs(node.target, node.args),\n"
                },
                {
                    "Id": 81,
                    "hunk size": 15,
                    "hunk": "@@ -497,17 +504,8 @@ class GraphModuleSerializer(metaclass=Final):\n                     Argument.create(\n                         as_sym_int=self.serialize_sym_int_output(node.name, meta_val)\n                     )\n-                ],\n-                metadata=self.serialize_metadata(node),\n-            )\n-        elif node.target in _SYM_BOOL_OPS:\n-            assert len(node.kwargs) == 0\n-            meta_val = node.meta[\"val\"]\n-            ex_node = Node(\n-                target=self.serialize_operator(node.target),\n-                inputs=self.serialize_sym_op_inputs(node.target, node.args),\n-                outputs=[\n-                    Argument.create(\n+                    if (node.target in _SYM_INT_OPS or isinstance(meta_val, torch.SymInt))\n+                    else Argument.create(\n                         as_sym_bool=self.serialize_sym_bool_output(node.name, meta_val)\n                     )\n                 ],\n"
                },
                {
                    "Id": 82,
                    "hunk size": 11,
                    "hunk": "@@ -1538,6 +1536,15 @@ class GraphModuleDeserializer(metaclass=Final):\n     def deserialize_sym_bool(self, s: SymBool) -> Union[bool, torch.SymBool]:\n         val = s.value\n         if s.type == \"as_expr\":\n+            # first we sympify this just to access any untracked symbols\n+            expr = sympy.sympify(val.expr_str)\n+            for sym in expr.free_symbols:\n+                if (\n+                    not isinstance(sym, sympy.Number)\n+                    and str(sym) not in self.symbol_name_to_symbol\n+                ):\n+                    self.deserialize_sym_int(SymInt.create(as_expr=SymExpr(str(sym))))\n+            # then we sympify again using locals to correctly reify with the constructed symbols\n             expr = sympy.sympify(val.expr_str, locals=self.symbol_name_to_symbol)\n             return self.shape_env.create_symboolnode(expr)\n         elif s.type == \"as_bool\":\n"
                },
                {
                    "Id": 83,
                    "hunk size": 8,
                    "hunk": "@@ -1661,7 +1668,11 @@ class GraphModuleDeserializer(metaclass=Final):\n         return self.graph\n \n     def deserialize_node(self, serialized_node: Node, target: Callable) -> None:\n-        if target in _SYM_BOOL_OPS or target in _SYM_INT_OPS:\n+        if (\n+            target in _SYM_BOOL_OPS\n+            or target in _SYM_INT_OPS\n+            or target == torch.ops.aten.item.default  # this can produce either SymInt or SymBool\n+        ):\n             name = serialized_node.outputs[0].value.as_name\n             args = self.deserialize_sym_op_inputs(serialized_node.inputs)\n \n"
                }
            ]
        },
        {
            "path": "torch/_export/utils.py",
            "patches": [
                {
                    "Id": 84,
                    "hunk size": 8,
                    "hunk": "@@ -148,9 +148,9 @@ def _check_input_constraints_for_graph(\n                                 )\n                 else:\n                     if arg_dim != node_dim:\n-                        if isinstance(\n-                            node_dim, torch.SymInt\n-                        ):  # this means we deferred a guard from export analysis to runtime, let this pass\n+                        if isinstance(node_dim, torch.SymInt):\n+                            # this means we deferred a guard from export analysis to runtime, let this pass\n+                            # we'll add a runtime assert checking equality to this replacement expression\n                             continue\n                         raise RuntimeError(\n                             f\"Expected input at {get_keystr(key_path)}.shape[{j}] to be equal to \"\n"
                }
            ]
        },
        {
            "path": "torch/export/_trace.py",
            "patches": [
                {
                    "Id": 85,
                    "hunk size": 7,
                    "hunk": "@@ -96,6 +96,11 @@ class ExportDynamoConfig:\n     reorderable_logging_functions: Set[Callable] = dataclasses.field(\n         default_factory=set\n     )\n+    # Emit runtime asserts after AOTAutograd instead.\n+    # This isn't really necessary, and isn't much more efficient since the runtime asserts pass does CSE,\n+    # but if we want to reason more about what guards/runtime asserts to emit,\n+    # this makes it a bit cleaner to do from the export side. Also no real point in running this twice.\n+    do_not_emit_runtime_asserts = True\n \n \n @dataclasses.dataclass\n"
                },
                {
                    "Id": 86,
                    "hunk size": 13,
                    "hunk": "@@ -1469,13 +1484,18 @@ def _export_to_aten_ir_make_fx(\n         input_specs=input_specs, output_specs=output_specs\n     )\n \n+    # See comment in _export_to_aten_ir()\n+    if produce_guards_callback:\n+        try:\n+            produce_guards_callback(gm)\n+        except (ConstraintViolationError, ValueRangeError) as e:\n+            raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904\n+\n     from torch._guards import detect_fake_mode\n \n     fake_mode = detect_fake_mode(flat_args)\n \n-    from torch._dynamo import config as _dynamo_config\n-\n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n         stack_trace = (\n             'File \"torch/fx/passes/runtime_assert.py\", line 24, '\n             \"in insert_deferred_runtime_asserts\"\n"
                },
                {
                    "Id": 87,
                    "hunk size": 9,
                    "hunk": "@@ -1864,13 +1879,6 @@ def _export(\n          Additionally, if TORCH_DYNAMO_DO_NOT_EMIT_RUNTIME_ASSERTS=1 is specified, we will allow complex constraints\n          while not emitting runtime asserts, returning a cleaner graph with lesser guarantees around dynamic shapes.\n \n-        _disable_forced_specializations:\n-         Similar to allow_complex_guards_as_runtime_asserts, but only avoids specializing to static values if set to True.\n-         For complex guards that don't specialize, this flag doesn't have any effect. Ideally this would be subsumed by\n-         allow_complex_guards_as_runtime_asserts, but this handles one additional case: single-variable equalities where\n-         the symbol is solvable for a concrete value (e.g. Eq(s0 // 4, 400) -> s0 = 1600). If set to True, this flag will\n-         avoid specializations. Direct equalities (e.g. s0 = 4), will still specialize.\n-\n     Returns:\n         An ExportedProgram containing the traced method.\n     \"\"\"\n"
                },
                {
                    "Id": 88,
                    "hunk size": 8,
                    "hunk": "@@ -1880,12 +1888,6 @@ def _export(\n             f\"Expecting `args` to be a tuple of example positional inputs, got {type(args)}\",\n         )\n \n-    if _disable_forced_specializations and strict:\n-        raise UserError(\n-            UserErrorType.INVALID_INPUT,\n-            \"_disable_forced_specializations can be only be specified in non-strict mode.\",\n-        )\n-\n     global _EXPORT_FLAGS, _EXPORT_MODULE_HIERARCHY\n     _EXPORT_MODULE_HIERARCHY = _get_module_hierarchy(mod)\n \n"
                }
            ]
        },
        {
            "path": "torch/export/exported_program.py",
            "patches": [
                {
                    "Id": 89,
                    "hunk size": 5,
                    "hunk": "@@ -502,14 +502,13 @@ def _decompose_and_get_gm_with_new_signature_constants(\n \n     # Run this pass before creating input/output specs, since size-related CSE/DCE might affect output signature.\n     # Overwrite output specs afterwards.\n-    from torch._dynamo import config as _dynamo_config\n     from torch._export.passes._node_metadata_hook import (\n         _node_metadata_hook,\n         _set_node_metadata_hook,\n     )\n     from torch._functorch._aot_autograd.input_output_analysis import _graph_output_names\n \n-    if not _dynamo_config.do_not_emit_runtime_asserts:\n+    if not torch._dynamo.config.do_not_emit_runtime_asserts:\n         stack_trace = (\n             'File \"torch/fx/passes/runtime_assert.py\", line 24, '\n             \"in insert_deferred_runtime_asserts\"\n"
                }
            ]
        },
        {
            "path": "torch/fx/experimental/symbolic_shapes.py",
            "patches": [
                {
                    "Id": 90,
                    "hunk size": 12,
                    "hunk": "@@ -1818,12 +1787,10 @@ class DimConstraints:\n             assert isinstance(solution, sympy.Eq), f\"Expected an equality constraint for {s}, got {solution}\"\n             symbol, val = solution.args\n             assert symbol == s, f\"Expected a constraint on {s} instead of on {symbol}\"\n-            # really don't force specializations here\n-            if not (_disable_forced_specializations and s in self._marked_dynamic):\n-                # because this is univariate, the solution is a specialization\n-                self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n-                # add this as a substitution to simplify other constraints\n-                self._substitutions[s] = val\n+            # because this is univariate, the solution is a specialization\n+            self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n+            # add this as a substitution to simplify other constraints\n+            self._substitutions[s] = val\n \n             # simplify multivariate inequalities: some of them will now become univariate!\n             multivariate_inequalities = self._multivariate_inequalities\n"
                },
                {
                    "Id": 91,
                    "hunk size": 5,
                    "hunk": "@@ -1832,9 +1799,6 @@ class DimConstraints:\n                 self.add(expr.xreplace({s: self._substitutions[s]}))\n             self._raise_inconsistencies()\n \n-        if not _disable_forced_specializations:\n-            self._specialize_divisor_symbols()\n-\n         # solve linear congruences\n         # NOTE(avik): We do not need to solve them for symbols that have already been specialized.\n         reduced_congruences = self._reduce_congruences()\n"
                },
                {
                    "Id": 92,
                    "hunk size": 5,
                    "hunk": "@@ -1850,9 +1814,6 @@ class DimConstraints:\n                         self._dcp.symbol_to_source[tmp] = [ConstantSource(tmp_name)]\n                         r = try_solve(sympy.Eq(base, divisor * tmp), s)\n                         self._dynamic_results.add(self._dcp.doprint(sympy.Eq(s, r[1])))\n-                    elif not _disable_forced_specializations:\n-                        self._force_specialization(s)\n-                        self._univariate_inequalities.pop(s, None)\n \n         # remaining symbols have only pure inequalities (no equalities)\n         for s, exprs in self._univariate_inequalities.items():\n"
                },
                {
                    "Id": 93,
                    "hunk size": 7,
                    "hunk": "@@ -1875,11 +1836,6 @@ class DimConstraints:\n         symbolic_equivalences = self._symbolic_equivalences\n         self._symbolic_equivalences = []\n         for source, expr in symbolic_equivalences:\n-            if not _disable_forced_specializations and not _is_supported_equivalence(expr):\n-                for s in expr.free_symbols:\n-                    self._force_specialization(s)\n-                    sexpr = self._dcp._print_Symbol(s)\n-                    self._dynamic_results = {r for r in self._dynamic_results if sexpr not in r}\n             self.add_equality(source, expr.xreplace(self._substitutions))\n \n         # remaining symbolic equivalences become dynamic equality constraints\n"
                },
                {
                    "Id": 94,
                    "hunk size": 15,
                    "hunk": "@@ -4096,13 +4052,12 @@ class ShapeEnv:\n                     constraints = symbol_to_constraints[symbol]\n                     for c in constraints:\n                         if isinstance(c, StrictMinMaxConstraint):\n-                            if not _disable_forced_specializations:\n-                                var_with_range = self._render_range_for_constraint_violation(source, c)\n-                                msg = (\n-                                    f\"Not all values of {var_with_range} \"\n-                                    f\"satisfy the generated guard {guard_expr}.\"\n-                                )\n-                                record_constraint_violation(c.warn_only, self._debug_name(source), msg)\n+                            var_with_range = self._render_range_for_constraint_violation(source, c)\n+                            msg = (\n+                                f\"Not all values of {var_with_range} \"\n+                                f\"satisfy the generated guard {guard_expr}.\"\n+                            )\n+                            record_constraint_violation(c.warn_only, self._debug_name(source), msg)\n                         elif isinstance(c, RelaxedUnspecConstraint):\n                             # This is fine, we allow guards here as long as it\n                             # didn't constrain it to one value  (we don't\n"
                },
                {
                    "Id": 95,
                    "hunk size": 13,
                    "hunk": "@@ -4123,6 +4078,17 @@ class ShapeEnv:\n                 continue\n             issue_guard(guard)\n \n+        # Because there are guards that export's constraint solver can suggest good fixes for, that we may have\n+        # deferred as runtime asserts, and that produce_guards() alone won't do anything with (e.g. divisiblity guards),\n+        # we want to send runtime asserts to export's constraint solver too. These will still stay in the graph as asserts,\n+        # but export's constraint solver can decide whether to do anything with them (i.e. raise an error and provide\n+        # suggested fixes, or decide it's out of scope and leave as a runtime assert in the graph).\n+        for ra in self.deferred_runtime_asserts.get(None, []):\n+            if self._maybe_evaluate_static(ra.expr, axioms=()) is not None:\n+                continue\n+            expr = self.simplify(ra.expr)\n+            self.dim_constraints.add(expr)\n+\n         # 3. Every symbol must be within its value range (this handles 0/1\n         # specialization too).\n         for symbol, sources in symbol_to_source.items():"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/096dc444cee3ebf8c1868ceb92ca4d6ede9f9cc1",
    "message": "Keep zero check be compatible with different sympy versions (#130729)\n\n# Motivation\nI found a difference between sympy 1.12 and 1.13.\n```python\n# for 1.12\n>>> import sympy\n>>> a = sympy.Number(0.0)\n>>> a == 0\nTrue\n```\n```python\n# for 1.13\n>>> import sympy\n>>> a = sympy.Number(0.0)\n>>> a == 0\nFalse\n```\nThe different behavior will impact the result of [safe_mul](https://github.com/pytorch/pytorch/blob/6beec34b1c6803d5f6648c3cd7c262d6432374c8/torch/utils/_sympy/value_ranges.py#L521-L528), resulting in an incorrect results when `a = sympy.Number(0.0)`, `b = inf` and the result is `nan` if sympy version is 1.13. (the expected result is **0**)\n```python\ndef safe_mul(a, b):\n    # Make unknown() * wrap(0.0) == wrap(0.0)\n    if a == 0.0:\n        return a\n    elif b == 0.0:\n        return b\n    else:\n        return a * b\n```\n\nIn different sympy versions, `sympy.Number(0)` always has the same behavior that equals to 0.0.\n```python\n>>> import sympy\n>>> a = sympy.Number(0)\n>>> a == 0.0\nTrue # for different sympy versions\n```\nSo, use 0.0 when checking zero in safe_mul to keep compatible with different sympy versions.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130729\nApproved by: https://github.com/lezcano, https://github.com/EikanWang",
    "date": "2024-07-16T08:39:00+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/utils/_sympy/value_ranges.py",
            "patches": [
                {
                    "Id": 96,
                    "hunk size": 8,
                    "hunk": "@@ -519,10 +519,10 @@ class SymPyValueRangeAnalysis:\n             return cls.and_(a, b)\n \n         def safe_mul(a, b):\n-            # Make unknown() * wrap(0) == wrap(0)\n-            if a == 0:\n+            # Make unknown() * wrap(0.0) == wrap(0.0)\n+            if a == 0.0:\n                 return a\n-            elif b == 0:\n+            elif b == 0.0:\n                 return b\n             else:\n                 return a * b"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/06f8af30fa125ca3bec56982e53d09d77fa1ec69",
    "message": "Change FakeTensor serialization to consider only an _active_ FakeTensor mode (#120848)\n\nSummary: https://github.com/pytorch/pytorch/pull/108186 make some changes related to FakeTensor serialization such that saving and loading a tensor will give us a meta tensor, even if FakeTensor mode is not enabled. This means we can't properly save and load Tensors as part of Fx graph caching. This PR changes the logic to check if there's an _active_ FakeTensor mode.\n\nTest Plan:\n* New unit tests\n* Validated unit tests introduced in https://github.com/pytorch/pytorch/pull/108186 still pass\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120848\nApproved by: https://github.com/eellison, https://github.com/thiagocrepaldi",
    "date": "2024-03-01T02:37:21+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_guards.py",
            "patches": [
                {
                    "Id": 97,
                    "hunk size": 17,
                    "hunk": "@@ -846,3 +846,18 @@ def detect_fake_mode(inputs: Any = None):\n         return fake_mode\n     else:\n         return None\n+\n+\n+def active_fake_mode():\n+    \"\"\"\n+    Inspects the dispatch mode stack for an active fake mode and returns it.\n+    Returns None if no fake mode is active.\n+    \"\"\"\n+    from torch._subclasses.fake_tensor import FakeTensorMode\n+    from torch.utils._python_dispatch import _get_current_dispatch_mode_stack\n+\n+    for _, m in enumerate(reversed(_get_current_dispatch_mode_stack())):\n+        if isinstance(m, FakeTensorMode):\n+            return m\n+\n+    return None\n"
                }
            ]
        },
        {
            "path": "torch/serialization.py",
            "patches": [
                {
                    "Id": 98,
                    "hunk size": 4,
                    "hunk": "@@ -1196,7 +1196,7 @@ def _legacy_load(f, map_location, pickle_module, **pickle_load_args):\n             nbytes = numel * torch._utils._element_size(dtype)\n \n             if root_key not in deserialized_objects:\n-                if torch._guards.detect_fake_mode(None) is not None:\n+                if torch._guards.active_fake_mode() is not None:\n                     obj = cast(Storage, torch.UntypedStorage(nbytes, device='meta'))\n                 else:\n                     obj = cast(Storage, torch.UntypedStorage(nbytes))\n"
                },
                {
                    "Id": 99,
                    "hunk size": 4,
                    "hunk": "@@ -1272,7 +1272,7 @@ def _legacy_load(f, map_location, pickle_module, **pickle_load_args):\n \n     deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)\n \n-    if torch._guards.detect_fake_mode(None) is None:\n+    if torch._guards.active_fake_mode() is None:\n         offset = f.tell() if f_should_read_directly else None\n         for key in deserialized_storage_keys:\n             assert key in deserialized_objects"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/1cae60a87e5bdda8bcf55724a862eeed98a9747e",
    "message": "Caching attr_proxy for nn_module attribute to fix guard check failure (#130280)\n\nFixes https://github.com/pytorch/pytorch/issues/129939\n\nDifferential Revision: [D59594605](https://our.internmc.facebook.com/intern/diff/D59594605)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130280\nApproved by: https://github.com/anijain2305",
    "date": "2024-07-11T18:21:35+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_dynamo/guards.py",
            "patches": [
                {
                    "Id": 100,
                    "hunk size": 8,
                    "hunk": "@@ -877,7 +879,11 @@ class GuardBuilder(GuardBuilderBase):\n         elif istype(source, AttrSource):\n             assert base_guard_manager  # to make mypy happy\n \n-            if isinstance(base_example_value, torch.nn.Module):\n+            if (\n+                isinstance(base_example_value, torch.nn.Module)\n+                and get_custom_getattr(base_example_value)\n+                is unpatched_nn_module_getattr\n+            ):\n                 out = self.getattr_on_nn_module(\n                     source,\n                     base_guard_manager,\n"
                },
                {
                    "Id": 101,
                    "hunk size": 8,
                    "hunk": "@@ -1162,7 +1168,11 @@ class GuardBuilder(GuardBuilderBase):\n \n                 # if the base value is nn.Module, check if we can speedup the\n                 # guard by going through __dict__ attrs.\n-                if isinstance(base_example_value, torch.nn.Module):\n+                if (\n+                    isinstance(base_example_value, torch.nn.Module)\n+                    and get_custom_getattr(base_example_value)\n+                    is unpatched_nn_module_getattr\n+                ):\n                     return self.getattr_on_nn_module(\n                         source,\n                         base_manager,\n"
                }
            ]
        },
        {
            "path": "torch/fx/experimental/proxy_tensor.py",
            "patches": [
                {
                    "Id": 102,
                    "hunk size": 20,
                    "hunk": "@@ -975,17 +977,33 @@ class _ModuleStackTracer(PythonKeyTracer):\n                 self.__dict__ = base.__dict__\n                 self.__class__.__module__ = base.__class__.__module__\n                 self.__class__.__qualname__ = base.__class__.__qualname__\n+                self.reset_proxy_mapping(base, path)\n+\n+            def reset_proxy_mapping(self, base, path):\n                 self_.proxy_paths[self] = path\n                 self_.proxy_modules[self] = base\n \n             def __getattr__(self, name):\n                 assert isinstance(self, torch.nn.Module)\n+                # Calling into torch.nn.Module.__getattr__ with super(),\n+                # That __getattr__ is patched to be module_getattr_wrapper in _symbolic_trace.py.\n+                # which then calls into _ModuleStackTracer.getattr\n                 attr_val = super().__getattr__(name)\n                 if isinstance(attr_val, AttrProxy):\n                     attr_val = self_.proxy_modules[attr_val]\n                 elif not isinstance(attr_val, torch.nn.Module):\n                     return attr_val\n-                return AttrProxy(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                if attr_val not in self_.attr_proxy_map:\n+                    self_.attr_proxy_map[attr_val] = AttrProxy(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                else:\n+                    # NOTE [caching AttrProxy]. Caching ensures a 1-1 mapping between AttrProxy and the actual attr_val.\n+                    # 1. We reset the proxy_mapping to solve the diamond shape reference problem: we want to record the\n+                    # path as A.B.D instead of A.C.D (the purpose of _ModuleStackTracer).\n+                    # 2. Instead of creating a new AttrProxy, we just reset the proxy_mapping of existing one. This is to avoid\n+                    # dynamo creating multiple guards for the same attr_val but different AttrProxy when exporting\n+                    # a model that calls torch.compile (e.g when a model uses torch.cond.)\n+                    self_.attr_proxy_map[attr_val].reset_proxy_mapping(attr_val, self_.proxy_paths[self] + \".\" + name)\n+                return self_.attr_proxy_map[attr_val]\n \n             @property\n             def _modules(self):\n"
                },
                {
                    "Id": 103,
                    "hunk size": 10,
                    "hunk": "@@ -1020,7 +1038,13 @@ class _ModuleStackTracer(PythonKeyTracer):\n             return super().getattr(attr, attr_val, parameter_proxy_cache)\n         if isinstance(attr_val, self.proxy_type):\n             return attr_val\n-        return self.proxy_type(attr_val, attr)\n+\n+        # See NOTE [caching AttrProxy].\n+        if attr_val not in self.attr_proxy_map:\n+            self.attr_proxy_map[attr_val] = self.proxy_type(attr_val, attr)\n+        else:\n+            self.attr_proxy_map[attr_val].reset_proxy_mapping(attr_val, attr)\n+        return self.attr_proxy_map[attr_val]\n \n     def trace(self, root, concrete_args):\n         res = super().trace(root, concrete_args)"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/be9de33240424c85cbd8dd11bd285830e8ad9b36",
    "message": "[Dynamo][9/N] Make SkipFilesVariable wrap functions only (#115963)\n\nMake ```SkipFilesVariable``` only handle function type, and route skipped classes to ```UserDefinedClassVariable```. The reasons behind this are:\n* We'd like to remove ```is_allowed```, so the allowed/disallowed torch classes should have a proper place to handle. We can put them in either ```SkipFilesVariable``` and ```UserDefinedClassVariable``` under the current architecture, but it's  confusing to have two places do one thing.\n   - Going forward, let's make ```SkipFilesVariable``` only handle functions, and probably I'll rename it to ```SkippedFunctionVariable``` in the following PRs.\n   - Let's do dispatch by value's type, all torch classes stuff would go to ```UserDefinedClassVariable``` in the next PR.\n* We'd merge in_graph/skip/inline trace decision into the same API ```trace_rule.lookup```, so probably we have to limit the input to only function for better organizing ```VariableBuilder._wrap``` logics.\n   - Next step, I'll merge ```skipfiles.check``` into ```trace_rules.lookup```, and do the skipfile check before wrapping them into correct variable tracker.\n   - Though the ```TorchCtxManagerClassVariable``` is decided by ```trace_rules.lookup```, I'll refactor it out in the following PRs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115963\nApproved by: https://github.com/jansel",
    "date": "2023-12-21T01:35:07+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/c993f1b37fe167c186911885dd680bba52471aeb",
    "message": "Fix edge cases for gather in inductor (#126893)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126893\nApproved by: https://github.com/peterbell10\nghstack dependencies: #126876",
    "date": "2024-06-10T15:31:03+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/lowering.py",
            "patches": [
                {
                    "Id": 104,
                    "hunk size": 17,
                    "hunk": "@@ -2781,18 +2781,29 @@ def gather(x, dim, index, sparse_grad=False):\n     # sparse_grad doesn't affect forward computation,\n     # and backward tracing is taken care of by AOT Autograd\n     assert isinstance(x, TensorBox)\n+    if index.get_numel() == 0:\n+        # Empty index case. Return an empty array with the same shape\n+        return new_empty(x, index.get_size())\n+\n     assert index.get_dtype() == torch.int64\n     size = x.get_size()\n     offset = len(size) == 0\n     dim = _validate_dim(x, dim, offset)\n \n+    if offset:\n+        x = expand(x, [1])\n+        size = [1]\n+\n     x_loader = x.make_loader()\n     index_loader = index.make_loader()\n \n     def fn(idx):\n         idx = list(idx)\n-        if len(idx) != 0:\n-            idx[dim] = ops.indirect_indexing(index_loader(idx), size[dim])\n+        gather_idx = ops.indirect_indexing(index_loader(idx), size[dim])\n+        if len(idx) == 0:\n+            idx = [gather_idx]\n+        else:\n+            idx[dim] = gather_idx\n         return x_loader(idx)\n \n     return Pointwise.create(\n"
                },
                {
                    "Id": 105,
                    "hunk size": 5,
                    "hunk": "@@ -3272,6 +3283,9 @@ def scatter_reduce_(self, dim: int, index, src, reduce, *, include_self: bool =\n     if isinstance(index, TensorBox) and len(index.get_size()) == 0:\n         index = view(index, [1])\n \n+    if index.get_numel() == 0:\n+        return self\n+\n     dim = _validate_dim(self, dim)\n \n     self.realize()\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/6eac3f45c7d6db45fa26507d0d8949ce6fb12dd7",
    "message": "Add basic sanity checks for graph ops to cache key (#124745)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124745\nApproved by: https://github.com/bdhirsh",
    "date": "2024-05-23T23:37:43+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_functorch/_aot_autograd/autograd_cache.py",
            "patches": []
        },
        {
            "path": "torch/_inductor/codecache.py",
            "patches": [
                {
                    "Id": 106,
                    "hunk size": 15,
                    "hunk": "@@ -566,6 +566,19 @@ class FxGraphCachePickler(pickle.Pickler):\n         return \"\\n\".join(lines)\n \n \n+def get_code_hash(roots):\n+    contents: Dict[str, bytes] = {torch.__version__: b\"\"}\n+    for lib in pkgutil.iter_modules(roots):\n+        spec = lib.module_finder.find_spec(lib.name, None)\n+        assert spec is not None\n+        module = spec.origin\n+        assert module is not None\n+        with open(module, \"rb\") as f:\n+            contents[module] = f.read()\n+\n+    return hashlib.sha256(pickle.dumps(contents)).digest()\n+\n+\n @functools.lru_cache(None)\n def torch_key():\n     \"\"\"\n"
                },
                {
                    "Id": 107,
                    "hunk size": 18,
                    "hunk": "@@ -573,23 +586,17 @@ def torch_key():\n     \"\"\"\n     if not config.is_fbcode():\n         inductor_root = os.path.dirname(__file__)\n-\n-        contents: Dict[str, bytes] = {torch.__version__: b\"\"}\n-        for lib in pkgutil.iter_modules([inductor_root]):\n-            spec = lib.module_finder.find_spec(lib.name, None)\n-            assert spec is not None\n-            module = spec.origin\n-            assert module is not None\n-            with open(module, \"rb\") as f:\n-                contents[module] = f.read()\n-\n-        return hashlib.sha256(pickle.dumps(contents)).digest()\n+        return get_code_hash([inductor_root])\n \n     from libfb.py import parutil\n \n     return parutil.get_file_contents(\"torch/src_hash.txt\").rstrip()\n \n \n+def get_inductor_root():\n+    return os.path.dirname(__file__)\n+\n+\n @dataclasses.dataclass\n class OrderedSetHolder:\n     \"\"\""
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0678742924ebdab90dd6ef1dcce64ffc61799d47",
    "message": "[MPS] Add Metal implementation of exp op (#128421)\n\nTo improve accuracy, use `precise::exp()` (and `precise::sin()`/`precise::cos()` for complex flavor)\nReuse `test_exp1` to check that accuracy of `exp` ops is sometimes closer to CPU\n\nFix bug in non-contiguous tensors handling\n\nFixes https://github.com/pytorch/pytorch/issues/84936\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128421\nApproved by: https://github.com/kulinseth\nghstack dependencies: #128373, #128375",
    "date": "2024-06-13T06:53:17+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "aten/src/ATen/native/mps/OperationUtils.mm",
            "patches": []
        },
        {
            "path": "aten/src/ATen/native/mps/UnaryConstants.h",
            "patches": []
        },
        {
            "path": "aten/src/ATen/native/mps/operations/UnaryKernel.mm",
            "patches": [
                {
                    "Id": 108,
                    "hunk size": 12,
                    "hunk": "@@ -15,14 +17,8 @@\n namespace at::native {\n static mps::MetalShaderLibrary lib(UNARY_KERNEL_TEMPLATE, 2);\n \n-TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n-  // handle erfinv ops using metal kernel\n-  // erfinv algorithm ported from aten/src/ATen/native/Math.h\n-  // https://github.com/pytorch/pytorch/blob/4154c8ea159fdaecc71ee9af820ac956193c875b/aten/src/ATen/native/Math.h#L152\n-\n-  TORCH_CHECK(self.scalar_type() != ScalarType::Double, \"MPS does not support erfinv op with scalar type: Double\");\n-\n-  Tensor inputTensor = self;\n+static void exec_unary_kernel(const Tensor& self, const Tensor& output_, const std::string& name) {\n+  Tensor inputTensor = self.contiguous();\n   Tensor outputTensor = output_;\n   bool needs_output_copy = false;\n   uint32_t length = output_.numel();\n"
                },
                {
                    "Id": 109,
                    "hunk size": 15,
                    "hunk": "@@ -31,11 +27,16 @@ TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n   }\n   using namespace mps;\n   @autoreleasepool {\n-    auto cplState = lib.getPipelineStateForFunc(\"erfinv_mps_kernel\",\n-                                                {scalarToMetalTypeString(outputTensor), scalarToMetalTypeString(self)});\n+    id<MTLComputePipelineState> cplState = nil;\n+    if (c10::isComplexType(self.scalar_type())) {\n+      auto scalarStr = self.scalar_type() == kComplexFloat ? \"float\" : \"half\";\n+      cplState = lib.getPipelineStateForFunc(name + \"_complex_kernel\", {scalarStr, scalarStr});\n+    } else {\n+      cplState = lib.getPipelineStateForFunc(name + \"_kernel\",\n+                                             {scalarToMetalTypeString(outputTensor), scalarToMetalTypeString(self)});\n+    }\n \n-    if (!self.is_contiguous()) {\n-      inputTensor = inputTensor.contiguous();\n+    if (!outputTensor.is_contiguous()) {\n       outputTensor = outputTensor.contiguous();\n       needs_output_copy = true;\n     }\n"
                },
                {
                    "Id": 110,
                    "hunk size": 17,
                    "hunk": "@@ -58,4 +59,19 @@ TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n     output_.copy_(outputTensor);\n   }\n }\n+TORCH_IMPL_FUNC(erfinv_out_mps)(const Tensor& self, const Tensor& output_) {\n+  // handle erfinv ops using metal kernel\n+  // erfinv algorithm ported from aten/src/ATen/native/Math.h\n+  // https://github.com/pytorch/pytorch/blob/4154c8ea159fdaecc71ee9af820ac956193c875b/aten/src/ATen/native/Math.h#L152\n+\n+  TORCH_CHECK(self.scalar_type() != ScalarType::Double, \"MPS does not support erfinv op with scalar type: Double\");\n+  exec_unary_kernel(self, output_, \"erfinv\");\n+}\n+\n+TORCH_IMPL_FUNC(exp_out_mps)(const Tensor& self, const Tensor& output_) {\n+  exec_unary_kernel(self, output_, \"exp\");\n+}\n+TORCH_IMPL_FUNC(tanh_out_mps)(const Tensor& self, const Tensor& output_) {\n+  exec_unary_kernel(self, output_, \"tanh\");\n+}\n } // namespace at::native\n"
                }
            ]
        },
        {
            "path": "aten/src/ATen/native/mps/operations/UnaryOps.mm",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/560213de2d8f734987e25680e72d565501ab8318",
    "message": "[export] Error on not pytree-flattened nodes (#117598)\n\nAttempts to make the input/output mismatch error better by first checking if the inputs/outputs are able to be pytree flattened into supporting types (tensors, symints, ...). So if user passes in some datastructure which does not have a pytree flatten registration, this will error with the message \"It looks like one of the inputs is with type CustomType is not supported or pytree flatten-able.... please register a pytree flatten/unflatten function using the pytree.register_pytree_node API\".\n\nThe check inside of produce_matching should now only error if something unexpected happens (dynamo accidentally adds an input or removes an output), and should be considered an internal error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117598\nApproved by: https://github.com/avikchaudhuri, https://github.com/BowenBao",
    "date": "2024-01-18T03:06:42+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_dynamo/eval_frame.py",
            "patches": []
        },
        {
            "path": "torch/_dynamo/exc.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e6f655697b904401a2685e60dbeeaa94483ce180",
    "message": "[AOTI] Fix unsupported type of output=s1 (#126797)\n\nFixes #123036\n\nIn unit test `DynamicShapesCudaWrapperCudaTests.test_scaled_dot_product_attention_cuda_dynamic_shapes_cuda_wrapper`, computed buffer buf3 is compiled to a fallback kernel `aoti_torch_cuda__scaled_dot_product_flash_attention`. It has 9 outputs whose types are `[MultiOutput, MultiOutput, None, None, s1, s1, MultiOutput, MultiOutput,MultiOutput]`. The type `s1` here is passed from [generate_output](https://github.com/pytorch/pytorch/blob/acfe237a71af609e837a34bb38048aa8acb8eb4d/torch/_inductor/ir.py#L5658).\n\nThey type check for Symbol is missing for fallback kernel output generation. This PR fixes this issue by checking `output.is_Symbol`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126797\nApproved by: https://github.com/desertfire",
    "date": "2024-05-22T02:15:43+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/codegen/cpp_wrapper_cpu.py",
            "patches": [
                {
                    "Id": 111,
                    "hunk size": 6,
                    "hunk": "@@ -1223,6 +1223,10 @@ class CppWrapperCpu(WrapperCodeGen):\n                 output_name = f\"{output_name_base}_{idx}\"\n                 self.writeline(f\"int64_t {output_name} = {output};\")\n                 output_args.append(f\"&{output_name}\")\n+            elif isinstance(output, sympy.Symbol):\n+                output_name = f\"{output_name_base}_{idx}\"\n+                self.writeline(f\"auto {output_name} = {output};\")\n+                output_args.append(f\"&{output_name}\")\n             elif output is None:\n                 output_args.append(\"nullptr\")\n             else:"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2fe672b146e776d18fcb794ca3cbb4e52d32ba8e",
    "message": "compile: ban mutations on non-compositional uses of as_strided (#122502)\n\nFixes https://github.com/pytorch/pytorch/issues/104505\n\nI was originally going to ban all usages of as_strided + mutation in functionalization. But I'm pretty sure that as_strided + mutation is fine when we are calling as_strided on a base tensor.\n\nSo in this PR I added a slightly more conservative check: if we see an as_strided + mutation, where the input to an as_strided was **another** view op, then I error loudly in functionalization and link to the github issue above (in case anyone runs into this in the real world)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122502\nApproved by: https://github.com/ezyang, https://github.com/albanD",
    "date": "2024-04-12T01:12:23+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "aten/src/ATen/FunctionalStorageImpl.cpp",
            "patches": [
                {
                    "Id": 112,
                    "hunk size": 14,
                    "hunk": "@@ -103,6 +103,18 @@ FunctionalStorageImpl::FunctionalStorageImpl(const Tensor& base)\n \n void FunctionalStorageImpl::add_update(const Tensor& updated_val, const std::vector<ViewMeta>& metas) {\n   TORCH_CHECK(!frozen_, \"cannot mutate tensors with frozen storage\");\n+\n+  if (metas.size() > 1) {\n+    for (size_t i = 1; i < metas.size(); ++i) {\n+      // Skipping this check for XLA. Would be good to add it back, but it is failing XLA CI\n+      TORCH_CHECK(updated_val.device().type() == c10::DeviceType::XLA || !metas[i].is_as_strided,\n+\"During torch.compile, encountered a mutation on a view chain of length \", metas.size(), \", where view \", i,\n+\" was an as_strided() call. as_strided() is non-compositional, and therefore is not possible to functionalize properly today,\"\n+\"so this behavior is banned in compile. As a workaround, you can either remove the mutation from the model code, or you \"\n+\"can insert a graph break right before the mutation with torch._dynamo.graph_break(). If you would like this behavior to \"\n+\"work properly, please comment on https://github.com/pytorch/pytorch/issues/104505.\");\n+    }\n+  }\n   updates_.push_back({updated_val, metas});\n   generation_++;\n }\n"
                }
            ]
        },
        {
            "path": "aten/src/ATen/FunctionalStorageImpl.h",
            "patches": []
        },
        {
            "path": "torch/csrc/autograd/FunctionsManual.cpp",
            "patches": []
        },
        {
            "path": "torchgen/gen_functionalization_type.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0156eeb56488dc203180bd19645f3c26e5a50f42",
    "message": "[dynamo] bugfix - make module setattr more restrictive (#107828)\n\nA check got missed in https://github.com/pytorch/pytorch/pull/106092\n\nFixes https://github.com/pytorch/pytorch/issues/107721\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107828\nApproved by: https://github.com/eellison",
    "date": "2023-08-24T16:00:29+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8c38d0cd648a8cef9518591f3b4dc257104a5fa8",
    "message": "[inductor] Fix edge case in JIT vs. AOT fusion after finalizing MultiTemplateBuffer (#126622)\n\n# Context\nHere's a peripheral scenario causing the JIT-pass and AOT-pass to pick different fusions.\n```py\n# JIT -- buf3 is a MultiTemplateBuffer\nV.graph.buffers = [buf0, buf1, buf2, buf3, buf4]\n                                ^          ^\n# JIT pass calls finalize_multi_template_buffers()\nV.graph.buffers = [buf0, buf1, buf2, buf4, *buf3*]\n\n# AOT, note proximity_score(buf2, buf4) is \"better\" for fusion than JIT\nV.graph.buffers = [buf0, buf1, buf2, buf4, *buf3*]\n                                ^    ^\n```\n\nIt happens like this:\n* JIT starts with the original set nodes using V.graph.buffers\n* In JIT, finalize_multi_template_buffers() is called which can change the order of the buffers.\n* This makes the order of buffers/scheduler nodes different.\n* Now, each node's min/max-order is different than before.\n* As a result, the proximity between two nodes is different. https://github.com/pytorch/pytorch/blob/ad67553c5c1672d65b810acd7a6a01e11695098b/torch/_inductor/scheduler.py#L2316-L2335\n\n# Error\n```\n$ TORCH_LOGS=\"+fusion\" python test/inductor/test_max_autotune.py -k test_jit_fusion_matches_aot_fusion\n======================================================================\nFAIL: test_jit_fusion_matches_aot_fusion (__main__.TestMaxAutotune)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  ...\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/graph.py\", line 1718, in compile_to_fn\n    code, linemap = self.codegen_with_cpp_wrapper()\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/graph.py\", line 1618, in codegen_with_cpp_wrapper\n    return self.codegen()\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/graph.py\", line 1636, in codegen\n    self.scheduler.codegen()\n  File \"/data/users/colinpeppler/pytorch/torch/_dynamo/utils.py\", line 210, in time_wrapper\n    r = func(*args, **kwargs)\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/scheduler.py\", line 2602, in codegen\n    self.get_backend(device).codegen_node(node)  # type: ignore[possibly-undefined]\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/codegen/cuda_combined_scheduling.py\", line 66, in codegen_node\n    return self._triton_scheduling.codegen_node(node)\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/codegen/triton.py\", line 3377, in codegen_node\n    return self.codegen_node_schedule(node_schedule, buf_accesses, numel, rnumel)\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/codegen/triton.py\", line 3602, in codegen_node_schedule\n    final_kernel.call_kernel(final_kernel.kernel_name)\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/codegen/triton.py\", line 3055, in call_kernel\n    grid = wrapper.generate_default_grid(name, grid)\n  File \"/data/users/colinpeppler/pytorch/torch/_inductor/codegen/cpp_wrapper_cuda.py\", line 174, in generate_default_grid\n    params is not None\nAssertionError: cuda kernel parameters for triton_poi_fused_add_0 should already exist at this moment, only found dict_keys(['Placeholder.DESCRIPTIVE_NAME', 'triton_poi_fused_add_mul_0', 'triton_poi_fused_pow_1'])\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126622\nApproved by: https://github.com/chenyang78\nghstack dependencies: #125982",
    "date": "2024-05-20T16:58:08+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/scheduler.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d72d99e5917a3d26e304dfb22d7cc86c99466528",
    "message": "Fix sparse compressed tensor invariants checks when nnz==0 (#115826)\n\nFixes https://github.com/pytorch/pytorch/issues/115755\n\nThis PR is a step toward deprecating `torch.empty(..., layout=<sparse compressed tensor layout>)` that usage should be minimized as it will produce invalid tensors, see also https://github.com/pytorch/pytorch/issues/90695 .\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115826\nApproved by: https://github.com/cpuhrsch, https://github.com/amjames",
    "date": "2023-12-20T12:16:07+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/f422027fceb71b1f2f7d940f3ade32ced2ed0ba0",
    "message": "fix torch.linalg.lstsq input check (#130612)\n\nFixes [#117236 ](https://github.com/pytorch/pytorch/issues/117236)\nThe current case does not meet the vector scenario requirements, and it lacks sufficient checks (relying solely on ```dim_diff``` is insufficient).  Consequently, it triggers an internal assertion error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130612\nApproved by: https://github.com/lezcano",
    "date": "2024-07-12T23:06:52+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "aten/src/ATen/native/BatchLinearAlgebra.cpp",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/f1aef2c0941139fcdc8d59208718127959f6a706",
    "message": "Don't check is_conj for `_refs.linalg.svd` (#117972)\n\nThe flag is not correctly set when PyTorch is compiled with GPU support resulting in failures in\n`test_ops.py::test_python_ref_meta__refs_linalg_svd_cpu_complex`.\n\nUse a similar approach to test_meta and skip the check for this function.\n\nWorkaround for #105068\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117972\nApproved by: https://github.com/lezcano",
    "date": "2024-01-26T15:24:29+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_prims_common/__init__.py",
            "patches": [
                {
                    "Id": 113,
                    "hunk size": 11,
                    "hunk": "@@ -171,10 +172,11 @@ def compare_tensor_meta(\n             msg = f\"Storage offset mismatch! Storage offsets are {a.storage_offset()} and {b.storage_offset()}!\"\n             raise RuntimeError(msg)\n \n-    if a.is_conj() != b.is_conj():\n-        raise RuntimeError(\n-            f\"Conj mismatch! is_conj is set to {a.is_conj()} and {b.is_conj()}\"\n-        )\n+    if check_conj:\n+        if a.is_conj() != b.is_conj():\n+            raise RuntimeError(\n+                f\"Conj mismatch! is_conj is set to {a.is_conj()} and {b.is_conj()}\"\n+            )\n \n     if a.is_neg() != b.is_neg():\n         raise RuntimeError("
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/097defb1608827d82b18b27adeec0a98b72a9281",
    "message": "[device mesh] only check when world size > num_devices per host (#111091)\n\nas titled\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111091\nApproved by: https://github.com/awgu, https://github.com/wz337\nghstack dependencies: #110898, #110900",
    "date": "2023-10-12T03:37:18+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/a7cfe40c9bb22eef588b1916a7f7e8b132919c8d",
    "message": "[dtensor] Improve from_local API with run_check (#130289)\n\nas titled, this PR:\n1. switch `run_check` to be by default False and add extra doc/comments\n   about the correctness guarantee. Since I observed so many calls\nforget to use run_check=False, we should simply switch to not perform\nmetadata check and make our documentation explicit\n2. Implement metadata check by picking up the changes from https://github.com/pytorch/pytorch/pull/115229\n3. Improve the from_local documentation\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130289\nApproved by: https://github.com/awgu, https://github.com/wz337\nghstack dependencies: #130286, #130287, #130288",
    "date": "2024-07-15T18:52:55+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/distributed/_tensor/_collective_utils.py",
            "patches": []
        },
        {
            "path": "torch/distributed/_tensor/api.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/38827141681397564ff4ef0163dc1f0d17cd0708",
    "message": "Fix check-labels.yml for ghstack PRs (#117680)\n\nOtherwise check-labels doesn't run on ghstack PRs, see https://github.com/pytorch/pytorch/pull/117609 for example: no Check Labels workflow run.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117680\nApproved by: https://github.com/izaitsevfb",
    "date": "2024-01-18T01:33:55+00:00",
    "label": "YES",
    "changes": [
        {
            "path": ".github/workflows/check-labels.yml",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2f219f7d79357a82c1caf2d4a8df540e97b56bce",
    "message": "Enforce unused-{variable/function} checks to all torch targets (#130189)\n\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130189\nApproved by: https://github.com/ezyang",
    "date": "2024-07-06T16:03:01+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "aten/src/ATen/native/quantized/cudnn/utils.h",
            "patches": []
        },
        {
            "path": "cmake/public/utils.cmake",
            "patches": [
                {
                    "Id": 114,
                    "hunk size": 8,
                    "hunk": "@@ -405,12 +405,6 @@ endmacro()\n #   torch_compile_options(lib_name)\n function(torch_compile_options libname)\n   set_property(TARGET ${libname} PROPERTY CXX_STANDARD 17)\n-  set(private_compile_options \"\")\n-\n-  # ---[ Check if warnings should be errors.\n-  if(WERROR)\n-    list(APPEND private_compile_options -Werror)\n-  endif()\n \n   # until they can be unified, keep these lists synced with setup.py\n   if(MSVC)\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/5c9d5272e43573269427cae5e38307faac3266f6",
    "message": "fixes #124582 (#128483)\n\nadded check for existence of outputs requiring grad to make_graphed_callables.\n\nadded new test case, updated existing test case to include parameterless modules.\n\nFixes #124582\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128483\nApproved by: https://github.com/eqy, https://github.com/ezyang",
    "date": "2024-07-02T08:45:59+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/cuda/graphs.py",
            "patches": [
                {
                    "Id": 115,
                    "hunk size": 4,
                    "hunk": "@@ -377,7 +387,7 @@ def make_graphed_callables(\n         static_grad_inputs = []\n         grad_idx = 0\n         for arg in static_input_surface:\n-            if arg.requires_grad:\n+            if arg.requires_grad and grad_inputs is not None:\n                 static_grad_inputs.append(grad_inputs[grad_idx])\n                 grad_idx += 1\n             else:"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/90ef3b82d1ea823add8f49ea58bd0df057dd00e1",
    "message": "[DeviceMesh] Add unique mesh_dim_name check in init_device_mesh() (#108326)\n\nEach mesh_dim_name in mesh_dim_names need to be unique. This PR adds check when calling init_device_mesh().\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108326\nApproved by: https://github.com/wanchaol",
    "date": "2023-09-01T02:14:18+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/51fa0bd436cf627bd0c8ccf3a3a8b9c07d260622",
    "message": "[pt2-bench] pass acc test if ref is NaN (#129996)\n\nI'm debugging the accuracy failure for training vision_maskrcnn.\n\nUnfortunately I could not succeed to run it locally (I've check pined commits for torchbenchmars/torchvision are correct, and reinstalled torchbenchmark for mask_rcnn). I get this error:\n```\neager run fail: AssertionError: targets should not be none when in training mode\n```\n(Command: time python benchmarks/dynamo/torchbench.py --backend inductor --amp --performance --training --only vision_maskrcnn )\n\nBut look at the log from the dashboard\n```\nE0623 19:17:59.085000 140114670171328 torch/_dynamo/utils.py:1468] RMSE (res-fp64): nan, (ref-fp64): nan and shape=torch.Size([1024, 256, 1, 1]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000\n```\n\nWe can see both the reference number and the pt2 number are NaN. I change torch._dynamo.utils.same to return true if both RMSE values are NaN.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129996\nApproved by: https://github.com/jansel",
    "date": "2024-07-04T01:14:29+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_dynamo/utils.py",
            "patches": [
                {
                    "Id": 116,
                    "hunk size": 9,
                    "hunk": "@@ -1466,6 +1466,13 @@ def same(\n                     multiplier = 3.0\n \n                 passes_test = res_error <= (multiplier * ref_error + tol / 10.0)\n+                if (\n+                    not passes_test\n+                    and equal_nan\n+                    and math.isnan(ref_error)\n+                    and math.isnan(res_error)\n+                ):\n+                    passes_test = True\n                 if not passes_test:\n                     log_error(\n                         \"RMSE (res-fp64): %.5f, (ref-fp64): %.5f and shape=%s. res.dtype: %s, multiplier: %f, tol: %f\","
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/bb13fad7aa7754042efe6e9465410cf5e543a77e",
    "message": "Share TCPStore by default when using c10d rdzv handler (#128096)\n\nSummary:\nNumber of features rely on TCP store as a control plane. By default TCPStore server is started on rank0 trainer and this can create a a race condition when rank0 may exit (error and graceful exit) and any other ranks reading/writing will fail.\n\nSolution: TCPStore server should outlive all the trainer processes. By moving the ownership TCPStore to torchelastic agent it naturally fixes the lifecycle of the server.\n\nStatic rendezvous in torchelastic does already support sharing of the TCPStore server. We are extending this to more commonly used c10d rendezvous handler.\n\nAny handler would like to manage tcp store has to:\n- Return true on `use_agent_store` property\n- `RendezvousInfo`.`RendezvousStoreInfo`#[`master_addr/master_port`] values refer to managed TCPStore (those are returned on `next_rendezvous` call)\n\nNote: in some instances users may want to use non-TCPStore based stores for the torchelastic rendezvous process, so the handler will need to create and hold a reference to TCPStore (as done in this change)\n\nTest Plan:\n`cat ~/workspace/dist-demo/stores.py`\n~~~\nimport torch\nimport logging\nimport sys\nimport torch.distributed as dist\nimport torch\n\nimport os\nimport time\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.StreamHandler(sys.stderr))\nlogger.setLevel(logging.INFO)\n\ndef _run_test(store):\n\n    if dist.get_rank() == 1:\n        logger.info(\"Rank %s is sleeping\", dist.get_rank())\n        time.sleep(5)\n        key = \"lookup_key\"\n        logger.info(\"Checking key %s in store on rank %s\", key, dist.get_rank())\n        store.check([key])\n    else:\n        logger.info(\"rank %s done\", dist.get_rank())\n\ndef main() -> None:\n    use_gpu = torch.cuda.is_available()\n    dist.init_process_group(backend=\"nccl\" if use_gpu else \"gloo\")\n    dist.barrier()\n\n    logger.info(f\"Hello World from rank {dist.get_rank()}\")\n\n    host = os.environ['MASTER_ADDR']\n    port = os.environ['MASTER_PORT']\n    world_size = os.environ['WORLD_SIZE']\n\n    logger.info(\"testing TCPStore\")\n    store = dist.TCPStore(\n        host_name=host, port=int(port), world_size=int(world_size),\n    )\n    _run_test(store)\n\nif __name__ == \"__main__\":\n    main()\n~~~\n\nWith the fix (TORCH_DISABLE_SHARE_RDZV_TCP_STORE=0 or just drop the option)\n~~~\n(pytorch_38) [kurman@devgpu011.cln5 ~/local/pytorch (main)]$ TORCH_DISABLE_SHARE_RDZV_TCP_STORE=0 python -m torch.distributed.run --rdzv-backend c10d --nproc-per-node 3 ~/workspace/dist-demo/stores.py\nmaster_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\nWARNING:__main__:\n*****************************************\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n*****************************************\nHello World from rank 1\nHello World from rank 2\nHello World from rank 0\ntesting TCPStore\ntesting TCPStore\ntesting TCPStore\nrank 2 done\nRank 1 is sleeping\nrank 0 done\nChecking key lookup_key in store on rank 1\n~~~\n\nTORCH_DISABLE_SHARE_RDZV_TCP_STORE=1\n~~~\n(pytorch_38) [kurman@devgpu011.cln5 ~/local/pytorch (main)]$ TORCH_DISABLE_SHARE_RDZV_TCP_STORE=1 python -m torch.distributed.run --rdzv-backend c10d --npro\nc-per-node 3 ~/workspace/dist-demo/stores.py\nmaster_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\nWARNING:__main__:\n*****************************************\n\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n*****************************************\nHello World from rank 0\nHello World from rank 2\nHello World from rank 1\ntesting TCPStore\ntesting TCPStore\ntesting TCPStore\nrank 0 done\nrank 2 done\nRank 1 is sleeping\nChecking key lookup_key in store on rank 1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/kurman/workspace/dist-demo/stores.py\", line 46, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/kurman/workspace/dist-demo/stores.py\", line 42, in main\n[rank1]:     _run_test(store)\n[rank1]:   File \"/home/kurman/workspace/dist-demo/stores.py\", line 22, in _run_test\n[rank1]:     store.check([key])\n[rank1]: torch.distributed.DistNetworkError: Connection reset by peer\nE0605 17:40:22.853277 140249136719680 torch/distributed/elastic/multiprocessing/api.py:832] failed (exitcode: 1) local_rank: 1 (pid: 2279237) of binary: /home/kurman/.conda/envs/pytorch_38/bin/python\nTraceback (most recent call last):\n  File \"/home/kurman/.conda/envs/pytorch_38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/kurman/.conda/envs/pytorch_38/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/data/users/kurman/pytorch/torch/distributed/run.py\", line 904, in <module>\n    main()\n  File \"/data/users/kurman/pytorch/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n    return f(*args, **kwargs)\n  File \"/data/users/kurman/pytorch/torch/distributed/run.py\", line 900, in main\n    run(args)\n  File \"/data/users/kurman/pytorch/torch/distributed/run.py\", line 891, in run\n    elastic_launch(\n  File \"/data/users/kurman/pytorch/torch/distributed/launcher/api.py\", line 132, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/data/users/kurman/pytorch/torch/distributed/launcher/api.py\", line 263, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n============================================================\n/home/kurman/workspace/dist-demo/stores.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-06-05_17:40:22\n  host      : devgpu011.cln5.facebook.com\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 2279237)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n~~~\n\nDifferential Revision: D58180193\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128096\nApproved by: https://github.com/shuqiangzhang",
    "date": "2024-06-12T21:49:42+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py",
            "patches": []
        },
        {
            "path": "torch/distributed/elastic/rendezvous/dynamic_rendezvous.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/5caf2e55d4c59d3562d5e3e91c22698451edcb80",
    "message": "[FSDP] fix: fix for fsdp zero2 validation error (#110139)\n\n# Problem\nWhen sharding_strategy is set to SHARD_GRAD_OP and forward_prefetch is turned on, the validation after the train has an incorrect weight shape.\n<img width=\"1508\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/41232043/57a9c3bb-cb5c-46df-ac26-922740686f9e\">\n\n# Analyze\nWhen using `SHARD_GRAD_OP`, the `free_unsharded_flat_param` in `_post_forward_reshard` is often False, so it does not set the handle's `_prefetched` flag to False after the forward.\n\nThe normal train phase sets this flag to False in the `_post_backward_final_callback`, and the validation phase doesn't execute the hook, so after the first iter of the validation is done, the flag of the handle of the prefetched will remain True.\n\nThis will cause the handle to skip the `_unshard` in the next `_pre_forward_unshard`, and the `_prefetch_handle` will not do a prefetch, which will result in an incorrect weight shape.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110139\nApproved by: https://github.com/awgu",
    "date": "2023-10-14T20:59:28+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/b5e83b8c50bb883c1749c2edf8add5bb37e284c4",
    "message": "Fix edge case for size 1 channels dim in AdaptiveMaxPool (#116482)\n\nFixes https://github.com/pytorch/pytorch/issues/107842\n\nUnlike `AdaptiveAvgPool`, `AdaptiveMaxPool` does not have a CUDA kernel for ChannelsLast. We workaround this by calling `contiguous()` on the input. However, there is an edge case when the channels dimension has size 1.\n\n```python\n>>> t = torch.randn(2, 1, 3, 3)\n>>> t.stride()\n(9, 9, 3, 1)\n>>> t_c =  t.to(memory_format=torch.channels_last)\n>>> t_c.stride()\n(9, 1, 3, 1)  # (CHW, 1, CW, C)\n>>> t_c.is_contiguous()\nTrue  # contiguity check doesn't check strides for singleton dimensions\n```\n\nSince the CUDA kernel treats the batch,`B`, and  channels,`C`, dimensions as implicitly flattened and increments the data pointer for `input` to the start of the next plane using\n\nhttps://github.com/pytorch/pytorch/blob/669b182d33f5e1368d8de6d86b891f65480c9b22/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu#L67\n\nIf our input falls into the aforementioned edge case, the `data_ptr` will not be incremented correctly. The simple fix for this is to calculate the stride for the channels dimension using $\\prod_{i > 1}size(i)$\n\nAnalogous fix for the 3D case.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116482\nApproved by: https://github.com/albanD",
    "date": "2023-12-28T15:02:29+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/ae2c219de2fd032036aa1d2a04101f1c23fd5bbe",
    "message": "Revert \"[BE] Remove stale CUDA version check from cpp_extension.py (#113447)\"\n\nThis reverts commit 7ccca60927cdccde63d6a1d40480950f24e9877a.\n\nReverted https://github.com/pytorch/pytorch/pull/113447 on behalf of https://github.com/malfet due to Broke ROCM ([comment](https://github.com/pytorch/pytorch/pull/113447#issuecomment-1806407892))",
    "date": "2023-11-10T20:46:13+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/04b04c068659127a53d659c44b0dd75fa9fd5887",
    "message": "[Compiled Autograd] Turn accumulate_grad into an op (#111271)\n\nRather than baking the behavior of `AccumulateGrad` nodes into the generated graph (either as `+=`, or as a return value of the graph).  This creates a new `accumulate_grad_` dispatcher op that is included in the generated graph like:\n```\ndef forward(self, inputs, sizes, hooks):\n    getitem = inputs[0]\n    getitem_1 = inputs[1]\n    getitem_2 = inputs[2]\n    getitem_3 = inputs[3]\n    getitem_4 = inputs[4]\n    getitem_5 = inputs[5]\n    getitem_6 = inputs[6]\n    getitem_7 = inputs[7]\n    getitem_8 = inputs[8]\n    getitem_9 = inputs[9];  inputs = None\n    expand = torch.ops.aten.expand.default(getitem, [2, 4]);  getitem = None\n    threshold_backward = torch.ops.aten.threshold_backward.default(expand, getitem_1, 0);  expand = getitem_1 = None\n    t = torch.ops.aten.t.default(getitem_3);  getitem_3 = None\n    mm = torch.ops.aten.mm.default(threshold_backward, t);  t = None\n    t_1 = torch.ops.aten.t.default(threshold_backward)\n    mm_1 = torch.ops.aten.mm.default(t_1, getitem_2);  t_1 = getitem_2 = None\n    t_2 = torch.ops.aten.t.default(mm_1);  mm_1 = None\n    sum_1 = torch.ops.aten.sum.dim_IntList(threshold_backward, [0], True);  threshold_backward = None\n    view = torch.ops.aten.view.default(sum_1, [4]);  sum_1 = None\n    t_3 = torch.ops.aten.t.default(t_2);  t_2 = None\n    accumulate_grad_ = torch.ops.inductor.accumulate_grad_.default(getitem_4, t_3);  getitem_4 = t_3 = None\n    threshold_backward_1 = torch.ops.aten.threshold_backward.default(mm, getitem_5, 0);  mm = getitem_5 = None\n    t_4 = torch.ops.aten.t.default(threshold_backward_1)\n    mm_2 = torch.ops.aten.mm.default(t_4, getitem_6);  t_4 = getitem_6 = None\n    t_5 = torch.ops.aten.t.default(mm_2);  mm_2 = None\n    sum_2 = torch.ops.aten.sum.dim_IntList(threshold_backward_1, [0], True);  threshold_backward_1 = None\n    view_1 = torch.ops.aten.view.default(sum_2, [4]);  sum_2 = None\n    t_6 = torch.ops.aten.t.default(t_5);  t_5 = None\n    accumulate_grad__1 = torch.ops.inductor.accumulate_grad_.default(getitem_7, t_6);  getitem_7 = t_6 = None\n    accumulate_grad__2 = torch.ops.inductor.accumulate_grad_.default(getitem_8, view_1);  getitem_8 = view_1 = None\n    accumulate_grad__3 = torch.ops.inductor.accumulate_grad_.default(getitem_9, view);  getitem_9 = view = None\n    return []\n\n```\n\nThe motivation here is `AccumulateGrad` nodes are causing trouble in FSDP tracing, since FSDP is in-place resizing parameters and parameter storage in hooks.  We will model this mutation in dynamo, but not during the initial compiled autograd capture.  This allows us to bypass failing shape checks in the initial capture.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111271\nApproved by: https://github.com/voznesenskym",
    "date": "2023-10-16T21:16:17+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/272cf29e4d0c9de3fb1631f766e8bb55d65944d6",
    "message": "[FSDP2][BE] Refactored `check_1d_sharded_parity` to use mesh (#121357)\n\nEventually, we should just have one unified way to check for parity between a `DTensor`-sharded model and a replicated model. This PR is a small refactor to work toward that. One current gap to use this `check_sharded_parity` function for 2D is that FSDP's `(Shard(0), Shard(0))` layout differs from that of the `DTensor` APIs since FSDP shards on dim-0 after TP shards on dim-0.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121357\nApproved by: https://github.com/weifengpy\nghstack dependencies: #121360",
    "date": "2024-03-11T22:34:42+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/d2215f14ba1cd45c79f778a804a6125f140461b3",
    "message": "Fix: transactional translation validation insertion. (#107523)\n\nThis PR fixes transactional behavior of translation validation insertion.\n\nPreviously, this transactional behavior was implemented by removing the FX node if any\nissues occurred until the end of `evaluate_expr`. However, since we cache FX nodes, we\nmight end up removing something that wasn't inserted in the same function call.\n\n**Solution:** when creating an FX node for `call_function`, we also return whether this is\na fresh FX node or not. Then, we can appropriately handle each case.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107523\nApproved by: https://github.com/ezyang",
    "date": "2023-08-22T12:38:05+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/0f887a6d1a62449c92ad22b7659c471797ed3762",
    "message": "limit fused kernel num args. (#113131)\n\nFixes #97361\n\nWhen fused kernel more than 1024 parameters, it should throw error from ctypes.\nLimit args number is should be a mechanism to protect stack memory. As we known, CPP is passing args via stack memory, and stack memory has size limitation.\n\nCode change:\n\n1. cpp backend will check the fused nodes' args number, if it is reach the limitation. It will status flush status to ready.\n2. scheduler will check `ready_to_flush` API and help backend flush codegen.\n3. Add `ready_to_flush` API to `BaseScheduling`, Triton backend will return False due to not support it yet.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113131\nApproved by: https://github.com/jgong5, https://github.com/mlazos",
    "date": "2023-11-22T18:05:33+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/44b98c09ca90ce4529d1a9792b01a0e8afe5345e",
    "message": "[BE] migrate all assertRaises tests to OptimizerInfo test_errors (#116315)\n\nRemoves a part of the sparse adam test and the following three tests: `test_fused_optimizer_raises`, `test_duplicate_params_across_param_groups`, `test_duplicate_params_in_one_param_group`\n\n```\n(pytorch-3.10) [janeyx@devgpu023.odn1 ~/local/pytorch (d2d129de)]$ python test/test_optim.py -k test_fused_optimizer_raises -k test_duplicate_params_across_param_groups -k test_duplicate_params_in_one_param_group\n/home/janeyx/.conda/envs/pytorch-3.10/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n...\n----------------------------------------------------------------------\nRan 3 tests in 0.023s\n\nOK\n```\n\nIncreases coverage by testing the duplicate param tests on ALL the optims instead of just one each. Also fixes SparseAdam bug which was accidentally calling torch.unbind through list instead of putting params in a list. This bug was caught by migrating the weird warning stuff to just one easy warning context manager, which checks that nothing else gets raised.\n\nThe new test_errors does not run slower than before, overhead is still king:\n```\n(pytorch-3.10) [janeyx@devgpu023.odn1 ~/local/pytorch (d2d129de)]$ python test/test_optim.py -k test_errors\n/home/janeyx/.conda/envs/pytorch-3.10/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n..........................\n----------------------------------------------------------------------\nRan 26 tests in 10.337s\n\nOK\n```\n\nCompared to test_errors BEFORE my commit :p\n```\n(pytorch-3.10) [janeyx@devgpu023.odn1 ~/local/pytorch (b47aa696)]$ python test/test_optim.py -k test_errors\n/home/janeyx/.conda/envs/pytorch-3.10/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n.............sssssssssssss\n----------------------------------------------------------------------\nRan 26 tests in 11.980s\n\nOK (skipped=13)\n(pytorch-3.10) [janeyx@devgpu023.odn1 ~/local/pytorch (b47aa696)]$\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116315\nApproved by: https://github.com/mikaylagawarecki",
    "date": "2023-12-27T00:08:31+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/29fa6fbc4eda6c02ecdfd73b74a8702187c4fc44",
    "message": "[Dynamo] Fix a corner case of reinplace_inplaceable_ops pass for triton kernels (#117612)\n\nSummary:\nWe saw the following failure when compiling custom triton kernels:\n```\nRuntimeError: Argument 'getitem_22' of Node 'triton_kernel_wrapper_functional_proxy_3' was used before it has been defined! Please check that Nodes in the graph are topologically ordered\n```\nThe root-cause is when doing the replacement, the replacement is replaced by another replacement. The fix will keep finding the replacement until it is not replaced\n\nTest Plan:\n\nAdded a test case\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117612\nApproved by: https://github.com/aakhundov",
    "date": "2024-01-17T18:41:42+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_inductor/fx_passes/post_grad.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/de4d48df34c0e8bcd03d96bcdfd12e263e9889ff",
    "message": "[c10d] Fix timeout dump path write path overlap when there are multiple PGs (#116218)\n\nBasically we observed that if there are multiple PGs and if the timeout happens on one of the subPG, we somehow use the local rank in the dump file. We realize that:\n1. For setting the timeout signal in the store, any watchdog thread from any PG can do that.\n2. For checking and dump, only the watchdog thread of default PG which we will always create and contain all ranks (no file name conflict) is needed here because the store signal and dump debug info are all global.\n3. Since dump is global, we want to avoid the case when ranks from sub-PG pollute logs from global ranks (local rank 0 vs global rank 0). So that we use global ranks here to initialize debug info writer. (Down the road, we are thinking about making it a singleton so that user only register it once for multi-PG case.)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116218\nApproved by: https://github.com/wconstab",
    "date": "2023-12-29T21:58:25+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8b00e5aa12cfbc29909a8697180621e7cd5d92d3",
    "message": "[FSDP2] Added pre/post-backward (#118004)\n\nThis PR adds the pre- and post-backward logic:\n- **Pre-backward hook:** `FSDPState` and `FSDPParamGroup` define this, and `FSDPState` is responsible for registering since its pre-backward should run even if the `FSDPState` does not manage any parameters (in case it is the root).\n- **Post-backward hook:** Only `FSDParamGroup` defines this since the post-backward hook reshards parameters and reduce-scatters gradients (functionality only needed with managed parameters). The `FSDPParamGroup` is responsible for registering this.\n- **Post-backward final callback:** `FSDPState` defines this, and each `FSDPParamGroup` defines a `finalize_backward()` to call in the final callback.\n\n### Pre-Backward\n\nThe pre-backward hook is registered on the module outputs (that require gradient), and it should run when the first such output has its gradient computed. The hook may run multiple times per backward, once per module forward. Specifically, there will be one `(pre-backward, post-backward)` interval for each of the module's `forward()` calls. This is contrast with the existing FSDP semantics, which only defines a single `(pre-backward, post-backward)` interval that is equivalent to the union of this FSDP's `(pre-backward, post-backward)` intervals. This avoids spiking memory from having multiple modules not resharding and avoids some autograd edge cases.\n\nWe implement the pre-backward hook by having a flag that is set upon the 1st calls to disable subsequent calls. This flag could be maintained by FSDP, but for a cleaner design, we augment `register_multi_grad_hook` with a `mode=\"any\"` option and use that instead.\n\n### Post-Backward\n\nThe post-backward hook is equivalent to a module full backward hook (`nn.Module.register_full_backward_hook`) except it adds pytree logic to work with data structures other than just flat `Tensor` args passed to `nn.Module.forward`. If we were to use `register_full_backward_hook`, then the hook could fire early (before all gradients for the module have been computed). Most internal models use custom data structures as `forward` inputs, and they find that unifying under pytree is an acceptable solution.\n\nUnlike existing FSDP, we are able to reshard the parameters in the post-backward hook _before_ 'concatenating' the autograd-computed gradients, achieving a lower peak memory usage. (Existing FSDP has `SplitWithSizesBackward` that calls a `CatArrayBatched`, and here we have the reduce-scatter copy-in.)\n\n### Final Callback\nThe final callback runs as a queued callback to the autograd engine, meaning that it runs at the end of backward.\n\nIn the future, if we do not want to wait for the reduce-scatter (or similar for CPU offloading), we can augment the final callback. The code is written such that each reduce-scatter can be waited on separately (via CUDA event).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118004\nApproved by: https://github.com/weifengpy, https://github.com/wanchaol\nghstack dependencies: #117950, #117955, #117973, #117975",
    "date": "2024-02-02T19:10:11+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/distributed/_composable/fsdp/_fsdp_collectives.py",
            "patches": []
        },
        {
            "path": "torch/distributed/_composable/fsdp/_fsdp_param.py",
            "patches": [
                {
                    "Id": 117,
                    "hunk size": 20,
                    "hunk": "@@ -286,6 +287,24 @@ class FSDPParam:\n             return self._sharded_param_data\n         return torch.empty(0)  # mypy\n \n+    @property\n+    def unsharded_param(self) -> nn.Parameter:  # ND\n+        self._assert_in_states(ShardedState.UNSHARDED)\n+        return self._unsharded_param\n+\n+    @property\n+    def unsharded_grad_data(self) -> torch.Tensor:\n+        grad = self.unsharded_param.grad\n+        assert grad is not None, \"Expects unsharded_param.grad to not be None\"\n+        return self._get_grad_inner_tensor(grad)\n+\n+    def _get_grad_inner_tensor(self, grad: torch.Tensor) -> torch.Tensor:\n+        if self.is_dtensor:\n+            if isinstance(grad, AsyncCollectiveTensor):\n+                grad = grad.wait()\n+            grad = cast(DTensor, grad)._local_tensor\n+        return grad\n+\n     def _assert_in_states(self, *states: ShardedState) -> None:\n         if self.sharded_state not in states:\n             _raise_assert_with_print(\n"
                }
            ]
        },
        {
            "path": "torch/distributed/_composable/fsdp/_fsdp_param_group.py",
            "patches": [
                {
                    "Id": 118,
                    "hunk size": 6,
                    "hunk": "@@ -112,9 +159,9 @@ class FSDPParamGroup:\n         if not self._all_gather_result:\n             return  # no preceding unshard\n         if self._training_state == TrainingState.FORWARD:  # implicit prefetch\n-            if prev_all_gather_state := self.all_gather_state.pop():\n+            if prev_all_gather_state := self.comm_ctx.all_gather_state:\n                 self._wait_all_gather_streams_on_event(prev_all_gather_state.event)\n-                del prev_all_gather_state  # free\n+                self.comm_ctx.all_gather_state = None  # free the all-gather result\n         foreach_all_gather_copy_out(\n             self._all_gather_result, self.fsdp_params, self._all_gather_process_group\n         )\n"
                }
            ]
        },
        {
            "path": "torch/distributed/_composable/fsdp/_fsdp_state.py",
            "patches": [
                {
                    "Id": 119,
                    "hunk size": 9,
                    "hunk": "@@ -52,9 +61,14 @@ class FSDPState(_State):\n         self, module: nn.Module, args: Tuple[Any, ...], kwargs: Dict[str, Any]\n     ) -> Tuple[Tuple[Any, ...], Dict[str, Any]]:\n         self._lazy_init()\n-        if not self._is_root:\n+        if self._state_ctx.iter_forward_root is not None:\n             return args, kwargs\n+        self._state_ctx.iter_forward_root = self\n         with torch.profiler.record_function(\"FSDP::root_pre_forward\"):\n+            # Wait for optimizer before implicitly prefetched all-gathers\n+            current_stream = torch.cuda.current_stream()\n+            self._comm_ctx.all_gather_copy_in_stream.wait_stream(current_stream)\n+            self._comm_ctx.all_gather_stream.wait_stream(current_stream)\n             if self._device.type == \"cuda\":\n                 with torch.profiler.record_function(\"FSDP::inputs_to_device\"):\n                     args_tuple, kwargs_tuple = _to_kwargs(\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/bc34f02c38485ff6b5d1255012c1dd2b8c59810d",
    "message": "[BE][Easy]: Apply RUF019: remove duplicate checks for dict access (#114478)\n\nApplies RUF019 nightly preview rule to the codebase\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114478\nApproved by: https://github.com/mikaylagawarecki",
    "date": "2023-11-29T00:14:02+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/1bb1e3463c95c75a8555fd4c796dad8a2762d667",
    "message": "Fix allowlisting of builtins for weights_only unpickler (#129244)\n\nSince we use [`DEFAULT_PROTOCOL=2`](https://github.com/pytorch/pytorch/blob/main/torch/serialization.py#L62), some functions/classes that were renamed from python 2-->3 will be pickled with their python2 name. This PR ensures that when a mod `GLOBAL <python2_mod>.<python2_name> ` is encountered, [following the strategy used by pickle](https://github.com/python/cpython/blob/main/Lib/pickle.py#L1590C13-L1593C63) it is properly mapped to `<python3_mod>.<python3_name>`.\n\nThis fix ensures that `add_safe_globals` works properly for such functions/classes (i.e. users will allowlist the python3 func and the weights_only unpickler will do the appropriate translation when checking whether a class was allowlisted).\n\nAn example is as follows:\n`__builtin__` was named to `builtins`, see the [release notes for Python 3.0](https://docs.python.org/3/whatsnew/3.0.html)\n\n> Renamed module `__builtin__` to [`builtins`](https://docs.python.org/3/library/builtins.html#module-builtins) (removing the underscores, adding an \u2018s\u2019). The __builtins__ variable found in most global namespaces is unchanged. To modify a builtin, you should use [builtins](https://docs.python.org/3/library/builtins.html#module-builtins), not `__builtins__`!\n\nHowever, since we use [`DEFAULT_PROTOCOL=2`](https://github.com/pytorch/pytorch/blob/main/torch/serialization.py#L62), builtins will be pickled with their module string as `__builtin__`.\n\n```python\n>>> import pickle\n>>> import pickletools\n>>> print.__module__\n'builtins'\n>>> with open('print.pkl', 'wb') as f:\n>>>      pickle.dump(print, f, protocol=2) # 2 because this is the default protocol used by pytorch\n>>> with open('print.pkl', 'rb') as f:\n>>>     pickletools.dis(f)\n0: \\x80 PROTO      2\n2: c    GLOBAL     '__builtin__ print' # pickle saves the module string as __builtin__ !!! :(\n21: q    BINPUT     0\n23: .    STOP\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129244\nApproved by: https://github.com/albanD",
    "date": "2024-06-25T04:19:44+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "torch/_weights_only_unpickler.py",
            "patches": [
                {
                    "Id": 120,
                    "hunk size": 10,
                    "hunk": "@@ -170,12 +182,20 @@ class Unpickler:\n         self.readline = file.readline\n         self.read = file.read\n         self.memo: Dict[int, Any] = {}\n+        self.proto: int = -1\n \n     def load(self):\n         \"\"\"Read a pickled object representation from the open file.\n \n         Return the reconstituted object hierarchy specified in the file.\n         \"\"\"\n+        if not has_compat_pickle:\n+            warnings.warn(\n+                \"Could not import IMPORT_MAPPING and NAME_MAPPING from _compat_pickle. \"\n+                \"If the default `pickle_protocol` was used at `torch.save` time, any functions or \"\n+                \"classes that are in these maps might not behave correctly if allowlisted via \"\n+                \"`torch.serialization.add_safe_globals()`.\"\n+            )\n         self.metastack = []\n         self.stack: List[Any] = []\n         self.append = self.stack.append\n"
                },
                {
                    "Id": 121,
                    "hunk size": 9,
                    "hunk": "@@ -190,6 +210,13 @@ class Unpickler:\n             if key[0] == GLOBAL[0]:\n                 module = readline()[:-1].decode(\"utf-8\")\n                 name = readline()[:-1].decode(\"utf-8\")\n+                # Patch since torch.save default protocol is 2\n+                # users will be running this code in python > 3\n+                if self.proto == 2 and has_compat_pickle:\n+                    if (module, name) in NAME_MAPPING:\n+                        module, name = NAME_MAPPING[(module, name)]\n+                    elif module in IMPORT_MAPPING:\n+                        module = IMPORT_MAPPING[module]\n                 full_path = f\"{module}.{name}\"\n                 if full_path in _get_allowed_globals():\n                     self.append(_get_allowed_globals()[full_path])\n"
                },
                {
                    "Id": 122,
                    "hunk size": 12,
                    "hunk": "@@ -334,8 +361,14 @@ class Unpickler:\n                 self.append(decode_long(data))\n             # First and last deserializer ops\n             elif key[0] == PROTO[0]:\n-                # Read and ignore proto version\n-                read(1)[0]\n+                self.proto = read(1)[0]\n+                if self.proto != 2:\n+                    warnings.warn(\n+                        f\"Detected pickle protocol {self.proto} in the checkpoint, which was \"\n+                        \"not the default pickle protocol used by `torch.load` (2). The weights_only \"\n+                        \"Unpickler might not support all instructions implemented by this protocol, \"\n+                        \"please file an issue for adding support if you encounter this.\"\n+                    )\n             elif key[0] == STOP[0]:\n                 rc = self.stack.pop()\n                 return rc"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/69cedc16c593e777cd36f064dd862c78d6291c0b",
    "message": "Add padding dimension checks and tests (#121298)\n\nFixes #121093\n\nPreviously, calling the following functions with invalid padding dimensions would cause a segmentation fault:\n```\ntorch._C._nn.replication_pad1d, torch._C._nn.replication_pad3d, torch._C._nn.replication_pad3d\n```\n\nTo fix, added condition checking to raise a runtime error with a debug message instead, specifying the correct dimensions necessary.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121298\nApproved by: https://github.com/mikaylagawarecki",
    "date": "2024-03-06T21:55:34+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "aten/src/ATen/native/ReplicationPadding.cpp",
            "patches": [
                {
                    "Id": 123,
                    "hunk size": 3,
                    "hunk": "@@ -25,6 +25,7 @@ namespace at::meta {\n TORCH_META_FUNC(replication_pad1d) (\n   const Tensor& input, IntArrayRef paddingSize  // no out argument!\n ) {\n+  TORCH_CHECK(paddingSize.size() == 2, \"padding size is expected to be 2\");\n \n   int64_t dimw = 1;\n   int64_t dimslices = 0;\n"
                },
                {
                    "Id": 124,
                    "hunk size": 3,
                    "hunk": "@@ -85,6 +86,7 @@ TORCH_META_FUNC(replication_pad1d_backward) (\n TORCH_META_FUNC(replication_pad2d) (\n   const Tensor& input, IntArrayRef paddingSize\n ) {\n+  TORCH_CHECK(paddingSize.size() == 4, \"padding size is expected to be 4\");\n   int64_t pad_l = paddingSize[0];\n   int64_t pad_r = paddingSize[1];\n   int64_t pad_t = paddingSize[2];\n"
                },
                {
                    "Id": 125,
                    "hunk size": 3,
                    "hunk": "@@ -124,6 +126,7 @@ TORCH_META_FUNC(replication_pad2d) (\n TORCH_META_FUNC(replication_pad3d) (\n   const Tensor& input, IntArrayRef paddingSize\n ) {\n+  TORCH_CHECK(paddingSize.size() == 6, \"padding size is expected to be 6\");\n   int64_t pleft = paddingSize[0];\n   int64_t pright = paddingSize[1];\n   int64_t ptop = paddingSize[2];\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/4ee179c9528c8c6aae17a01f2b0d7e8235219219",
    "message": "Fix `ConstantVariable` init method if NumPy is missing (#109388)\n\nBy adding `np is not None` check before `isinstance(value, np.number)`\n\nPartially addresses https://github.com/pytorch/pytorch/issues/109387\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109388\nApproved by: https://github.com/ezyang",
    "date": "2023-09-16T00:07:19+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/addb8e29cd842e1a290cb0b55662ee0423ab2498",
    "message": "Enable 2d + AC torch.compile (#112536)\n\nThis PR enables AC + torch.compile to work with FSDP + TP, the fix to\nhigh order op path is that we need to check both tensor and tensor\nsubclass bases to make sourceless builder\n\nNOTE: selective AC + 2D is still not working, need to fix this\nseparately\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112536\nApproved by: https://github.com/yf225",
    "date": "2023-11-09T06:12:13+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/b4a008209a451d6fde94f09601ad179a274c315c",
    "message": "Expose tensor check from guard for reusing (#124836)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124836\nApproved by: https://github.com/jgong5, https://github.com/jansel, https://github.com/desertfire",
    "date": "2024-04-27T18:35:35+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/csrc/dynamo/cache_entry.cpp",
            "patches": []
        },
        {
            "path": "torch/csrc/dynamo/extra_state.cpp",
            "patches": []
        },
        {
            "path": "torch/csrc/dynamo/guards.cpp",
            "patches": []
        },
        {
            "path": "torch/csrc/dynamo/guards.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/045309aa3575b86eb6fd259bfaa336f723eb7b2f",
    "message": "[MPS] Enable toch.mm and friends for complex dtypes (#127241)\n\n- Add `supportedFloatingOrComplexType`\n- Change dtype check to those\n- Extend low-precision fp32 list to complex types\n- Mark conv2d as supported now, as it was failing due to the tighter accuracy constrains than the same op for float32 dtype\n\nFixes https://github.com/pytorch/pytorch/issues/127178\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127241\nApproved by: https://github.com/janeyx99",
    "date": "2024-05-28T17:56:13+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "aten/src/ATen/native/mps/OperationUtils.h",
            "patches": [
                {
                    "Id": 126,
                    "hunk size": 13,
                    "hunk": "@@ -419,6 +419,17 @@ inline bool supportedFloatingType(const Tensor& t) {\n   return supportedFloatingType(t.scalar_type());\n }\n \n+inline bool supportedFloatingOrComplexType(ScalarType dtype) {\n+  if (dtype == kComplexFloat || dtype == kComplexHalf) {\n+    return supportsComplex();\n+  }\n+  return supportedFloatingType(dtype);\n+}\n+inline bool supportedFloatingOrComplexType(const Tensor& t) {\n+  return supportedFloatingOrComplexType(t.scalar_type());\n+}\n+\n+\n inline bool needsGather(const Tensor& t) {\n   return !t.is_contiguous() || t.storage_offset();\n }\n"
                }
            ]
        },
        {
            "path": "aten/src/ATen/native/mps/operations/Linear.mm",
            "patches": [
                {
                    "Id": 127,
                    "hunk size": 8,
                    "hunk": "@@ -15,16 +15,16 @@ Tensor _mps_linear(const Tensor& input, const Tensor& weight_arg, const c10::opt\n \n   auto weight = (weight_arg.dim() == 1) ? weight_arg.view({1, weight_arg.size(0)}) : weight_arg;\n \n-  TORCH_CHECK(supportedFloatingType(input), \"MPS device does not support linear for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(input), \"MPS device does not support linear for non-float inputs\");\n   TORCH_CHECK(input.is_mps(), \"Tensor for argument input is on \", input.device(), \" but expected on mps\");\n-  TORCH_CHECK(supportedFloatingType(weight_arg), \"MPS device does not support linear for non-float weights\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(weight_arg), \"MPS device does not support linear for non-float weights\");\n   TORCH_CHECK(weight_arg.is_mps(), \"Tensor for argument weight is on \", weight_arg.device(), \" but expected on mps\");\n \n   const Tensor& bias = *(at::borrow_from_optional_tensor(bias_opt));\n   const bool is_bias_defined = bias.defined();\n   if (is_bias_defined) {\n     TORCH_CHECK(bias.is_mps(), \"Tensor for argument bias is on \", bias.device(), \" but expected on mps\");\n-    TORCH_CHECK(supportedFloatingType(bias), \"MPS device does not support linear for non-float bias\");\n+    TORCH_CHECK(supportedFloatingOrComplexType(bias), \"MPS device does not support linear for non-float bias\");\n   }\n \n   auto input_size = input.sizes();\n"
                },
                {
                    "Id": 128,
                    "hunk size": 7,
                    "hunk": "@@ -128,11 +128,12 @@ Tensor _mps_linear(const Tensor& input, const Tensor& weight_arg, const c10::opt\n \n static Tensor _mps_linear_backward_input(IntArrayRef input_size, const Tensor& grad_output, const Tensor& weight) {\n   TORCH_CHECK(grad_output.is_mps(), \"mps_linear_backward: grad_output needs to be mps layout\");\n-  TORCH_CHECK(weight.device().is_mps() && supportedFloatingType(weight),\n+  TORCH_CHECK(weight.device().is_mps() && supportedFloatingOrComplexType(weight),\n               \"mps_linear_backward: unsupported weights data type: \",\n               weight.scalar_type());\n \n-  TORCH_CHECK(supportedFloatingType(grad_output), \"MPS device does not support linear backward for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(grad_output),\n+              \"MPS device does not support linear backward for non-float inputs\");\n \n   const Tensor weight_reshaped = weight.is_contiguous() ? weight : weight.contiguous();\n \n"
                },
                {
                    "Id": 129,
                    "hunk size": 5,
                    "hunk": "@@ -193,7 +194,8 @@ static std::tuple<Tensor, Tensor> _mps_linear_backward_weights(const Tensor& gra\n   TORCH_CHECK(grad_output.is_mps() && input.is_mps(),\n               \"_mps_linear_backward: grad_output and input needs to be mps layout\");\n \n-  TORCH_CHECK(supportedFloatingType(grad_output), \"MPS device does not support linear backward for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(grad_output),\n+              \"MPS device does not support linear backward for non-float inputs\");\n \n   struct CachedGraph : public MPSCachedGraph {\n     CachedGraph(MPSGraph* graph) : MPSCachedGraph(graph) {}\n"
                }
            ]
        },
        {
            "path": "aten/src/ATen/native/mps/operations/LinearAlgebra.mm",
            "patches": [
                {
                    "Id": 130,
                    "hunk size": 4,
                    "hunk": "@@ -131,7 +131,7 @@ static Tensor& mm_out_mps_impl(const Tensor& self, const Tensor& other, Tensor&\n   using namespace mps;\n   using CachedGraph = MPSBinaryCachedGraph;\n   TORCH_CHECK(self.dim() == 2 && other.dim() == 2, \"tensors must be 2-D\");\n-  TORCH_CHECK(supportedFloatingType(self), \"MPS device does not support mm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(self), \"MPS device does not support mm for non-float inputs\");\n \n   TensorArg args[]{{output, \"out\", 0}, {self, \"mat1\", 1}, {other, \"mat2\", 2}};\n   checkAllSameGPU(\"mm\", args);\n"
                },
                {
                    "Id": 131,
                    "hunk size": 5,
                    "hunk": "@@ -185,7 +185,8 @@ static Tensor& addbmm_or_baddbmm_out_mps_impl(const Tensor& input,\n   TORCH_CHECK(batch2.is_mps());\n   TORCH_CHECK(result.is_mps());\n \n-  TORCH_CHECK(supportedFloatingType(batch1), \"MPS device does not support addbmm or baddbmm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(batch1),\n+              \"MPS device does not support addbmm or baddbmm for non-float inputs\");\n \n   TORCH_CHECK(batch1.dim() == 3, \"batch1 must be a 3D tensor\");\n   TORCH_CHECK(batch2.dim() == 3, \"batch2 must be a 3D tensor\");\n"
                },
                {
                    "Id": 132,
                    "hunk size": 4,
                    "hunk": "@@ -292,7 +293,7 @@ static Tensor& addmm_out_mps_impl(const Tensor& bias,\n \n   TORCH_CHECK(output.is_mps());\n   TORCH_CHECK(self.dim() == 2 && other.dim() == 2, \"tensors must be 2-D\");\n-  TORCH_CHECK(supportedFloatingType(self), \"MPS device does not support addmm for non-float input\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(self), \"MPS device does not support addmm for non-float input\");\n \n   TensorArg args[]{{output, \"out\", 0}, {bias, \"self\", 1}, {self, \"mat1\", 2}, {other, \"mat2\", 3}};\n   checkAllSameGPU(__func__, args);\n"
                },
                {
                    "Id": 133,
                    "hunk size": 4,
                    "hunk": "@@ -387,7 +388,7 @@ static Tensor& addmm_out_mps_impl(const Tensor& bias,\n static Tensor& bmm_out_mps_impl(const Tensor& batch1, const Tensor& batch2, Tensor& result) {\n   using namespace mps;\n \n-  TORCH_CHECK(supportedFloatingType(batch1), \"MPS device does not support bmm for non-float inputs\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(batch1), \"MPS device does not support bmm for non-float inputs\");\n \n   if (batch1.numel() == 0 || batch2.numel() == 0) {\n     result.zero_();\n"
                },
                {
                    "Id": 134,
                    "hunk size": 4,
                    "hunk": "@@ -567,7 +568,7 @@ Tensor& addr_out_mps(const Tensor& self,\n \n   TORCH_CHECK(result.is_mps());\n   TORCH_CHECK(vec1.dim() == 1 && vec2.dim() == 1, \"tensors must be 1-D\");\n-  TORCH_CHECK(supportedFloatingType(vec1), \"MPS device does not support addr for non-float input\");\n+  TORCH_CHECK(supportedFloatingOrComplexType(vec1), \"MPS device does not support addr for non-float input\");\n \n   TensorArg args[]{{result, \"out\", 0}, {self, \"self\", 1}, {vec1, \"vec1\", 2}, {vec2, \"vec2\", 3}};\n   checkAllSameGPU(__func__, args);\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/5262484ecefb03da62ed36053c9a61c1ca938ede",
    "message": "[easy][aotinductor] fix typos & add static typing (#114728)\n\n```\n// check all references\n$ grep -rl 'cpp_kernel_overlad_name' *\nir.py\n```\n\n```\n$ lintrunner --take MYPYINDUCTOR torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py\nok No lint issues.\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114728\nApproved by: https://github.com/Skylion007, https://github.com/chenyang78",
    "date": "2023-11-30T02:10:56+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2cd3ef47779d9c4337dc0edba50ac84ffdce0ede",
    "message": "Check scale dtype for fake_quantize_per_channel_affine_cachemask (#120987)\n\nFixes #120903\n\nScale for fake quant is assumed FP32 but not checked. If scales of double dtype are passed in, an internal error is raised: `TORCH_INTERNAL_ASSERT(!needs_dynamic_casting<func_t>::check(iter));` in aten/src/ATen/native/cpu/Loops.h\nThis PR adds a check of scale dtype.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120987\nApproved by: https://github.com/jgong5, https://github.com/jerryzh168",
    "date": "2024-03-30T07:32:32+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp",
            "patches": [
                {
                    "Id": 135,
                    "hunk size": 4,
                    "hunk": "@@ -48,6 +48,8 @@ std::tuple<Tensor, Tensor> fake_quantize_per_channel_affine_cachemask(\n     int64_t axis,\n     int64_t quant_min,\n     int64_t quant_max) {\n+  TORCH_CHECK(scale.scalar_type() == ScalarType::Float,\n+              \"Scale must be Float, found \", scale.scalar_type());\n   TORCH_CHECK(zero_point.scalar_type() == ScalarType::Int || zero_point.scalar_type() == ScalarType::Float || zero_point.scalar_type() == ScalarType::Half,\n               \"Zero-point must be Int32, Float or Half, found \", zero_point.scalar_type());\n   TORCH_CHECK(scale.dim() == 1, \"scale should be a 1-D tensor\");\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/8013c4409f3a37452e71ebb2e2a135cc40978b3e",
    "message": "[inductor] config to control whether we assume inputs are aligned (#122158)\n\n**Motivation**: https://github.com/pytorch/pytorch/issues/112771\n\n**Summary**: Inductor generates triton that assumes that inputs are going to be 16-byte aligned. If the inputs aren't aligned, Inductor clones the inputs. This PR introduces a config option to not do this: when assume_aligned_inputs=False, Inductor will _not_ pass inputs as being divisible_by_16, and Inductor will not make clones. This an can generate code that might be a bit slower, but this tradeoff can be worth it in some scenarios where you might otherwise make a lot of clones.\n\nIdeally, we could do this on a per-tensor basis. But this would be a lot of work, and attempts to add guards on storage offsets to do this automatically have run into issues: recompilations and excessive time to generate/check guards.\n\n**Tests** https://github.com/pytorch/pytorch/pull/122159 flips this to False. It didn't run through all errors, but the ones we see are all expected failures: divisible_by_16 changes; triton kernel caching fails if we call the same triton kernel multiple times (this makes sense because the first call will have unaligned inputs, but subsequent calls have aligned inputs); and some xfailed tests start passing.\n\n**Alternatives/RFC**:\n* Is this the right thing to do with cudagraphs?\n* Elias and Jason mentioned that we probably still want to make clones if we're dealing with unaligned inputs to matmuls. Is this something we should add in this config option? (In the use case I'm targeting, it seems like we don't need this optimization right now)\n\nDifferential Revision: [D55079094](https://our.internmc.facebook.com/intern/diff/D55079094)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122158\nApproved by: https://github.com/ezyang",
    "date": "2024-03-22T20:03:38+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_inductor/compile_fx.py",
            "patches": [
                {
                    "Id": 136,
                    "hunk size": 7,
                    "hunk": "@@ -805,7 +805,10 @@ def align_inputs(\n     inputs: List[torch.Tensor],\n     static_input_idxs: Sequence[int] = (),\n ):\n-    inputs_to_check = get_input_idxs_to_check(inputs, static_input_idxs)\n+    if config.assume_aligned_inputs:\n+        inputs_to_check = get_input_idxs_to_check(inputs, static_input_idxs)\n+    else:\n+        inputs_to_check = []\n     return align_inputs_from_check_idxs(model, inputs_to_check)\n \n \n"
                }
            ]
        },
        {
            "path": "torch/_inductor/config.py",
            "patches": [
                {
                    "Id": 137,
                    "hunk size": 7,
                    "hunk": "@@ -463,6 +463,11 @@ use_minimal_arrayref_interface: bool = False\n # decompose some memory bound matmul/bmm to mul\n decompose_mem_bound_mm: bool = False\n \n+# assume_aligned_inputs means that we assume that inputs will be aligned; we generate\n+# code using this assumption, and clone tensors before use if they aren't aligned.\n+# In the common case, most inputs will be aligned.\n+assume_aligned_inputs: bool = True\n+\n \n # config specific to codegen/cpp.py\n class cpp:\n"
                }
            ]
        },
        {
            "path": "torch/_inductor/scheduler.py",
            "patches": [
                {
                    "Id": 138,
                    "hunk size": 8,
                    "hunk": "@@ -2450,8 +2450,10 @@ class Scheduler:\n         self.flush()\n \n     def is_unaligned_buffer(self, buf_name):\n-        if buf_name in V.graph.graph_inputs or buf_name in V.graph.constants:\n-            # all graph inputs or constants are assumed to be aligned\n+        if buf_name in V.graph.graph_inputs:\n+            return not config.assume_aligned_inputs\n+        if buf_name in V.graph.constants:\n+            # all constants are assumed to be aligned\n             return False\n         node = self.name_to_node[buf_name]\n         layout = node.node.get_layout()"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/394ec2da300e8933d8184ba862daed3a115d7fd9",
    "message": "Remove GPU Check from Basic Chrome Trace test (#125430)\n\nSummary: Remove the check to make sure all GPU labels are enumerated when CUDA is available. There are some systems where CUDA is available but we do not print any GPU labels (because GPU is not available).\n\nTest Plan: Test in regression with ciflow/periodic label\n\nDifferential Revision: D56906893\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125430\nApproved by: https://github.com/izaitsevfb",
    "date": "2024-05-03T04:51:10+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/283ce12aa9c4ab0f734dba63007b2c6de7c655b8",
    "message": "Add channels_last3d support for mkldnn conv and mkldnn deconv (#95271)\n\n### Motivation\n\n- Add channels_last3d support for mkldnn conv and mkldnn deconv.\n- Use `ideep::convolution_transpose_forward::compute_v3` instead of `ideep::convolution_transpose_forward::compute`.  compute_v3 uses `is_channels_last` to notify ideep whether to go CL or not to align with the memory format check of PyTorch.\n\n### Testing\n1 socket (28 cores):\n\n- memory format: torch.contiguous_format\n\nmodule | shape | forward / ms | backward / ms\n-- | -- | -- | --\nconv3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 64.56885 | 150.1796\nconv3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3) | 100.6754 | 231.8883\nconv3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 19.31751 | 68.31131\n\nmodule | shape | forward / ms | backward / ms\n-- | -- | -- | --\nConvTranspose3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 122.7646 | 207.5125\nConvTranspose3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3) | 202.4542 | 368.5492\nConvTranspose3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 122.959 | 84.62577\n\n- memory format: torch.channels_last_3d\n\nmodule | shape | forward / ms | backward / ms\n-- | -- | -- | --\nconv3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 40.06993 | 114.317\nconv3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3 | 49.08249 | 133.4079\nconv3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 5.873911 | 17.58647\n\nmodule | shape | forward / ms | backward / ms\n-- | -- | -- | --\nConvTranspose3d | input size: (32, 32, 10, 100, 100), weight size: (32, 32, 3, 3, 3) | 88.4246 | 208.2269\nConvTranspose3d | input size: (32, 16, 10, 200, 200), weight size: (16, 16, 3, 3, 3 | 140.0725 | 270.4172\nConvTranspose3d | input size: (16, 4, 5, 300, 300), weight size: (4, 4, 3, 3, 3) | 23.0223 | 37.16972\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95271\nApproved by: https://github.com/jgong5, https://github.com/cpuhrsch",
    "date": "2023-08-30T02:53:30+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/b816760a2f27adafb0b1dac4c032a2e97c690b29",
    "message": "More progress on type checking ValueRanges (#118870)\n\nType checking Python is a pain. Here are my learnings:\n\n* The types for heavily polymorphic code is going to be verbose, no way around it. I originally was hoping I could lean on polymorphism with a bounded TypeVar to compactly write signatures for many of the ValueRanges methods, but I ran into some unworkaroundable mypy bugs. Writing out all the types explicitly and using `@overload` liberally works pretty well, so I think I recommend people do that instead of trying to do fancy things.\n* Sympy is missing annotations for assumptions, because they are all metaprogrammed. I don't really relish maintaining a typeshed for sympy, so I wrote a small mypy plugin to add them in.\n* GADT style refinement is... just not a good idea in practice. Mypy easily gets confused whether or not a return value from a refined section is allowed for the outer return type. So many of these have been replaced with less informative implementation types and more informative external types via overloads. Hopefully this is good for use sites.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118870\nApproved by: https://github.com/Skylion007, https://github.com/albanD",
    "date": "2024-02-05T20:29:25+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "mypy.ini",
            "patches": []
        },
        {
            "path": "mypy_plugins/sympy_mypy_plugin.py",
            "patches": []
        },
        {
            "path": "torch/utils/_sympy/value_ranges.py",
            "patches": [
                {
                    "Id": 139,
                    "hunk size": 19,
                    "hunk": "@@ -70,8 +70,25 @@ def vr_is_expr(vr: ValueRanges[_T]) -> TypeGuard[ValueRanges[sympy.Expr]]:\n     return not vr.is_bool\n \n \n+ExprIn = Union[int, float, sympy.Expr]\n+BoolIn = Union[bool, SympyBoolean]\n+AllIn = Union[ExprIn, BoolIn]\n+ExprFn = Callable[[sympy.Expr], sympy.Expr]\n+ExprFn2 = Callable[[sympy.Expr, sympy.Expr], sympy.Expr]\n+BoolFn = Callable[[SympyBoolean], SympyBoolean]\n+BoolFn2 = Callable[[SympyBoolean, SympyBoolean], SympyBoolean]\n+AllFn = Union[ExprFn, BoolFn]\n+AllFn2 = Union[ExprFn2, BoolFn2]\n+\n+\n @dataclasses.dataclass(frozen=True)\n class ValueRanges(Generic[_T]):\n+    if TYPE_CHECKING:\n+        # ruff doesn't understand circular references but mypy does\n+        ExprVR = ValueRanges[sympy.Expr]  # noqa: F821\n+        BoolVR = ValueRanges[SympyBoolean]  # noqa: F821\n+        AllVR = Union[ExprVR, BoolVR]\n+\n     # Although the type signature here suggests you can pass any\n     # sympy expression, in practice the analysis here only works\n     # with constant sympy expressions\n"
                },
                {
                    "Id": 140,
                    "hunk size": 8,
                    "hunk": "@@ -92,15 +117,15 @@ class ValueRanges(Generic[_T]):\n         object.__setattr__(self, \"is_bool\", isinstance(lower, SympyBoolean))\n         assert isinstance(upper, SympyBoolean) == self.is_bool\n \n-    def boolify(self):\n-        if self.is_bool:\n+    def boolify(self) -> ValueRanges[SympyBoolean]:\n+        if vr_is_bool(self):\n             return self\n         elif self == ValueRanges.unknown():\n             return ValueRanges.unknown_bool()\n         else:\n             raise AssertionError(f\"not bool like {self}\")\n \n-    def __contains__(self, x):\n+    def __contains__(self, x: AllIn) -> bool:\n         x = simple_sympify(x)\n         return sympy_generic_le(self.lower, x) and sympy_generic_le(x, self.upper)\n \n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/10c646295d3512237cfb3ab44aa21dfcc9832441",
    "message": "When doing typed typecheck, also check signature with symint removed (#109727)\n\nSee the test case for what we didn't catch (SymInt vs const SymInt&\nmismatch.)\n\nIt's necessary to test for both, because we will fall back to the\nnon-SymInt signature if there is no SymInt unboxed kernel available.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109727\nApproved by: https://github.com/zou3519",
    "date": "2023-09-27T07:29:46+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/03ff44c958913dedbf160b2c2f6b9107cfd160c1",
    "message": "[c10d] Fix Store check condition in NCCL PG watchdog (#115475)\n\nIn https://github.com/pytorch/pytorch/pull/115449/ somehow after turning on `DUMP_ON_TIMEOUT=1`, some existing tests failed. Upon checking, the failing is because of TCPStore check call within watchdog thread.\n\n1. It's not because of TCPStore creation has not completed, because if I put it sleep for a long time, the test still failed. Rather, it's because we query TCPStore after we shutdown the PG.\n\n2. The reason for that is: The `std::chrono::steady_clock::now()` function in C++ returns a `time_point` object representing the current point in time according to the steady clock. The default unit of this time_point is not directly specified in terms of seconds or nanoseconds; rather, it is dependent on the internal representation of the steady clock, which can vary between implementations. In reality it's actually nanosecs which makes the delta so big that we are checking the store every time when watchdog thread wakes up. To make things even worse, `terminateProcessGroup_` might be turned to be `True` before the next check for the outmost while but before TCPStore check, so watchdog gets stuck because we are checking a TCPStore which is already deleted. And main thread is still waiting for watchdog to join.\n\nThe solution here is:\n1. Add back `std::chrono::duration_cast` to ensure the delta is indeed mil_sec, so that the timeout check logic is working as expected.\n2. Check `terminateProcessGroup_` as well so that, we don't do any dump when main thread has already mark the process exited.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115475\nApproved by: https://github.com/wconstab",
    "date": "2023-12-11T21:06:05+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/02a410ee12e6f7318c2abb463edfaff456a0bdb4",
    "message": "Enable TORCH_TRACE by default in all Tupperware like environments (#120915)\n\nSummary:\nThis is a reimplemented version of the FB specific code in https://www.internalfb.com/diff/D54230697\n\nThe new strategy is that we unconditionally install an FB handler to trace_log logger (and always set level to DEBUG). When the first log message is emitted, we check the JK/filesystem to see if we should actually do logging. If we decide we don't do logging, we remove the handler from trace_log and are done.\n\nbuild_only[github-export-checks,executorch,pytorch_benchmark,pytorch_quantization,pytorch_distributed,pytorch_distributed_gpu,pytorch_dynamo_inductor,pytorch_functorch,pytorch_fx2trt,pytorch_diff_train_tests_ads,glow_fb_pytorch_tests,training_platform,training_platform_compatibility,training_toolkit_applications,training_toolkit_examples,training_toolkit_model_optimization,dper3_pytorch,xplat_caffe2,pytorch_dev,android-pytorch-instrumentation-tests,smartpytorchgithub_first_try_merge,frl-target-determinator,f6-buck,training_platform_for_github,sigmoid_cpu,sigmoid_gpu,aiplatform_modelprocessing_for_github,accelerators_workloads_models_slimdsnn,ae_aotinductor_benchmark_test,aps_,aps_deterministic_ne_tests,dper_lib_silvertorch,torchrec,torchrec_fb,deeplearning_aot_inductor]\n\nTest Plan:\nsandcastle\n\n```\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//torchrec/inference/tests:test_single_gpu_executor -- --exact 'torchrec/inference/tests:test_single_gpu_executor - TorchDeployGPUTest.NestedModelSingleGPU'\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//dper_lib/silvertorch/modules/dynamic_stats/tests:accumulators_test -- --exact 'dper_lib/silvertorch/modules/dynamic_stats/tests:accumulators_test - test_global_fixed_interval_accumulator (dper_lib.silvertorch.modules.dynamic_stats.tests.accumulators_test.GlobalFixedIntervalUnivalentAcculumatorTest)'\n```\n\nAlso running a test flow with/without JK enabled\n\nDifferential Revision: D54275086\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120915\nApproved by: https://github.com/yanboliang",
    "date": "2024-03-01T04:47:13+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_logging/_internal.py",
            "patches": [
                {
                    "Id": 141,
                    "hunk size": 6,
                    "hunk": "@@ -1004,7 +1053,9 @@ def trace_structured(\n     assert callable(\n         payload_fn\n     ), f\"payload_fn should be callable, but got {type(payload_fn)}\"\n-    if trace_log.isEnabledFor(logging.DEBUG):\n+    # trace_log never propagates and is ALWAYS DEBUG, so also check that there\n+    # are handlers instead of checking the log level\n+    if trace_log.handlers:\n         record: Dict[str, object] = {}\n         record[name] = metadata_fn()\n         if not suppress_context:\n"
                }
            ]
        },
        {
            "path": "torch/_utils_internal.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/3f1f057adfcd4cef67fff9605a894cb075c02881",
    "message": "Remove parent device mesh check (#118620)\n\nRemoves raising error if a device_mesh has a parent.\n\nThe comment says that HSDP + TP is not supported, but I'm able to do 2D parallelism + HSDP fine. The only issues are:\n- this check\n- https://github.com/pytorch/pytorch/pull/118618\n- a series of PRs related to checkpointing with 3D meshes that I will open\nWe currently monkeypatch for the above which I am slowly upstreaming.\n\nI imagine torch will have a better, native integration eventually, but this check seems too aggressive in the meantime given DTensor now lets users do some things themselves (which is amazing \ud83c\udf89)!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118620\nApproved by: https://github.com/wz337, https://github.com/wanchaol",
    "date": "2024-02-02T05:29:49+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/distributed/fsdp/_init_utils.py",
            "patches": [
                {
                    "Id": 142,
                    "hunk size": 8,
                    "hunk": "@@ -204,12 +204,6 @@ def _is_valid_hybrid_shard_pg_type(process_group: Any) -> bool:\n \n @no_type_check\n def _is_valid_hybrid_shard_device_mesh(device_mesh: DeviceMesh) -> bool:\n-    parent_mesh = _mesh_resources.get_parent_mesh(device_mesh)\n-    if parent_mesh is not None:\n-        raise RuntimeError(\n-            f\"Found device_mesh {device_mesh} passed in has a parent device_mesh {parent_mesh}.\",\n-            \"Hybrid sharding + TP is not supported yet.\",\n-        )\n     return isinstance(device_mesh, DeviceMesh) and device_mesh.ndim == 2\n \n "
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/491292229782e06b1d99684fc591fabae5445cdd",
    "message": "Fake Tensor refactors part 1 (#116344)\n\nThese are mostly small performance optimizations to move constant list construction into global scope and replace O(n) `x in list` checks with O(1) `x in dict` checks.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116344\nApproved by: https://github.com/yanboliang",
    "date": "2023-12-23T08:38:26+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/993e4f3911856be3a93746f6ed6a13f25de6ff65",
    "message": "[c10d] relax the nccl error check for nonblocking mode (#118254)\n\nresolve https://github.com/pytorch/pytorch/issues/117749\n\nSummary:\nThis is the first step to enable NCCL nonblocking mode.\n\nIn NCCL nonblocking mode,  ncclInProgress is an expected return value\nwhen checking communicators. Without this relaxation, watchdog thread\nwould throw NCCL errors during work checking while it is expected.\n\nTest Plan:\nSet nonblocking mode in unit tests, and make sure all existing NCCL\ntests pass\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118254\nApproved by: https://github.com/kwen2501",
    "date": "2024-01-27T03:49:00+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
            "patches": [
                {
                    "Id": 143,
                    "hunk size": 7,
                    "hunk": "@@ -1702,7 +1702,10 @@ std::exception_ptr ProcessGroupNCCL::checkForNCCLErrorsInternal(\n               *commFailureReason)));\n     }\n     ncclResult_t ncclAsyncErr = ncclComm->checkForNcclError();\n-    if (ncclAsyncErr != ncclSuccess) {\n+    // When nonblocking mode is enabled by TORCH_NCCL_USE_COMM_NONBLOCKING,\n+    // ncclInProgress could be returned when there are pending NCCL calls.\n+    // In this case, no exception should be thrown\n+    if (ncclAsyncErr != ncclSuccess && ncclAsyncErr != ncclInProgress) {\n       return std::make_exception_ptr(C10_BUILD_ERROR(\n           DistBackendError,\n           \"NCCL error: \" + ncclGetErrorWithVersion(ncclAsyncErr) + \"\\n\" +"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/741c1710e81bcc4034440d87ebbd23ec4c89cd81",
    "message": "[cond] inlining into one of the branches when pred is a python constant (#130493)\n\nReland https://github.com/pytorch/pytorch/pull/128709.\n\nWhen the input predicate is a python constant, we specialize into one of the branches and warn users that torch.cond is not preserving the dynamism. The previous behavior is that we baked in True/False in the cond operator. This can be confusing. In this PR, we change it to be specializing into one of the branches when the inputs are constants.\n\nWe additionally change the naming of cond operator to default one without overriding its name. This allows better testing on de-serialized graph.\n\nTest Plan:\nThe predicate in some existing tests is the result of a shape comparison. When no dynamic shape is involved, the predicate is a python bool. To fix them, we either change the predicate to be some data-dependent tensor or change the test to check cond is specialized as one of the branches,\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130493\nApproved by: https://github.com/BoyuanFeng",
    "date": "2024-07-12T18:02:09+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/_dynamo/variables/higher_order_ops.py",
            "patches": [
                {
                    "Id": 144,
                    "hunk size": 14,
                    "hunk": "@@ -632,6 +632,18 @@ class CondHigherOrderVariable(TorchHigherOrderOperatorVariable):\n                 f\"Expected 4 arguments but got {len(args)}.\\n\"\n                 f\"Usage: cond(pred, true_fn, false_fn, operands)\",\n             )\n+\n+        # Specialize into one of the branches since pred is constant\n+        if type(args[0]) is ConstantVariable:\n+            log.warning(\n+                \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+                \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+            )\n+            if args[0].as_python_constant():\n+                return args[1].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+            else:\n+                return args[2].call_function(tx, args[3].unpack_var_sequence(tx), {})\n+\n         # predicate\n         if type(args[0]) not in (ConstantVariable, TensorVariable, SymNodeVariable):\n             unimplemented(\n"
                }
            ]
        },
        {
            "path": "torch/_higher_order_ops/cond.py",
            "patches": [
                {
                    "Id": 145,
                    "hunk size": 12,
                    "hunk": "@@ -107,6 +110,16 @@ def cond(pred, true_fn, false_fn, operands):\n     if torch.compiler.is_dynamo_compiling():\n         return cond_op(pred, true_fn, false_fn, operands)\n \n+    if isinstance(pred, (bool, int, float)):\n+        log.warning(\n+            \"Pred is a Python constant. When used with torch.cond, it executes only one of the branches.\"\n+            \" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.\"\n+        )\n+        if pred:\n+            return true_fn(*operands)\n+        else:\n+            return false_fn(*operands)\n+\n     def _validate_input(pred, true_fn, false_fn, operands):\n         if not isinstance(pred, (bool, torch.Tensor, torch.SymBool)):\n             raise RuntimeError(f\"Expected pred to be bool or tensor, but got {pred}.\")\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/75d3bbaaa23a5b3febb0264df0742b6b6e4c916e",
    "message": "Fix cudagraph check message (#115664)\n\nThis error message is printed when CUDAGraph trees are used with multiple device indices.\n\nHowever, the message seems to say the opposite.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115664\nApproved by: https://github.com/soulitzer",
    "date": "2023-12-13T18:44:43+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fff633f087fa5fb8ac69038fadbe88ccc54e614a",
    "message": "[CI] Enable AOT inductor FP32 accuracy test for CPU (#129040)\n\nThis PR enabled AOT inductor backend FP32 accuracy check for CPU in CI workflow, which could catch AOT inductor issue at early stage.\n\n**Test Time cost:**\n| Suite       \t| Precision \t| Time cost \t|\n|-------------\t|-----------\t|-----------\t|\n| Huggingface \t| FP32      \t|   1h12m   \t|\n| Timm models \t| FP32      \t|   1h32m   \t|\n|  Torchbench \t| FP32      \t|   1h40m   \t|\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129040\nApproved by: https://github.com/chuanqi129, https://github.com/desertfire, https://github.com/malfet",
    "date": "2024-06-30T14:00:09+00:00",
    "label": "NO",
    "changes": [
        {
            "path": ".github/workflows/inductor.yml",
            "patches": []
        },
        {
            "path": "benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_huggingface_freezing_inference.csv",
            "patches": []
        },
        {
            "path": "benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_timm_freezing_inference.csv",
            "patches": []
        },
        {
            "path": "benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_torchbench_freezing_inference.csv",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/07f0f383fa23e63eca164036ab58ab983e9437eb",
    "message": "update tensor-like to check instance for torch function impl (#111087)\n\ntensor like should check the instance for a torch function impl, not the type\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111087\nApproved by: https://github.com/ezyang",
    "date": "2023-10-12T02:14:38+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e29eb39e04a9318abae5be131ec361690443fc7d",
    "message": "[EZ] Fix typo in gcc version detection (#120489)\n\nIt should be `FATAL_ERROR` rather than `FATAL`\n\nI wish cmakelint would have detected it\n\nAlso, downgrade this check to 9.3, as all our binary builds are using 9.3 at the moment (will update in a followup PR)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120489\nApproved by: https://github.com/DanilBaibak, https://github.com/Skylion007",
    "date": "2024-02-23T20:31:21+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "CMakeLists.txt",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/7979ba7b4327d1fabbeb06bafb2f1d2a2269f8b9",
    "message": "[inductor] Add dropout type check to match eager (#115040)\n\nFixes #98970\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115040\nApproved by: https://github.com/oulgen",
    "date": "2023-12-03T23:05:02+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/fdc29f58c6c1b4f1a7215573d18551e63071f1a0",
    "message": "[TP] Refactor style to make it work with torch.compile (#111625)\n\nWe are refactoring parallel style to solve the following things:\n1. To further simplifying code logic to make more readable for users.\n2. To remove tuple check so that we can work with dynamo for now. Ideally dynamo needs to support this case and we will fix it in parallel.\n3. Add tests for newly added parallel style in UT and torch compile test so that we can capture regression due to code change.\n4. Move placements early return check into DTensor since it is by passed by dynamo.\n5. Remove PairwiseParallelStyle from unit tests to use the new Col/Rowwise parallel style.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111625\nApproved by: https://github.com/wanchaol",
    "date": "2023-10-20T19:20:43+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/b1657beac174811acb03024327b8d6e972e9eca9",
    "message": "feat: Add min, max ranges to mark_dynamic API (#119737)\n\nFixes https://github.com/pytorch/pytorch/issues/115137\n\nThis PR adds:\n\n- mark_dynamic API will accept `min`, `max` values to create a bounded constraint on the dim.\n- test case in test_misc.py which checks if `ConstraintViolationError` is triggered if `torch.compile` gets a input dimension out of bounds.\n\nCo-authored-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119737\nApproved by: https://github.com/ezyang, https://github.com/jansel",
    "date": "2024-03-07T23:26:03+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "docs/source/torch.compiler_dynamic_shapes.rst",
            "patches": []
        },
        {
            "path": "torch/_dynamo/decorators.py",
            "patches": []
        },
        {
            "path": "torch/_dynamo/variables/builder.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/6b4c686b9a33a1503a4a4133f9067dd31e0822f7",
    "message": "[aotindutor] Forward fix a performance regression (#110800)\n\nSummary: Forward fix a performance regression caused by https://github.com/pytorch/pytorch/pull/110510. When a model is run once, all those kernel pointers are initialized and removing the if-nullptr check will cause those loadKernel be unnecessarily executed again when we rerun the foward function. Another way to do this is to codegen loadKernel in the initializer, which I may do in a later PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110800\nApproved by: https://github.com/jansel",
    "date": "2023-10-08T04:06:44+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/e0e2d897ed519e65b097fa811abb5db04e6576b2",
    "message": "Handle Tensor returns in PropagateUnbackedSymInts (#124297)\n\nThis subsumes https://github.com/pytorch/pytorch/pull/124069\n\nIn the original PR, my idea was that when we run PropagateUnbackedSymInts, we check that the sizes before and after are exactly the same. This ended up turning up lots of bugs that I didn't feel like fixing. Separately, Ivan let me know that this pass was quite expensive in terms of compile time, since we spent a lot of time thinking about the equalities.\n\nTo kill two birds with one stone, we now only check for equality precisely when an unbacked SymInt was bound (thanks to the previous PR in this stack, we now have this information). Specifically, we look to see if `meta[\"unbacked_bindings\"]` is set on the old node, and if it is, we assert the old value is equal to the new value from the repropagation. Note that the pytree key is used to actually extract the new value from the example value, as it may be nested inside an, e.g., tensor size.\n\nWe do something a bit naughty at the end: we use `defer_runtime_assert` to actually teach ShapeEnv about the equality. This is implementationally equivalent to what we used to do, but we're going to change this later soon.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124297\nApproved by: https://github.com/lezcano\nghstack dependencies: #124290",
    "date": "2024-04-24T12:18:33+00:00",
    "label": "YES",
    "changes": [
        {
            "path": "docs/source/fx.experimental.rst",
            "patches": []
        },
        {
            "path": "torch/_functorch/_aot_autograd/traced_function_transforms.py",
            "patches": [
                {
                    "Id": 146,
                    "hunk size": 12,
                    "hunk": "@@ -675,16 +679,8 @@ def aot_dispatch_subclass(\n \n class PropagateUnbackedSymInts(torch.fx.Interpreter):\n     def run_node(self, n: torch.fx.Node):\n-        import sympy\n-\n         result = super().run_node(n)\n-        # TODO: handle Tensor returns\n-        if \"example_value\" in n.meta:\n-            if isinstance(result, torch.SymInt) and isinstance(\n-                result.node.expr, sympy.Symbol\n-            ):\n-                torch._check(result == n.meta[\"example_value\"])\n-\n+        rebind_unbacked(detect_fake_mode().shape_env, n, result)\n         return result\n \n \n"
                }
            ]
        },
        {
            "path": "torch/fx/experimental/symbolic_shapes.py",
            "patches": [
                {
                    "Id": 147,
                    "hunk size": 4,
                    "hunk": "@@ -1229,7 +1253,7 @@ def _eval_is_non_overlapping_and_dense(sizes, strides):\n \n def cast_symbool_to_symint_guardless(symbool: torch.SymBool) -> torch.SymInt:\n     int_sym = sympy.Piecewise((1, symbool.node.expr), (0, True))\n-    return symbool.node.shape_env.create_symintnode(int_sym, hint=int(symbool.node.require_hint()))\n+    return symbool.node.shape_env.create_symintnode(int_sym, hint=int(symbool.node.require_hint()) if has_hint(symbool) else None)\n \n SYMPY_INTERP = {\n     'Abs': operator.abs,"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/2a51ccc77e26de66c35e3308aab4412986f1b0c7",
    "message": "When translation validation is enabled, assert that hint is consistent (#130478)\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130478\nApproved by: https://github.com/lezcano",
    "date": "2024-07-11T13:02:31+00:00",
    "label": "NO",
    "changes": [
        {
            "path": "torch/fx/experimental/sym_node.py",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/PyTorch/PyTorch/commit/b51e0246b7f119770c47183b230c553f15ab4fbb",
    "message": "sccache version update (#119554)\n\nFixes #37928\n\n`sccache` is updated to the newer version (`v0.7.4`) to fix non-cacheable calls `multiple input files`  for `CUDA` builds.\n\nThis should make `Cache hits (CUDA)`  work as expected and improve the speed dramatically.\n\n---\n\nAdditional information:\n\n- Modified `install_sccache.bat` check structure due to GitHub Action error `Process completed with exit code 255.`\n    - Error is occurring when freshly downloaded `sccache` is being called with `--show-stats` or `--start-server` arguments within the script\n    - Now, it is checking file's existence and killing/deleting executable before the download\n\n- Removed `sccache-cl` since it is no longer needed with newer versions of `sccache`\n\n---\n\n`win-vs2019-cpu-py3 / build` - `16m 27s`\n\n![image](https://github.com/pytorch/pytorch/assets/148207261/b5628e6c-64bb-4293-9d07-480f56df44f1)\n\n`win-vs2019-cuda11.8-py3 / build` - `17m 4s` **(previously ~45 mins - 1h30mins)**\n\n![image](https://github.com/pytorch/pytorch/assets/148207261/e4ab01cb-0f56-41e8-984f-110e643b9c09)\n\nNow `Cache Hits (CUDA)` hits all `304` object and the error `Non-cacheable reasons` is fixed.\n\n![image](https://github.com/pytorch/pytorch/assets/148207261/c8c25d2e-3fc1-4edb-8982-99c1f490cb54)\n\n---\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119554\nApproved by: https://github.com/malfet",
    "date": "2024-02-13T23:50:40+00:00",
    "label": "YES",
    "changes": [
        {
            "path": ".github/workflows/_win-build.yml",
            "patches": []
        }
    ]
}]