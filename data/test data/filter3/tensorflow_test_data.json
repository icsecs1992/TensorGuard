{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8c3822edbb31cf71cedaf49f2167e45c1e2d0b83",
    "message": "Update the is_dtensor check to only run in eager mode.\n\nPiperOrigin-RevId: 516294602",
    "date": "2023-03-13T12:53:59-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e597f7dc6bbad2659af8f099b8e91e168a04bbf0",
    "message": "[XLA:GPU] Add check that computation is found in TritonFilecheckTest.\n\nTritonFileCheckTest::CreateTritonIrAndFilecheck fails with an obscure segmentation fault in TritonFusionAnalysis::Execute if an incorrect name for the triton function is passed in. This makes it seem like the issue is around tiling analysis when it is just a test typo. This change adds a ret_check to clarify that the failure is from the call to GetComputationWithName.\nPiperOrigin-RevId: 605007087",
    "date": "2024-02-07T09:30:20-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/0a1a2f68de95cda539858eaf1dddbe512dbac4db",
    "message": "gpu_delegate: Update GPU MUL compatibility checker\n\nADD, MUL only works when two inputs has the same number of dimension on GPU.\nMake sure if the condition is checked properly.\n\nPiperOrigin-RevId: 632015698",
    "date": "2024-05-08T21:03:51-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/lite/tools/versioning/gpu_compatibility.cc",
            "patches": [
                {
                    "Id": 1,
                    "hunk size": 10,
                    "hunk": "@@ -446,6 +447,14 @@ absl::Status CheckGpuDelegateCompatibility(const OpSignature& op_sig) {\n       if (op_sig.inputs.size() != 2) {\n         return absl::UnimplementedError(\"ADD requires two input tensors.\");\n       }\n+      const auto& input0 = op_sig.inputs.at(0);\n+      const auto& input1 = op_sig.inputs.at(1);\n+      if (input0.dims.size() != input1.dims.size()) {\n+        return absl::UnimplementedError(\n+            absl::StrCat(\"ADD doesn't support broadcasting - input0: [\",\n+                         absl::StrJoin(input0.dims, \",\"), \"], input1: [\",\n+                         absl::StrJoin(input1.dims, \",\"), \"]\"));\n+      }\n       const TfLiteAddParams* tf_options;\n       return RetrieveBuiltinData(op_sig, &tf_options);\n     }\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/4fe875c71d97e84bb380539fb64179d01deb4f15",
    "message": "Provide a set of TF_LITE_ENSURE macros that work with the opaque context.  This allows clients that use the stable delegate API to check conditions within their code and report errors if the conditions don't hold.\n\nPiperOrigin-RevId: 503411126",
    "date": "2023-01-20T04:36:45-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1e7fd2f447e37761b6f6c673de77d392b6d0650d",
    "message": "Add checkers before derefence in conv Prepare()",
    "date": "2022-12-16T13:14:12+03:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/bc9028e1c9f1fa43c85e606bf8d367214db8ca23",
    "message": "[XLA] Detect duplicate parameter numbers in HLO parser.\n\nThis enables early and more informative detection of wrong computations instead of crashing on a later check in the HLO computation's constructor.\n\nPiperOrigin-RevId: 546716740",
    "date": "2023-07-09T15:33:35-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f32eaafca6748212d625738d6bd35330b1dfa82a",
    "message": "[XLA:GPU] Outline cuBLAS padding requirements; make cuBLAS/Triton GEMM decision more robust.\n\nThe present order of passes in the compiler (padding for cuBLAS -> ... -> Triton GEMM rewrite -> cuBLAS GEMM rewrite) has a problem that padding for cuBLAS skips padding for those GEMMs which will be executed with Triton GEMM based on a complex check; if the condition for this check changes after that and before Triton GEMM rewrite and Triton rewriter skips that GEMM we end up with one that has to be executed by cuBLAS but is not padded. This change solves the problem by catching all non-padded ones in the Triton GEMM rewrite.\n\nPiperOrigin-RevId: 545729159",
    "date": "2023-07-05T11:08:53-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2f95c49f1e72e1c95d150686a6ffeb23e76e3e71",
    "message": "Make checks which opcodes support to_apply consistent.\n\nThere were switch statements in 3 different places, each with a different set\nof ops that are considered to have a to_apply() subcomputation. Add a\nhas_to_apply() function for checking whether an op has a to_apply()\nsubcomputation, and use it in these 3 places.\n\nPiperOrigin-RevId: 479284174",
    "date": "2022-10-06T05:11:34-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c9666f72eb0c97846c82bc3aadff9214f466585e",
    "message": "This CL adds a new RewritePattern to check if a BatchMatMulOp, with no broadcasting necessary, can be lowered to FullyConnectedOp. If possible, lowers a TF::BatchMatMulOp to TFL::FullyConnectedOp.\n\nPiperOrigin-RevId: 503305389",
    "date": "2023-01-19T17:19:30-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/94eb84b051eb1435c1930ddad8f474c188a234b9",
    "message": "[XLA:GPU][NFC] Refactor Triton GEMM rewriter: use a context class to hold attributes of a fusion scope.\n\nIn addition generalize the way the parameters of the dimension split are passed between dot fusion scopes and update the method checking for supported dimension orders.\n\nPiperOrigin-RevId: 559358482",
    "date": "2023-08-23T02:22:34-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/3a8b4ffe6f1402b967709d2fc7120f808c5c198c",
    "message": "PR #3980: [python:xla_extension] Handle unbounded recursion in cyclical PyTrees\n\nImported from GitHub PR https://github.com/openxla/xla/pull/3980\n\nProvides more graceful handling of an unbounded recursion error that can occur when attempting to flatten a PyTree with cyclical node references.\n\nExample from https://github.com/google/jax/issues/15711:\n```Python\nimport jax.tree_util as jtu\na = []\na.append(a)\njtu.tree_flatten(a)\n# \u201cpython\u201d terminated by signal SIGSEGV (Address boundary error)\n```\n\nWith this pull, the above snippet now returns the error message:\n```Python\nRecursionError: maximum recursion depth exceeded in flatten; PyTree may have cyclical node references.\n```\n\nThe maximum recursion depth before the error is throw, is controlled by the Python interpreter.\nCopybara import of the project:\n\n--\n73f9a28d3dd69025a9e90815183dc79e0dddcc0a by tttc3 <T.Coxon2@lboro.ac.uk>:\n\nHandle unbounded recursion in cyclical PyTrees\n\nMerging this change closes #3980\n\nPiperOrigin-RevId: 547950031",
    "date": "2023-07-13T15:59:53-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/662128e8ca3411286b234553a7efc1356353d0f5",
    "message": "add rank checking for MEAN op\n\nThe MEAN op of NNAPI only supports a tensor with rank <= 4.\nCheck the rank of the input tensor before delegating the op.\n\n```\n...\n12-22 09:22:24.514  6130  6130 E ModelBuilder: Invalid Operation: NN_RET_CHECK failed (packages/modules/NeuralNetworks/common/types/operations/src/SimpleMath.cpp:30): inputRank <= 4u (inputRank = 5, 4u = 4) Unsupported input tensor rank for operation MEAN\n12-22 09:22:24.514  6130  6130 E tflite  : NN API returned error ANEURALNETWORKS_BAD_DATA at line 1131 while adding operation.\n12-22 09:22:24.515  6130  6130 E tflite  : Restored original execution plan after delegate application failure.\n...\n```",
    "date": "2022-12-22T09:22:55+08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1b0db88af3638acb44a96332802ab005c616e8a8",
    "message": "Return invalid argument status when input validation fails.\n\nPiperOrigin-RevId: 510582268",
    "date": "2023-02-17T20:20:22-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2a052ebf19897e0dc91b8b18f8e23ecc35e7a5fe",
    "message": "Modify LiteralTestUtil to ensure dynamic dimensions are equivalent when checking equality. Previously the LiteralTestUtil would consider two dynamic literals equal as long as they had identical elements (even if they had different dynamic dimensions).\n\nPiperOrigin-RevId: 506359222",
    "date": "2023-02-01T10:52:35-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fdf0f5d3e67ccd6064ac09256ab78d7031d2d38d",
    "message": "Fix checkfail in Conv2DBackpropFilter\n\nThe API tf.raw_ops.Conv2DBackpropFilter not have a check for strides size which may be causing checkfail as reported in #63076.",
    "date": "2024-03-01T12:39:55+05:30",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/core/kernels/conv_grad_filter_ops.cc",
            "patches": [
                {
                    "Id": 2,
                    "hunk size": 5,
                    "hunk": "@@ -114,6 +114,9 @@ class Conv2DBackpropFilterOp : public OpKernel {\n     OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                 errors::InvalidArgument(\"Invalid data format\"));\n     OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides_));\n+    OP_REQUIRES(context, strides_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window strides field must \"\n+                                        \"specify 4 dimensions\"));\n     int stride_n = GetTensorDim(strides_, data_format_, 'N');\n     int stride_c = GetTensorDim(strides_, data_format_, 'C');\n     int stride_h = GetTensorDim(strides_, data_format_, 'H');"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2b164c8cd6b5ae3dd6c664127ebdf1104836eeda",
    "message": "Fix checkfail in DenseBincount\n\nThe API raw_ops.DenseBincount lacks validation of input to be vector. It does have checking for rank<=2 but not for rank>0.\r\nPassing a scalar value causes checkfail with debug build.\r\n\r\nReported at #63068",
    "date": "2024-03-04T20:34:41+05:30",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/core/ops/math_ops.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a773acefff893ea3435c0c2eb763ddef21408db5",
    "message": "[xla] hlo_computation: drop instruction_indices_\n\nIt's redundant given that we have HloInstruction.index_in_parent_.\nThere is only one caller (a TF_RET_CHECK) that needs it; I think\nwe can survive without that check.\n\nThis reduces the size of the struct from 176 to 152 bytes.\n\nWhile at it, move to_be_deleted_ up in the struct so that\ninstructions_, instruction_count_ and to_be_deleted_ are now\ncontiguous.\n\nPiperOrigin-RevId: 622709022",
    "date": "2024-04-07T19:58:17-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_computation.cc",
            "patches": [
                {
                    "Id": 3,
                    "hunk size": 7,
                    "hunk": "@@ -453,9 +454,8 @@ Status HloComputation::RemoveInstructionImpl(HloInstruction* instruction,\n       << \"instruction \" << instruction->name()\n       << \" has control successors and cannot be removed\";\n \n-  auto inst_it = instruction_indices_.find(instruction);\n-  TF_RET_CHECK(inst_it != instruction_indices_.end());\n-  HloInstructionInfo* info = &instructions_[inst_it->second];\n+  HloInstructionInfo* info = &instructions_[instruction->index_in_parent_];\n+  DCHECK_EQ(info->inst(), instruction);\n   info->inst()->set_parent(nullptr);\n   to_be_deleted_.push_back(info->inst());  // Takes ownership\n   to_be_deleted_.back()->DetachFromOperandsAndUsers();\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_computation.h",
            "patches": [
                {
                    "Id": 4,
                    "hunk size": 5,
                    "hunk": "@@ -1010,9 +1012,6 @@ Status HloComputation::AcceptOrdered(\n   absl::flat_hash_set<const HloInstruction*> visited;\n   for (const HloInstruction* instruction : order) {\n     VLOG(3) << \"Visiting ordered: \" << instruction->ToString();\n-    TF_RET_CHECK(instruction_indices_.contains(instruction))\n-        << \"Instruction \" << instruction->name() << \" is not in computation \"\n-        << name();\n     TF_RET_CHECK(!visited.contains(instruction))\n         << \"Instruction \" << instruction->name()\n         << \" appears more than once in order\";"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fe32e229b996dd04abfc0c3d60018aa6f5c31bf2",
    "message": "A couple minor bug fixes:\n1. When adding unknown shardings to input parameter tuples, ignore modules with no input parameters.\n2. Skip misaligned sharding checks for outfeed, send and send-done ops.\n\nPiperOrigin-RevId: 621345164",
    "date": "2024-04-02T18:43:48-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e3559b8dc1522c0f5bd8b044b788e2107f71cde3",
    "message": "Fix the minibenchmark unit test.\nCurrently for custom validation case, accuracy validation is not supported, and the BenchmarkResult doesn't have meaning.\n\nPiperOrigin-RevId: 482501534",
    "date": "2022-10-20T08:56:24-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7ee6022b17404a7232c887002ff6eb43d008be6d",
    "message": "Validate null pointer dereference",
    "date": "2023-04-11T15:58:05-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ef070b94c1341986ec8f1469d0104f73c567de05",
    "message": "ScatterNd, BroadcastTo, Reshape and ExpandDims call IsConstantOrPersistentTensor to check if the output tensor should be dynamic.\n\nThis removes unnecessary dynamic tensors from the graph allowing delegates to be applied and may reduce memory usage. It will also mean that Reshape and ExpandDims will always be in-place ops.\n\nPiperOrigin-RevId: 491864928",
    "date": "2022-11-30T03:02:33-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/98293dec27f9f68f932ad658bff82c8fffda5406",
    "message": "removing the check for dtypes",
    "date": "2023-02-21T17:33:30-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9243f0660cc5368da2e1f0c925b2c41c486bca26",
    "message": "Attempt to be less restrictive in FusionCanShareBufferHint().\n\nInitially I added a restriction that a fusion parameter should not be\n(transitively) used by more than one fusion output. I added this restriction\nbecause there was a test that was failing otherwise. After thinking about this\nagain, the real bug was just that I didn't check whether any of the users of the\nfusion output for which we want to see whether it can share the buffer with the\nfusion operand has some non-elementwise user.\n\nPiperOrigin-RevId: 541560719",
    "date": "2023-06-19T02:01:00-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/860538f62384fc4d9427aeac6f20ff7a87bcd21b",
    "message": "Test dynamic_arg_shardings only for '==' equality not also the default pointer equality. Also add tests which checks this behavior and makes sure that we don't fallback to python\n\nPiperOrigin-RevId: 485656967",
    "date": "2022-11-02T12:01:29-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/aba50056eff560a068b29f13f34c5c3f7ab8e3c4",
    "message": "#tf-data-service Explicitly checks if snapshot metadata exists.\n\nThe `GFile` documentation does not mention what exceptions it raises:\nhttps://github.com/tensorflow/tensorflow/blob/38b17d708344a91234dad879794735e79f9af42a/tensorflow/python/platform/gfile.py#L37.\n\nIt would be safer to explicitly check for file existence and return the\nappropriate result.\n\nPiperOrigin-RevId: 623961044",
    "date": "2024-04-11T16:01:43-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/python/data/ops/load_op.py",
            "patches": [
                {
                    "Id": 5,
                    "hunk size": 10,
                    "hunk": "@@ -106,10 +106,12 @@ def _load_distributed_snapshot_metadata(\n     DistributedSnapshotMetadata if the snapshot is a distributed snapshot.\n     Returns None if it is a non-distributed snapshot.\n   \"\"\"\n+  metadata_file = _pywrap_snapshot_utils.TF_DATA_SnapshotMetadataFilePath(path)\n+  if not gfile.Exists(metadata_file):\n+    return None\n+\n   try:\n-    with gfile.GFile(\n-        _pywrap_snapshot_utils.TF_DATA_SnapshotMetadataFilePath(path), \"r\"\n-    ) as f:\n+    with gfile.GFile(metadata_file, \"r\") as f:\n       return text_format.ParseLines(\n           f, snapshot_pb2.DistributedSnapshotMetadata())\n   except (\n"
                },
                {
                    "Id": 6,
                    "hunk size": 8,
                    "hunk": "@@ -149,6 +151,12 @@ def _load_element_spec(path: str) -> Any:\n     NotFoundError if the element spec file does not exist or cannot be decoded.\n   \"\"\"\n   dataset_spec_filename = os.path.join(path, dataset_ops.DATASET_SPEC_FILENAME)\n+  if not gfile.Exists(dataset_spec_filename):\n+    raise errors.NotFoundError(\n+        node_def=None, op=None,\n+        message=\"tf.data snapshot element_spec file not found: \"\n+                f\"{dataset_spec_filename}.\")\n+\n   with gfile.GFile(dataset_spec_filename, \"rb\") as f:\n     encoded_spec = f.read()\n   try:"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a9aa884e1eae90bc473a8c78372d4ce3750af947",
    "message": "[PJRT] Add send/recv callback API where impl does host/device layout rewrites.\n\nThe new API is ExecuteOptions::use_major_to_minor_data_layout_for_callbacks. It\ndefaults to false (the behavior prior to this change), meaning the caller is\nresponsible for rewriting data using PjRtTransferMetadata::device_shape and\nPjRtHostMemoryForDeviceManager. When set to true, the implementation assumes the\nhost is using major-to-minor layout and handles the rewrite internally.\n\nThis change also adds a new method `PjRtClient::SupportsSendRecvCallbacks`.\nPrior to this change, callers would check if\n`PjRtClient::GetPjRtHostMemoryForDeviceManager` returned nullptr. Since callers\nno longer necessarily need the manager to use host callbacks, the more explicit\nmethod is necessary.\n\nFinally, this change makes jax always use the new option.\n\nMost of the complexity in this change is plumbing the option everywhere that\nneeds it, since even though jax always set\nuse_major_to_minor_data_layout_for_callbacks = true now, we still need to\nsupport other callers.\n\nThe motivation for this change is avoiding passing xla::Shapes across the PJRT\nAPI for the sake of the C API. C API users will not have the option of setting\nuse_major_to_minor_data_layout_for_callbacks = false.\nPiperOrigin-RevId: 518733871",
    "date": "2023-03-22T19:29:19-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/63feaf321165e1e2795f43e3834c007364921df6",
    "message": "Add check for raster bits.",
    "date": "2023-09-29T14:53:21+03:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a34c51c30ceb98a02124da2c18301abd1aa76e60",
    "message": "Only unfollow when numbers of strategies are different. Do not check output sharding consistency anymore because when the input and output shardings are consistent, they can be different if input and outputs have different number of dimensions.\n\nPiperOrigin-RevId: 479492433",
    "date": "2022-10-06T22:14:59-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/91b04be7fdac04467d537b81826057f8fc26dea0",
    "message": "[XLA:GPU] Clean up cudnn runtime fusion, but don't enable it.\n\nIn a previous CL I disabled cudnn runtime fusion because we found some convs\nthat weren't working -- cudnn returned 0 algorithms, so you couldn't run the\nconv.  It was unclear at the time why this was happening.\n\nAfter reading the code today, I noticed that kElu convs have a check that was\nmissing in kRelu6 and kLeakyRelu convs.  Namely, kElu fusion skips convs with\nan odd number of input/output features.\n\nWhen I look at the failing testcases we had for kRelu6 and kLeakyRelu, they\nboth had an odd number of features.  Bingo!\n\nUnfortunately, I cannot enable this by default, because it's very slow to\ncompile these convs on Ampere -- 7s with runtime fusion, as opposed to ~50ms\nwithout.  This causes a 100x slowdown in compilation time.\n\nPiperOrigin-RevId: 552642750",
    "date": "2023-07-31T17:58:32-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/cf5c14e10a465a9e6ede1640d08289e7e68be324",
    "message": "Return error on invalid input in `tfl.atan2`\n\nPiperOrigin-RevId: 555461404",
    "date": "2023-08-10T06:13:29-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1668cc3c7371287fdd17988eb0e218c8231fc10f",
    "message": "Fix checkfail in tf.raw_ops.Substr \n\nThe API tf.raw_ops.Substr  currently validates whether the input args pos and len are of same shape or not.Its not checking whether these tensors are empty or not and trying to access the Tensor values directly without validating.If a user passes empty tensors it will lead to assertion failure causing core dumped error.\r\n\r\nMay fixes #63036",
    "date": "2024-02-29T20:37:10+05:30",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/core/kernels/substr_op.cc",
            "patches": [
                {
                    "Id": 7,
                    "hunk size": 9,
                    "hunk": "@@ -56,7 +56,12 @@ class SubstrOp : public OpKernel {\n                 errors::InvalidArgument(\n                     \"pos and len should have the same shape, got: \",\n                     pos_shape.DebugString(), \" vs. \", len_shape.DebugString()));\n-\n+    OP_REQUIRES(context, pos_tensor.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor pos_tensor: \",\n+                                        pos_tensor.DebugString()));\n+    OP_REQUIRES(context, len_tensor.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor len_tensor: \",\n+                                        len_tensor.DebugString()));\n     bool is_scalar = TensorShapeUtils::IsScalar(pos_shape);\n \n     if (is_scalar || input_shape == pos_shape) {"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1b1d9407422ef2a8b6a04c6918d75459de9b6355",
    "message": "#tf-data Add a range check for `index_flat_map`.\n\nWithout this check, it could crash if `index_map_fn` returns an\nout-of-bound index due to:\nhttps://github.com/tensorflow/tensorflow/blob/69a908420c5c5b90027f23905cd842c76ca3955c/tensorflow/core/framework/tensor.cc#L1104\n\nPiperOrigin-RevId: 625476301",
    "date": "2024-04-16T15:58:14-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/core/kernels/data/experimental/index_flat_map_dataset_op.cc",
            "patches": [
                {
                    "Id": 8,
                    "hunk size": 16,
                    "hunk": "@@ -89,15 +89,21 @@ absl::StatusOr<size_t> GetValue(const Tensor& tensor) {\n }\n \n // Returns the `offset`-th element from `tensors`.\n-std::vector<Tensor> GetSlice(const std::vector<Tensor>& tensors,\n-                             size_t offset) {\n+absl::StatusOr<std::vector<Tensor>> GetSlice(const std::vector<Tensor>& tensors,\n+                                             size_t offset) {\n   std::vector<Tensor> result;\n   for (size_t i = 0; i < tensors.size(); ++i) {\n     if (tensors[i].dims() == 0) {  // Scalar.\n       result.push_back(tensors[i]);\n-    } else {\n-      result.push_back(MaybeCopySubSlice(tensors[i], offset));\n+      continue;\n+    }\n+    if (offset > tensors[i].dim_size(0)) {\n+      return absl::InvalidArgumentError(absl::StrCat(\n+          \"`index_flat_map` got invalid `index_map_fn` which returns offset \",\n+          offset, \", but the input element has \", tensors[i].dim_size(0),\n+          \" elements: \", tensors[i].DebugString()));\n     }\n+    result.push_back(MaybeCopySubSlice(tensors[i], offset));\n   }\n   return result;\n }\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/85c7ae4332f54fb6259186e37e774c9b4a9bd6b0",
    "message": "Reenable tsan for distributed snapshots fault tolerance tests.\n\n- Keep track of dead workers in the snapshot manager to ensure stream ownership isn't retained after timeout.\n- Change the orphan test to check for the dead worker's stream having been assigned to any remaining worker (not just the newest one, as tsan causes workers to intermittently timeout).\n- To prevent timeouts, decrease the size of the test datasets and increase the number of shards.\n- Use zero workers in on-disk state recovery tests.  The worker is irrelevant since it's just a test of the dispatcher reading the on-disk state.  And with a worker, there is some bad condition under tsan ~1% of the time, but presumably only when the on-disk state is bad anyway -- will figure this out later.\n\nPiperOrigin-RevId: 516868639",
    "date": "2023-03-15T10:53:55-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/20c05898b344c0fe17fab13cc355b1e94e3864ec",
    "message": "[XLA:GPU] Fix fusion parameter limit again and remove hard check\n\nI removed the hard check, because this seems to break frequently, but it's not really an error.\n\nAlso had to fix some tests, because the fix changes the order of parameters in the fusion.\n\nAlso fixed TensorIterationSpec::IterationSpecFragment::ToString() during the debugging.\n\nPiperOrigin-RevId: 586770496",
    "date": "2023-11-30T13:33:31-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/255d8a4c173e7946885abaac9f12d3b1059ca9b5",
    "message": "Fixes strict subclass check for checkpoint callback field.\n\nPiperOrigin-RevId: 615096147",
    "date": "2024-03-12T11:32:15-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/python/checkpoint/restore.py",
            "patches": [
                {
                    "Id": 9,
                    "hunk size": 14,
                    "hunk": "@@ -113,16 +113,16 @@ class CheckpointPosition(object):\n      callback: Reshard callback for resharding this checkpoint position. Maybe\n        None.\n     \"\"\"\n-    if not isinstance(\n-        self.callback, checkpoint_adapter.ReshardCallback\n-    ):\n-      raise TypeError(\"Cannot override resharding callback.\")\n+    if not issubclass(checkpoint_adapter.ReshardCallback, type(self.callback)):\n+      raise TypeError(\n+          \"Cannot override resharding callback, already set to non trivial.\"\n+      )\n     self.callback = callback\n \n   def has_non_trivial_reshard_callback(self) -> bool:\n     \"\"\"Determine whether this value has a non-trivial resharding callback.\"\"\"\n-    return not isinstance(\n-        self.callback, checkpoint_adapter.ReshardCallback\n+    return not issubclass(\n+        checkpoint_adapter.ReshardCallback, type(self.callback)\n     )\n \n   def is_simple_variable(self) -> bool:\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/4edf32dad2f84cb824a3fe186fc3fa3fd53929be",
    "message": "Add tests to check if pjit handles deleted array inputs gracefully and consistently\n\npjit dispatch paths should check deleted array inputs when attempting to use\nthem. These new tests ensure that various pjit dispatch paths detect and handle\nthem gracefully and consistently.\n\nAdd a check to the PyArray argument handling to make the tests pass.\n\nPiperOrigin-RevId: 492605524",
    "date": "2022-12-02T18:46:52-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/5ed3c7881f1f039b1bb502eb68c65250de3bbac8",
    "message": "Fix ThreadPoolHandle 0 nthreads argument.\n\nIt was reported that a value of 0 leads to a check failure.  Using 0 to indicate\n`port::MaxParallelism`, for consistency with `Dataset`.\n\nFixes #59162\n\nPiperOrigin-RevId: 508092599",
    "date": "2023-02-08T08:56:20-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a0c74d196a21b9d1230820d31e15492ec84d07a6",
    "message": "[PJRT:C] Check PJRT_Api struct_size to detect version incompatibilities.\n\nI just ran into this, and it's very confusing without this check.\n\nThis also moves `CheckMatchingStructSizes` from the wrapper impl to\nthe helper functions file.\n\nPiperOrigin-RevId: 497254775",
    "date": "2022-12-22T15:56:04-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/974f0a2279317cc3a4190559951f7a2cafdde197",
    "message": "[XLA:GPU] Cleanup fusion checks.\n\nCreatesHeavyComputation() check is not needed by fusion merger anymore (calls IsProducerConsumerFusible()) and multi-output fusion (calls IsProducerConsumerMultiOutputFusible()) - now covered by the cost analysis. It might still make sense in instruction fusion which calls IsProducerConsumerFusible() too, so the call to CreatesHeavyComputation() moved there.\n\nPiperOrigin-RevId: 488953965",
    "date": "2022-11-16T09:17:02-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e56206755ff0c98269eb3e50c98fccbaadb6884d",
    "message": "Support quantized i64 during flatbuffer import\n\nSometimes tflite flatbuffer represent `i32` quantized values as `i64`s. In these cases we should truncate down to a lower bit width to avoid creating illegal types.\n\nWe check the bitwidth of the type at load time and pick a power-of-2 bit\nwidth where the value can be safely truncated.\n\nPiperOrigin-RevId: 558958449",
    "date": "2023-08-21T18:50:57-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b7d44fb5b5aeb132bfa6ae700981b0e1ca0609b1",
    "message": "Relax the isolated process check in NNAPI delegate for Android 14 (U) and above\n\nPiperOrigin-RevId: 530432149",
    "date": "2023-05-08T16:06:11-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1b1ebd2b295ae199bddda268d579f4a4cbfccefd",
    "message": "Merge pull request #55677 from PatriceVignola:add-get-input-tensor-from-variable-validation\n\nPiperOrigin-RevId: 481375400",
    "date": "2022-10-15T12:36:48-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/dce9eeac57d02fd34bc3b146a55c583c38cb490c",
    "message": "Adding null pointer checks.\n\nPiperOrigin-RevId: 623312795",
    "date": "2024-04-09T16:43:03-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/hlo_verifier.cc",
            "patches": [
                {
                    "Id": 10,
                    "hunk size": 20,
                    "hunk": "@@ -2010,17 +2010,21 @@ std::string ComputationsToString(\n \n // Verifies various invariants about the structure of the HLO:\n //\n-// (1) each instruction has a non-null parent() set to the HloComputation\n-// which\n-//     contains it.\n+// (1) each instruction is non-null and has a non-null parent() set to the\n+// HloComputation which contains it.\n //\n-// (2) each computation has a non-null parent() set to the HloModule which\n-//     contains it.\n+// (2) each computation is non-null and has a non-null parent() set to the\n+// HloModule which contains it.\n //\n-// (3) the operands of each instruction are in the same computation as the\n-//     instruction.\n+// (3) the operands of each instruction are non-null and are in the same\n+// computation as the instruction.\n Status VerifyHloStructure(HloModule* module) {\n   for (const HloComputation* computation : module->computations()) {\n+    if (computation == nullptr) {\n+      return Internal(\"Computation in module %s is a null pointer\",\n+                      module->name());\n+    }\n+\n     if (computation->parent() == nullptr) {\n       return Internal(\"Computation %s has a null parent pointer\",\n                       computation->name());\n"
                },
                {
                    "Id": 11,
                    "hunk size": 6,
                    "hunk": "@@ -2031,6 +2035,10 @@ Status VerifyHloStructure(HloModule* module) {\n     }\n \n     for (const HloInstruction* instruction : computation->instructions()) {\n+      if (instruction == nullptr) {\n+        return Internal(\"Instruction in computation %s is a null pointer\",\n+                        computation->name());\n+      }\n       if (instruction->parent() == nullptr) {\n         return Internal(\"Instruction %s has a null parent pointer\",\n                         instruction->name());\n"
                },
                {
                    "Id": 12,
                    "hunk size": 13,
                    "hunk": "@@ -2051,6 +2059,17 @@ Status VerifyHloStructure(HloModule* module) {\n     for (const HloInstruction* instruction : computation->instructions()) {\n       for (int i = 0; i < instruction->operand_count(); ++i) {\n         const HloInstruction* operand = instruction->operand(i);\n+        if (operand == nullptr) {\n+          return Internal(\n+              \"Operand %d (out of %d) of instruction: %s is a null pointer\", i,\n+              instruction->operand_count(), instruction->name());\n+        }\n+        if (operand->parent() == nullptr) {\n+          return Internal(\n+              \"Operand %d (out of %d) of instruction: %s has a null pointer \"\n+              \"parent\",\n+              i, instruction->operand_count(), instruction->name());\n+        }\n         if (operand->parent() != instruction->parent()) {\n           return Internal(\n               \"Operand %d (%s) of instruction %s is in a different \""
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fcc97a32a8671814935e735492258089976c4457",
    "message": "Fix RaggedTensor slicing (`__getitem__`) for additional supported value types\n\nWe now check if the data is a RaggedTensor or not, rather than checking if the data is a Tensor. This properly dispatches `__getitem__` to the underlying type rather than sometimes treating it as a RaggedTensor. This previously led to `Ellipsis` not being supported.\n\nPiperOrigin-RevId: 484179745",
    "date": "2022-10-27T01:05:19-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2547314a01549add2dde112279badee2c3c860ae",
    "message": "Add NumElements == 1 check for each input_mins/maxes tensors for QuantizedConcatOp.\nReplace silent OP_REQUIRES in void utility functions to return error Status.\n\nPiperOrigin-RevId: 512623798",
    "date": "2023-02-27T07:46:28-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/00517642a356c5e04f009ea61c74638d89746392",
    "message": "Return error on invalid input in `tfl.splitv`\n\nPiperOrigin-RevId: 555138718",
    "date": "2023-08-09T06:07:32-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/232fffd51f6a8ac6b7ce14944d264827155adcea",
    "message": "Enhanced the future test by checking error strings.\n\nPiperOrigin-RevId: 503470419",
    "date": "2023-01-20T10:22:05-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/25a7a3324ad566c5f3a10be506fb6d3dfddd482c",
    "message": "Move `is_deleted()` to C++ so that we can check if an Array is deleted without materializing `_arrays`.\n\nAlso raise a better error message when doing operations of a deleted Array rather than the current thing which says: `NoneType has no len()`. Now it says: `Array has been deleted`.\n\nPiperOrigin-RevId: 482497114",
    "date": "2022-10-20T08:34:04-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/bbbe095a3ae243d10752ca16b914a8fa11221d07",
    "message": "Fix crash in ragged_cross where the ragged tensor input is invalid\n\nThis PR tries to address the issue raised in 59114 where\nragged_cross will crash when input is invalid.\n\nThis PR fixes 59114.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\n\nAdd test case for GitHub issue 59114.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\n\nAdd additional check to return back immediately\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "date": "2023-02-13T19:40:36+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/105d5f3137d75f873fe3d7ad9aab1cb98356488d",
    "message": "Rollback of PR #58391\nPR #58391: OP_REQUIRES for dtype check in AssignAddVariableOp\n\nImported from GitHub PR https://github.com/tensorflow/tensorflow/pull/58391\n\nFixes https://github.com/tensorflow/tensorflow/issues/58318\nCopybara import of the project:\n\n--\n0dd390280ec46773f859c66d2f835ac2fdc4d977 by bhack <bhack@users.noreply.github.com>:\n\nOP_REQUIRES for dtype check in AssignAddVariableOp\n\nMerging this change closes #58391\n\nPiperOrigin-RevId: 499259827",
    "date": "2023-01-03T10:41:29-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/917c9e33a40cf07725f86700ab3960681a98180c",
    "message": "Add memory kind check in `PjRtArray::Create`.\n\nPiperOrigin-RevId: 566851924",
    "date": "2023-09-19T23:06:27-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/3dc509f31848c7778dc68fabc59ab39c2e0d1e4a",
    "message": "[tflite-gpu] Memory leak in Nvidia driver 525.85.12; disable heap check until resolved.\n\nPiperOrigin-RevId: 528878157",
    "date": "2023-05-02T13:49:26-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/d3ffd32e33f129c2cce47d20adede1c3430cc865",
    "message": "Fix the segfault crash when validating model flatbuffer.\n\nPiperOrigin-RevId: 576995368",
    "date": "2023-10-26T15:01:27-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/920f576f4a20c4e95acc3e140512affb2434a5d0",
    "message": "[XLA:GPU] Improve fusion kernel reuse (runtime and compile time)\n\n1. If multiple arguments have the same buffer, pass it only once and potentially mark the argument as `noalias`. For example this speeds up the runtime of computations where the output buffer is same as the input buffer. Some of our models are sped up by up to 3% (reaching the same speed as before adding kernel reuse). This may make the compile time a bit slower but we didn't encounter that.\n2. Mark parameters noalias if their buffer is not modified in the kernel, even if they overlap. This may provide both runtime and compile time speedup, but we are not sure how many HLOs are affected.\n3. Don't print operand shapes in the fingerprint. It may provide some compilation speedup, and the fingerprints are still unique.\n4. NFC: Remove an unneeded `b_.SetInsertPoint(b_.GetInsertBlock()->getTerminator());`.\n5. NFC: Make checks/logs nicer.\n6. NFC: Remove unneeded \"IrEmitterUnnested::\" from header.\n\nPiperOrigin-RevId: 516500042",
    "date": "2023-03-14T05:54:48-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/cd93d80b50aaf728bb3af862134a9c4b6661e284",
    "message": "Replaced errors::* with absl::* to pass code checks",
    "date": "2024-01-02T13:56:17-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "tensorflow/core/kernels/control_flow_ops.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/6045ccea11b31174d5d9b147fddedf1b5f5af384",
    "message": "Add utility functions to check if a fusion has collective ops inside recursively.\n\nPiperOrigin-RevId: 605777706",
    "date": "2024-02-09T18:00:51-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/collective_ops_utils.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/collective_ops_utils.h",
            "patches": [
                {
                    "Id": 13,
                    "hunk size": 6,
                    "hunk": "@@ -172,6 +172,10 @@ inline constexpr absl::string_view kNopReturnTokenCustomCallTarget =\n // Returns true if instruction is a collective op or a collective fusion.\n bool IsCollective(const HloInstruction* instruction);\n \n+// Returns true if instruction is a collective op (or a collective fusion) with\n+// channel_id.\n+bool IsCollectiveWithChannelId(const HloInstruction* instruction);\n+\n // Returns true if instruction is a synchronous collective op.\n bool IsSyncCollective(const HloInstruction* instr);\n \n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/140d4a5814d62e2640cf376ab1c3ca9c32d9182c",
    "message": "[xla:ffi] NFC: Optimize arguments decoding check\n\nUse `&` instead of `&&` to reduce the number of branches as we don't care about performance of the error handling path.\n\nname                old cpu/op   new cpu/op   delta\nBM_AnyBufferArgX1   13.8ns \u00b111%  13.7ns \u00b1 9%     ~     (p=0.909 n=80+80)\nBM_AnyBufferArgX4   15.2ns \u00b1 8%  15.1ns \u00b1 8%     ~     (p=0.096 n=79+80)\nBM_AnyBufferArgX8   20.0ns \u00b1 3%  17.6ns \u00b1 9%  -11.82%  (p=0.000 n=64+80)\nBM_BufferArgX1      14.3ns \u00b111%  14.3ns \u00b110%     ~     (p=0.604 n=80+80)\nBM_BufferArgX4      16.5ns \u00b1 8%  16.5ns \u00b1 7%     ~     (p=0.289 n=80+80)\nBM_BufferArgX8      24.5ns \u00b1 8%  22.6ns \u00b1 7%   -7.57%  (p=0.000 n=80+80)\nBM_TupleOfI32Attrs  67.9ns \u00b1 1%  67.9ns \u00b1 2%     ~     (p=0.961 n=66+68)\n\nPiperOrigin-RevId: 649197305",
    "date": "2024-07-03T14:28:15-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/ffi/api/api.h",
            "patches": [
                {
                    "Id": 14,
                    "hunk size": 13,
                    "hunk": "@@ -1331,10 +1331,13 @@ class Handler : public Ffi {\n     std::tuple<std::optional<FnArgType<Ts>>...> args = {\n         internal::Decode<Ts>::call(offsets, ctx, diagnostic)...};\n \n-    bool all_decoded = (std::get<Is>(args).has_value() && ...);\n-    if (XLA_FFI_PREDICT_FALSE(!all_decoded)) {\n-      return FailedDecodeError(call_frame, {std::get<Is>(args).has_value()...},\n-                               diagnostic);\n+    if constexpr (sizeof...(Ts) > 0) {\n+      // We intentionally use `&`, as it generates fewer branch instructions.\n+      bool all_decoded = (std::get<Is>(args).has_value() & ...);\n+      if (XLA_FFI_PREDICT_FALSE(!all_decoded)) {\n+        return FailedDecodeError(\n+            call_frame, {std::get<Is>(args).has_value()...}, diagnostic);\n+      }\n     }\n \n     auto result = fn_(std::move(*std::get<Is>(args))...);"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8ac217e9cf2af6f519e3adc6790dfcd9a86440b6",
    "message": "Fixes the Memory Term Reducer to properly handle the case of invalid intervals in the input.\n\nPiperOrigin-RevId: 628413845",
    "date": "2024-04-26T08:56:23-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/hlo/experimental/auto_sharding/auto_sharding_memory.cc",
            "patches": [
                {
                    "Id": 15,
                    "hunk size": 3,
                    "hunk": "@@ -114,6 +114,7 @@ std::pair<int64_t, int64_t> MemoryTermReducer::Reduce(\n   reduced_intervals_.reserve(num_primitives);\n   for (PrimIdx prim_idx = 0; prim_idx < num_primitives; ++prim_idx) {\n     reduced_intervals_.push_back(intervals(prim_idx));\n+    if (!IsValid(reduced_intervals_.back())) continue;\n     num_terms += length(reduced_intervals_.back());\n   }\n \n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/6e153325b66330dafea4e4e8b67b5d56b1a37852",
    "message": "[XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast produces a\nscalar. This avoids crashing within last_dimension when attempting to match.\n\nPiperOrigin-RevId: 548090995",
    "date": "2023-07-14T04:46:03-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b86b0fc59db40956f36e2566e5c6d6bc4221da4e",
    "message": "Rewrite check in SubBuffer() constructor to satisfy ASAN checks\n\nCompare elements count instead of pointers to omit default\nostream<<(signed char*) behaviour and fix undefined behaviour -\npointer should point into the allocated memory or one element past it.",
    "date": "2023-02-02T18:50:19+02:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1a1a381b5be7701843c3f1e34aa1846ae2a1d0ce",
    "message": "Fix a SIGSEGV bug in `InferShapeForXlaGatherOp`\n\nSince `ComputeOutputComponent` may return nullptr, we need to check for null attributes explicitly to be safe.\n\nPiperOrigin-RevId: 601487562",
    "date": "2024-01-25T10:27:28-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/compiler/mlir/tensorflow/transforms/shape_inference.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ae709a6f0780160d9451fa1b81cc25aef2a5d2aa",
    "message": "PR #6377: Fix the check failure where while loop is not rooted on a tuple\n\nImported from GitHub PR https://github.com/openxla/xla/pull/6377\n\nSome while loops might not be rooted on a tuple.\nThis removes the check to assert on such situation.\nAddressed issue: https://github.com/openxla/xla/issues/6353\nCopybara import of the project:\n\n--\nd454d73618f606360f2eff896093045896419c46 by TJ <tjx@nvidia.com>:\n\nFix the check failure where while loop is not rooted on a tuple\n\n--\nc5fccd1f594cf2bf1b54daa60f05e71eecea58e8 by TJ <tjx@nvidia.com>:\n\nremoved redundant code in tests\n\nMerging this change closes #6377\n\nPiperOrigin-RevId: 574847170",
    "date": "2023-10-19T06:37:32-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8cdcf82b1f07d5f76614eb5d30a2c4f2304e35f9",
    "message": "[XLA:GPU] In Multi-output fusion, use DFS to check reachability\n\nCurrently, MultiOutputFusion uses HloReachabilityMap, which builds reachability matrix on creation and element lookup on request. With multi-output fusion rebuilding the reachability after every step, that may take time.\n\nThis change makes MultiOutputFusion to use a simple depth-first search reachability check, bounded by the post-order index. Namely,\n* To build the \"map\", the instructions are sorted in the post order, and indices are stored.\n* On the request, the DFS from the destination node is started, and only goes through nodes which have post-order index >= source node.\n\nWith this approach, the MultiOutputFusion pass of one example HLO module goes from 34 minutes to 4 minutes. There are more advanced data structures that help even more (0.5 minutes), this will be done in the subsequent changes.\n\nPiperOrigin-RevId: 615730140",
    "date": "2024-03-14T04:38:29-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/hlo/ir/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_dfs_reachability.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_dfs_reachability.h",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/multi_output_fusion.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/multi_output_fusion.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/675d73c284396a100555300fe2eab5d02f4bd847",
    "message": "[XLA:GPU] Relaxing shape check on collective ops during setting operand layouts. This was triggering for semantically correct ReduceScatter HLO.\n\nPiperOrigin-RevId: 555169308",
    "date": "2023-08-09T08:26:55-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/397096ed623d97bd9a44dbd4c9ac91f7a1455449",
    "message": "PR #13109: Add flag for termination on nccl error\n\nImported from GitHub PR https://github.com/openxla/xla/pull/13109\n\nThis introduces a flag for termination on NCCL async error. With the flag on, XLA will terminate the process on NCCL error. With the flag off, the existing behavior should remain unchanged.\n\nThe patch is motivated by several problems:\n\n- Without this patch, the heartbeat monitor only checks communicators that are currently not use by the running executable (because it obtains the communicators with TryAcquire). Since NCCL errors cause a hang in the running communicator, most failing communicators are locked, so their async errors just go undetected. As a result, XLA often hangs until Grpc timeout even in cases when ncclCommGetAsyncError would report an error.\n\n- Ideally we would recover by aborting the faulty communicators, but that seems to be unreliable (aborts can cause hangs if NCCL currently hangs on a different communicator than the one being aborted). NCCL team is aware of this and working on a fix (https://github.com/NVIDIA/nccl/issues/1013). At the moment, there does not seem to be a reliable fast recovery mechanism short of process termination.\n\nWe propose to expose a flag for terminating the process on failure so that there is some way to detect and recover from a NCCL failure. Once the comm-abort works reliably, we will use that and propagate the error to the API user.\n\nThe patch is based on a PoC from pshamis@nvidia.com and vincentz@nvidia.com.\nCopybara import of the project:\n\n--\n858aeacb2d689e4b03f4e3bcc0595223119143d5 by Jaroslav Sevcik <jsevcik@nvidia.com>:\n\nAdd flag for termination on nccl error\n\nMerging this change closes #13109\n\nPiperOrigin-RevId: 640085317",
    "date": "2024-06-04T03:40:55-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/debug_options_flags.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/runtime/nccl_clique.cc",
            "patches": [
                {
                    "Id": 16,
                    "hunk size": 9,
                    "hunk": "@@ -180,6 +184,13 @@ static absl::Status CheckComm(NcclApi::NcclCommHandle comm) {\n // Runs async check on all communicators in a clique.\n static void CheckClique(const NcclCliqueKey& clique_key,\n                         NcclClique& lockable_clique) {\n+  if (TerminateOnNcclError()) {\n+    absl::Status status = lockable_clique.CheckAsyncErrors();\n+    if (!status.ok()) {\n+      LOG(FATAL) << \"Terminating process due to async NCCL error: \" << status;\n+    }\n+    return;\n+  }\n   if (NcclClique::Lock clique = lockable_clique.TryAcquire()) {\n     VLOG(5) << \"Checking NCCL clique \" << clique_key.ToString()\n             << \" for async errors; num_communicators=\"\n"
                },
                {
                    "Id": 17,
                    "hunk size": 20,
                    "hunk": "@@ -510,4 +521,22 @@ absl::StatusOr<std::shared_ptr<NcclClique::Lock>> AcquireNcclClique(\n                               num_local_participants, rank, config);\n }\n \n+absl::Status NcclClique::CheckAsyncErrors() {\n+  return async_error_checker_.Check();\n+}\n+\n+absl::Status NcclCliqueCommunicators::AsyncErrorChecker::Check() {\n+  absl::Status status = absl::OkStatus();\n+  communicators_.ForEachComm(\n+      [&status](int32_t rank, NcclApi::NcclCommHandle comm) {\n+        // Do not overwrite previous errors.\n+        if (!status.ok()) return;\n+        status = NcclApi::Default()->CommGetAsyncError(comm);\n+        if (!status.ok()) {\n+          LOG(ERROR) << \"NCCL async error (rank \" << rank << \"): \" << status;\n+        }\n+      });\n+  return status;\n+}\n+\n }  // namespace xla::gpu\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/gpu/runtime/nccl_clique.h",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/xla.proto",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/558192ef2a0945b8149e646fd462b441a9ff3574",
    "message": "Improve LiteralTestUtil support for dynamic literals. Previously, the utility would check every element of each dynamic literal against one another, even if those elements were beyond the specified dynamic bound. Now the LiteralTestUtil only checks up until the maximum index specified by the Literal's dynamic dimension.\n\nA previous fix was added for LiteralTestUtil::Equal. This is the same change but for LiteralTestUtil::Near.\n\nPiperOrigin-RevId: 474915396",
    "date": "2022-09-16T15:36:35-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1e40c1b0f8b6154db39dcd38767286ab307a0686",
    "message": "[XLA] Make num_devices optional in sharding validation\n- This will allow validating sharding even if num_devices is not known\n- Also fix validation to use \"devices\" instead of \"core\" in error messages\n- When num_devices is known, check that all devices are listed in the sharding.\n\nPiperOrigin-RevId: 485916872",
    "date": "2022-11-03T10:55:57-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fbc02b521b121bb8f080d4ade5d543ff7594caa7",
    "message": "Arena planner optimizations\n\nCache num_tensors, return status without checking as it is checked by caller and reserve vector.\n\nPiperOrigin-RevId: 471585854",
    "date": "2022-09-01T11:36:39-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/82481f6e967b0523165067687384395570712236",
    "message": "Add minimum supported version checking to XlaCallModuleOp.\n\nAlso stop supporting version 1 (no StableHLO).\n\nPiperOrigin-RevId: 515147273",
    "date": "2023-03-08T14:53:13-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/aee99960dc090f7f9f18fa37e889689eede0aab8",
    "message": "Fix shape mismatch when handling reshape in SPMD partitioner.\n\n1. Move shape check for reshape from `HloInstruction::CreateShape` into the constructor, so that illegal reshape is caught at creation time from clone.\n2. Enhance `hlo_sharding_util::ReshapeSharding`. It can handle the cases where\n* `source_shape_size % source_tile_size == 0`\n* `source_shape_size % target_shape_size == 0`\n* `(source_tile_size % target_shape_size != 0) || (target_shape_size % source_tile_size != 0)`\n3. Enhance `SpmdPartitioningVisitor::HandleReshape`. Unify the two cases in one lambda function. It attempts the original output sharding and then the desired output sharding.\n\nPiperOrigin-RevId: 631996882",
    "date": "2024-05-08T19:35:34-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_instruction.cc",
            "patches": [
                {
                    "Id": 18,
                    "hunk size": 8,
                    "hunk": "@@ -2097,12 +2097,6 @@ HloInstruction::CreateBroadcastSequence(\n \n /* static */ std::unique_ptr<HloInstruction> HloInstruction::CreateReshape(\n     const Shape& shape, HloInstruction* operand, int64_t inferred_dimension) {\n-  CHECK(operand->shape().is_unbounded_dynamic() ||\n-        ShapeUtil::StaticExtentProduct(shape) ==\n-            ShapeUtil::StaticExtentProduct(operand->shape()))\n-      << \"shape: \" << ShapeUtil::HumanString(shape)\n-      << \" operand: \" << ShapeUtil::HumanString(operand->shape());\n-\n   return std::make_unique<HloReshapeInstruction>(shape, operand,\n                                                  inferred_dimension);\n }\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_instructions.cc",
            "patches": [
                {
                    "Id": 19,
                    "hunk size": 7,
                    "hunk": "@@ -1555,6 +1555,11 @@ HloReshapeInstruction::HloReshapeInstruction(const Shape& shape,\n                                              int64_t inferred_dimension)\n     : HloInstruction(HloOpcode::kReshape, shape),\n       inferred_dimension_(inferred_dimension) {\n+  CHECK(operand->shape().is_unbounded_dynamic() ||\n+        ShapeUtil::StaticExtentProduct(shape) ==\n+            ShapeUtil::StaticExtentProduct(operand->shape()))\n+      << \"shape: \" << ShapeUtil::HumanString(shape)\n+      << \" operand: \" << ShapeUtil::HumanString(operand->shape());\n   AppendOperand(operand);\n }\n \n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/hlo/utils/hlo_sharding_util.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/61ad42acb4fa48e2fc6140a4fad41aa8ad85ea52",
    "message": "Merge int16 EXP zero-point check into the else if",
    "date": "2022-12-06T10:46:53+00:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/43fd10302bcc8447e7a7205bae848a3a88624775",
    "message": "Return error on invalid input in `tfl.atan2_custom`\n\nPiperOrigin-RevId: 556797683",
    "date": "2023-08-14T08:27:11-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/788486f6c3c5ae651cf0cb409c5d85286d2f61ce",
    "message": "Fix NCCL and CUDA definition check.\n\nPiperOrigin-RevId: 596612409",
    "date": "2024-01-08T09:15:22-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_driver.cc",
            "patches": [
                {
                    "Id": 20,
                    "hunk size": 6,
                    "hunk": "@@ -57,9 +57,9 @@ limitations under the License.\n #include \"tsl/platform/statusor.h\"\n #include \"tsl/platform/threadpool.h\"\n \n-#ifdef XLA_ENABLE_XCCL\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n #include \"third_party/nccl/nccl.h\"\n-#endif  // XLA_ENABLE_XCCL\n+#endif  // XLA_ENABLE_XCCL && XLA_ENABLE_CUDA\n \n static constexpr bool FLAGS_gpuexec_cuda_driver_inject_init_error = false;\n static constexpr bool FLAGS_gpuexec_cuda_sync_around_driver_calls = false;\n"
                },
                {
                    "Id": 21,
                    "hunk size": 4,
                    "hunk": "@@ -1636,7 +1636,7 @@ struct BitPatternToValue {\n   ScopedActivateContext activated{context};\n   void* ptr = nullptr;\n \n-#ifdef XLA_ENABLE_XCCL\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n   ncclResult_t res = ncclMemAlloc(&ptr, bytes);\n   if (res != ncclSuccess) {\n     return absl::InternalError(absl::StrFormat(\n"
                },
                {
                    "Id": 22,
                    "hunk size": 4,
                    "hunk": "@@ -1659,7 +1659,7 @@ struct BitPatternToValue {\n     GpuContext* context, void* location) {\n   ScopedActivateContext activation(context);\n \n-#ifdef XLA_ENABLE_XCCL\n+#if defined(XLA_ENABLE_XCCL) && defined(XLA_ENABLE_CUDA)\n   ncclResult_t res = ncclMemFree(location);\n   if (res != ncclSuccess) {\n     return absl::InternalError(absl::StrFormat("
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7186b5b24a616a4ad44c703677a7a74c10ae8b64",
    "message": "diag_rank validation in MatrixDiagV3\n\nSimplified diag_rank validation in MatrixDiagV3.",
    "date": "2024-04-15T09:58:31+05:30",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/core/kernels/linalg/matrix_diag_op.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c3b618f959f8afcec8c42a6ab9639c047e61c518",
    "message": "Move optional checking for creating GpuTimers into callsites to remove convenience method.\n\nPiperOrigin-RevId: 653681508",
    "date": "2024-07-18T11:14:30-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_blas.cc",
            "patches": [
                {
                    "Id": 23,
                    "hunk size": 14,
                    "hunk": "@@ -718,12 +718,12 @@ absl::Status CUDABlas::DoBlasGemmWithAlgorithm(\n       cublasMath_t math_type,\n       GetMathTypeForGemmEx(stream, algorithm, type_a, type_b, numeric_options));\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          output_profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   // Since we are converting 'algorithm' to cublasGemmAlgo_t by static_cast,\n   // we do the following compile-time check on the default value:\n"
                },
                {
                    "Id": 24,
                    "hunk size": 14,
                    "hunk": "@@ -753,12 +753,12 @@ absl::Status CUDABlas::DoBlasGemmStridedBatchedWithAlgorithm(\n   TF_ASSIGN_OR_RETURN(\n       cublasMath_t math_type,\n       GetMathTypeForGemmEx(stream, algorithm, type_a, type_b, numeric_options));\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          output_profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n   cudaDataType_t cuda_in_type = AsCudaDataType(type_a);\n \n #if CUDA_VERSION >= 11000\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_blas_lt.cc",
            "patches": [
                {
                    "Id": 25,
                    "hunk size": 13,
                    "hunk": "@@ -406,11 +406,12 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n     std::optional<DeviceMemoryBase> workspace,\n     std::optional<ScratchAllocator*> scratch_allocator,\n     blas::ProfileResult* profile_result = nullptr) const {\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<gpu::GpuTimer> timer,\n-      gpu::GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n+  std::optional<gpu::GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        gpu::GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   void* workspace_addr;\n   uint64_t workspace_size = 0;\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "patches": [
                {
                    "Id": 26,
                    "hunk size": 15,
                    "hunk": "@@ -2306,13 +2306,12 @@ absl::Status CudnnSupport::DoRnnForwardImpl(\n       stream, cudnn, rnn_desc, model_dims, input_desc, workspace_allocator,\n       reserve_space_allocator, is_training, &workspace, &reserve_space));\n \n-  const bool is_profiling = output_profile_result != nullptr;\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   if (input_desc.is_var_seq_lengths()) {\n     // In CUDNN v8, the cudnnRNNForward*** and cudnnRNNForward***Ex have been\n"
                },
                {
                    "Id": 27,
                    "hunk size": 4,
                    "hunk": "@@ -2409,7 +2408,7 @@ absl::Status CudnnSupport::DoRnnForwardImpl(\n #endif  // CUDNN_VERSION >= 90000\n   }\n \n-  if (is_profiling) {\n+  if (timer.has_value()) {\n     TF_RETURN_IF_ERROR(PopulateProfileFromTimer(\n         timer, *rnn_desc.algorithm_config().algorithm(),\n         output_profile_result));\n"
                },
                {
                    "Id": 28,
                    "hunk size": 15,
                    "hunk": "@@ -2460,13 +2459,12 @@ absl::Status CudnnSupport::DoRnnBackwardImpl(\n                                         input_desc, workspace_allocator,\n                                         nullptr, true, &workspace, nullptr));\n \n-  const bool is_profiling = output_profile_result != nullptr;\n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  std::optional<GpuTimer> timer;\n+  if (output_profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   if (input_desc.is_var_seq_lengths()) {\n     // In CUDNN v8, the cudnnRNNBackward*** and cudnnRNNBackward***Ex have\n"
                },
                {
                    "Id": 29,
                    "hunk size": 4,
                    "hunk": "@@ -2604,7 +2602,7 @@ absl::Status CudnnSupport::DoRnnBackwardImpl(\n #endif  // CUDNN_VERSION >= 90000\n   }\n \n-  if (is_profiling) {\n+  if (timer.has_value()) {\n     TF_RETURN_IF_ERROR(PopulateProfileFromTimer(\n         timer, *rnn_desc.algorithm_config().algorithm(),\n         output_profile_result));\n"
                },
                {
                    "Id": 30,
                    "hunk size": 15,
                    "hunk": "@@ -5589,12 +5587,13 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n                      ? static_cast<void*>(&dbeta)\n                      : static_cast<void*>(&fbeta);\n \n-    const bool is_profiling = profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            is_profiling));\n+    std::optional<GpuTimer> timer = std::nullopt;\n+\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n \n     const auto get_fwd_bugs = [&]() -> absl::Status {\n #if CUDNN_VERSION < 8000\n"
                },
                {
                    "Id": 31,
                    "hunk size": 4,
                    "hunk": "@@ -5671,7 +5670,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n                                      static_cast<int>(kind_));\n     }\n \n-    if (is_profiling) {\n+    if (timer.has_value()) {\n       TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer, algo, profile_result,\n                                                   scratch_memory.size()));\n     }\n"
                },
                {
                    "Id": 32,
                    "hunk size": 16,
                    "hunk": "@@ -6035,18 +6034,18 @@ class CudnnExecutionPlanRunner<void(Args...)>\n             << \"\\nWorkspace size in bytes: \" << workspace_size\n             << \"\\nVariantPack: \" << variantPack.describe();\n \n-    const bool is_profiling = profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            is_profiling));\n+    std::optional<GpuTimer> timer = std::nullopt;\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n \n     cudnnStatus_t status = cudnnBackendExecute(\n         cudnn.handle(), plan_.get_raw_desc(), variantPack.get_raw_desc());\n     RETURN_IF_CUDNN_ERROR(status);\n \n-    if (is_profiling) {\n+    if (timer.has_value()) {\n       TF_ASSIGN_OR_RETURN(auto desc, ToAlgorithmDesc());\n       TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer, desc, profile_result,\n                                                   scratch_memory.size()));\n"
                },
                {
                    "Id": 33,
                    "hunk size": 13,
                    "hunk": "@@ -6616,12 +6615,13 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n     }\n \n     auto algo = MakeAlgorithmDesc();\n+    std::optional<GpuTimer> timer = std::nullopt;\n \n-    TF_ASSIGN_OR_RETURN(\n-        std::optional<GpuTimer> timer,\n-        GpuTimer::CreateIfNeeded(\n-            stream, profile_result && profile_result->warmup_run_executed(),\n-            profile_result != nullptr));\n+    if (profile_result != nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer,\n+          GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+    }\n     auto side_input_data_ptr = (side_input_scale_ == 0)\n                                    ? output_data.opaque()\n                                    : side_input_data.opaque();\n"
                },
                {
                    "Id": 34,
                    "hunk size": 4,
                    "hunk": "@@ -6675,7 +6675,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n     }\n     RETURN_IF_CUDNN_ERROR(status);\n \n-    if (profile_result) {\n+    if (timer.has_value()) {\n       TF_RETURN_IF_ERROR(PopulateProfileFromTimer(timer, algo, profile_result,\n                                                   scratch_memory.size()));\n       VLOG(4) << \"conv with algorithm \" << ToConvForwardAlgo(algo)\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer.cc",
            "patches": [
                {
                    "Id": 35,
                    "hunk size": 11,
                    "hunk": "@@ -119,15 +119,6 @@ bool ShouldLaunchDelayKernel() {\n                                   stop_event,     stream, std::move(semaphore)};\n }\n \n-/*static*/ absl::StatusOr<std::optional<GpuTimer>> GpuTimer::CreateIfNeeded(\n-    Stream* stream, bool use_delay_kernel, bool is_needed) {\n-  if (is_needed) {\n-    TF_ASSIGN_OR_RETURN(GpuTimer t, GpuTimer::Create(stream, use_delay_kernel));\n-    return {std::make_optional(std::move(t))};\n-  }\n-  return std::nullopt;\n-}\n-\n /*static*/ void GpuTimer::ReturnRandomDurationsForTesting() {\n   return_random_durations = true;\n }\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer.h",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/rocm/hip_blas_lt.cc",
            "patches": [
                {
                    "Id": 36,
                    "hunk size": 13,
                    "hunk": "@@ -395,12 +395,13 @@ absl::Status BlasLt::MatmulPlan::DoMatmul(\n       blas_lt_ref_.parent_->RecordApiTrace(StreamExecutor::GemmCallTrace{\n           StreamExecutor::GemmCallTrace::GemmType::kBlasLt, 0, a.size(),\n           b.size()});\n+  std::optional<gpu::GpuTimer> timer = std::nullopt;\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<gpu::GpuTimer> timer,\n-      gpu::GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result));\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        gpu::GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   void* workspace_addr = nullptr;\n   uint64_t workspace_size = 0;\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/stream_executor/rocm/rocm_blas.cc",
            "patches": [
                {
                    "Id": 37,
                    "hunk size": 12,
                    "hunk": "@@ -544,11 +544,11 @@ absl::Status ROCMBlas::DoBlasGemmWithAlgorithm(\n         \"datatypes for the inputs a (%d) and b (%d) are unsupported\",\n         static_cast<int>(type_a), static_cast<int>(type_b)));\n   }\n-  TF_ASSIGN_OR_RETURN(\n-      auto timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer, GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   // fall back to the default implementation\n   if (algorithm == blas::kDefaultAlgorithm && type_a == type_c) {\n"
                },
                {
                    "Id": 38,
                    "hunk size": 12,
                    "hunk": "@@ -605,11 +605,11 @@ absl::Status ROCMBlas::DoBlasGemmStridedBatchedWithAlgorithm(\n         \"datatypes for the inputs a (%d) and b (%d) are unsupported\",\n         static_cast<int>(type_a), static_cast<int>(type_b)));\n   }\n-  TF_ASSIGN_OR_RETURN(\n-      auto timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream, profile_result && profile_result->warmup_run_executed(),\n-          profile_result != nullptr));\n+  std::optional<GpuTimer> timer = std::nullopt;\n+  if (profile_result != nullptr) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer, GpuTimer::Create(stream, profile_result->warmup_run_executed()));\n+  }\n \n   // fall back to the default implementation\n   if (algorithm == blas::kDefaultAlgorithm && type_a == type_c) {\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/stream_executor/rocm/rocm_dnn.cc",
            "patches": [
                {
                    "Id": 39,
                    "hunk size": 14,
                    "hunk": "@@ -2494,13 +2494,13 @@ absl::Status MIOpenSupport::DoRnnForwardImpl(\n   }\n \n   const bool is_profiling = output_profile_result != nullptr;\n+  std::optional<GpuTimer> timer = std::nullopt;\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  if (is_profiling) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   // make the forward call\n   if (!is_training) {\n"
                },
                {
                    "Id": 40,
                    "hunk size": 14,
                    "hunk": "@@ -2626,13 +2626,13 @@ absl::Status MIOpenSupport::DoRnnBackwardImpl(\n         stream->MemZero(input_c_backprop_data, size_data * type_size));\n \n   const bool is_profiling = output_profile_result != nullptr;\n+  std::optional<GpuTimer> timer = std::nullopt;\n \n-  TF_ASSIGN_OR_RETURN(\n-      std::optional<GpuTimer> timer,\n-      GpuTimer::CreateIfNeeded(\n-          stream,\n-          output_profile_result && output_profile_result->warmup_run_executed(),\n-          is_profiling));\n+  if (is_profiling) {\n+    TF_ASSIGN_OR_RETURN(\n+        timer,\n+        GpuTimer::Create(stream, output_profile_result->warmup_run_executed()));\n+  }\n \n   // make the backward data call\n   auto status = wrap::miopenRNNBackwardData(\n"
                },
                {
                    "Id": 41,
                    "hunk size": 14,
                    "hunk": "@@ -3326,12 +3326,12 @@ class RocmConvRunner : public dnn::ConvRunner {\n     float beta = 0.0;\n \n     const bool is_profiling = output_profile_result != nullptr;\n-    TF_ASSIGN_OR_RETURN(std::optional<GpuTimer> timer,\n-                        GpuTimer::CreateIfNeeded(\n-                            stream,\n-                            output_profile_result &&\n-                                output_profile_result->warmup_run_executed(),\n-                            is_profiling));\n+    std::optional<GpuTimer> timer = std::nullopt;\n+    if (is_profiling) {\n+      TF_ASSIGN_OR_RETURN(\n+          timer, GpuTimer::Create(\n+                     stream, output_profile_result->warmup_run_executed()));\n+    }\n \n     miopenStatus_t status = miopenStatusSuccess;\n     switch (kind_) {"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e41bd3f8ddf9a85356f10977b432afb4b4f6d6a6",
    "message": "Explicitly check if the input module to auto-sharding uses manual sharding, and crash if so as we currently do not support handling such modules.\n\nPiperOrigin-RevId: 565712685",
    "date": "2023-09-15T10:32:48-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8dcc7fc1fb1aa0274bed7c401a9f2bc344f16609",
    "message": "Add validation to check the order of xspace paths and preload xplane.\n\nPiperOrigin-RevId: 542298563",
    "date": "2023-06-21T10:43:39-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c17d13a42c578fc1038af42fb2170a9f9a312347",
    "message": "[NFC] Add variant of IsCustomCall that checks one of several targets.\n\nPiperOrigin-RevId: 472608375",
    "date": "2022-09-06T18:54:03-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ad2702d2ea290544ea4b72b9aded9b56e2ee8853",
    "message": "PR #4528: [NVIDIA XLA:GPU] Enable reduction epilogue fusion for some ops\n\nImported from GitHub PR https://github.com/openxla/xla/pull/4528\n\nEnable reduction epilogue fusion for some ops, convert, bitcast, reshape that's actually a bitcast.\nBefore enabling\nother_ops -> reduce -> convert\nwould become fusion_kernel->convert\nafter enabling\nIt will become a single fusion kernel.\nWe will progressively enable this for more elementwise ops with more benchmarking.\nCopybara import of the project:\n\n--\n11730ca050fdc4bd60f1b8b0e760090415682cd6 by TJ <tjx@nvidia.com>:\n\nEnable reduction epilogue fusion for some ops\n\n--\nea6ba4d7df79066d231b1d18bea20ecc8f579fae by TJ <tjx@nvidia.com>:\n\nremoved kcopy from reduction consumer check\n\n--\nd583df0196e74a67c8eda0d4ebca25b187beb5f6 by TJ <tjx@nvidia.com>:\n\nadded codegen support for reduction epilogue fusion\n\n--\n7aae75c86013dfef4332b2db7bbbf4c411187f0e by TJ <tjx@nvidia.com>:\n\nexplicitly check for reduce op in hlo fusion analysis when emitting kind\n\n--\n421549b62fde13e7f6d1e4301c3e9da5351da3e6 by TJ <tjx@nvidia.com>:\n\nfixed parallel reduction test failure\naddressed pr comments\n\nMerging this change closes #4528\n\nPiperOrigin-RevId: 554742300",
    "date": "2023-08-08T01:54:45-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fc640c7c9ef550ea27d1ba417ea5febdf97fcff2",
    "message": "[PJRT C API] Add a version check for GetCompiledMemoryStats to return an error instead of crashing for old plugins without this method.\n\nPiperOrigin-RevId: 609560110",
    "date": "2024-02-22T18:04:33-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/pjrt/c/pjrt_c_api_helpers.cc",
            "patches": [
                {
                    "Id": 42,
                    "hunk size": 9,
                    "hunk": "@@ -944,6 +944,13 @@ absl::Span<PJRT_DeviceDescription* const> DeviceDescriptions(\n \n absl::StatusOr<xla::CompiledMemoryStats> GetCompiledMemoryStats(\n     const PJRT_Api* api, PJRT_Executable* executable) {\n+  // TODO(jieying): To be removed after 03/2024.\n+  if (api->pjrt_api_version.major_version == 0 &&\n+      api->pjrt_api_version.minor_version < 40) {\n+    return absl::UnimplementedError(\n+        \"GetCompiledMemoryStats requires a plugin with PJRT C API version >= \"\n+        \"0.40\");\n+  }\n   PJRT_Executable_GetCompiledMemoryStats_Args args;\n   args.struct_size = PJRT_Executable_GetCompiledMemoryStats_Args_STRUCT_SIZE;\n   args.extension_start = nullptr;"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7ad5a7612918285ddca49633d63c462262bd3dc9",
    "message": "Fix SegFault in Python InterpreterWrapper\n\nIf `InterpreterWrapper::TensorSparsityParameters` encounters Tensors which do not have a `block_map`, a `nullptr` is dereferenced causing AccViol/SegFault.\n\nAdd a check for `nullptr`.",
    "date": "2024-03-09T09:28:26-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc",
            "patches": [
                {
                    "Id": 43,
                    "hunk size": 10,
                    "hunk": "@@ -135,9 +135,11 @@ PyObject* PyDictFromSparsityParam(const TfLiteSparsity& param) {\n   PyDict_SetItemString(result, \"traversal_order\",\n                        PyArrayFromIntVector(param.traversal_order->data,\n                                             param.traversal_order->size));\n-  PyDict_SetItemString(\n-      result, \"block_map\",\n-      PyArrayFromIntVector(param.block_map->data, param.block_map->size));\n+  if (param.block_map != nullptr) {\n+    PyDict_SetItemString(\n+        result, \"block_map\",\n+        PyArrayFromIntVector(param.block_map->data, param.block_map->size));\n+  }\n   PyObject* dim_metadata = PyList_New(param.dim_metadata_size);\n   for (int i = 0; i < param.dim_metadata_size; i++) {\n     PyObject* dim_metadata_i = PyDict_New();"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/512f4b101762dba010e75f93b84cb096b075992c",
    "message": "Update `CalibrationWrapper` to support models larger than 2GB using offset buffers.\n\nAdd implementations to handle the case where the calibration wrapper accepts a TFLite flatbuffer model that uses offset buffers.\nOffset buffers are enabled by the TFLite converter's `_experimental_use_buffer_offset` flag.\nWhenever the model's base buffer (that contains the serialized flatbuffer) is modified, the offset values to the offset buffer of the tensor buffers should be updated.\n\nPiperOrigin-RevId: 561251067",
    "date": "2023-08-30T00:10:55-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/bb9b9f32bd08bc5660343cdc09f4b467d7297e3c",
    "message": "Change a recently introduced DCHECK from O(N) cost to O(1).\n\nA recent change added a DCHECK that took O(N) time for an\ninstruction with N users every time a user was added. This\nbecame quadratic and caused timeouts in some tests that\nenable assertion checking. Made the DCHECK O(1) to fix it.\n\nPiperOrigin-RevId: 595834589",
    "date": "2024-01-04T17:05:02-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_instruction.cc",
            "patches": [
                {
                    "Id": 44,
                    "hunk size": 16,
                    "hunk": "@@ -206,17 +206,9 @@ void HloInstruction::Users::RebuildMap() {\n \n bool HloInstruction::Users::CheckInvariants() {\n   if (user_map_ != nullptr) {\n-    int64_t index = 0;\n-    for (const HloInstruction* u : users_) {\n-      CHECK(user_map_->contains(u));\n-      CHECK_EQ((*user_map_)[u], index);\n-      index++;\n-    }\n-    for (auto [u, index] : *user_map_) {\n-      CHECK_GE(index, 0);\n-      CHECK_LT(index, users_.size());\n-      CHECK_EQ(users_[index], u);\n-    }\n+    // Avoid quadratic behavior by doing a quick and dirty check on\n+    // size instead of actually comparing mapped indices.\n+    CHECK_EQ(users_.size(), user_map_->size());\n   }\n   return true;\n }"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/63410d7f0c444d9081ff674bef024cfbfc939027",
    "message": "Minor fix: Support q/dq ops after a weight when checking if the weight is constant\n\nPiperOrigin-RevId: 490665558",
    "date": "2022-11-23T23:56:06-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/5fc08bdd5eef1e5a42d97b10c53536c6cdaf5425",
    "message": "Add a capacity check when scheduling a low priority task\n\nPiperOrigin-RevId: 617961589",
    "date": "2024-03-21T14:41:30-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "tensorflow/core/kernels/batching_util/BUILD",
            "patches": []
        },
        {
            "path": "tensorflow/core/kernels/batching_util/batch_resource_base.cc",
            "patches": [
                {
                    "Id": 45,
                    "hunk size": 9,
                    "hunk": "@@ -532,6 +532,13 @@ BatchResourceBase::GetBatcherQueueOptions(\n       low_priority_max_enqueued_batches;\n   batcher_queue_options.low_priority_queue_options.batch_timeout_micros =\n       low_priority_batch_timeout_micros;\n+  if (low_priority_allowed_batch_sizes.empty()) {\n+    batcher_queue_options.low_priority_queue_options.max_execution_batch_size =\n+        low_priority_max_batch_size;\n+  } else {\n+    batcher_queue_options.low_priority_queue_options.max_execution_batch_size =\n+        *low_priority_allowed_batch_sizes.rbegin();\n+  }\n   batcher_queue_options.enable_large_batch_splitting =\n       enable_large_batch_splitting;\n   if (enable_large_batch_splitting) {\n"
                },
                {
                    "Id": 46,
                    "hunk size": 10,
                    "hunk": "@@ -554,14 +561,6 @@ BatchResourceBase::GetBatcherQueueOptions(\n           .max_execution_batch_size = *allowed_batch_sizes.rbegin();\n       batcher_queue_options.allowed_batch_sizes = allowed_batch_sizes;\n     }\n-    if (low_priority_allowed_batch_sizes.empty()) {\n-      batcher_queue_options.low_priority_queue_options\n-          .max_execution_batch_size = low_priority_max_batch_size;\n-    } else {\n-      batcher_queue_options.low_priority_queue_options\n-          .max_execution_batch_size =\n-          *low_priority_allowed_batch_sizes.rbegin();\n-    }\n   }\n   batcher_queue_options.disable_padding = disable_padding;\n \n"
                }
            ]
        },
        {
            "path": "tensorflow/core/kernels/batching_util/shared_batch_scheduler.h",
            "patches": [
                {
                    "Id": 47,
                    "hunk size": 11,
                    "hunk": "@@ -475,6 +476,15 @@ class Queue {\n   Status ValidateBatchTaskQueueCapacity(TaskType* task) const\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n+  // Returns an error if the low priority task queue doesn't have capacity for\n+  // this task using the low priority batch options. Since the low priority\n+  // tasks are not batched until they get scheduled, it only checks that a\n+  // single task does not it exceed input batch size limit and the total size of\n+  // the tasks in the queue does not exceed the max batch size * max enqueued\n+  // batch sizes.\n+  Status ValidateLowPriorityTaskQueueCapacity(const TaskType& task) const\n+      TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n+\n   // The task size of the last batch in the queue.\n   size_t tail_batch_task_size() const TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n \n"
                },
                {
                    "Id": 48,
                    "hunk size": 7,
                    "hunk": "@@ -884,11 +894,6 @@ Queue<TaskType>::~Queue() {\n \n template <typename TaskType>\n Status Queue<TaskType>::Schedule(std::unique_ptr<TaskType>* task) {\n-  if ((*task)->size() > options_.input_batch_size_limit) {\n-    return errors::InvalidArgument(\"Task size \", (*task)->size(),\n-                                   \" is larger than maximum input batch size \",\n-                                   options_.input_batch_size_limit);\n-  }\n   if (options_.enable_lazy_split) {\n     return ScheduleWithLazySplit(std::move(task));\n   }\n"
                },
                {
                    "Id": 49,
                    "hunk size": 3,
                    "hunk": "@@ -1048,6 +1053,7 @@ Status Queue<TaskType>::ScheduleWithoutOrEagerSplit(\n     if (IsLowPriorityTask(task)) {\n       // Insert the task to the low priority task queue instead of the high\n       // priority batch queue below.\n+      TF_RETURN_IF_ERROR(ValidateLowPriorityTaskQueueCapacity(**task));\n       low_priority_tasks_.AddTask(std::move(*task), env_->NowMicros());\n     } else {\n       TF_RETURN_IF_ERROR(ScheduleWithoutOrEagerSplitImpl(task));\n"
                },
                {
                    "Id": 50,
                    "hunk size": 10,
                    "hunk": "@@ -1109,6 +1115,14 @@ size_t Queue<TaskType>::SchedulingCapacityInternal() const {\n \n template <typename TaskType>\n Status Queue<TaskType>::ValidateBatchTaskQueueCapacity(TaskType* task) const {\n+  // Check if the task size is larger than the batch size limit, regardless of\n+  // the batch capacity.\n+  if (task->size() > options_.input_batch_size_limit) {\n+    return absl::InvalidArgumentError(absl::StrFormat(\n+        \"Task size %d is larger than maximum input batch size %d\", task->size(),\n+        options_.input_batch_size_limit));\n+  }\n+\n   // Queue creation requires that `enable_large_batch_splitting` is true\n   // when `enable_lazy_split` is true, so this covers both eager split and\n   // lazy split.\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c0a3117d09daf89717441cb6b302439486732500",
    "message": "Adjust FindNonTrivialHero check for reductions.\n\nFor reductions, we don't need to check whether there is a non-elementwise\ndirect or indirect user. We only allow unary ops in the reduction epilogue, so\nany non-elementwise user would be coming from a different fusion root. In that\ncase, the reduction would be code generated as loop.\n\nPiperOrigin-RevId: 600410667",
    "date": "2024-01-22T03:55:32-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "patches": [
                {
                    "Id": 51,
                    "hunk size": 16,
                    "hunk": "@@ -1051,6 +1051,20 @@ static std::optional<HloInstructionAdaptor> FindTransposeHero(\n     return TraversalResult::kAdvance;\n   };\n   HloBfsConsumersFirstTraversal({root}, fusion, visit);\n+  if (transpose) {\n+    // Make sure that no non-elementwise op is reachable from the transpose.\n+    auto visit = [](HloInstructionAdaptor node) {\n+      return node.instruction().opcode() != HloOpcode::kTuple &&\n+             node.instruction().opcode() != HloOpcode::kParameter &&\n+             !IsIntermediate(&node.instruction(),\n+                             /*allowed_operand_count=*/3);\n+    };\n+    bool has_nontrivial_user = HloAnyOf(transpose->GetUsers(), fusion, visit,\n+                                        /*visit_operands=*/false);\n+    if (has_nontrivial_user) {\n+      return std::nullopt;\n+    }\n+  }\n   return transpose;\n }\n \n"
                },
                {
                    "Id": 52,
                    "hunk size": 10,
                    "hunk": "@@ -1058,13 +1072,13 @@ const HloInstruction& FindNonTrivialHero(const HloInstruction& instr,\n                                          const HloFusionAdaptor& fusion) {\n   HloInstructionAdaptor idx{instr};\n \n-  // Go up the chain of trivial element-wise(+bitcast, -copy) operations. Such\n-  // chains are bound to be quite small, as we restrict the number of users as\n-  // well. Note that no memoization is needed due to user number constraints: we\n+  // Go up the chain of trivial element-wise(+bitcast, -copy) operations. Note\n+  // that no memoization is needed due to number of operands constraints: we\n   // never have to revisit same nodes.\n   auto get_intermediate_arg =\n       [&](HloInstructionAdaptor node) -> std::optional<HloInstructionAdaptor> {\n-    if (IsIntermediate(&node.instruction(), 1, &fusion) &&\n+    if (IsIntermediate(&node.instruction(), /*allowed_operand_count=*/1,\n+                       &fusion) &&\n         fusion.ContainsInstruction(node.GetOperand(0))) {\n       return node.GetOperand(0);\n     }\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b97f0f708cb9a7039a12c605391c6ec90f767f49",
    "message": "Add check to see if key exists before calling at.\n\nPiperOrigin-RevId: 542306890",
    "date": "2023-06-21T11:10:38-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9ceb4517a93c612c3b90c72a0cf54b3bad1bde97",
    "message": "PR #10668: [XLA:GPU] Fix cuDNN FMHA rewriter sequence length checks and Bump up minimum flash attn cuDNN version to 8.9.4\n\nImported from GitHub PR https://github.com/openxla/xla/pull/10668\n\n* If only seqlen_q or seqlen_kv is larger than 512, it will be lowered to fused attn instead of flash attn. This is incorrect as fused attn does not support this case. Add checks to lower to flash attn if one seqlen is larger than 512.\n* While fixing the seqlen checks, also relax the seqlen/head dim constraints to support any seqlen % 2 == 0, head_dim <= 128 and head_dim % 8 = 0. (Since 8.9.4).\n* Consolidate multiple get seqlen/head dim from fused attn/flash attn checks into one `getBHSD` function and rewrite both fused attn/flash attn checks to make it easier to understand.\n* Bump up minimum cuDNN version require for flash attn from 8.9.3 to 8.9.4. (More seqlen/head dim support and cross attn support).\n* Remove cross attn checks from rewriter and rewriter test since it is now default supported with 8.9.4.\n* Add one testcase to cover the first bullet point.\nCopybara import of the project:\n\n--\nbfdce45d669fe899dd845f497df783f014558384 by cjkkkk <ske@nvidia.com>:\n\nrewrite seqlen checks\n\n--\n8a0e8b465cc1790953b1dc5f8740adfa7cf1124a by cjkkkk <ske@nvidia.com>:\n\nreturn vector directly\n\n--\n72abf544d2f962c5554fe00ffea8d3167a9cde81 by cjkkkk <ske@nvidia.com>:\n\nuse struct for qkv_layout && add () for extra &&\n\n--\n0cac35422315180fcc9b04372916e061ebe17754 by cjkkkk <ske@nvidia.com>:\n\nfix head dim check with cuDNN < 8.9.6\n\nMerging this change closes #10668\n\nPiperOrigin-RevId: 619703747",
    "date": "2024-03-27T16:58:42-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/cudnn_fused_mha_rewriter.cc",
            "patches": [
                {
                    "Id": 53,
                    "hunk size": 17,
                    "hunk": "@@ -1239,10 +1239,19 @@ absl::StatusOr<bool> IsMHABlockSupported(\n     return false;\n   }\n \n+  // get batch/num heads/sequence length/hidden dim from bmm1 and bmm2\n+  // also make sure they are the same between bmm1 and bmm2\n+  TF_ASSIGN_OR_RETURN(std::optional<QKVLayout> qkv_layout,\n+                      GetQKVLayout(bmm_1, bmm_2, need_canonicalization));\n+  if (!qkv_layout.has_value()) {\n+    VLOG(2) << \"bmm1 and bmm2 have different qkv layout.\";\n+    return false;\n+  }\n+\n   // check if matched attention block is supported by cuDNN flash attention.\n-  TF_ASSIGN_OR_RETURN(is_flash_attention,\n-                      IsFlashAttention(bmm_1, is_causal_mask, custom_call_name,\n-                                       cc, cudnn_version));\n+  TF_ASSIGN_OR_RETURN(\n+      is_flash_attention,\n+      IsFlashAttention(qkv_layout.value(), is_training, cc, cudnn_version));\n   if (is_flash_attention) {\n     if (is_causal_mask) {\n       // if bias is causal mask, needs to remove bias from name\n"
                },
                {
                    "Id": 54,
                    "hunk size": 15,
                    "hunk": "@@ -1259,14 +1268,11 @@ absl::StatusOr<bool> IsMHABlockSupported(\n     }\n     return true;\n   }\n-  // otherwise check if it is supported by regular attention\n-  TF_ASSIGN_OR_RETURN(bool is_bmm1_supported,\n-                      IsSupportedBMM1(bmm_1, is_training));\n-  if (!is_bmm1_supported) return false;\n-  TF_ASSIGN_OR_RETURN(bool is_bmm2_supported,\n-                      IsSupportedBMM2(bmm_2, need_canonicalization));\n-  if (!is_bmm2_supported) return false;\n-  return true;\n+  // check if matched attention block is supported by cuDNN fused attention.\n+  TF_ASSIGN_OR_RETURN(\n+      bool is_fused_attention,\n+      IsFusedAttention(qkv_layout.value(), is_training, cc, cudnn_version));\n+  return is_fused_attention;\n }\n \n absl::StatusOr<HloInstruction*> CanonicalizeBatchedGemmForcuDNNFMHA(\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a3ef3ad4db453ca64453ad7c8cc3a219e70887c8",
    "message": "Fix checks whether conversions are supported.\n\nPiperOrigin-RevId: 611457501",
    "date": "2024-02-29T07:09:59-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/fusions/concatenate_mlir.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/fusions/fusions.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/fusions/transpose_mlir.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9a4b6b6bcc7a813162bf0378727950e321aca19c",
    "message": "Add stricter type checking for tf.math.real (using is_numeric)",
    "date": "2023-05-09T14:49:51-04:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/64c943c7633c79d97fc7205c8801a054e354b1e1",
    "message": "Add GetParameters() and FusionInstruction() methods to fusion adaptors.\n\nAlso let GetRoots() for ProducerConsumer fusion handle the case where producer\nis a multi-output fusion.\n\nPiperOrigin-RevId: 630936590",
    "date": "2024-05-05T23:05:05-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/hlo_traversal.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/hlo_traversal.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/5c6dc6c062ac1d59176d0a8d75b547bb42104421",
    "message": "[xla:gpu] Update check in AddressComputationFusionRewriter to process legacy custom calls too\n\nPiperOrigin-RevId: 607206363",
    "date": "2024-02-14T21:51:26-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/address_computation_fusion_rewriter.cc",
            "patches": [
                {
                    "Id": 55,
                    "hunk size": 4,
                    "hunk": "@@ -253,7 +281,7 @@ absl::StatusOr<bool> AddressComputationFusionRewriter::Run(\n   for (HloComputation* computation : module->computations()) {\n     if (computation->IsFusionComputation()) continue;\n     for (HloInstruction* instr : computation->instructions()) {\n-      if (IsLegacyCublasMatmul(*instr) || IsCustomCall(instr)) {\n+      if (IsLegacyCublasMatmul(*instr) || IsCustomCall(instr, platform_name_)) {\n         auto sliced_operand_chains = GetSlicedOperandChains(instr);\n         if (!(sliced_operand_chains.size() == 1 &&\n               sliced_operand_chains.front() == instr)) {\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/gpu/address_computation_fusion_rewriter.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e0b4ce7751e929ae4de6a0e43bf6b8081c8e5d25",
    "message": "PR #13360: [oneDNN][BugFix] Fix dtype utility function for F16 AVX2 DL extension\n\nImported from GitHub PR https://github.com/openxla/xla/pull/13360\n\nIntel's Efficiency cores do not support AVX512 instructions. However, some of them do support lower precisions (BF16/FP16) by converting to and from FP32. Due to the lack of Byte and Word instruction set support, the current condition check always returns False, even on supporting E-cores. This PR fixes the condition to correctly return True on all compatible hardware.\nCopybara import of the project:\n\n--\nd138f97cb5004b199e626d4f705e50269b79e160 by Akhil Goel <akhil.goel@intel.com>:\n\nFix AVX2 FP16\n\nMerging this change closes #13360\n\nPiperOrigin-RevId: 640097657",
    "date": "2024-06-04T05:39:39-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/cpu/onednn_util.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/64740d5dfdc689ba04f2a95c2ff565ef2c0a9f53",
    "message": "Avoid overflows in check whether there is enough available input space\n\nbytes_to_write can overflow int32 in a way that makes it look smaller than\nAvailableInputSpace(). Instead do the comparison using size_t.\n\nPiperOrigin-RevId: 626402022",
    "date": "2024-04-19T10:36:43-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/third_party/tsl/tsl/lib/io/zlib_outputbuffer.cc",
            "patches": [
                {
                    "Id": 56,
                    "hunk size": 4,
                    "hunk": "@@ -155,7 +155,7 @@ absl::Status ZlibOutputBuffer::Append(StringPiece data) {\n \n   size_t bytes_to_write = data.size();\n \n-  if (static_cast<int32>(bytes_to_write) <= AvailableInputSpace()) {\n+  if (bytes_to_write <= static_cast<size_t>(AvailableInputSpace())) {\n     AddToInputBuffer(data);\n     return absl::OkStatus();\n   }\n"
                },
                {
                    "Id": 57,
                    "hunk size": 4,
                    "hunk": "@@ -163,7 +163,7 @@ absl::Status ZlibOutputBuffer::Append(StringPiece data) {\n   TF_RETURN_IF_ERROR(DeflateBuffered(zlib_options_.flush_mode));\n \n   // At this point input stream should be empty.\n-  if (static_cast<int32>(bytes_to_write) <= AvailableInputSpace()) {\n+  if (bytes_to_write <= static_cast<size_t>(AvailableInputSpace())) {\n     AddToInputBuffer(data);\n     return absl::OkStatus();\n   }"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/a2383e62036c7a4036a10755a8de54855e9ac9a5",
    "message": "[XLA:GPU] Add proper checks for code duplication to priority_fusion.\n\nWe were using fusion_node_indexing_evaluation which just supports checks\nwhether a single producer can be fused into a fusion, but we called it also\nwith producer fusions which were then not properly evaluated. We already have a\ndifferent way to check this via cost_analysis which is also used in\nFusionMerger, so we should be using that instead.\n\nPiperOrigin-RevId: 603267214",
    "date": "2024-01-31T22:53:26-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/priority_fusion.cc",
            "patches": [
                {
                    "Id": 58,
                    "hunk size": 16,
                    "hunk": "@@ -471,18 +465,8 @@ class GpuPriorityFusionQueue : public FusionQueue {\n     // have exponential time/memory requirements for emitting certain fusion\n     // kernels, in which case we don't want to fuse.\n     // TODO(b/119692968): Remove this once we have fixed our fusion emitter.\n-    if (consumer->opcode() == HloOpcode::kFusion) {\n-      absl::MutexLock lock(&fusion_node_evaluations_mutex_);\n-      if (fusion_node_evaluations_.find(consumer) ==\n-          fusion_node_evaluations_.end()) {\n-        // We have no cached results for this fusion node yet. Compute it now.\n-        fusion_node_evaluations_.emplace(\n-            consumer, FusionNodeIndexingEvaluation(consumer));\n-      }\n-      if (fusion_node_evaluations_.at(consumer).CodeDuplicationTooHigh(\n-              producer)) {\n-        return \"the fusion would result in an overly large code duplication\";\n-      }\n+    if (cost_analysis_.ProducerConsumerMergedTooLarge(*producer, *consumer)) {\n+      return \"the fusion would result in an overly large code duplication\";\n     }\n \n     // Don't fuse across a root instruction. There are situation when a root\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/gpu/priority_fusion.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9910b18b4bc3066352ec02b227d450c4f7749c6b",
    "message": "stop kernel execution of validation fails",
    "date": "2023-02-08T05:15:42+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/07e48f1c97963b9918e9ae6aaba79143226162de",
    "message": "Fix scatter bounds checks for unsigned indices.\n\nI accidentally used sle where we need ule, resulting in incorrect\nresults for values >= 2**31 / 2**63.\n\nPiperOrigin-RevId: 634651640",
    "date": "2024-05-17T00:18:09-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/fusions/scatter_mlir.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/72df60c5451ccec269d071cbff30038abb4e7d0b",
    "message": "Change 4/6 for making MSA repacking slice aware.\n\nSplit FindChunkCandidates() into 3 methods:\n- GetMaxColocationSize\n- CreateSlicedAllocationFinder\n- PostProcessFindChunkCandidatesResult\n\nWe do this so that slice-aware repacking can make use of the individual methods. In particular, when more than 1 sliced allocation is colocated, we need to create a sliced allocation finder for each so that we can check sliced fits individually.\n\nPiperOrigin-RevId: 565471831",
    "date": "2023-09-14T14:13:18-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/55cce91af005a2d2ab3a0f9c7546a86be3ee3187",
    "message": "[XLA:GPU] Fix a bug in ProvideParameters\n\nIt did not handle the case when the subgraph has more than one root.\n\nPiperOrigin-RevId: 626298839",
    "date": "2024-04-19T02:38:33-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/fusions/concatenate_mlir.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/fusions/in_place_dynamic_update_slice_mlir.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/fusions/mlir/elemental_hlo_to_mlir.cc",
            "patches": [
                {
                    "Id": 59,
                    "hunk size": 20,
                    "hunk": "@@ -1140,20 +1140,26 @@ SmallVector<Value> ProvideParameter(\n   SmallVector<Value> operands(\n       this_fn.getArguments().take_front(instr->parent()->num_parameters()));\n   absl::c_copy(indices, std::back_inserter(operands));\n-  return builder.create<PureCallOp>(callee, operands).getResults();\n+  auto results = builder.create<PureCallOp>(callee, operands).getResults();\n+  if (results.size() == 1) {\n+    return results[0];\n+  }\n+  auto callee_subgraph = computation.FindSubgraph(operand);\n+  auto it = absl::c_find(callee_subgraph.roots, operand);\n+  CHECK(it != callee_subgraph.roots.end());\n+  return results[std::distance(callee_subgraph.roots.begin(), it)];\n }\n \n SmallVector<Value> ProvideParameterRange(\n-    const PartitionedComputation::Subgraph& caller, const HloInstruction* instr,\n+    const PartitionedComputation& computation, const HloInstruction* instr,\n     int start, int num, ValueRange indices,\n     const CallTargetProvider& call_target_provider, mlir::func::FuncOp this_fn,\n     ImplicitLocOpBuilder& builder) {\n   SmallVector<Value> scalars;\n+  scalars.reserve(num);\n   for (int i = 0; i < num; ++i) {\n-    auto scalar = ProvideParameter(caller, instr, i + start, indices,\n-                                   call_target_provider, this_fn, builder);\n-    CHECK_EQ(scalar.size(), 1);\n-    scalars.push_back(scalar.front());\n+    scalars.push_back(ProvideParameter(computation, instr, i + start, indices,\n+                                       call_target_provider, this_fn, builder));\n   }\n   return scalars;\n }\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/gpu/fusions/mlir/elemental_hlo_to_mlir.h",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/fusions/reduction_mlir.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/fusions/scatter_mlir.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/fusions/transpose_mlir.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/0b86e66216e547c8dd5c825457ecbd54b40c8ffe",
    "message": "PR #5914: [ROCm] fixing build errors for ROCm nightly tests\n\nImported from GitHub PR https://github.com/openxla/xla/pull/5914\n\nThis commit fixes current build errors for ROCm platform.\n\n@akuegel @ddunl @ezhulenev : can someone of you check this commit please ?\n\nHere I have addressed the issues mentioned by Eugene from https://github.com/openxla/xla/pull/5867\nabout stream_executor targets.\n\nCopybara import of the project:\n\n--\n307781ca50e80fc870c9b2593458a25ea7c2321a by Pavel Emeliyanenko <pavel.emeliyanenko@amd.com>:\n\nfixing build errors for ROCm nightly tests\n\nMerging this change closes #5914\n\nPiperOrigin-RevId: 568775978",
    "date": "2023-09-27T01:28:30-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/4e9db184297de547884b4b3c1d1422658ec37a6e",
    "message": "#tf-data Output of a model node can be deleted by a thread while some of its input nodes are in the process of being deleted. If such a case happens, the `output_` of these input nodes becomes a dangling pointer. This CL adds a check for such a scenario.\n\nPiperOrigin-RevId: 564583894",
    "date": "2023-09-11T20:52:35-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/adb0d424d4c2312f39def8360db82e01c0c1e073",
    "message": "Fix NCCL definition check.\n\nPiperOrigin-RevId: 596567472",
    "date": "2024-01-08T05:57:42-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_driver.cc",
            "patches": [
                {
                    "Id": 60,
                    "hunk size": 6,
                    "hunk": "@@ -57,9 +57,9 @@ limitations under the License.\n #include \"tsl/platform/statusor.h\"\n #include \"tsl/platform/threadpool.h\"\n \n-#ifdef NCCL_ENABLED\n+#ifdef XLA_ENABLE_XCCL\n #include \"third_party/nccl/nccl.h\"\n-#endif  // NCCL_ENABLED\n+#endif  // XLA_ENABLE_XCCL\n \n static constexpr bool FLAGS_gpuexec_cuda_driver_inject_init_error = false;\n static constexpr bool FLAGS_gpuexec_cuda_sync_around_driver_calls = false;\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/0e74b6ecb39527636d894e3530789899e97172c4",
    "message": "[XLA:GPU] Simplify EmitF32ToBF16 in GpuElementalIrEmitter for Cuda capability >= 8.0\n\nWe are using the more complex default operation for Cuda capability < 8.0,\nand a faster hardware solution for Cuda capability >= 8.0.\n\nWe are now running the elemental_ir_emitter_test on gpu_sm70only and gpu_sm80only in addition to cpu and gpu.\n\nWe also pass down the IrEmitterContext to GpuElementalIrEmitter to be able to check the Cuda capability.\n\nPiperOrigin-RevId: 493008856",
    "date": "2022-12-05T08:17:57-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9bef202848856f4c08ac29a1d874241813a3f6a0",
    "message": "Do not allow to multi-output fuse two transposes with different permutations.\n\nThe fast transpose emitter relies on all transpose heroes in the multi-output\nfusion to be of the same kind. The previous check for equality of hero shapes\nand operand shapes was not good enough.\n\nPiperOrigin-RevId: 513771585",
    "date": "2023-03-03T02:30:14-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/85d5a350394b8b26baed04fbba0c019e1a5693b0",
    "message": "Added unit tests, aot check for loadsavedmodel, deserialization process for aot_packages if found, a new aot model and according unit testing.\n\nPiperOrigin-RevId: 555736121",
    "date": "2023-08-10T18:27:12-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/71af215f4ed19707309bea5dcc34adc2e489b5b5",
    "message": "This is change 2/2 for allowing the user to tell the HeapSimulator how to iterate over slice time permutations. (A permutation of slice times tells the compiler when to start each slice allocation.) In the future, we want MSA to be able to trim the iteration space for the HeapSimulator based on higher-level knowledge.\n\nIn this change we make the SlicedAllocationFinder explicitly take a SliceTimePermutationIterator, so the user can specify which permutations to try. In addition, we move the code to check for legal permutations from the best fit repacker to the HeapSimulator, so all iterators can use it.\n\nThis is mostly a refactoring.\n\nPiperOrigin-RevId: 597340523",
    "date": "2024-01-10T13:48:21-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/allocation_block.h",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/heap_simulator.cc",
            "patches": [
                {
                    "Id": 61,
                    "hunk size": 15,
                    "hunk": "@@ -1586,11 +1686,14 @@ GlobalDecreasingSizeBestFitHeap<BufferType>::SlicedAllocationFinder::FindInRoot(\n   CHECK_EQ(first_offset % alignment_, 0);\n   for (int64_t offset = first_offset; offset + max_colocation_size_ <= last_end;\n        offset += alignment_) {\n-    for (SliceTimePermutationIterator permutation_it(\n-             LatestSliceTime(), is_slice_time_permutation_allowed_);\n-         !permutation_it.Done(); permutation_it.Next()) {\n-      if (DoesPermutationFit(permutation_it.Get(), root, offset).ok()) {\n-        return PermutationToChunks(permutation_it.Get(), offset);\n+    for (slice_time_permutation_iterator_->Begin();\n+         !slice_time_permutation_iterator_->Done();\n+         slice_time_permutation_iterator_->Next()) {\n+      if (DoesPermutationFit(slice_time_permutation_iterator_->Get(), root,\n+                             offset)\n+              .ok()) {\n+        return PermutationToChunks(slice_time_permutation_iterator_->Get(),\n+                                   offset);\n       }\n     }\n \n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/heap_simulator.h",
            "patches": [
                {
                    "Id": 62,
                    "hunk size": 11,
                    "hunk": "@@ -629,12 +655,13 @@ class GlobalDecreasingSizeBestFitHeap : public HeapAlgorithm<BufferType> {\n     //   with the fully allocated sliced allocation.\n     // - preferred_offset: The preferred starting offset for the fully allocated\n     //   sliced allocation.\n+    // - slice_time_permutation_iterator: An iterator for iterating over the\n+    //   different slice time permutations for slices. Users may specify the\n+    //   order in which different permutations are tried by the HeapSimulator.\n+    //   Users are also responsbile for ensuring that returned permutations are\n+    //   legal.\n     // - is_offset_allowed: Indicates if a the entire sliced allocation is\n     //   allowed to be allocated at a given offset.\n-    // - is_slice_time_permutation_allowed: Indicates if the permutation of a\n-    //   vector of slice times is allowed. The vector V of slice times contains\n-    //   the values {0, ..., num_slices-1}. If V[i] = j, the slice at the ith\n-    //   smallest offset will start at the jth earliest slice start time.\n     //\n     // REQUIRES:\n     // - sorted_slice_sizes.size() == free_chunks_per_slice_time.size()\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/memory_space_assignment/best_fit_repacker.cc",
            "patches": [
                {
                    "Id": 63,
                    "hunk size": 10,
                    "hunk": "@@ -161,6 +161,14 @@ std::vector<const AllocationBlock*> SortAllocationBlocks(const T& container) {\n   return result;\n }\n \n+const SlicedAllocationData* GetSlicedAllocationDataPointer(\n+    const std::optional<SlicedAllocationData>& sliced_allocation_data) {\n+  if (!sliced_allocation_data.has_value()) {\n+    return nullptr;\n+  }\n+  return &(*sliced_allocation_data);\n+}\n+\n // A slice-aware best-fit repacker.\n class BestFitRepacker\n     : public GlobalDecreasingSizeBestFitHeap<AllocationBlock> {\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2dcf5bb323ab773ad13633cd0e4998d420eff0b4",
    "message": "Update type annotations and checks for FunctionType\n\nPiperOrigin-RevId: 542969197",
    "date": "2023-06-23T15:16:23-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1cd945adaf5fcee4bc4497c87529586765e52e10",
    "message": "[pjrt] Add a run time check to guarantee that only one unique future created from a promise for move-only types\n\nPiperOrigin-RevId: 628114989",
    "date": "2024-04-25T10:41:07-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/pjrt/pjrt_future.h",
            "patches": [
                {
                    "Id": 64,
                    "hunk size": 13,
                    "hunk": "@@ -348,8 +361,15 @@ class PjRtFuture : public internal::PjRtFutureBase<T> {\n       Promise promise,\n       PjRtFutureHelpers::OnBlockStartFn on_block_start = nullptr,\n       PjRtFutureHelpers::OnBlockEndFn on_block_end = nullptr)\n-      : Base(std::move(promise).ExtractRef(), std::move(on_block_start),\n-             std::move(on_block_end)) {}\n+      : Base(promise.release(), std::move(on_block_start),\n+             std::move(on_block_end)) {\n+#ifndef NDEBUG\n+    if constexpr (Base::is_unique()) {\n+      DCHECK_EQ(promise.AddFuture(), 0)\n+          << \"Unique PjRtFuture cannot share a promise object\";\n+    }\n+#endif\n+  }\n \n   // Blocks the calling thread until the future is ready, then returns the\n   // final value.\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9b342f538c9de4ded35975803450e015b18f1029",
    "message": "Part of a larger change to implement sliced prefetching.\n\nAdd a check to make sure repacking is disabled when slicing is enabled.\n\nPiperOrigin-RevId: 541069948",
    "date": "2023-06-16T19:05:42-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/61163051af7315f10c18d4fd99d7a6f40d8cd766",
    "message": "Fix segment reduction overflow.\n\nThe `nsegments` argument type must be the same as the `segment_ids`\n(since the latter are just indices into `nsegments`).  For large\ninputs that would overflow `Index` (`long`) but still fit into `int64_t`,\nthis was previously causing a uncaught failure, since it would pass\nthe validation checks that used `int64_t`, but then would result in\na negative value when it's actually used.\n\nFixes #64023.\n\nPiperOrigin-RevId: 632523124",
    "date": "2024-05-10T09:52:04-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/core/kernels/segment_reduction_ops_impl.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/98fb4f6e48627cf30cd747848505b7eae28f1ded",
    "message": "PR #8073: ReplaceInstructionWithDifferentShape should return false if old_instruction HasControlDependencies\n\nImported from GitHub PR https://github.com/openxla/xla/pull/8073\n\nThere are two `ReplaceInstructionWithDifferentShape` functions.\nThe default value for `relay_control_dependency` is `false`.\n```\n  StatusOr<bool> ReplaceInstructionWithDifferentShape(\n      HloInstruction* old_instruction,\n      HloInstruction* new_instruction,\n      bool preserve_sharding,\n      bool relay_control_dependency = false,\n      bool remove_unused_operands = true\n  );\n\n  StatusOr<bool> ReplaceInstructionWithDifferentShape(\n      HloInstruction* old_instruction,\n      HloInstruction* new_instruction\n  );\n```\nMany users use `ReplaceInstructionWithDifferentShape` functions providing only old and new instructions and leave all other parameters with default values.\n\nIf old_instruction has ControlDependencies then it will be impossible to replace it because ReplaceInstruction copies and drops ControlDependencies from old to new instruction only if `relay_control_dependency` is set to `true`.\nCurrently the function will Fail if we try to use it for the case when old_instruction has ControlDependencies. It will fail because `RemoveInstruction(old_instruction)` will fail because `IsSafelyRemovable` check will return false.\n\nInstead of failing, ReplaceInstructionWithDifferentShape should simply return false - indicating that the validation checks for ReplaceInstruction was not satisfied.\n\n@Tongfei-Guo @jurahul Can you have a look?\n\nCopybara import of the project:\n\n--\nfbe2fb2da6a51e73a6913b1f67c8d1587abbe049 by Alexander Pivovarov <pivovaa@amazon.com>:\n\nReplaceInstructionWithDifferentShape should return false if old_instruction HasControlDependencies\n\nMerging this change closes #8073\n\nPiperOrigin-RevId: 598560578",
    "date": "2024-01-15T03:11:08-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_computation.cc",
            "patches": [
                {
                    "Id": 65,
                    "hunk size": 6,
                    "hunk": "@@ -1182,6 +1182,10 @@ StatusOr<bool> HloComputation::ReplaceInstructionWithDifferentShape(\n     TF_RETURN_IF_ERROR(\n         new_instruction->CopyAllControlDepsFrom(old_instruction));\n     TF_RETURN_IF_ERROR(old_instruction->DropAllControlDeps());\n+  } else if (old_instruction->HasControlDependencies()) {\n+    VLOG(10) << \"Skipping replacement because old instruction has \"\n+                \"control dependencies\";\n+    return false;\n   }\n   VLOG(10) << \"transformed \" << old_instruction->ToString() << \" to \"\n            << new_instruction->ToString();"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e8cc764cd7dab7495b91e0c973a41d258f18c568",
    "message": "Support mocking number of GPUs in CUDA plugin.\n\nAlso move reading jax config value to be right before the client is created. Previously they were read before calling register_plugin, which happens during import and before any call of jax.config.update.\n\nThe decorator in mock_gpu_test was used wrongly. jtu.run_on_devices will create the client before jax.config.update is called, which is not desired. Remove the decorator will not fail CPU/TPU tests because the mesh will check the num_shard and the number of devices in the client and skip it if it does not match.\n\ngenerate_pjrt_gpu_plugin_options is only used in places that do not require compatibility so do not need to update xla_client version.\n\nPiperOrigin-RevId: 611610915",
    "date": "2024-02-29T15:25:27-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/pjrt/c/pjrt_c_api_gpu_internal.cc",
            "patches": [
                {
                    "Id": 66,
                    "hunk size": 7,
                    "hunk": "@@ -125,6 +126,11 @@ PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {\n   if (auto it = create_options.find(\"num_nodes\"); it != create_options.end()) {\n     num_nodes = std::get<int64_t>(it->second);\n   }\n+  bool enable_mock_nccl = false;\n+  if (auto it = create_options.find(\"enable_mock_nccl\");\n+      it != create_options.end()) {\n+    enable_mock_nccl = std::get<bool>(it->second);\n+  }\n \n   xla::GpuClientOptions options;\n   options.allocator_config = allocator_config;\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/python/xla_client.py",
            "patches": [
                {
                    "Id": 67,
                    "hunk size": 11,
                    "hunk": "@@ -205,21 +205,14 @@ def make_tpu_client(library_path: Optional[str] = None):\n   return make_tfrt_tpu_c_api_client()\n \n \n-def generate_pjrt_gpu_plugin_options(\n-    visible_devices: str = 'all',\n-) -> _NameValueMapping:\n+def generate_pjrt_gpu_plugin_options() -> _NameValueMapping:\n   \"\"\"Generates the PjRt GPU plugin options.\n \n-  Args:\n-    visible_devices: A string of visible cuda devices.\n-\n   Returns:\n     A dictionary of plugin options.\n   \"\"\"\n \n   options = {}\n-  if visible_devices != 'all':\n-    options['visible_devices'] = [int(x) for x in visible_devices.split(',')]\n   options['platform_name'] = 'cuda'\n   allocator = os.getenv('XLA_PYTHON_CLIENT_ALLOCATOR', 'default').lower()\n   memory_fraction = os.getenv('XLA_PYTHON_CLIENT_MEM_FRACTION', '')\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/python/xla_client.pyi",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/85ea63741928b4ff2182112f8cf7f12673cc9fe0",
    "message": "Fix a crash in unoptimized builds.\n\ngetIntOrFloatBitWidth asserts that the type is an integer, but\nwe only check the type after calling it.\n\nPiperOrigin-RevId: 636531275",
    "date": "2024-05-23T07:01:54-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/fusions/mlir/lower_tensors.cc",
            "patches": [
                {
                    "Id": 68,
                    "hunk size": 8,
                    "hunk": "@@ -463,9 +463,11 @@ struct RemoveUnusedIndexSwitchResults\n };\n \n bool IsAtomicIntegral(Type element_type) {\n+  if (!element_type.isInteger()) {\n+    return false;\n+  }\n   unsigned element_bitwidth = element_type.getIntOrFloatBitWidth();\n-  return element_type.isInteger() &&\n-         (element_bitwidth == 32 || element_bitwidth == 64);\n+  return element_bitwidth == 32 || element_bitwidth == 64;\n }\n \n Value CreateBitcast(mlir::ImplicitLocOpBuilder& b, Value value, Type ty) {"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7494487575eecff08ddbffc12e038661876ee835",
    "message": "Fix race condition in kernel caching.\n\nIf two identical kernels are created at the same time, they will both observe that the cache doesn't contain their cache_key, and both try to create the kernel at the same time. The first kernel to be created gets added to the cache, then the second kernel overwrites the first kernel in the cache.\n\nThis can cause issues for stateful kernels such as kernels that own resources. Destroying the first kernel in the above example could cause in-use resources to be destroyed.\n\nThis CL resolves the issue by checking if a cache entry already exists right before adding to the cache. If we create a new kernel but discover a cached entry has been added during the kernel creation, we now discard the new kernel and use the cached entry. This way we only ever use one kernel per cache key.\n\nPiperOrigin-RevId: 596762121",
    "date": "2024-01-08T18:57:48-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/core/common_runtime/eager/context.cc",
            "patches": [
                {
                    "Id": 69,
                    "hunk size": 14,
                    "hunk": "@@ -1203,10 +1203,16 @@ Device* EagerContext::GetCachedDevice(Fprint128 device_cache_key) {\n   return iter->second;\n }\n \n-void EagerContext::AddKernelToCache(Fprint128 cache_key,\n-                                    KernelAndDevice* kernel) {\n+core::RefCountPtr<KernelAndDevice> EagerContext::AddKernelToCache(\n+    Fprint128 cache_key, core::RefCountPtr<KernelAndDevice> kernel) {\n   mutex_lock ml(cache_mu_);\n-  core::RefCountPtr<KernelAndDevice> new_ref(kernel);\n+  auto iter = kernel_cache_.find(cache_key);\n+  if (iter != kernel_cache_.end()) {\n+    core::RefCountPtr<KernelAndDevice> new_ref(iter->second.get());\n+    new_ref->Ref();\n+    return new_ref;\n+  }\n+  core::RefCountPtr<KernelAndDevice> new_ref(kernel.get());\n   new_ref->Ref();\n   kernel_cache_[cache_key] = std::move(new_ref);\n   auto* registered_function =\n"
                }
            ]
        },
        {
            "path": "tensorflow/core/common_runtime/eager/context.h",
            "patches": []
        },
        {
            "path": "tensorflow/core/common_runtime/eager/execute.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/49d64711af8a28aafb1eca4cdf20661a5511700e",
    "message": "[xla:cpu] Do not check buffer slices in optimized builds\n\nOverheads of checking buffer slices add up, and incorrect buffer slice is a serious compiler bug that should be discovered in debug builds.\n\nname                                     old cpu/op   new cpu/op   delta\nBM_SelectAndScatterF32/128/process_time   965\u00b5s \u00b1 1%   806\u00b5s \u00b120%     ~\nBM_SelectAndScatterF32/256/process_time  3.79ms \u00b1 2%  2.93ms \u00b1 0%  -22.64%\nBM_SelectAndScatterF32/512/process_time  15.5ms \u00b1 2%  12.0ms \u00b1 2%  -22.50%\n\nPiperOrigin-RevId: 650128768",
    "date": "2024-07-07T23:06:24-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/cpu/runtime/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/cpu/runtime/buffer_allocations.h",
            "patches": [
                {
                    "Id": 70,
                    "hunk size": 11,
                    "hunk": "@@ -72,16 +81,17 @@ absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n   return buffers_[index];\n }\n \n-absl::StatusOr<se::DeviceMemoryBase> BufferAllocations::GetDeviceAddress(\n-    const BufferAllocation::Slice& buffer_slice) const {\n+inline ABSL_ATTRIBUTE_ALWAYS_INLINE absl::StatusOr<se::DeviceMemoryBase>\n+BufferAllocations::GetDeviceAddress(\n+    const BufferAllocation::Slice& slice) const {\n   // Handle empty slices explicitly and return a null pointer device memory to\n   // guarantee that we do not accidentally write through the empty slice which\n   // would hide a real bug in the code.\n-  if (ABSL_PREDICT_FALSE(buffer_slice.size() == 0)) {\n+  if (ABSL_PREDICT_FALSE(slice.size() == 0)) {\n     return se::DeviceMemoryBase(nullptr, 0);\n   }\n \n-  int64_t index = buffer_slice.index();\n+  int64_t index = slice.index();\n   if (ABSL_PREDICT_FALSE(index < 0 || index >= num_buffers_)) {\n     return InvalidArgument(\n         \"Invalid buffer index %d. It must be in the range [0, %d)\", index,\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/cpu/runtime/kernel_thunk.cc",
            "patches": [
                {
                    "Id": 71,
                    "hunk size": 18,
                    "hunk": "@@ -158,6 +163,22 @@ tsl::AsyncValueRef<Thunk::ExecuteEvent> KernelThunk::Execute(\n   return OkExecuteEvent();\n }\n \n+absl::Status KernelThunk::CheckBufferAlignment(\n+    absl::Span<const SE_HOST_KernelArg> kernel_args) {\n+  if (min_alignment_.has_value()) {\n+    for (int64_t i = 0; i < num_kernel_args_; ++i) {\n+      auto ptr = reinterpret_cast<uintptr_t>(kernel_args[i].data);\n+      if (ABSL_PREDICT_FALSE((ptr & (*min_alignment_ - 1)) != 0)) {\n+        return Internal(\n+            \"Host kernel %s buffer argument #%d (%p) is not aligned to a \"\n+            \"required minimum alignment of %d bytes\",\n+            info().op_name, i, kernel_args[i].data, *min_alignment_);\n+      }\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+\n KernelThunk::BufferUses KernelThunk::buffer_uses() const {\n   BufferUses buffer_uses;\n   for (const BufferAllocation::Slice& buffer : arguments_buffers_) {\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/cpu/runtime/kernel_thunk.h",
            "patches": [
                {
                    "Id": 72,
                    "hunk size": 8,
                    "hunk": "@@ -56,6 +58,12 @@ class KernelThunk final : public Thunk {\n               std::string kernel_name, se::ThreadDim thread_dim,\n               std::optional<uint64_t> min_alignment);\n \n+  // Checks that all buffers are aligned to the minimum alignment. We codegen\n+  // with the assumption that all buffers are aligned, and if they are not, we\n+  // will crash with a segmentation fault, or worse, produce incorrect results.\n+  absl::Status CheckBufferAlignment(\n+      absl::Span<const SE_HOST_KernelArg> kernel_args);\n+\n   std::vector<BufferAllocation::Slice> arguments_buffers_;\n   std::vector<BufferAllocation::Slice> results_buffers_;\n \n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/cpu/runtime/thunk.h",
            "patches": [
                {
                    "Id": 73,
                    "hunk size": 14,
                    "hunk": "@@ -225,6 +225,18 @@ class Thunk {\n   // Encodes thunk info into the TraceMe compatible format.\n   std::string TraceMeEncode() const;\n \n+  // Returns `true` if thunk should check buffer slices bounds, alignment, etc.\n+  // In optimized builds, we skip buffer slices checks, and assume that all\n+  // buffer slices are valid, as overhead of buffer slices checks adds up and\n+  // become measurable on a hot path of executing tiny thunks.\n+  static constexpr bool ShouldCheckBufferSlices() {\n+#ifdef NDEBUG\n+    return false;\n+#else\n+    return true;\n+#endif  // NDEBUG\n+  }\n+\n  private:\n   Kind kind_;\n   Info info_;\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/stream_executor/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/device_memory.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f849de0732b5bc677311a3d4b9fe55ecf5b68adf",
    "message": "[XLA:GPU] Refactor code that sets priorities into a function.\n\nThis change lookups and erases a few more elements from `reverse_map_`, but overall it shouldn't make a dent in the compile time.\n\nWith this change, we check if priority is negative before inserting into the queue, not after. This also doesn't affect compile time, but arguably makes the code easier to understand.\n\nPiperOrigin-RevId: 617478676",
    "date": "2024-03-20T05:44:31-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/priority_fusion.cc",
            "patches": [
                {
                    "Id": 74,
                    "hunk size": 9,
                    "hunk": "@@ -195,18 +219,11 @@ class GpuPriorityFusionQueue {\n \n     while (!producer_priority_queue_.empty() && current_consumers_.empty()) {\n       auto next_it = std::prev(producer_priority_queue_.end());\n-      auto priority = next_it->first.first;\n \n       current_producer_ = next_it->second;\n       producer_priority_queue_.erase(next_it);\n       reverse_map_.erase(current_producer_);\n \n-      // If the priority is negative, it's not helpful to perform fusion on this\n-      // instruction.\n-      if (priority < 0) {\n-        continue;\n-      }\n-\n       current_consumers_ = current_producer_->users();\n \n       if (current_producer_->opcode() == HloOpcode::kBitcast) {\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/6a6af6b4729ea1532a1eafe50149664347c45bdb",
    "message": "- Simplify the logics to check if graph input arg is a capture\n- Fix the bug of by-ref captures are not identified\n\nPiperOrigin-RevId: 501419075",
    "date": "2023-01-11T17:23:16-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/685418cd85e09bc2117fa15bc1b6a75d21248348",
    "message": "maxpooling op should check that ksize must be positive.\n\nPiperOrigin-RevId: 520539022",
    "date": "2023-03-29T22:25:19-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7175c60d92eef9d4971157be27c17899b7b32d7c",
    "message": "Add check to ensure number of parameters in HLO Computation in equal to number of op operands.\n\nPiperOrigin-RevId: 530963153",
    "date": "2023-05-10T11:51:38-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/03d38ef5265d84d486d99aae9edd2040f79328af",
    "message": "Remove duplicate checks. `std::optional<T>::operator bool` and `std::optional<T>::has_value` (HloInputOutputAliasConfig::OutputHasAlias) are equivalent.\n\nPiperOrigin-RevId: 536425731",
    "date": "2023-05-30T09:39:09-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c2da6c528594f361d03fb106e6db0279ea9fec22",
    "message": "Update isinstance checks in TF-NumPy to use core.Tensor instead of tensor.Tensor.\nThis allows WeakTensor to pass the instance checks.\n\nPiperOrigin-RevId: 554636220",
    "date": "2023-08-07T17:08:03-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2aa9e7e4c7581cb47fc69324d98883c252bee263",
    "message": "[XLA:GPU] Allow partially uncoalesced reads in transpose->reduction fusion merging.\n\nProfiling of real models and small test cases shows that merging fusions with a transpose and fusions resulting in a reduction may be beneficial for not materializing intermediate results even though reads become uncoalesced. The check for transposes is improved therefore to estimate what percentage of the data of the producer fusion is being transposed.\n\nTest GpuFusibleTest.DoNotFuseLayoutChangingOpWithReduce was removed for being a duplicate of InstructionFusionTest.DoNotFuseLayoutChangingOpWithReduce: it is about instruction fusion, whereas gpu_fusible checks are used for both instruction fusion and fusion merging.\n\nTest GpuFusibleTest.TransposingCopyNotFused was removed for being almost a duplicate of FusionMergerTest.WillNotMergeReduceUnfriendlyLayouts: it is about fusion merging, whereas again gpu_fusible checks are common.\n\nPiperOrigin-RevId: 475580193",
    "date": "2022-09-20T10:06:48-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/422d09c0badb4bc56fb554a8a50e7e7346a2c972",
    "message": "[XLA:GPU][NFC] Always profile the cuBLAS version of GEMMs in TritonAutotuner\n\nEven if no validation is requested.\nThis is a preparation for a later CL.\n\nPiperOrigin-RevId: 567587347",
    "date": "2023-09-22T04:51:41-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/30b8166b0ed8e61260e961e3cd60a61678544316",
    "message": "Fix bug in FusionCanShareBufferHint() for transpose multi-output fusions.\n\nWe did not correctly check whether a fusion root is accessed in two different\niteration orders. If there is another root that has a transpose hero, the\niteration orders will be different.\n\nPiperOrigin-RevId: 636096047",
    "date": "2024-05-22T03:19:45-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/buffer_sharing.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/98830e91ac878d1ee15b7539dc72bb06adb2fc2f",
    "message": "[xla:hlo] Use llvm::BitVector instead of a set when checking reachability\n\nname                               old cpu/op   new cpu/op   delta\nBM_HloDfsReachabilityBuild/1        109ns \u00b1 4%   111ns \u00b1 4%     ~\nBM_HloDfsReachabilityBuild/64      1.71\u00b5s \u00b1 6%  1.71\u00b5s \u00b1 4%     ~\nBM_HloDfsReachabilityBuild/128     3.38\u00b5s \u00b1 3%  3.43\u00b5s \u00b1 3%   +1.54%\nBM_HloDfsReachabilityBuild/256     6.80\u00b5s \u00b1 4%  6.95\u00b5s \u00b1 5%   +2.25%\nBM_HloDfsReachabilityBuild/512     13.8\u00b5s \u00b1 4%  14.2\u00b5s \u00b1 6%   +2.63%\nBM_HloDfsReachabilityBuild/4096     155\u00b5s \u00b1 4%   157\u00b5s \u00b1 4%     ~\nBM_HloDfsReachabilityBuild/32768   1.42ms \u00b1 5%  1.45ms \u00b1 3%   +1.94%\nBM_HloDfsReachabilityBuild/262144  32.2ms \u00b1 4%  32.1ms \u00b1 4%     ~\nBM_HloDfsReachabilityCheck/1       7.37ns \u00b1 3%  7.41ns \u00b1 4%     ~\nBM_HloDfsReachabilityCheck/64       295ns \u00b1 5%   139ns \u00b1 8%  -52.78%\nBM_HloDfsReachabilityCheck/128      679ns \u00b1 3%   278ns \u00b1 7%  -59.05%\nBM_HloDfsReachabilityCheck/256     1.53\u00b5s \u00b1 5%  0.61\u00b5s \u00b1 6%  -60.06%\nBM_HloDfsReachabilityCheck/512     3.06\u00b5s \u00b1 5%  1.31\u00b5s \u00b1 6%  -57.27%\nBM_HloDfsReachabilityCheck/4096    30.2\u00b5s \u00b1 7%  17.9\u00b5s \u00b1 4%  -40.53%\nBM_HloDfsReachabilityCheck/32768    532\u00b5s \u00b1 4%   327\u00b5s \u00b1 5%  -38.52%\nBM_HloDfsReachabilityCheck/262144  8.72ms \u00b1 3%  6.66ms \u00b1 4%  -23.59%\n\nPiperOrigin-RevId: 616956892",
    "date": "2024-03-18T15:10:23-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_dfs_reachability.cc",
            "patches": [
                {
                    "Id": 75,
                    "hunk size": 16,
                    "hunk": "@@ -45,16 +45,20 @@ bool HloDfsReachability::IsReachable(const HloInstruction* from,\n \n   // Note that the DFS goes from the \"uses\" root towards the \"defs\", i.e. from\n   // `to` node to `from` node, so the node indices are decreasing.\n-  if (target_node_idx > dfs_root_idx) {\n+  if (dfs_root_idx < target_node_idx) {\n     return false;\n   }\n \n-  // We use LLVM support library here because it has stack-allocated maps (in\n-  // contrast to absl) which significantly improves performance by avoiding heap\n-  // allocations when instructions are reachable via a short chain.\n-  llvm::SmallDenseSet<size_t, 8> visited_idxs{dfs_root_idx};\n+  // We use LLVM support library here because it has stack-allocated bit vector\n+  // which significantly improves performance by avoiding heap allocations when\n+  // instructions are reachable via a short chain.\n   llvm::SmallVector<const HloInstruction*> stack{to};\n \n+  // We will visit instructions in the [target_node_idx, dfs_root_idx] range, so\n+  // we can construct a smaller bit vector.\n+  llvm::BitVector visited_idxs(1 + (dfs_root_idx - target_node_idx));\n+  visited_idxs.set(dfs_root_idx - target_node_idx);\n+\n   auto check_and_enqueue = [&](const HloInstruction* instr) {\n     if (instr == from) {\n       return true;\n"
                },
                {
                    "Id": 76,
                    "hunk size": 6,
                    "hunk": "@@ -63,9 +67,11 @@ bool HloDfsReachability::IsReachable(const HloInstruction* from,\n     if (instr_idx < target_node_idx) {\n       return false;\n     }\n-    if (auto [_, inserted] = visited_idxs.insert(instr_idx); !inserted) {\n+    size_t visited_idx = instr_idx - target_node_idx;\n+    if (visited_idxs.test(visited_idx)) {\n       return false;\n     }\n+    visited_idxs.set(visited_idx);\n     stack.push_back(instr);\n     return false;\n   };"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/924f80a4fdb34230965a7a8a4476901847463645",
    "message": "Add stricter type checking for tf.math.real\n\nFix for tf.math.real so that it only accepts tensors with numeric entries as input. This makes it consistent with its documentation at https://www.tensorflow.org/api_docs/python/tf/math/real and raises a TypeError saying input must have numeric entries when called incorrectly.",
    "date": "2023-05-08T16:20:25-04:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/89f97d1aa691a1042705134ddf70a51d8a86e591",
    "message": "Move fuel consumption point to an earlier stage in the cudnn fused convolution rewriter\n\nThe call to `ConsumeFuel` happened after `EnsureIfConvBiasActivation` had already rewritten\nthe convolution instruction. In order to effectively use `ConsumeFuel` for model\nbisection I'm moving the check to before any changes are made to the instruction.\n\nPiperOrigin-RevId: 631755068",
    "date": "2024-05-08T04:56:06-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/cudnn_fused_conv_rewriter.cc",
            "patches": [
                {
                    "Id": 77,
                    "hunk size": 8,
                    "hunk": "@@ -782,6 +782,12 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {\n       continue;\n     }\n \n+    if (!ConsumeFuel(\"cudnn-fused-convolution-rewriter\", [&] {\n+          return absl::StrCat(\"FuseBiasOrSideInput: \", conv->ToString());\n+        })) {\n+      continue;\n+    }\n+\n     // If it's a vanilla forward conv, upgrade it to a bias-activation conv.  We\n     // only want to do this if the fusion will succeed, but we're guaranteed\n     // that it will, because the only reason we'll bail at this point is if\n"
                },
                {
                    "Id": 78,
                    "hunk size": 8,
                    "hunk": "@@ -848,12 +854,6 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {\n       continue;\n     }\n \n-    if (!ConsumeFuel(\"cudnn-fused-convolution-rewriter\", [&] {\n-          return absl::StrCat(\"FuseBiasOrSideInput: \", conv->ToString());\n-        })) {\n-      continue;\n-    }\n-\n     HloInstruction* new_conv = comp->AddInstruction(\n         conv->CloneWithNewOperands(conv->shape(), new_operands));\n     comp->parent()->SetAndUniquifyInstrName(new_conv, conv->name());"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/5322fd40cd58cfa8c551e602fede7a3be19fff95",
    "message": "[PJRT] Fix checking for output sharding\n\nOutput sharding for empty tuple needs to have one \"replicated\" element.\n\nPiperOrigin-RevId: 559899447",
    "date": "2023-08-24T16:12:36-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ab60b0ee51a8924a0f02b0152cd6a78ba64d3e94",
    "message": "[tfg] Fix named-attribute token check.\n\nSince the name tokens are being indexed directly, we should check that list of tokens is not empty to prevent an out-of-bounds error.\n\nPiperOrigin-RevId: 511553573",
    "date": "2023-02-22T11:57:10-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ba451bde470e0d5cfe6d40c42f17035b17fd9538",
    "message": "PR #13310: [NVIDIA GPU] Added a rewrite logic in gpu_windowned_einsum_handler to handle all2all\n\nImported from GitHub PR https://github.com/openxla/xla/pull/13310\n\nAdded a rewrite logic in gpu_windowned_einsum_handler to decompose a2a+gemm or gemm+a2a into smaller a2a+gemm/gemm+a2a to hide communuication overhead. Partial results will be aggregated at the end.\nAn example will be:\n```\n\ninput\n   |\na2a{replica_groups={{0,1}}}\n   |\ngemm\n\n```\n\ndecomposed into\n```\n\n                  input\n                 /         \\\n           slice1      slice2\n              /               \\\n           a2a1             a2a2\n            /                  \\\n       gemm1                 gemm2\n                \\           /\n                    add\n```\n\nAll partial gemms will be dispatched to parallel streams too to achieve gemm-gemm overlap.\n\nPerformance metrics:\nFor an unit with just a2a+gemm or gemm+a2a, we see from 5-15% speedup depending on the size by doing this type of composition.\nCopybara import of the project:\n\n--\n557c540df51b3c238f87ae01262f1a6000ee4499 by TJ <tjx@nvidia.com>:\n\nAdded a rewrite logic in gpu_windowned_einsum_handler to decompose\na2a+gemm or gemm+a2a into smaller a2a+gemm/gemm+a2a to hide\ncommunuication overhead.\n\n--\nd3e9b2fc28b484f263609c21b6177ea948aa8e01 by TJ <tjx@nvidia.com>:\n\nChanged testing to use file check\n\n--\n3de9fbb962438837e77353c3c0b2a96e3e0d397e by TJ Xu <tjx@nvidia.com>:\n\nAdded e2e tests\naddress recent changes to thunk emission with execution stream id\n\n--\nd7790ed5e206c5e1ebf33afa8e34d7faedff4d47 by TJ Xu <tjx@nvidia.com>:\n\nAdded file check to BUILD file\n\nMerging this change closes #13310\n\nPiperOrigin-RevId: 644291310",
    "date": "2024-06-18T02:36:27-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/gpu_windowed_einsum_handler.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/ir_emitter_unnested.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/ir_emitter_unnested.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/84d7bf6f64fd9c8677f7f26511ce3031fe8d35a6",
    "message": "Add is_numeric to dtypes.cc to check whether a data type is numeric",
    "date": "2023-05-09T14:44:28-04:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/fc24a5a3b3b79cb6eec10d39990d4eea3412b45b",
    "message": "[XLA] Propagate side effects for async ops\n\nSince *-{update,done} ops no longer have a direct reference to the wrapped computation, the existing check for op side effects isn't able to trivially find the computation. As a side effect of this, DCE ends up purging these async ops.\n\nDeffer side effect checks for async ops to their respective wrapped op.\n\nPiperOrigin-RevId: 604707924",
    "date": "2024-02-06T11:44:09-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_instruction.h",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_instructions.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9da5526ee0e967a829c730f9851f7d83871a1dde",
    "message": "Remove math_ops.py's indirect dependency on resource_variable_ops.py by replacing the isinstance checks with checks in the C++ layer.\n\nPiperOrigin-RevId: 586466978",
    "date": "2023-11-29T15:19:42-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/86aa7ef960146497a436ffb0bc3369706f930a40",
    "message": "[PJRT C API] Change CheckMatchingStructSizes to be ActualStructSizeIsGreaterOrEqual.\n\nWith this change, it is no longer required that the plugin has the same struct size as the framework. Because ActualStructSizeIsGreaterOrEqual is called in the plugin, this means it will check whether framework has a greater or equal struct size than the plugin (meaning the framework is newer than the plugin).\n\nPiperOrigin-RevId: 573059095",
    "date": "2023-10-12T17:53:05-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/73a8d4db003c1b4389c841988814e9972509555b",
    "message": "Reland PR #9757: GpuTimer: improve kernel execution time measurement accuracy\n\nImported from GitHub PR https://github.com/openxla/xla/pull/9757\n\nThis PR changes how `GpuTimer` measures execution time, which should improve the measurement accuracy.\nQuoting my comment in the code:\n```c++\n// When a timer is created it launches a delay kernel into the given stream and\n// queues a start event immediately afterwards. This delay kernel blocks\n// execution on the stream until GetElapsedDuration() is called, at which point\n// an end event is queued and the delay kernel exits. This allows the device\n// execution time of the tasks queued to the stream while the timer is active\n// to be measured more accurately.\n```\nthis should improve the accuracy of the measurements that are used to make auto-tuning decisions, especially for small kernels.\n\nThere are a couple of edge cases that have special treatment:\n- if `CUDA_LAUNCH_BLOCKING=1` then the delay kernel will not achieve anything, so we don't launch it\n- if any code in the timed region synchronises the device then the host will wait for the delay kernel to time out before actually launching the kernels to be measured. This will lead to a poor quality measurement, and an error message is printed. This condition can be met if one of the kernels being measured is lazily loaded, as lazy loading can trigger synchronisation. Best practice is to execute a warmup run (without timing enabled) before the timed execution.\n\nTwo parts of the autotuning code are updated to follow this best practice.\n\nThe `GpuTimer::Create*` signatures that were deprecated in https://github.com/openxla/xla/pull/9841 do not benefit from these accuracy improvements.\n\ncc: @sergachev @nouiz\nCopybara import of the project:\n\n--\n2b834f7154158ddc7294d45bb5481b8c38efdc61 by Olli Lupton <olupton@nvidia.com>:\n\nGpuTimer: use delay kernel to improve accuracy\n\nThis ensures that all the device operations to be timed are queued to\nthe relevant stream before any of them are executed, resulting in a more\naccurate measurement. This is skipped if CUDA_LAUNCH_BLOCKING=1 or\nunified addressing is not available.\n\nIn addition, make sure that auto-tuning code paths have warm-up\nexecutions of the kernels being measured. The warm-up executions should\nnot be inside GpuTimer regions.\n\nWith the example HLO:\n  parameter_0 = bf16[128,12,128]{2,1,0} parameter(0)\n  parameter_1 = bf16[12,128,128]{2,1,0} parameter(1)\n  ROOT dot.26 = bf16[12,128,128]{2,1,0} dot(parameter_0, parameter_1),\n    lhs_batch_dims={1}, lhs_contracting_dims={0}, rhs_batch_dims={0},\n    rhs_contracting_dims={2}\na cuBLAS kernel was selected with a CUPTI/NSys-measured runtime of\n3.9\u00b5s and before this change, GpuTimer measured a runtime of 17.5\u00b5s.\nWith this change, GpuTimer measures a runtime of 8.1\u00b5s.\n\nWith the example HLO:\n  p0 = bf16[128,1536]{0,1} parameter(0)\n  p1 = s8[1536,12288]{0,1} parameter(1)\n  c = bf16[1536,12288]{0,1} convert(p1)\n  ROOT d = bf16[128,12288]{1,0} dot(p0, c), lhs_contracting_dims={1},\n    rhs_contracting_dims={0}\nfive cuDNN plans were auto-tuned, with CUPTI/Nsys-measured runtimes of\n{34.9, 18.5, 30.9, 32.4, 31.5}\u00b5s. Before this change, GpuTimer measured\nruntimes of {76.6, 40.0, 52.5, 55.7, 51.8}\u00b5s. With this change, GpuTimer\nmeasures runtimes of {39.6, 23.0, 35.4, 35.1, 35.1}\u00b5s.\n\nIn summary, with this change, GpuTimer gives results that are much\ncloser to the CUPTI/Nsys measurements, with a ~uniform offset of ~4\u00b5s. A\nconstant offset doesn't matter for auto-tuning.\n\n--\n8f5ce0e7e309a46bf69c369bc4b1da8c26fd1579 by Olli Lupton <olupton@nvidia.com>:\n\nAddress CR\n\nMerging this change closes #9757\n\nPiperOrigin-RevId: 615693482",
    "date": "2024-03-14T01:46:28-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/conv_algorithm_picker.cc",
            "patches": [
                {
                    "Id": 79,
                    "hunk size": 13,
                    "hunk": "@@ -625,15 +624,20 @@ absl::StatusOr<AutotuneResult> GpuConvAlgorithmPicker::AutotuneOneConvRunner(\n   // Dry-run to warmup the plan.\n   launch_status = RunGpuConv(config, operand_buffers, result_buffers,\n                              scratch_memory, stream, options);\n+  // It is intentional that the warm-up run does not have a profile result.\n+  // This avoids a timeout and error message if lazy module loading is enabled\n+  // by ensuring that lazy loading happens outside the GpuTimer region.\n+  options.profile_result = &profile_result;\n   constexpr int kMaxIter = 10;\n   // Iterate until the new measurement is within kThreshold of the current\n   // minimum.\n   int num_iters = 0;\n-  for (;\n-       num_iters < kMaxIter && launch_status.ok() && profile_result.is_valid();\n-       num_iters++) {\n+  for (; num_iters < kMaxIter && launch_status.ok(); ++num_iters) {\n     launch_status = RunGpuConv(config, operand_buffers, result_buffers,\n                                scratch_memory, stream, options);\n+    if (!profile_result.is_valid()) {\n+      break;\n+    }\n     float old_min_time = min_time;\n     min_time = std::min(min_time, profile_result.elapsed_time_in_ms());\n     max_time = std::max(max_time, profile_result.elapsed_time_in_ms());\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/gpu/gemm_algorithm_picker.cc",
            "patches": [
                {
                    "Id": 80,
                    "hunk size": 11,
                    "hunk": "@@ -240,6 +240,15 @@ class GemmAutotuner {\n \n     auto tuned_func = [&](const se::blas::AlgorithmType& algorithm)\n         -> absl::StatusOr<se::blas::ProfileResult> {\n+      // Do a warm-up run first, without a profile result. This avoids a timeout\n+      // and error message if lazy module loading is enabled by ensuring that\n+      // lazy loading happens outside the GpuTimer. RunGemm swallows error codes\n+      // when profile_result is passed, as it is in the measurement below, but\n+      // not otherwise. It is, therefore, consistent to ignore the error code\n+      // here.\n+      static_cast<void>(RunGemm(gemm_config, lhs_buffer_, rhs_buffer_,\n+                                output_buffer_, workspace_buffer,\n+                                deterministic_ops_, stream_, algorithm));\n       se::blas::ProfileResult profile_result;\n       // We expect GemmWithAlgorithm to fail sometimes -- in fact, it will fail\n       // for all algorithms if we're targeting < sm_50. But because we pass a\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/stream_executor/build_defs.bzl",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer.cc",
            "patches": [
                {
                    "Id": 81,
                    "hunk size": 13,
                    "hunk": "@@ -51,10 +51,21 @@ absl::Duration RandomDuration() {\n   return absl::Microseconds(distribution(rng));\n }\n \n+bool ShouldLaunchDelayKernel() {\n+  // Only launch the delay kernel if CUDA_LAUNCH_BLOCKING is not set to 1.\n+  static bool value = [] {\n+    const char* blocking = std::getenv(\"CUDA_LAUNCH_BLOCKING\");\n+    return !blocking || std::string_view{blocking} != \"1\";\n+  }();\n+  return value;\n+}\n+\n }  // namespace\n \n /*deprecated*/ /*static*/ absl::StatusOr<GpuTimer> GpuTimer::Create(\n     GpuStream* stream) {\n+  // This deprecated factory does not launch the delay kernel and may lead to\n+  // reduced measurement accuracy.\n   GpuExecutor* parent = stream->parent();\n   GpuContext* context = parent->gpu_context();\n   GpuEventHandle start_event;\n"
                },
                {
                    "Id": 82,
                    "hunk size": 13,
                    "hunk": "@@ -97,6 +172,17 @@ GpuTimer::CreateIfNeeded(GpuStream* stream, bool is_needed) {\n \n GpuTimer::~GpuTimer() {\n   GpuContext* context = parent_->gpu_context();\n+  if (semaphore_ && !is_stopped_) {\n+    // Signal the delay kernel that it can exit\n+    *semaphore_ = GpuSemaphoreState::Release;\n+    // Wait for the delay kernel to exit before destroying the value that it is\n+    // watching.\n+    absl::Status status =\n+        GpuDriver::SynchronizeStream(context, stream_->gpu_stream());\n+    if (!status.ok()) {\n+      LOG(ERROR) << status;\n+    }\n+  }\n   if (start_event_ != nullptr) {\n     absl::Status status = GpuDriver::DestroyEvent(context, &start_event_);\n     if (!status.ok()) {\n"
                },
                {
                    "Id": 83,
                    "hunk size": 14,
                    "hunk": "@@ -117,6 +203,18 @@ absl::StatusOr<absl::Duration> GpuTimer::GetElapsedDuration() {\n   }\n   TF_RETURN_IF_ERROR(GpuDriver::RecordEvent(parent_->gpu_context(), stop_event_,\n                                             stream_->gpu_stream()));\n+  // If we launched the delay kernel then check if it already timed out.\n+  if (semaphore_) {\n+    if (*semaphore_ == GpuSemaphoreState::TimedOut) {\n+      // The delay kernel did not achieve the intended result.\n+      LOG(ERROR) << \"Delay kernel timed out: measured time has sub-optimal \"\n+                    \"accuracy. There may be a missing warmup execution, please \"\n+                    \"investigate in Nsight Systems.\";\n+    } else {\n+      // Signal that the kernel can exit\n+      *semaphore_ = GpuSemaphoreState::Release;\n+    }\n+  }\n   float elapsed_milliseconds = NAN;\n   if (!GpuDriver::GetEventElapsedTime(parent_->gpu_context(),\n                                       &elapsed_milliseconds, start_event_,\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer.h",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer_kernel.cu.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/gpu/gpu_timer_kernel.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f28921d9bb59159eef5731e035d7640aadf8f2cb",
    "message": "Pass MLIR bytecode across XLA Extension boundary for JAX when converting StableHLO<->MHLO\n\nPiperOrigin-RevId: 622294316",
    "date": "2024-04-05T16:00:26-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/python/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/python/mlir.cc",
            "patches": [
                {
                    "Id": 84,
                    "hunk size": 12,
                    "hunk": "@@ -92,6 +94,16 @@ std::string PrintModule(mlir::ModuleOp module) {\n   return s;\n }\n \n+absl::StatusOr<std::string> SerializeUsingBytecode(mlir::ModuleOp module) {\n+  std::string bytecode;\n+  llvm::raw_string_ostream os(bytecode);\n+  mlir::BytecodeWriterConfig config;\n+  if (mlir::failed(mlir::writeBytecodeToFile(module, os, config))) {\n+    return absl::InvalidArgumentError(\"mlir::writeBytecodeToFile failed\");\n+  }\n+  return bytecode;\n+}\n+\n void EnablePrintBeforeAndAfter(mlir::PassManager& pm) {\n   auto print_before = [](mlir::Pass*, mlir::Operation*) { return true; };\n   auto print_after = [](mlir::Pass*, mlir::Operation*) { return true; };\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/python/xla_client.py",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/python/xla_extension/mlir.pyi",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ba549a246d9d466de305a42640c878e43cbaf796",
    "message": "Adds the necessary logic for the creation, deletion and for checking the readiness of a `BasicStringArray`, a simple `ifrt::Array` implementation that wraps a local (or host) string buffer.\n\nPiperOrigin-RevId: 632270272",
    "date": "2024-05-09T15:12:29-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/python/pjrt_ifrt/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/python/pjrt_ifrt/basic_string_array.cc",
            "patches": [
                {
                    "Id": 85,
                    "hunk size": 11,
                    "hunk": "@@ -43,17 +45,14 @@ char BasicStringArray::ID = 0;\n absl::StatusOr<tsl::RCReference<BasicStringArray>> BasicStringArray::Create(\n     Client* client, Shape shape, std::shared_ptr<const Sharding> sharding,\n     Future<Buffers> buffers, OnDoneWithBuffer on_done_with_buffer) {\n+  if (!buffers.IsValid()) {\n+    return absl::InvalidArgumentError(\"Got buffers_ future is invalid\");\n+  }\n   return tsl::MakeRef<BasicStringArray>(client, std::move(shape),\n                                         std::move(sharding), std::move(buffers),\n                                         std::move(on_done_with_buffer));\n }\n \n-absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::FullyReplicatedShard(\n-    ArrayCopySemantics semantics) {\n-  // Make a single sharded BasicStringArray from the first shard.\n-  return absl::UnimplementedError(\"Not implemented\");\n-}\n-\n BasicStringArray::BasicStringArray(Client* client, Shape shape,\n                                    std::shared_ptr<const Sharding> sharding,\n                                    Future<Buffers> buffers,\n"
                },
                {
                    "Id": 86,
                    "hunk size": 19,
                    "hunk": "@@ -85,19 +131,14 @@ absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::Reshard(\n   return absl::UnimplementedError(\"Not implemented\");\n }\n \n-Future<> BasicStringArray::GetReadyFuture() const {\n-  DCHECK(this);\n-  return Future<>(absl::UnimplementedError(\"Not implemented\"));\n-}\n-\n-Future<> BasicStringArray::Delete() {\n-  DCHECK(this);\n-  return Future<>(absl::UnimplementedError(\"Not implemented\"));\n+absl::StatusOr<tsl::RCReference<Array>> BasicStringArray::FullyReplicatedShard(\n+    ArrayCopySemantics semantics) {\n+  // Make a single sharded BasicStringArray from the first shard.\n+  return absl::UnimplementedError(\"Not implemented\");\n }\n \n-bool BasicStringArray::IsDeleted() const {\n-  DCHECK(this);\n-  return false;\n+absl::StatusOr<std::unique_ptr<PjRtLayout>> BasicStringArray::layout() const {\n+  return absl::UnimplementedError(\"Not implemented\");\n }\n \n std::string BasicStringArray::DebugString() const {\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/python/pjrt_ifrt/basic_string_array.h",
            "patches": [
                {
                    "Id": 87,
                    "hunk size": 7,
                    "hunk": "@@ -80,7 +86,10 @@ class BasicStringArray final\n     return DType(DType::kString);\n   }\n \n-  const Shape& shape() const override { return shape_; }\n+  const Shape& shape() const override {\n+    DCHECK(this);\n+    return shape_;\n+  }\n \n   const Sharding& sharding() const override {\n     DCHECK(this);\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/python/pjrt_ifrt/pjrt_client.cc",
            "patches": [
                {
                    "Id": 88,
                    "hunk size": 7,
                    "hunk": "@@ -209,6 +265,11 @@ absl::StatusOr<tsl::RCReference<Array>> PjRtClient::MakeArrayFromHostBuffer(\n     Client::HostBufferSemantics semantics,\n     std::function<void()> on_done_with_host_buffer) {\n   DCHECK(this);\n+  if (dtype.kind() == DType::kString) {\n+    return MakeStringArrayFromHostBuffer(this, data, dtype, shape, byte_strides,\n+                                         sharding, semantics,\n+                                         on_done_with_host_buffer);\n+  }\n   if (!llvm::isa<const SingleDeviceSharding>(sharding.get())) {\n     return InvalidArgument(\n         \"Only SingleDeviceSharding is supported: sharding=%s\",\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/python/pjrt_ifrt/pjrt_client.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b59abe629e614d9cacf1169c0cbdc9070f23dd48",
    "message": "Fix a bug in the mhlo quant unpacking pass\n\nThe bug occurs when output has dynamic shape but input doesn't. The fix checks output shape to determine whether to use dynamic broadcast op.\n\nPiperOrigin-RevId: 588612987",
    "date": "2023-12-06T19:11:12-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/de03551e0b32eab46372ae6e00a47b5b6b612b29",
    "message": "Fix logic for finding first non-zero and slicing information for\nhttps://github.com/google/jax/issues/13824.\n\nThe problem is that the code to scan the literal for non-zeros was completely broken. This made it find negative bounds when scanning (because it never would check the final dimension for -1's and simply scan memory arbitrarily until it found a non-zero).\nI also fixed how the non-zero checker works for an entirely zero weight matrix. In this case, the bounds would all loop (which is wrong).\nAlso, when checking the slicing to see if only the feature dim was sliced, the output shape was used instead of the input shape making the check trivially true (slice-dims always equal the input-dims).\n\nPiperOrigin-RevId: 503555714",
    "date": "2023-01-20T17:33:07-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f5381e0e10b5a61344109c1b7c174c68110f7629",
    "message": "Fix OOB error when op input sizes do not match.\n\nIn cases where op input sizes are specified as in\n```\nREGISTER_OP(\"DynamicStitch\")\n    .Input(\"indices: N * int32\")\n    .Input(\"data: N * T\")\n    .Output(\"merged: T\")\n    .Attr(\"N : int >= 1\")\n    .Attr(\"T : type\")\n    .SetShapeFn(DynamicStitchShapeFunction);\n```\nif differing number of inputs are provided (e.g. 3 for `indices` and 4 for `data`)\nwe can get a crash in the executor when parsing the inputs, even before the kernel\ncalled.  Here we avoid this by checking the return code for the argument id and\nexit early.\n\nPiperOrigin-RevId: 478068540",
    "date": "2022-09-30T13:39:28-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/24b8ded9e921e78f91ed3fec50d7623803ac4bd4",
    "message": "Replace instance checks for `ops.EagerTensor` with `core.Value`.\n\nPiperOrigin-RevId: 508793367",
    "date": "2023-02-10T17:43:14-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/5006a30cce50b6f613317a2ba5db16edd34cf02e",
    "message": "Improve ragged_cross_op input ragged splits check and fix flaky ragged_cross_op_tests.\n\nPiperOrigin-RevId: 548243985",
    "date": "2023-07-14T16:03:18-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f52a6ffe40e525254b54ac371e6a8e3531489655",
    "message": "[MacOS][Pluggable Device] Fixes for macos pluggable device.\n\n1. Disable the Eigen max align check as we use custom device memory\n   allocator in Metal plugin\n2. Disable list kernels on Pluggable devices as Device memory on MacOS\n   pluggable device doesn't allow us to index at a offset from the base\n   MTLBuufer location.\n3. Disable XLA compilation path as its not supported in pluggable\n   architecture",
    "date": "2023-01-20T14:44:24-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/22ca232e2f59fa9a9c49100e7976a9a82f026f69",
    "message": "Added additional checks to model initialization. Corrected error codes for the checks inside model loader.\n\nPiperOrigin-RevId: 479595356",
    "date": "2022-10-07T09:17:31-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e44f8a08051baa58bde9130a844a1b82a8179526",
    "message": "check hasattr on the type, not the instance.\n\nhasattr on the instance triggers __getattr__ which carries very undesirable\neffects, such as running Ops on a donated buffer.\n\nLong term, we may want to audit all uses of hasattr on TensorFlow instances\nthat overrides __getattr__ in nontrival (e.g. running tf Ops) ways. They will\nalmost always cause trouble here and there because TensorFlow is quite far\nfrom being able guarantee if an Op returns or consumes is actually valid in all cases. Things will improve give it time, but if we can avoid such strong assumptions the system tend to get more robust.\n\nPiperOrigin-RevId: 578261984",
    "date": "2023-10-31T12:18:59-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8b742f8559e88474735d0a2c03e00da65e40b412",
    "message": "Fix check error on shape overflow.\n\nFixes #60198, #60275\n\nPiperOrigin-RevId: 529127714",
    "date": "2023-05-03T10:28:05-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/48d3e51a1bd128554dd129251a51b6e12918a604",
    "message": "Add a check to HandleFromInput to ensure that the resource isn't empty.\n\nPiperOrigin-RevId: 510250667",
    "date": "2023-02-16T14:56:06-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/bc195281edfbe5c93fe1e7a05354d3747215ea91",
    "message": "Ensure that all instructions which are inserted by HloRematerialization have .remat in their name.\n\nCloned computations are excluded from this check.\n\nPiperOrigin-RevId: 629553524",
    "date": "2024-04-30T16:52:49-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/hlo_rematerialization.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/d070aa7f0f409825e42358b6b29491f4842ceba3",
    "message": "Use the generic printer and disable validation when printing debug operations.\n\nOp validation can be time-consuming on large models.\n\nPiperOrigin-RevId: 532602582",
    "date": "2023-05-16T16:18:40-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/83f96361c8ea039f8a44aad7f378dc805f3ed3ce",
    "message": "Update Eigen to commit:dceb779ecd822f55b4ae78f760371b0e08a889f2\n\nCHANGELOG\n=========\ndceb779ec - Fix test for pow with mixed integer types. We do not convert the exponent if it is an integer type.\nafc014f1b - Allow mixed types for pow(), as long as the exponent is exactly representable in the base type.\nb2c82a934 - Remove bad skew_symmetric_matrix3 test.\ne8a2aa24a - Fix a couple of issues with unary pow():\n07d075995 - [ROCm] Fix for sparse matrix related breakage on ROCm.\nfb212c745 - Fix g++-6 constexpr and c++20 constexpr build errors.\nec9c7163a - Feature/skew symmetric matrix3\n311ba66f7 - Fix realloc for non-trivial types.\n3c37dd2a1 - Tweak bound for pow to account for floating-point types.\nf9dfda28a - Add missing comparison operators for GPU packets.\n242325eca - Remove unused variable.\n133498c32 - Add constexpr, test for C++14 constexpr.\n69f50e3a6 - Adjust overflow threshold bound for pow tests.\n3e44f960e - Reduce compiler warnings for tests.\nb7e21d4e3 - Call check_that_malloc_is_allowed() in aligned_realloc()\n6e83e906c - fix typo in doc/TutorialSparse.dox\n525f06667 - fixed msvc compilation error in GeneralizedEigenSolver.h\nf241a2c18 - Add asserts for index-out-of-bounds in IndexedView.\nf5364331e - Fix some cmake issues.\nd816044b6 - Fix mixingtypes tests.\n94cc83faa - 2 typos fix in the 3rd table.\n30c42222a - Fix some test build errors in new unary pow.\nbd393e15c - Vectorize acos, asin, and atan for float.\ne5af9f87f - Vectorize pow for integer base / exponent types\n8acbf5c11 - re-enable pow for complex types\n7064ed134 - Specialize psign<Packet8i> for AVX2, don't vectorize psign<bool>.\n98e51c9e2 - Avoid undefined behavior in array_cwise test due to signed integer overflow\na7c1cac18 - Fix GeneralizedEigenSolver::info() and Asserts\n714678fc6 - Add missing ptr in realloc call.\nb2a13c9dd - Sparse Core: Replace malloc/free with conditional_aligned\n6aad0f821 - Fix psign for unsigned integer types, such as bool.\n1a09defce - Protect new pblend implementation with EIGEN_VECTORIZE_AVX2\n7c67dc67a - Use proper double word division algorithm for pow<double>. Gives 11-15% speedup.\n7a3b667c4 - Add support for AVX512-FP16 for vectorizing half precision math\n76a669fb4 - add fixed power unary operation\n39fcc8979 - Removed unnecessary checks for FP16C\n2f7cce2dd - [SYCL] Fix some SYCL tests\n27367017b - Disable bad \"deprecated warning\" edge-case in BDCSVD\nb8e93bf58 - Eliminate bool bitwise warnings.\n66ea0c09f - Don't double-define Half functions on aarch64\n97e0784dc - Vectorize the sign operator in Eigen.\nbe20207d1 - Fix vectorized Jacobi Rotation\n7a87ed1b6 - Fix code and unit test for a few corner cases in vectorized pow()\n9e0afe0f0 - Fix non-VSX PowerPC build\n84a9d6fac - Fix use of Packet2d type for non-VSX.\nce60a7be8 - Partial Packet support for GEMM real-only (PowerPC).  Also fix compilation warnings & errors for some conditions in new API.\n5a1c7807e - Fix inner iterator for sparse block.\n39d22ef46 - Fix flaky packetmath_1 test.\n7896c7dc6 - Use numext::sqrt in ConjugateGradient.\ne618c4a5e - Improve pblend AVX implementation\nef4654bae - Add true determinant to QR and it's variants\nb7668c037 - Avoid including <sstream> with EIGEN_NO_IO\n7dd3dda3d - Updated AccelerateSupport documentation after PR 966.\n69714ff61 - Add Sparse Subset of Matrix Inverse\n\nPiperOrigin-RevId: 474559050",
    "date": "2022-09-15T07:47:37-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/1123fb3da1b2a8977bb01fc285a7be8f81914201",
    "message": "Replace Interval::NumElements with ::GetLoopTripCount.\n\nThe latter has some safety checks built in. Replace unsafe\nuses of NumElements with `IsFeasible`.\n\nPiperOrigin-RevId: 641840653",
    "date": "2024-06-10T04:10:55-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/fusions/mlir/elemental_hlo_to_mlir.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/model/coalescing_analysis.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/model/indexing_map.cc",
            "patches": [
                {
                    "Id": 89,
                    "hunk size": 13,
                    "hunk": "@@ -784,8 +784,17 @@ void Interval::Print(std::ostream& out) const {\n   out << '[' << lower << \", \" << upper << \"]\";\n }\n \n+int64_t Interval::GetLoopTripCount() const {\n+  if (!IsFeasible()) {\n+    return 0;\n+  }\n+  DCHECK((static_cast<absl::int128>(upper) - lower + 1) <=\n+         std::numeric_limits<int64_t>::max());\n+  return upper - lower + 1;\n+}\n+\n Interval::ComparisonResult Interval::operator>(const Interval& b) const {\n-  if (NumElements() == 0 || b.NumElements() == 0) {\n+  if (!IsFeasible() || !b.IsFeasible()) {\n     return {std::nullopt};\n   }\n   if (lower > b.upper) {\n"
                },
                {
                    "Id": 90,
                    "hunk size": 4,
                    "hunk": "@@ -799,7 +808,7 @@ Interval::ComparisonResult Interval::operator>(const Interval& b) const {\n \n Interval::ComparisonResult Interval::operator==(const Interval& b) const {\n   Interval intersection = Intersect(b);\n-  if (intersection.NumElements() == 0) return {false};\n+  if (!intersection.IsFeasible()) return {false};\n   if (intersection.IsPoint() && IsPoint() && b.IsPoint()) {\n     return {true};\n   }\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/gpu/model/indexing_map.h",
            "patches": [
                {
                    "Id": 91,
                    "hunk size": 9,
                    "hunk": "@@ -46,9 +46,14 @@ struct Interval {\n   void Print(std::ostream& out) const;\n \n   bool IsPoint() const { return lower == upper; }\n-  int64_t NumElements() const { return upper - lower + 1; }\n   bool IsFeasible() const { return lower <= upper; }\n \n+  // Returns the number of elements in the interval. Asserts that the number of\n+  // elements fits in an int64_t. For this reason, this should only be used for\n+  // intervals corresponding to symbols, not for general intervals. Use\n+  // `IsFeasible` to check if the interval is non-empty.\n+  int64_t GetLoopTripCount() const;\n+\n   bool Contains(int64_t value) const {\n     return value >= lower && value <= upper;\n   }"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7e49357b9ebb225708fd75a29c261bb43f63e2a0",
    "message": "Fix a bug in the `xla-call-module-deserialization` pass where function name uniquification didn't consider name conflicts correctly\n\nThe current `NewFuncName()` implementation identifies a new function name that is unique in the target module to which StableHLO functions are inserted, but it does not check whether the newly generated name clashes with any existing function in the original StableHLO module. For example, if there are two functions `@foo` and `@foo0` in the StableHLO module and the target TF module has `@foo`, `NewFuncName()` will rename the first function `@foo` into `@foo0` as this name does not exist in the TF module, but this causes name collision between the two StableHLO functions.\n\nThis CL rewrites the renaming logic to leverage `mlir::SymbolTable::renameToUnique()`. This allows us to rename func names to be unique to both modules without having to manually generate func names, which simplifies the code.\n\nPiperOrigin-RevId: 578922049",
    "date": "2023-11-02T11:38:51-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f25c22bc494573699c05ee7c38b6154da8c8fc62",
    "message": "Support per-channel quantization for data movement ops\n\nThis is needed for per-channel quantized weight. Skip op/result type check in the pattern because some op may change for per-channel quantized type. E.g. Quantization axis may change after broadcast_in_dims.\n\nPiperOrigin-RevId: 582064121",
    "date": "2023-11-13T13:26:44-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/53fb0130851ad40d544105432f414b4ebe9e729d",
    "message": "Sliced prefetch tuning: When we check if we have enough copy resources for a sliced prefetch, inflate the required amount of resources for slices.\n\nPiperOrigin-RevId: 553382368",
    "date": "2023-08-03T00:19:20-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ae74390104b5c7bc2c78da5cf39c7e93ff422958",
    "message": "Add 64-bit triton matmul indexing mode\n\nIf any of the (M*K, K*N, M*N) sizes cross the 32 bit boundary, we must use 64-bit indexing, otherwise integer overflow will cause illegal memory accesses.\n\nPiperOrigin-RevId: 527242751",
    "date": "2023-04-26T06:04:06-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/7b9d773d28ba4062b79a75408f58a2143e80d64d",
    "message": "Disable fallback legalization for ops containing a list of SymbolRef attributes\n\nFallback legalization is not supported for such ops and this check makes sure that it is not attempted even if the op is added to the allow list.\n\nPiperOrigin-RevId: 494306878",
    "date": "2022-12-09T18:22:31-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/586392c0fdff62e8086267a5bf8327bc372366e7",
    "message": "Reland: Fix check in ShouldFuseInPlaceOp().\n\nWhen checking whether the consumer has an in-place operand that is shared with\nthe producer, it was missing the case that the in-place operand was the\nproducer itself.\n\nPiperOrigin-RevId: 545364078",
    "date": "2023-07-04T00:39:52-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/9bb678d2bd3d27e2ddc74fcf7270b0834cf008ad",
    "message": "[xla:gpu:runtime] Remove incorrect channel id check\n\nChannel id properly handled by GetNcclCollectiveConfig and there is no need to pass it to the custom call handler.\n\nPiperOrigin-RevId: 481928094",
    "date": "2022-10-18T08:39:39-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/87ded6c7697af6fdb9dbc022b385dbbaf7a4892a",
    "message": "Fix bool type inference.\n\nisinstance(True, int) == True, so check bool type before int type check.\n\nPiperOrigin-RevId: 554619002",
    "date": "2023-08-07T16:02:21-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/d4fc6f9cad02c96a69dff07d5daa0cd04929aeb0",
    "message": "Fix `FuseFullyConnectedAndMul` to check if FullyConnected has only one use, that is the LHS of Mul Op. Otherwise this will duplicate the fullyconnected op to serve the remaining uses.\n\nFor example, consider this MLIR graph with the output of FC used by multiple ops:\n```\n  %0 = \"tfl.fully_connected\"(%arg0, %cst0, %cst1) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<4x2xf32>, tensor<2x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n  %1 = \"tfl.reshape\"(%0, %cst3) : (tensor<4x2xf32>, tensor<2xi32>) -> tensor<1x8xf32>\n  %2 = \"tfl.mul\"(%0, %cst2) {fused_activation_function = \"RELU6\"} : (tensor<4x2xf32>, tensor<2xf32>) -> tensor<4x2xf32>\n```\n\nThe expected output for the fusion is a `NEW` FC op with the Mul op fused into the FC kernel, and the `OLD` FC op deleted. But the reshape-op above does not need/expect a fc with mul fused. So, the 'OLD' fc op is still around to feed into the ReshapeOp. Hence we are computing two fully-connected ops in the graph instead of one.\n\nThis particular bug has always been there but got triggered on this customer model(b/318735772) by this `recent` change cl/553508906.\n\nThis is a broader issue that needs to be fixed for all the BinaryOp(*FC_Op*) fusion optimization variants on FullyConnected Op, Convolutions, etc.\n\nPiperOrigin-RevId: 602536477",
    "date": "2024-01-29T17:18:36-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/compiler/mlir/lite/transforms/optimize.cc",
            "patches": [
                {
                    "Id": 92,
                    "hunk size": 8,
                    "hunk": "@@ -1249,6 +1249,12 @@ struct FuseFullyConnectedAndMul : public OpRewritePattern<TFL::MulOp> {\n     auto fc_op = dyn_cast_or_null<TFL::FullyConnectedOp>(\n         mul_op.getLhs().getDefiningOp());\n     if (!fc_op) return failure();\n+\n+    // Check if FullyConnected has only one use, that is the LHS of Mul Op.\n+    // Otherwise this will duplicate the fullyconnected op to serve the\n+    // remaining uses.\n+    if (!fc_op->hasOneUse()) return failure();\n+\n     Value filter = fc_op.getFilter();\n     Value bias = fc_op.getBias();\n     ElementsAttr cst_tmp;"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/f3819de491f740984b2975294bed0e642c66c802",
    "message": "[PJRT C API] Fix missing mesh shape and mesh ids in ExecutableBuildOptions proto.\n\nAlso check whether compile_thread_pool is nullptr and fail the serialization if it is not nullptr.\n\nPiperOrigin-RevId: 590380587",
    "date": "2023-12-12T16:51:32-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b7e107eaa6dffb649d055d893a1fce734ee50d55",
    "message": "PR #7922: [XLA:GPU] Error out for ptxas version - Update ptxas version check\n\nImported from GitHub PR https://github.com/openxla/xla/pull/7922\n\nThe version check is extended to all ptxas version `12.3.103`\nCopybara import of the project:\n\n--\n50c9aeb611685f1ee91771f031f9b8cafc58cb10 by Ayan Moitra <amoitra@nvidia.com>:\n\nUpdate ptxas version check\n\n--\n429b0866625761d13f51a70b404ab13714bba25b by Ayan Moitra <amoitra@nvidia.com>:\n\nnit fix\n\n--\n5dab1394c0a0370219c1d0b35f3cd9a949b5f3bd by Ayan Moitra <amoitra@nvidia.com>:\n\nSpecific ptxas version affected\n\nMerging this change closes #7922\n\nPiperOrigin-RevId: 595407179",
    "date": "2024-01-03T08:27:12-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/stream_executor/gpu/asm_compiler.cc",
            "patches": [
                {
                    "Id": 93,
                    "hunk size": 15,
                    "hunk": "@@ -258,11 +258,14 @@ tsl::StatusOr<std::vector<uint8_t>> CompileGpuAsm(int cc_major, int cc_minor,\n                                                   const char* ptx_contents,\n                                                   GpuAsmOpts options,\n                                                   bool cancel_if_reg_spill) {\n-  auto ptxas_version_tuple = GetAsmCompilerVersion(options.preferred_cuda_dir);\n-  if (ptxas_version_tuple.value() == std::array<int64_t, 3>{12, 3, 1}) {\n-    return tsl::errors::Internal(\n-        absl::StrFormat(\"ptxas 12.3.1 has a bug that we think can affect XLA. \"\n-                        \"Please use a different version.\"));\n+  TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n+                      GetAsmCompilerVersion(options.preferred_cuda_dir));\n+  if (ptxas_version_tuple == std::array<int64_t, 3>{12, 3, 103}) {\n+    return absl::InternalError(absl::StrFormat(\n+        \"ptxas %d.%d.%d has a bug that we think can affect XLA. \"\n+        \"Please use a different version.\",\n+        std::get<0>(ptxas_version_tuple), std::get<1>(ptxas_version_tuple),\n+        std::get<2>(ptxas_version_tuple)));\n   }\n   std::string ptxas_path =\n       FindCudaExecutable(\"ptxas\", options.preferred_cuda_dir);"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/d1ad87b61c28834a48d84d709f21aeedbd9c6521",
    "message": "[XLA:GPU][NFC] Transform dimension propagation to the functional paradigm\n\nThis intends to make it easier to implement new fusion strategies, such as \"Separate multiple uses of nodes within one scope when they are incompatible in Triton GEMM fusion\".\n\n\"Renames\":\nAnalyzeForFusion -> GetPropagatedDimOrdersAndRequirementsIfProfitablyFusible\nRequireSupportedInstruction -> GetPropagatedDimOrdersAndRequirements\nHandleInstruction -> GetPropagatedDimOrders\nRequireSupportedDimOrder -> GetRequirementsIfSupportedOrder\nRequireSupportedDimOrders -> GetRequirementsIfSupportedOrders\nDimOrderUpdates -> DimOrdersAndReqs\n\nNotable logic changes:\n\nI split out the splittable_dimension_major_part_size from DotProperties to DotRequirements, because it's not really a property of the dot, but rather a requirement which can be imposed by the instructions of the fusion.\n\nI explicitly return an error if a dimension split would be needed for Softmax in GetRequirementsIfSupportedOrder.\n\nI don't check IsSupportedSplittableDimensionMajorPartSize in GetRequirementsIfSupportedOrder anymore, I just check that in CombineDimOrdersAndReqs after the propagation is done.\n\nPiperOrigin-RevId: 585650283",
    "date": "2023-11-27T07:54:13-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/09b2e52e6dd1fd746503d8d457f16d09e272b03c",
    "message": "Do not match `RewriteQuantizedDotGeneralOpToTflFullyConnectedOrBatchMatmulOp` when dot_general is not followed by a requantization.\n\nThe case without a following requantization is partially supported by the other pattern, `RewriteUpstreamQuantizedDotGeneralOpToTflFullyConnectedOp`, but this change guards against an edge case where the output is an i32 quantized tensor.\nWithout this change, the program will crash when a `dot_general` produces an `i32` quantized tensor but is not followed by a requantization.\n\nPiperOrigin-RevId: 601407496",
    "date": "2024-01-25T04:30:59-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/compiler/mlir/lite/stablehlo/transforms/uniform_quantized_stablehlo_to_tfl_pass.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b45878744c636d5f32cc7a48e348c3dbfbe6d160",
    "message": "[xla:gml_st] Relax a check that refuses to tile parallel dimensions when op contains a reduction dimension\n\nThe user is responsible for making sure to not ask to generate parallel loops on reduction dimension.\n\nPiperOrigin-RevId: 481366310",
    "date": "2022-10-15T10:30:00-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c510c1b8b1ef5be1d65971f5b9e21e61becd0bb3",
    "message": "[XLA] Remove IsCalledComputation from HloComputation.\n\nThe function doesn't do what it seems. There are more types of called instruction that are not accounted in this check.\n\nPiperOrigin-RevId: 605270151",
    "date": "2024-02-08T04:11:31-08:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/hlo/ir/hlo_computation.h",
            "patches": [
                {
                    "Id": 94,
                    "hunk size": 10,
                    "hunk": "@@ -807,18 +807,14 @@ class HloComputation {\n   HloInstruction* AsyncStart() const { return async_start_; }\n \n   void AddAsyncStart(HloInstruction* async_instruction) {\n-    CHECK(!IsCalledComputation());\n+    // TODO: Add instruction type for async instructions.\n+    CHECK(instruction_type() == InstructionType::kUnset);\n     CHECK(async_instruction->opcode() == HloOpcode::kAsyncStart);\n     async_start_ = async_instruction;\n   }\n \n   void RemoveAsyncStart() { async_start_ = nullptr; }\n \n-  // Returns if this computation is invoked by an Hlo instruction.\n-  bool IsCalledComputation() const {\n-    return IsFusionComputation() || IsCustomCallComputation();\n-  }\n-\n   // Clear the unique ID of the computation so that it can be re-assigned, such\n   // as for the purpose of compacting the unique IDs.\n   void ClearUniqueIdInternal() { unique_id_ = -1; }"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/74a206527e98a8da65a8c341a8ac182b2c8ee377",
    "message": "[tflite-gpu] Push select_v2 dim check up from inference to parser.\n\nPiperOrigin-RevId: 565113938",
    "date": "2023-09-13T11:47:13-07:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/c9cb6d66bc77519202b98097f83fdd07241292a2",
    "message": "[XLA:GPU] Remove an extraneous check which mistakenly prevents fusing a relu with a gemm when the inputs (bias included) are square.\n\nPiperOrigin-RevId: 493393214",
    "date": "2022-12-06T13:18:28-08:00",
    "label": "NO",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8a2e7deb21f02e4072d6b62cf7f447b9264afe01",
    "message": "Adjust checks for `type(Tensor)` to isinstance or is_eager/is_symbolic_tensor.\n\nPiperOrigin-RevId: 525801792",
    "date": "2023-04-20T11:41:01-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e1ad3b74ad44b883c7b3fdc3a19adcea1d28bfbc",
    "message": "[XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast is an\neffective scalar. This short-circuit avoids crashing within last_dimension when\nattempting to match and either the operand or the result of the bitcast has a\nshape with rank 0.\n\nPiperOrigin-RevId: 548645429",
    "date": "2023-07-17T04:14:46-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/bc27c5a90f3a009c63072a57495f6b7b8d9f1dc1",
    "message": "Avoid forming multi-output fusions with incompatible non-trivial heroes.\n\nOne part of the fix is to run CSE before MultiOutputFusion, so that we can more\nreliably detect tiled transpose fusions. If we only run CSE after\nMultiOutputFusion, we would potentially detect more tiled transpose roots as\nbefore, some of them may be incompatible.\nThe second part of the fix is to add additional checks to FusionMerger, so that\nit does not merge two fusions with incompatible non-trivial heroes.\n\nPiperOrigin-RevId: 539875397",
    "date": "2023-06-13T00:21:25-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/8e8ee3834146461a7472de086742752eb7aca245",
    "message": "PR #9688: [XLA:CPU] Add F16 support for oneDNN matmul\n\nImported from GitHub PR https://github.com/openxla/xla/pull/9688\n\nThis PR enables F16 support for dot operation when onednn is enabled and adds tests. In the existing pipeline, F16 dot is converted to FP32 through the change-op-data-type pass. With the changes in this PR, if onednn is enabled and the conditions for rewriting dot to onednn matmul are met then F16 dot is not upcast to FP32.\nCopybara import of the project:\n\n--\n190e446d920fb82732054383c81545c528d6d9d9 by Kanvi Khanna <kanvi.khanna@intel.com>:\n\nAdd F16 support for matmul\n\n--\nf99f2bb30febc5c818c28c379232151085deebd6 by Kanvi Khanna <kanvi.khanna@intel.com>:\n\nfix formatting\n\n--\naef8cb1685539a5e1c4765262b8ed5f537613d58 by Kanvi Khanna <kanvi.khanna@intel.com>:\n\nfix for failing test\n\n--\n72dd379ce16eab6838ef568b20ab15e0e0ea0449 by Kanvi Khanna <kanvi.khanna@intel.com>:\n\nAdd check in ChangeOpDataType pass for F16 support as a separate pass for F16 support is not needed; undo changes for BMM;\n\nMerging this change closes #9688\n\nPiperOrigin-RevId: 617161505",
    "date": "2024-03-19T08:10:13-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/change_op_data_type.cc",
            "patches": [
                {
                    "Id": 95,
                    "hunk size": 5,
                    "hunk": "@@ -18,6 +18,9 @@ limitations under the License.\n #include <optional>\n \n #include \"xla/service/hlo_creation_utils.h\"\n+#if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)\n+#include \"xla/service/cpu/onednn_matmul_rewriter.h\"\n+#endif  // INTEL_MKL && ENABLE_ONEDNN_V3\n \n namespace xla {\n namespace {\n"
                },
                {
                    "Id": 96,
                    "hunk size": 8,
                    "hunk": "@@ -59,6 +62,12 @@ absl::StatusOr<bool> ChangeOpDataType::Run(\n       if (it == to_type_map_.end()) {\n         continue;\n       }\n+#if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)\n+      if (instr->opcode == HloOpcode::kDot &&\n+          OneDnnMatMulRewriter::ShouldRewrite(instr)) {\n+        continue;\n+      }\n+#endif  // INTEL_MKL && ENABLE_ONEDNN_V3\n       const PrimitiveType to_type = it->second;\n       absl::InlinedVector<HloInstruction*, 8> new_operands;\n       for (HloInstruction* operand : instr->mutable_operands()) {\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/cpu/onednn_matmul_rewriter.cc",
            "patches": [
                {
                    "Id": 97,
                    "hunk size": 12,
                    "hunk": "@@ -55,16 +55,6 @@ inline Status ValidateDotDimensionNumbers(\n   return OkStatus();\n }\n \n-// We also check if the convert instruction has only one use.\n-inline bool AllOperandsConvertedFromBF16ToF32(const HloInstruction* instr) {\n-  return absl::c_all_of(instr->operands(), [](HloInstruction* operand) {\n-    return Match(operand,\n-                 m::Convert(m::Op().WithElementType(PrimitiveType::BF16))\n-                     .WithElementType(PrimitiveType::F32)\n-                     .WithOneUse());\n-  });\n-}\n-\n template <typename Pattern>\n auto ElementwiseSafeIntermediate(HloInstruction** instr, Pattern pattern) {\n   return m::AnyOf<HloInstruction>(m::Broadcast(instr, pattern.WithOneUser()),\n"
                },
                {
                    "Id": 98,
                    "hunk size": 8,
                    "hunk": "@@ -548,11 +519,11 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n       // for bf16 case to avoid datatype mismatch.\n       if (optional_dot_bitcast != nullptr &&\n           optional_dot_bitcast->opcode() == HloOpcode::kBitcast) {\n-        if (matmul_call->shape().element_type() == PrimitiveType::BF16) {\n+        if (LowPrecisionType(matmul_call)) {\n           auto bitcast_call =\n               matmul_call->AddInstruction(HloInstruction::CreateBitcast(\n-                  ShapeUtil::ChangeElementType(instr->shape(),\n-                                               PrimitiveType::BF16),\n+                  ShapeUtil::ChangeElementType(\n+                      instr->shape(), matmul_call->shape().element_type()),\n                   matmul_call));\n           new_instr =\n               bitcast_call->AddInstruction(HloInstruction::CreateConvert(\n"
                },
                {
                    "Id": 99,
                    "hunk size": 4,
                    "hunk": "@@ -564,7 +535,7 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n               HloInstruction::CreateBitcast(instr->shape(), matmul_call));\n         }\n       } else {\n-        if (matmul_call->shape().element_type() == PrimitiveType::BF16) {\n+        if (LowPrecisionType(matmul_call)) {\n           new_instr = matmul_call->AddInstruction(HloInstruction::CreateConvert(\n               ShapeUtil::ChangeElementType(matmul_call->shape(),\n                                            PrimitiveType::F32),\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/cpu/onednn_matmul_rewriter.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2800ecbea2e747fcf2ccb79f63c5299221b1c1b6",
    "message": "Remove shape checks on tensor shapes which may not yet be correctly set.\n\nSlice cannot accept negative sizes as the input shape may not be correctly set.\n\nPiperOrigin-RevId: 615396964",
    "date": "2024-03-13T07:27:22-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc",
            "patches": [
                {
                    "Id": 100,
                    "hunk size": 5,
                    "hunk": "@@ -3190,9 +3190,6 @@ class Subgraph {\n       TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(\n           delegate, logging_context, input_tensor, node->inputs->data[i],\n           node_index));\n-\n-      TF_LITE_ENSURE_EQ(logging_context, NumDimensions(&input_tensor),\n-                        NumDimensions(&output_tensor));\n     }\n \n     if (subgraph != nullptr) {\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/ef7c0b3680c6446fa83649184f50cb1edfbc3aef",
    "message": "Fix multi-client D2H transfer when Coordination service is enabled.\n\nThe added test case fails without the fixes.\nBaseRendezvousMgr does not implement the heuristics for 'empty' src device, which is used by TPU multiclient to represent the local host device. This CL adds that\nknowledge to it.\n\nI think this Rendezvous path is triggered under coordination service because\nnow the device manager observes remote devices too. My theory is without coordination service the local path is always triggered, hence there were no need to check for 'empty' src device.\n\nPiperOrigin-RevId: 490307160",
    "date": "2022-11-22T12:38:43-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/70b2702091d8842d0ca853d5d1cd8d1ad2c3a983",
    "message": "Reland clean up wrong callers of FindNonTrivialHero.\n\nWe had several callers of FindNonTrivialHero which called it with a fusion\ninstruction. This was usually an indicator that the call was not actually needed\nat all and we should just use the instruction itself as the hero.\nAlso adjust ChooseFusionKind to check the producer, too, whether it is a kInput\nfusion.\n\nPiperOrigin-RevId: 599125417",
    "date": "2024-01-17T04:18:09-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/fusion_merger.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/fusion_wrapper.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/gpu_fusible.cc",
            "patches": [
                {
                    "Id": 101,
                    "hunk size": 15,
                    "hunk": "@@ -390,12 +387,19 @@ bool IsLoopFusibleAsConsumer(const HloInstruction& instr,\n   // Any reduction can be fused as a consumer.\n   if (instr.opcode() == HloOpcode::kReduce) return true;\n \n-  return IsUniversallyLoopFusible(instr, hero);\n+  // We may have input fusions which effectively have turned into loop\n+  // fusions. Those should still be considered as loop fusible consumers,\n+  // but they are not universally loop fusible.\n+  if (!IsInputFusible(instr) && instr.opcode() == HloOpcode::kFusion &&\n+      instr.fusion_kind() == HloInstruction::FusionKind::kInput) {\n+    return true;\n+  }\n+\n+  return IsUniversallyLoopFusible(instr);\n }\n \n // Returns true if `instr` can be fused as a producer into a kLoop fusion.\n-bool IsLoopFusibleAsProducer(const HloInstruction& instr,\n-                             const HloInstruction& hero) {\n+bool IsLoopFusibleAsProducer(const HloInstruction& instr) {\n   // Instr should be fusible.\n   if (!instr.IsFusible()) return false;\n \n"
                },
                {
                    "Id": 102,
                    "hunk size": 10,
                    "hunk": "@@ -452,12 +456,8 @@ FusionDecision CanEmitInputFusedScatter(const HloInstruction& producer,\n \n FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n                                          const HloInstruction& consumer) {\n-  const auto& producer_hero = FindNonTrivialHero(producer);\n-  const auto& consumer_hero = FindNonTrivialHero(consumer);\n-  if (!IsLoopFusibleAsProducer(producer, producer_hero) &&\n-      !(GetDescriptionForTiledTransposeEmitter(producer, producer_hero)\n-            .has_value() &&\n-        &consumer_hero == &producer)) {\n+  if (!IsLoopFusibleAsProducer(producer) &&\n+      !IsInputFusibleTranspose(producer)) {\n     return \"the producer is not loop-fusible\";\n   }\n \n"
                },
                {
                    "Id": 103,
                    "hunk size": 5,
                    "hunk": "@@ -494,8 +492,7 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,\n     return can_fuse;\n   }\n \n-  if (!IsInputFusible(consumer) &&\n-      !IsLoopFusibleAsConsumer(consumer, consumer_hero)) {\n+  if (!IsInputFusible(consumer) && !IsLoopFusibleAsConsumer(consumer)) {\n     return \"the consumer is not input-fusible and not loop-fusible\";\n   }\n \n"
                },
                {
                    "Id": 104,
                    "hunk size": 4,
                    "hunk": "@@ -556,7 +553,7 @@ FusionDecision IsProducerMultiOutputFusible(const HloInstruction& producer) {\n     return \"In-place operations are present\";\n   }\n \n-  if (!IsLoopFusibleAsProducer(producer, FindNonTrivialHero(producer))) {\n+  if (!IsLoopFusibleAsProducer(producer)) {\n     return \"producer is not loop-fusible\";\n   }\n \n"
                },
                {
                    "Id": 105,
                    "hunk size": 17,
                    "hunk": "@@ -567,11 +564,18 @@ FusionDecision IsProducerMultiOutputFusible(const HloInstruction& producer) {\n   return {};\n }\n \n-// Returns shared memory usage for a given instruction in bytes.\n+// Returns an estimate of the shared memory usage for a given instruction in\n+// bytes.\n static int64_t SharedMemoryUsageNoCache(const HloInstruction& instr) {\n-  // For now we are only fusing reductions.\n-  if (instr.opcode() == HloOpcode::kReduce &&\n-      IsReductionFromOrToContiguousDimensions(instr)) {\n+  if (instr.opcode() == HloOpcode::kFusion) {\n+    int64_t sum = 0;\n+    for (const HloInstruction* hlo :\n+         instr.fused_instructions_computation()->instructions()) {\n+      sum += SharedMemoryUsageNoCache(*hlo);\n+    }\n+    return sum;\n+  } else if (instr.opcode() == HloOpcode::kReduce &&\n+             IsReductionFromOrToContiguousDimensions(instr)) {\n     ReductionDimensions reduction_info =\n         GetReductionKindAndContiguousComponents(instr);\n     int64_t primitive_size = ShapeUtil::ByteSizeOfPrimitiveType(\n"
                },
                {
                    "Id": 106,
                    "hunk size": 13,
                    "hunk": "@@ -586,20 +590,11 @@ static int64_t SharedMemoryUsageNoCache(const HloInstruction& instr) {\n       // from potential x-tiling).\n       return 2 * 32 * 33 * primitive_size * num_variadic;\n     }\n-  } else if (GetDescriptionForTiledTransposeEmitter(instr,\n-                                                    FindNonTrivialHero(instr))\n-                 .has_value()) {\n+  } else if (GetDescriptionForTiledTransposeEmitter(instr, instr).has_value()) {\n     // Tile size for transposition.\n     int64_t primitive_size =\n         ShapeUtil::ByteSizeOfPrimitiveType(instr.shape().element_type());\n     return 32 * 33 * primitive_size;\n-  } else if (instr.opcode() == HloOpcode::kFusion) {\n-    int64_t sum = 0;\n-    for (const HloInstruction* hlo :\n-         instr.fused_instructions_computation()->instructions()) {\n-      sum += SharedMemoryUsageNoCache(*hlo);\n-    }\n-    return sum;\n   }\n   // Other fused expressions for now don't need the shared memory budget.\n   return 0;\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/gpu/gpu_fusible.h",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/instruction_fusion.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/ir_emission_utils.cc",
            "patches": [
                {
                    "Id": 107,
                    "hunk size": 8,
                    "hunk": "@@ -1084,11 +1084,7 @@ const HloInstruction& FindNonTrivialHero(const HloInstruction& instr,\n }\n \n const HloInstruction& FindNonTrivialHero(const HloInstruction& instr) {\n-  // It doesn't really make sense to call this function with a fusion, but it\n-  // happens. Return the fusion itself for historical reasons.\n-  // TODO(jreiffers): Clean this up.\n-  if (instr.opcode() == HloOpcode::kFusion) return instr;\n-\n+  CHECK_NE(instr.opcode(), HloOpcode::kFusion);\n   return FindNonTrivialHero(instr,\n                             *HloFusionAdaptor::ForComputation(instr.parent()));\n }\n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/service/gpu/multi_output_fusion.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/service/gpu/triton_autotuner.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/6ecc07270e41eae2b7aaca1482bad9c3462c3c9c",
    "message": "remove check",
    "date": "2023-03-20T19:38:21+00:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/aa89782b20e7f4a9aa8ffbebb06ffb0232b69eee",
    "message": "Add version checks to FindCudaExecutable\n\nCurrently we look for ptxas and nvlink in a few different places on the host machine, then we choose the first found binary without taking its version into account. If the chosen binary doesn't fulfill our version requirements we will later fail even if there was a suitable ptxas or nvlink in the search path in the first place.\n\nThis change makes it take the version of each binary into account when going through the search path. Unsuitable binaries will be discarded right away and the search continues until we are out of locations to check.\n\nThis should help with host environments that have multiple CUDA toolkits installed and should make ptxas and nvlink selection more robust.\n\nThe concreate changes:\n\n1. `FindCudaExecutable` now also takes a minimum version and a list of forbidden (think buggy) versions that are supposed to be skipped.\n2. `WarnIfBadPtxAsVersion` has been removed. It was checking for ptxas < 11.1 which is way older than our minimum supported version of 11.8 and was not doing anything given the check described in #3.\n3. There was another version check for `ptxas` in `NVPTXCompiler::ChooseLinkingMethod` which was checking for `version(ptxas)` < 11.8. This has also been removed/replace by the version check described in #4.\n4. Version checking for `ptxas` and `nvlink` has been consolidated into 2 methods `FindPtxAsExectuable` and `FindNvLinkExecutable`. These methods hard code the current minimum version (and the list of excluded versions) of each tool in one place. It's still not great but at least less spaghetti-like.\n\nPiperOrigin-RevId: 618797392",
    "date": "2024-03-25T04:32:33-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/nvptx_compiler.cc",
            "patches": [
                {
                    "Id": 108,
                    "hunk size": 15,
                    "hunk": "@@ -710,16 +691,9 @@ absl::StatusOr<NVPTXCompiler::LinkingMethod> ChooseLinkingMethodImpl(\n   TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n                       se::GetAsmCompilerVersion(preferred_cuda_dir));\n \n-  // ptxas versions prior to 11.8 are not supported anymore. We check this here,\n-  // since we are fetching the ptxas version anyway. Catching the error\n-  // elsewhere might introduce unnecessary overhead.\n-  if (ptxas_version_tuple < std::array<int64_t, 3>{11, 8, 0}) {\n-    return absl::InternalError(\"XLA requires ptxas version 11.8 or higher\");\n-  }\n-\n-  std::optional<std::array<int64_t, 3>> nvlink_version =\n-      GetNvLinkVersion(preferred_cuda_dir);\n-  if (nvlink_version && *nvlink_version >= ptxas_version_tuple) {\n+  auto nvlink_version = stream_executor::GetNvLinkVersion(preferred_cuda_dir);\n+  if (IsNvlinkEnabled() && nvlink_version.ok() &&\n+      nvlink_version.value() >= ptxas_version_tuple) {\n     return LinkingMethod::kNvLink;\n   }\n \n"
                }
            ]
        },
        {
            "path": "third_party/xla/xla/stream_executor/cuda/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_asm_compiler.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/gpu/BUILD",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/gpu/asm_compiler.cc",
            "patches": []
        },
        {
            "path": "third_party/xla/xla/stream_executor/gpu/asm_compiler.h",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/26715f1df5f4c16aad62474c6ecca28f2ece02de",
    "message": "PR #13527: [XLA:CPU][oneDNN] Enable mm-bias-add fusion\n\nImported from GitHub PR https://github.com/openxla/xla/pull/13527\n\nThis PR enables matmul followed by bias-add + binary-add fusion and tests. It removes the check that was blocking this fusion.\nCopybara import of the project:\n\n--\n7b916f2c9ffc9bb703a4650be9be99a1c0cb1696 by Kanvi Khanna <kanvi.khanna@intel.com>:\n\nEnable mm-bias-add fusion\n\nMerging this change closes #13527\n\nPiperOrigin-RevId: 649355747",
    "date": "2024-07-04T04:10:32-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/cpu/onednn_matmul_rewriter.cc",
            "patches": [
                {
                    "Id": 109,
                    "hunk size": 16,
                    "hunk": "@@ -324,6 +324,20 @@ absl::StatusOr<Shape> AdjustBiasShape(const HloInstruction* broadcast_instr,\n   return new_shape;\n };\n \n+// Compute new shape for the binary operand when dot's outer dims\n+// are flattened/unflattened with respect to the binary operand dims.\n+// Adjusting the operand shape to the dot's shape enables fusion in oneDNN.\n+absl::StatusOr<Shape> AdjustBinaryOperandShape(\n+    const HloInstruction* operand_instr, const Shape& dot_shape) {\n+  if (ShapeUtil::ElementsIn(operand_instr->shape()) !=\n+      ShapeUtil::ElementsIn(dot_shape)) {\n+    return absl::CancelledError(\n+        \"Number of elements in operand and dot instruction do not match.\");\n+  }\n+  Shape new_shape = dot_shape;\n+  return new_shape;\n+};\n+\n inline bool IsOperandFusible(HloInstruction* operand, HloInstruction* dot) {\n   // Check if the operand's shape is compatible with matmul for fusion.\n   // An operand is fusable if\n"
                },
                {
                    "Id": 110,
                    "hunk size": 15,
                    "hunk": "@@ -499,19 +521,6 @@ class OneDnnMatMulRewriteVisitor : public DfsHloRewriteVisitor {\n     if (Match(instr, pattern)) {\n       if (!IsSupportedType(dot->shape().element_type()))\n         return absl::OkStatus();\n-      // TODO(intel-tf): Remove the condition below when the fusion Dot +\n-      // Add(bias) + Add(e.g., residual) is enabled.\n-      if (!dot->backend_config<BackendConfig>()\n-               ->mutable_onednn_matmul_config()\n-               ->mutable_fusions()\n-               ->ops()\n-               .empty() &&\n-          dot->backend_config<BackendConfig>()\n-                  ->mutable_onednn_matmul_config()\n-                  ->mutable_fusions()\n-                  ->ops(0) == OneDnnFusionConfig::BIAS) {\n-        return absl::OkStatus();\n-      }\n       std::vector<HloInstruction*> new_operands;\n       for (auto operand : dot->operands()) {\n         new_operands.push_back(operand);\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/48cc5bba3e1c6b32488020840e27c46ea3131ed8",
    "message": "Remove check that the first thread pool is the default, as it is no longer needed.\n\nPiperOrigin-RevId: 619704113",
    "date": "2024-03-27T17:32:10-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "tensorflow/core/tfrt/tfrt_session/tfrt_session.cc",
            "patches": [
                {
                    "Id": 111,
                    "hunk size": 11,
                    "hunk": "@@ -604,15 +604,6 @@ class TfrtSessionFactory::ThreadPoolManager {\n         auto pool_index = it.index();\n         auto num_threads = pool_options.num_threads();\n \n-        // For the current use cases the first thread pool is always the default\n-        // thread pool. We add this check here to verify the assumption. We can\n-        // remove this check once the code stablizes, since it is semantically\n-        // meaningful to use non-default thread pool as the first thread pool.\n-        if (pool_index == 0 && num_threads != 0) {\n-          return errors::InvalidArgument(\n-              \"The first thread pool must have num_threads = 0\");\n-        }\n-\n         if (num_threads != 0) {\n           TF_ASSIGN_OR_RETURN(\n               auto* thread_pool,"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/e99e31597c1b5cc9f0cbc8a3dea71674d81c20b1",
    "message": "Fix GRUCellBlockOp message for invalid rank of x\n\nThe validation checks that x is a matrix, so rank must be 2. ff45913\nfixed the crash in #58261 but left this typo in an exception message.\n\nFixes #58261\n\nSigned-off-by: Reid Wahl <nrwahl@protonmail.com>",
    "date": "2023-02-15T12:44:47-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/258233804f2bc92b4bdb9714b396aed34b53ff0d",
    "message": "sanity check of empty tensor on avgpool3d_grad",
    "date": "2022-11-10T09:37:36-08:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/d669ad1858a759cc2f916a5da02c081b62546d52",
    "message": "Fix shape values profile handling\n\n- Recognize if input tensor changes size, and mark it as non-shape tensor\n- Do not check shape value profiles for tensors that are not shape value",
    "date": "2023-02-17T02:15:30+01:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/b40420ef4c2064b7eb5e26f9d1a1c6abdc3db0f4",
    "message": "Adds zero checks for groups and filters_per_group\n\nPiperOrigin-RevId: 538064566",
    "date": "2023-06-05T21:28:31-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/77531d7d6e887ece06177d28cfac76accdde3e85",
    "message": "[xla:gpu] cudnn_fused_mha_rewriter_test: consistently enforce version checks\n\nAlso add comments to clarify why we enforce these checks.\n\nWhile at it, centralize the setting of skip conditions.\n\nPiperOrigin-RevId: 611229558",
    "date": "2024-02-28T14:32:31-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/BUILD",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/662e615ccfa2ba1aa20c52be122a81343558d9cf",
    "message": "Remove broadcast checking early on in SupportedOpForPropagation for space-to-batch conversion. We check this in CanPropagate anyway.\n\nPiperOrigin-RevId: 643056256",
    "date": "2024-06-13T11:36:37-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/space_to_batch_converter.cc",
            "patches": [
                {
                    "Id": 112,
                    "hunk size": 11,
                    "hunk": "@@ -1620,15 +1620,6 @@ bool ConvolutionVisitor::SupportedOpForPropagation(HloInstruction* consumer,\n   }\n \n   if (IsTrivialElementwise(consumer)) {\n-    for (int64_t i = 0; i < consumer->operand_count(); ++i) {\n-      if (consumer->operand(i)->opcode() == HloOpcode::kBroadcast) {\n-        if (!IsBroadcastPropagatable(consumer->mutable_operand(i), producer)) {\n-          VLOG(2) << \"Could not propagate through broadcast \"\n-                  << consumer->operand(i)->ToString();\n-          return false;\n-        }\n-      }\n-    }\n     return true;\n   }\n "
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/6c1a8e4dcc8b6a05cfbb3c47bb85ef5f9e8d8800",
    "message": "Followup to Scatter Layout normalization\n\nThe code already implicitly assumes that ScatterSimplifier has run before. We\ncannot normalize if there are more than 1 \"scatter\" (batch) dimensions. Add a\ncheck for that and adjust a test case that would in fact be incorrectly\nnormalized (but hidden from the verifier by running ScatterSimplifier\nafterwards).\n\nPiperOrigin-RevId: 634695331",
    "date": "2024-05-17T03:35:38-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/layout_normalization.cc",
            "patches": [
                {
                    "Id": 113,
                    "hunk size": 18,
                    "hunk": "@@ -404,6 +404,22 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {\n       TF_ASSIGN_OR_RETURN(auto normalized_update, GetNormalizedInput(operand));\n       normalized_updates.push_back(normalized_update);\n     }\n+\n+    // Since normalization might reorder the 'scatter_updates' operands\n+    // differently than the 'scatter_indices' update, we have no way to specify\n+    // the order of 'scatter' (batch) dimensions, as that is not an attribute in\n+    // ScatterDimensionNumbers. Scatter implicitly assumes that the 'scatter'\n+    // dimensions appear in the same order in 'scatter_updates' and\n+    // 'scatter_indices'. So we require that there is just a single\n+    // 'scatter' dimension. This is ensured by the ScatterSimplifier pass.\n+    const auto& dims = scatter->scatter_dimension_numbers();\n+    if (scatter->scatter_updates().front()->shape().rank() -\n+            dims.update_window_dims_size() >\n+        1) {\n+      return FailedPrecondition(\n+          \"There should be just a single scatter dimension. Make sure to run \"\n+          \"ScatterSimplifier before LayoutNormalization\");\n+    }\n     TF_ASSIGN_OR_RETURN(auto normalized_indices,\n                         GetNormalizedInput(scatter->scatter_indices()));\n \n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/86f124fe7d01d8de95a9457f99b8a8d499dafc65",
    "message": "[XLA:FFI] Fix annotating buffer `dimensions` memory\n\nThe last dimension of FFI buffers `dimensions` member variable was not annotated as initialized memory, because of an off-by-one code error.\nIt went unnoticed because of usage absl::c_accumulate algorithm in tests. Modified one test to directly access dimensions in a for loop, cause that triggers sanitizer check.\n\nPiperOrigin-RevId: 635840115",
    "date": "2024-05-21T10:17:10-07:00",
    "label": "YES",
    "changes": [
        {
            "path": "third_party/xla/xla/service/cpu/runtime_handle_ffi_call.cc",
            "patches": []
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/2f972a133a1fac16a73fd5e2df722664cf7c1523",
    "message": "Remove a check for single user of an operand in convert-memory-placement-to-internal-annotations.\n\nPiperOrigin-RevId: 625183655",
    "date": "2024-04-15T21:16:44-07:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/convert_memory_placement_to_internal_annotations.cc",
            "patches": [
                {
                    "Id": 114,
                    "hunk size": 6,
                    "hunk": "@@ -79,10 +79,6 @@ absl::StatusOr<bool> ConvertMemoryPlacementToInternalAnnotations::Run(\n         } else if (is_to_device_case) {\n           VLOG(1) << \"Process backward case: \" << instruction->ToString();\n           HloInstruction* custom_call_operand = instruction->mutable_operand(0);\n-          if (custom_call_operand->users().size() != 1) {\n-            VLOG(1) << \"Skip because operand is used by more than one user\";\n-            continue;\n-          }\n           HloInstruction* new_result =\n               c->AddInstruction(HloInstruction::CreateCustomCall(\n                   custom_call_operand->shape(), {custom_call_operand},"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/66d401491c7bd9380d9c87a21b4a0cb40fbad14a",
    "message": "Relax the check for valid reduce fusions.\n\nWe can allow also combinations of reshape bitcast and transpose bitcast, which\nessentially means the shapes just need to have the same number of elements.\n\nPiperOrigin-RevId: 604638258",
    "date": "2024-02-06T07:23:06-08:00",
    "label": "NO",
    "changes": [
        {
            "path": "third_party/xla/xla/service/gpu/hlo_fusion_analysis.cc",
            "patches": [
                {
                    "Id": 115,
                    "hunk size": 10,
                    "hunk": "@@ -239,10 +239,10 @@ HloFusionAnalysis::EmitterFusionKind HloFusionAnalysis::GetEmitterFusionKind()\n         continue;\n       }\n       if (!IsRealReductionHero(*root, *hero)) {\n-        // Needs to have a compatible shape to the reduce operand.\n-        if (!ShapeUtil::IsReshapeOrTransposeBitcast(\n-                root->shape(), hero_operand_shape,\n-                /*ignore_element_type=*/true)) {\n+        // Needs to have a compatible shape to the reduce operand (compatible\n+        // meaning same number of elements).\n+        if (ShapeUtil::ElementsIn(root->shape()) !=\n+            ShapeUtil::ElementsIn(hero_operand_shape)) {\n           valid_shapes = false;\n           break;\n         }\n"
                }
            ]
        }
    ]
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/dac9af0b3e861336b7db97171a19e83f6365ed51",
    "message": "[TF:PJRT] Use ShapeUtil::Compatible when checking the compatibility between buffer on_device_shape and expected execution shape.\n\nBuffer on_device_shape may be dynamic. ShapeUtil::Equal will fail if it is dynamic. An alternative fix is to compare logical_on_device_shape. But getting logical_on_device_shape is blocking and may have performance impact.\n\nPiperOrigin-RevId: 547327689",
    "date": "2023-07-11T16:24:28-07:00",
    "label": "YES",
    "changes": []
},
{
    "commit_link": "https://github.com/TensorFlow/TensorFlow/commit/660ce5a89eb6766834bdc303d2ab3902aef99d3d",
    "message": "[Security] Add a check for empty variant tensor input to CompositeTensorVariantToComponents.\n\nSo an exception will be raised instead of segfault.\n\nPiperOrigin-RevId: 474397914",
    "date": "2022-09-14T14:59:17-07:00",
    "label": "YES",
    "changes": []
},
